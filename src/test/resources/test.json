{
  "59.pdf": {
    "forks": 6,
    "URLs": ["github.com/liyao001/BioQueue"],
    "contactInfo": [
      "li_yao@outlook.com",
      "gcsui@nefu.edu.cn"
    ],
    "subscribers": 5,
    "programmingLanguage": "Python",
    "shortDescription": "A novel pipeline framework to accelerate bioinformatics analysis",
    "publicationTitle": "BioQueue: a novel pipeline framework to accelerate bioinformatics analysis",
    "title": "BioQueue: a novel pipeline framework to accelerate bioinformatics analysis",
    "publicationDOI": "10.1093/bioinformatics/btx403",
    "codeSize": 2517,
    "publicationAbstract": "Motivation: With the rapid development of Next-Generation Sequencing, a large amount of data is now available for bioinformatics research. Meanwhile, the presence of many pipeline frameworks makes it possible to analyse these data. However, these tools concentrate mainly on their syntax and design paradigms, and dispatch jobs based on users' experience about the resources needed by the execution of a certain step in a protocol. As a result, it is difficult for these tools to maximize the potential of computing resources, and avoid errors caused by overload, such as memory overflow. Results: Here, we have developed BioQueue, a web-based framework that contains a checkpoint before each step to automatically estimate the system resources (CPU, memory and disk) needed by the step and then dispatch jobs accordingly. BioQueue possesses a shell command-like syntax instead of implementing a new script language, which means most biologists without computer programming background can access the efficient queue system with ease. Availability and implementation: BioQueue is freely available at https://github.com/liyao001/BioQueue. The extensive documentation can be found at http://bioqueue.readthedocs.io.Contact: li_yao@outlook.com or gcsui@nefu.edu.cn Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-10-18T13:21:39Z",
    "institutions": [
      "Northeast Forestry University",
      "Shanghai Tech University"
    ],
    "license": "Apache License 2.0",
    "dateCreated": "2017-03-12T06:46:59Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx403   BioQueue: a novel pipeline framework to accelerate bioinformatics analysis     Li Yao  0    Heming Wang  1    Yuanyuan Song  0    Guangchao Sui  0    0  College of Life Science, Northeast Forestry University ,  Harbin 150040 ,  China    1  School of Life Science and Technology, Shanghai Tech University ,  Shanghai 200031 ,  China     2017   1  1  3   Motivation: With the rapid development of Next-Generation Sequencing, a large amount of data is now available for bioinformatics research. Meanwhile, the presence of many pipeline frameworks makes it possible to analyse these data. However, these tools concentrate mainly on their syntax and design paradigms, and dispatch jobs based on users' experience about the resources needed by the execution of a certain step in a protocol. As a result, it is difficult for these tools to maximize the potential of computing resources, and avoid errors caused by overload, such as memory overflow. Results: Here, we have developed BioQueue, a web-based framework that contains a checkpoint before each step to automatically estimate the system resources (CPU, memory and disk) needed by the step and then dispatch jobs accordingly. BioQueue possesses a shell command-like syntax instead of implementing a new script language, which means most biologists without computer programming background can access the efficient queue system with ease. Availability and implementation: BioQueue is freely available at https://github.com/liyao001/BioQueue. The extensive documentation can be found at http://bioqueue.readthedocs.io.Contact: li_yao@outlook.com or gcsui@nefu.edu.cn Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: John Hancock    1 Introduction\r\n  Using pipeline frameworks to analyze data produced by the NextGeneration Sequencing techniques is now a common task in current biomedical research. Basically, a pipeline framework provides two functions; the explicit one is the description of protocols (or workflow, pipe-line called in other software), and the ambiguous one is how to dispatch jobs to achieve a maximum efficiency in analyzing data.  Actually, many frameworks focus on a design philosophy of describing a protocol to make it compatible with various demands and easy to scale up from a single laptop to clouds or clusters. For example, Galaxy  (Goecks et al., 2010)  provides pre-defined toolsheds and a web-based Graphical User Interface (GUI) to help users to create protocols. Since GUI may limit the flexibility, other queue systems emerged with command line interface and a build-in domain-specific language (DSL) to describe protocols, such as Ruffus  (Goodstadt, 2010)  and BigDataScript (Cingolani et al., 2015). This improvement can ensure the flexibility, but it leads to a steep learning curve. On the other hand, it is common for pipelines to run multiple jobs in parallel to improve the efficiency of analysis. Importantly, all tools mentioned above assume that users have extensive knowledge in properly allocating system resources for each job when analyzing data, which obviously does not apply to many circumstances.  Here, we introduce BioQueue, a novel queue system that, when dispatching jobs, can automatically estimate the system resources needed by the execution of a certain step and then optimize the execution of the queue to maximize the efficiency. Additionally, based on a design paradigm of configuration, BioQueue provides a web-based workbench and implements an explicit syntax  (Leipzig, 2016)  to keep an optimal balance between flexibility and simplicity.    2 Job dispatch\r\n  The aim of BioQueue is to improve the efficiency and robustness of bioinformatics research. One common strategy is to use as many as CPU cores available to reduce time elapsed on context switching. The other strategy is to run different jobs in parallel to efficiently use computing resources. However, due to the design of a software or limited system resources, many currently available tools have following flaws: cannot fully use the system resources allotted to them (see Supplementary Fig. S1). may not achieve an ideal efficiency or can even generate errors (such as memory overflow) when running multiple jobs simultaneously.  To circumvent these problems, BioQueue will calculate the hash for a certain step and then check the presence of any existing prediction model for it. If a model of predicting the sources required by the step is generated, BioQueue will use it; otherwise, BioQueue will collect the size of inputs, CPU utilization, memory usage (Supplementary Note S1) and disk usage information of this step as training materials. Resources needed by the step will be estimated by the following function:  ( The mean of training data jrj &lt; threshold f ðxÞ ¼ ax þ b jrj threshold When running on a personal computer or clouds, BioQueue implements a greedy algorithm to dispatch jobs with these estimated data to achieve an optimal balance between the efficiency and system resources (CPU, memory and disk). One special circumstance is that, when running a step for the first time, BioQueue will strictly limit the parallelism to reduce the possibility of creating an error. We benchmarked BioQueue against other four popular pipeline frameworks following the protocol presented in Supplementary Figure S2 and found that BioQueue can significantly shorten the elapsed time (Fig. 1 and Supplementary Fig. S3).  We also developed a plug-in system (Supplementary Note S2) to provide support to various distributed resource managers (DRMs), so BioQueue can run on clusters smoothly. On these platforms, BioQueue cooperates with DRMs by guiding them to allocate a proper quantity of CPU cores and memory for the execution of each step. More detailed information regarding how BioQueue generates a prediction model is depicted in Supplementary Figures S4 and S5.    3 Language design\r\n  One very conspicuous characteristic of data analysis in bioinformatics is that researchers usually need to analyze a large amount of data by using a particular protocol and other researchers may also need to use the same protocol to analyze their own data. Therefore, improving the reusability of a protocol is very crucial. To achieve this goal, BioQueue explicitly differentiates two concepts. One concept is a 'protocol', a chain of steps consisting of software and its parameters that define the behavior of the analysis. The second concept is a 'job'. When a 'protocol' is assigned with specific experimental variables, like input files, output files or sample name, the protocol will turn into a runnable 'job'.  When creating a protocol, BioQueue implements a shell commandlike syntax with wildcards (Table 1 shows a typical protocol in BioQueue) rather than a new script language. This is user-friendly to general biologists, because they do not have to learn programming, but only need to know the parameters of the tools that are easy to be found at the tools' papers. Though BioQueue explicitly separates the concepts of 'protocol' and 'job', a protocol can be converted into a runnable job without assigning any experimental values. However, we strongly recommend researchers to replace those values with wildcards (strings embraced with braces, like '{ThreadN}') to make the protocol reusable and reproducible. There are three types of wildcards in BioQueue: \u2022 Pre-defined wildcards, which provide support for file mapping and multithread. \u2022 References, the reference data which might be cited in multiple protocols, such as reference human genome and gene annotation. \u2022 User-defined wildcards, or experimental variables, which need to be assigned when creating a job.    4 Auxiliary functions\r\n  To facilitate biologists to use BioQueue, we provide auxiliary instructions that cover the entire process from writing a protocol to creating a job.  Firstly, we have made the protocols in BioQueue transplantable, which allows users who are using different instances to import suitable protocols created by others and process the same analysis. As a result, any BioQueue user can export a protocol with a generated model as a plain text file, and then upload the protocol to any forum or our open platform (http://bioqueue.nefu.edu.cn).  Secondly, BioQueue offers an open knowledge base to all biologists. On BioQueue's workbench, users can not only search for the usages of bioinformatics software suitable to their own analysis, but also contribute to this knowledge base by sharing new usage information, which will be beneficial to subsequent users. In this manner, the knowledge base will be vigorously maintained by the community with the most updated software and protocols.  Thirdly, we have embedded an autocomplete widget to provide suggestions among pre-defined wildcards and references when users type parameters in each step.  Lastly, when creating a new job, an auto-detecting widget can remind the user of the wildcards defined in a selected protocol.    Funding\r\n  This work was supported by the National Nature Science Foundation of China [81472635 and 81672795] to G.S.  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/liyao001/BioQueue",
    "publicationDate": "0",
    "authors": [
      "Li Yao",
      "Heming Wang",
      "Yuanyuan Song",
      "Guangchao Sui"
    ],
    "status": "Success",
    "toolName": "BioQueue",
    "homepage": ""
  },
  "24.pdf": {
    "forks": 2,
    "URLs": [
      "www.jax.org/strain/024105",
      "github.com/samuroi/SamuROI"
    ],
    "contactInfo": ["friedrich.johenning@charite.de"],
    "subscribers": 4,
    "programmingLanguage": "Python",
    "shortDescription": "SamuROI - Structured analysis of multiple user-defined ROIs, an open source Python-based analysis environment for imaging data. ",
    "publicationTitle": "SamuROI, a Python-Based Software Tool for Visualization and Analysis of Dynamic Time Series Imaging at Multiple Spatial Scales",
    "title": "SamuROI, a Python-Based Software Tool for Visualization and Analysis of Dynamic Time Series Imaging at Multiple Spatial Scales",
    "publicationDOI": "10.3389/fninf.2017.00044",
    "codeSize": 3342,
    "publicationAbstract": "The measurement of activity in vivo and in vitro has shifted from electrical to optical methods. While the indicators for imaging activity have improved significantly over the last decade, tools for analysing optical data have not kept pace. Most available analysis tools are limited in their flexibility and applicability to datasets obtained at different spatial scales. Here, we present SamuROI (Structured analysis of multiple user-defined ROIs), an open source Python-based analysis environment for imaging data. SamuROI simplifies exploratory analysis and visualization of image series of fluorescence changes in complex structures over time and is readily applicable at different spatial scales. In this paper, we show the utility of SamuROI in Ca2C-imaging based applications at three spatial scales: the micro-scale (i.e., sub-cellular compartments including cell bodies, dendrites and spines); the meso-scale, (i.e., whole cell and population imaging with single-cell resolution); and the macro-scale (i.e., imaging of changes in bulk fluorescence in large brain areas, without cellular resolution). The software described here provides a graphical user interface for intuitive data exploration and region of interest (ROI) management that can be used interactively within Jupyter Notebook: a publicly available interactive Python platform that allows simple integration of our software with existing tools for automated ROI generation and post-processing, as well as custom analysis pipelines. SamuROI software, source code and installation instructions are publicly available on GitHub and documentation is available online. SamuROI reduces the energy barrier for manual exploration and semi-automated analysis of spatially complex Ca2C imaging datasets, particularly when these have been acquired at different spatial scales.",
    "dateUpdated": "2017-09-16T16:17:30Z",
    "institutions": [
      "Berlin Institute of Health (BIH)",
      "Bernstein Center for Computational Neuroscience",
      "Cluster of Excellence 'Neurocure'",
      "DZNE-German Center for Neurodegenerative Disease",
      "University of Otago",
      "Einstein Center for Neuroscience",
      "Humboldt Universität Berlin",
      "Neuroscience Research Center",
      "These authors have contributed"
    ],
    "license": "MIT License",
    "dateCreated": "2016-09-05T15:55:20Z",
    "numIssues": 9,
    "downloads": 0,
    "fulltext": "     provided the original author(s) or licensor are credited\r\nand that the original publication in this journal is cited     10.3389/fninf.2017.00044   SamuROI, a Python-Based Software Tool for Visualization and Analysis of Dynamic Time Series Imaging at Multiple Spatial Scales     Martin Rueckl  4  6  8    Stephen C. Lenzi  4  6  7  8    Laura Moreno-Velasquez  0  4  7    Daniel Parthier  4  7    Dietmar Schmitz  1  2  3  4  5  7    Sten Ruediger  4  6    Friedrich W. Johenning  friedrich.johenning@charite.de  0  4  5  7    0  Berlin Institute of Health (BIH) ,  Berlin ,  Germany    1  Bernstein Center for Computational Neuroscience ,  Berlin ,  Germany    2  Cluster of Excellence 'Neurocure' ,  Berlin ,  Germany    3  DZNE-German Center for Neurodegenerative Disease ,  Berlin ,  Germany    4  Edited by: Andrew P. Davison, Centre National de la Recherche Scientifique (CNRS), France Reviewed by: Michael Denker, Forschungszentrum Jülich, Germany Malinda Lalitha Suvimal Tantirigama, University of Otago ,  New Zealand Friedrich W. Johenning    5  Einstein Center for Neuroscience ,  Berlin ,  Germany    6  Institute of Physics, Humboldt Universität Berlin ,  Berlin ,  Germany    7  Neuroscience Research Center ,  Charité Universitätsmedizin Berlin, Berlin ,  Germany    8  These authors have contributed     2017   11   The measurement of activity in vivo and in vitro has shifted from electrical to optical methods. While the indicators for imaging activity have improved significantly over the last decade, tools for analysing optical data have not kept pace. Most available analysis tools are limited in their flexibility and applicability to datasets obtained at different spatial scales. Here, we present SamuROI (Structured analysis of multiple user-defined ROIs), an open source Python-based analysis environment for imaging data. SamuROI simplifies exploratory analysis and visualization of image series of fluorescence changes in complex structures over time and is readily applicable at different spatial scales. In this paper, we show the utility of SamuROI in Ca2C-imaging based applications at three spatial scales: the micro-scale (i.e., sub-cellular compartments including cell bodies, dendrites and spines); the meso-scale, (i.e., whole cell and population imaging with single-cell resolution); and the macro-scale (i.e., imaging of changes in bulk fluorescence in large brain areas, without cellular resolution). The software described here provides a graphical user interface for intuitive data exploration and region of interest (ROI) management that can be used interactively within Jupyter Notebook: a publicly available interactive Python platform that allows simple integration of our software with existing tools for automated ROI generation and post-processing, as well as custom analysis pipelines. SamuROI software, source code and installation instructions are publicly available on GitHub and documentation is available online. SamuROI reduces the energy barrier for manual exploration and semi-automated analysis of spatially complex Ca2C imaging datasets, particularly when these have been acquired at different spatial scales.    calcium imaging  analysis software  Python programming  Open Source Software  microscopy   fluorescence       INTRODUCTION\r\n  Monitoring fluorescence changes of indicator molecules over time is one of the primary tools by which neuroscientists try to understand the function of neurons and neuronal networks. Small molecule indicators including Ca2C and direct voltage sensors can be used to read out the spatiotemporal code of neuronal activity in a non-invasive way  (Scanziani and Häusser, 2009)  and are now routinely used in the study of brain activity at different spatial scales  (Grienberger and Konnerth, 2012) . As technological improvements allow imaging datasets to increase in their complexity (larger fields of view, longer permissible recording times, better temporal resolution, parallel use of different indicators at different wavelength), there is a growing need for tools that enable efficient data exploration. Furthermore, these tools must be applicable to datasets acquired at different spatial scales, because scientific questions increasingly require an understanding of processes at many scales sequentially or, if technically feasible, simultaneously.  Here, we would like to distinguish three spatial scales based on the existing terminology and the physical boundaries in conventional fluorescence microscopy with respect to resolution and field of view size: the subcellular or micro-scale, which includes subcellular structures like dendrites and spines  (Jia et al., 2010; Kleindienst et al., 2011; Takahashi et al., 2012) ; the meso-scale, which comprises populations of individual cell bodies  (Garaschuk et al., 2000; Stettler and Axel, 2009; Sofroniew et al., 2016) ; and the macro-scale, which is imaging of activity over several brain regions without cellular resolution  (Conhaim et al., 2010; Busche et al., 2015) . Datasets from each of these scales pose a different analytical challenge when extracting meaningful information about neuronal activity patterns from spatially defined regions of interest (ROIs).  In the last decade, we have seen major technical advances of genetically encoded Ca2C indicators (GECIs) and the refinement of the multi cell bolus loading technique for in vivo Ca2C imaging  (Stosiek et al., 2003) . These technical developments have led to a surge of Ca2C imaging data at the meso-scale. Manual analysis of these datasets is labor intensive and can be prone to bias. This has driven the development of a wide variety of excellent tools that permit automated event detection and structure recognition for defining ROIs at the meso-scale  (Junek et al., 2009; Mukamel et al., 2009; Tomek et al., 2013; Kaifosh, 2014; Hjorth et al., 2015; Pnevmatikakis et al., 2016) .  While being tailor-made for meso-scale population Ca2C imaging, these tools do not cover the requirements of other spatial scales. Batch processing and automation have enabled time-effective data analysis for large populations of cells, but similar advances have not been made in terms of data exploration and visualization of spatiotemporal structure. Both quality control  (Harris et al., 2016)  and manual identification of patterns in imaging data require intuitive and effective visualization. As far as we are aware, few analytical tools exist that provide users with an analysis environment that can be applied at different spatial scales. We hope that a user's proficiency in handling data with SamuROI in a Python-based environment at one scale will greatly facilitate data analysis at other scales. This way, users should be able to reduce time and resources necessary to acquaint themselves with different analysis packages.  Furthermore, technological advances in microscopy have enabled longer observation periods in larger fields of view  (Sofroniew et al., 2016) , which together permit acquisition of spatiotemporally complex datasets. For example, it will soon be possible to routinely image thousands of cells at once  (Harris et al., 2016) . In this context, it becomes possible to address questions about spontaneous patterns of activity across multiple brain regions at different scales. The informational structure in spontaneous datasets is less predictable or manageable, and exploratory analysis is an essential step in making sense of the data. Data browsing tools are limited in this domain, and there is a need for tools that allow scientists to efficiently identify spatiotemporal structure within their data. We developed SamuROI to fill this niche: to provide a tool that enables analysis at multiple scales, and convenient data visualization for datasets with complex spatiotemporal structure.  SamuROI was designed for use on a standard desktop PC or laptop and focuses on intuitive data exploration and effective semi-automated ROI management. The built-in graphical user interface (GUI) displays data in the space, time and amplitude domains in a way that allows the user to easily connect fluorescence changes with their morphological location of origin and vice versa. This makes data inspection and manual curating of automated ROI generation easier and facilitates the rapid identification of data patterns during exploratory analysis.  SamuROI has been designed to work alongside other software, and to link analytical tools developed for the micro-, meso-, and macro-scales. ROIs generated from other tools can be imported, and also modified manually. Datasets can be saved as hdf5 files in which both structural and dynamic information can be organized together. Hdf5 is also a suitable format for automated post-processing of the analyzed data using Python or other scripting languages. We take advantage of the interactive workflow provided by Jupyter Notebook, which allows seamless integration of the SamuROI GUI with custom pre- and postprocessing analysis pipelines. This way, SamuROI bridges the gap between batch processing and data inspection while providing a versatile analysis environment for application in a range of imaging applications at different scales. SamuROI source code is publicly available on GitHub1 and licensed under the MIT license.  Detailed installation instructions and usage documentation are also available online2. In this paper, we describe the software architecture and the general data processing workflow. We also provide examples of its application at the micro-, meso-, and macro-scale using Ca2C imaging data obtained in acute slices.    MATERIALS AND METHODS\r\n    Experimental Procedures\r\n  Experimental data used to demonstrate and evaluate the functionality of SamuROI was generated in accordance with the national and international guidelines. All procedures were approved by the local health authority and the local ethics committee (Landesamt für Gesundheit und Soziales, Berlin; animal license number T100/03).  Dendritic and spine calcium signals were obtained in layer 2 cells of the medial entorhinal cortex (MEC) in acute brain slices.  Slices were prepared from juvenile Wistar rats (postnatal day 16 to 25) following the procedures as described in  Beed et al. (2010) . 1https://github.com/samuroi/SamuROI 2https://samuroi.readthedocs.io  SamuROI To provide optimal imaging conditions for small subcellular to build on top of the most recent versions of those projects. structures, we filled single cells with synthetic dyes. For dye filling, The source code of SamuROI is publicly available on GitHub4 we either performed whole-cell patch clamp recordings or single (see README.md for installation instructions). SamuROI is cell electroporation for measurements where we did not want licensed under the MIT license. The documentation of SamuROI to interfere with the intracellular composition of the cell. The is automatically built via Sphinx and available online5. Unit intracellular solution for filling patch clamp pipettes (3-6 M) tests and a continuous integration pipeline of new releases are contained: 130 K-gluconate, 20 KCl, 10 HEPES, 4 MgATP, 0.3 currently not available. Contributions in the form of bug reports, NaGTP, and 10 phosphocreatine (in mM; pH: 7.3) and 30 mM pull requests and proposed improvements are highly appreciated. Alexa 594 and 100 mM Oregon-Green BAPTA-6F (OGB6F).  Electroporation pipettes were filled with 1 mM Oregon-Green Basic Software Design of SamuROI BAPTA-1 (OGB1) and 150 mM Alexa 594 dissolved in ddH2O. When designing SamuROI, a key objective was to provide easy The single 10 V electroporation pulse lasted 10 ms  (Lang et al., extensions for custom functionality such as data pre- and post2006; Nevian and Helmchen, 2007) . processing, visualization and data curation. For this, toggling  For population Ca2C imaging of neonatal spontaneous between the GUI and python code is central. We therefore synchronous network events, we used the genetically encoded encourage running SamuROI from a Jupyter Notebook, which Ca2C indicator (GECI) GCaMP6f. NEX-Cre mice  (Goebbels provides easy access to all aspects of data management. et al., 2006)  were crossed with Ai95 animals3  (Madisen et al., We did not want the user to have to keep modifications via 2015)  for constitutive GCaMP6f expression in excitatory cells the GUI and the Jupyter Notebook in sync. For coordinating the only. Neonatal slices were cut horizontally for piriform cortex different levels of interaction with the data via widgets in the GUI and sagittally for the parahippocampal formation at p0-10. We and the Jupyter Notebook, we implemented a strict separation used the same ringer at all stages of preparation and recording. of data and its presentation. Technically speaking, we used the This solution consists of 125 mM NaCl, 25 NaHCO3, 10 mM \u201cdocument-view\u201d also known as \u201cmodel view (controller)\u201d design glucose, 4 mM KCl, 1.25 mM NaH2PO4, 2 mM CaCl2 and 1 mM pattern  (Gamma et al., 2015) . In document view, data (Document MgCl2, bubbled with carbogen (5% CO2 and 95% O2). in Figure 1A, i.e., the SamuROIData class), and its presentation  For all experiments, Ca2C imaging was performed using a to the user (Views in Figure 1A, i.e., the GUI and its widgets) do Yokogawa CSU-22 spinning disk microscope at 5000 rpm. The not depend on one another. For communication between these spinning disk confocal permitted the generation of a large field of parts we use a signal slot pattern (as in Qt, sometimes also called view time series at a high acquisition rate. A 488 nm LASER was \u201cObserver pattern\u201d)  (Gamma et al., 2015)  (Figure 1A). As data is focused onto the field of view using a 4 , 40 , or 60 objective. mutated, the data object calls all slots of the respective signal, i.e., Emission light was filtered using a 515 15 nm band-pass filter. it informs all 'listeners' that some aspect of the data has changed. Fluorescence was detected using an Andor Ixon DU-897D back- The right hand side of Figure 1A represents a set of different illuminated CCD, with a pixel size of 16 mm. Andor iQ software views. All views 'listen' to signals of the data that are of relevance was used for data acquisition. In order to prevent photo bleaching for their visualized content upon application start up, which while producing the clearest images possible, we minimized the means they are slots of the signal. If the data changes, signals illumination power. will be emitted to all slots and, consequently, all listening views will be notified and will modify their presentation to the user accordingly.  The SamuROIData class on the left hand side of Figure 1A holds all relevant data and provides functionality for mutating and extracting subsets of data (Figure 1A, left). The most important data members of this class are:    Software Architecture\r\n  Requirements for Running SamuROI In order to provide maximum backward compatibility, SamuROI is completely developed and tested using Python version 2.7. It should be possible to use SamuROI with Python versions 3.x but we have not tested this specifically. The efficient and effective use of SamuROI depends on four freely available python libraries: - Numpy and scipy are libraries for dealing with numerical data in Python. They provide numerical routines for array manipulation and are capable of handling large datasets. - PyQt, the bindings for the CCC widget library Qt is used for putting together windows, widgets, and other GUI elements. - Matplotlib, a plotting library which allows plots to be embedded in PyQt widgets.  All four modules are widely used, under active development and have been rigorously tested and validated by the open source community. Throughout the development of SamuROI, we tried 3https://www.jax.org/strain/024105 - the 3D numpy array containing the video data - the 2D numpy array containing the overlay mask - multiple python containers holding user defined ROI objects   The extensive use of python properties within the\r\n  SamuROIData class allows mutations of the data to be intercepted and the respective signals to be triggered. For the full API of the SamuROIData class and the mask sets, the reader is referred to the online documentation, especially the examples section. The signals provided by the SamuROIData class are trivially implemented as lists of python functions where the arguments of the signal invocation get perfectly forwarded to 4https://github.com/samuroi/SamuROI 5https://samuroi.readthedocs.io all functions contained in the list: class Signal(list): def__call__(self, args, kwargs): for func in self:  func( args, kwargs) # usage example def do_something(message):  print message sig D Signal() sig.append(do_something) sig(\u201chello world\u201d)    The above class is not to be mistaken with the Qt signals that\r\n  are used to connect to events originating from user input. The rationale behind using two different signal types is simple: the SamuROIData class was designed such that it is independent of any user frontend, and hence must not have a dependency on Qt.  This document view based design pattern permits the desired synchronized cooperation between the GUI and the Jupyter Notebook: as can be seen in Figure 1A, the Jupyter Notebook permits tuning of the GUI as well as data mutation and extraction via the SamuROIData class at the document level. Further, if user interaction with the GUI updates the SamuROIData class the same signal cascade as described above will be triggered.  To give an example of signal slot communication, we would like to describe in detail what happens during ROI mask addition. When a user adds a ROI mask to the data using the GUI widget, the widget's Qt signal is triggered, which adds the ROI mask to the SamuROIData class, triggers its internal signal and notifies all interested listeners (for example the widget which displays the list of masks as a tree structure; upper part of Figure 1B). The widget, which adds the ROI mask, therefore, does not need to know about other components which also require notification: the logic is confined in the signals from the SamuRoiData object.  Because of this signaling structure, updates originating from the interactive shell will invoke the same mechanism and update relevant GUI elements (lower part of Figure 1B). On the other hand, one can also use the interactive shell to add custom GUI elements to an existing window, or connect post-processing and export functions to the data (see example in the online documentation). Another advantage of the separation between data and view is the future possibility to reuse the GUI code, e.g., in a cloud computing scenario. Then the data object behind the client GUI would simply defer all calculations and memory limitations to a server and present only 2D slices and the calculated traces of the data to the GUI.  SamuROI   Performance Considerations\r\n  For a smooth user experience and fast calculation of traces from the defined ROIs SamuROI always holds the full 3D video array in memory. With the use of double precision floating points and an assumed video size of 512 512 pixels and 1000 frames this results in about 2 GB of required RAM. Hence, long-term recordings with high frame rates are likely to exceed an average workstations system memory. Since features as memory mapped files are not supported in SamuROI, such datasets need to be split to fit into memory.  Calculating the time series of ROIs makes extensive use of numpy routines and has negligible CPU cost: due to numpy's underlying C implementation a decent machine needs only a couple of milliseconds per ROI. Further, calculated traces get cached by SamuROI and hence need not be calculated twice. The only relevant computation times arise from the pre-processing of data (stabilization, filtering and/or renormalization) which can grow up to a couple of minutes per dataset. However, because pre-processing is usually run from an interactive python shell, it can easily be done in batch mode or distributed to dedicated machines. Then, the saved pre-processed data can be loaded into the GUI with minimal delay.      FUNCTIONALITY AND RESULTS\r\n  We will now illustrate the functionality of SamuROI by describing the general workflow of data processing. After explaining data import and pre-processing, we describe the different widgets of the GUI and explain data export. We then provide three application cases at the micro-, meso- and macroscale. We provide the most detailed description of micro-scale imaging, as we are not aware of any standardized freeware software solutions facilitating the analysis of fluorescence changes in complex dendritic structures.    General Workflow\r\n  The first step of any image analysis software is the conversion of the acquired raw data into a format compatible with the analysis software. Depending on the data acquisition system used, dynamic image series are saved in a variety of data formats. We therefore needed to define a format that works with SamuROI.  As an interface with SamuROI, we chose multiple image tif files.  When it becomes necessary to convert data from other time series formats into multiple image tif files, we recommend the use of Fiji  (Schindelin et al., 2012) .  The first step after loading the multiple image tif file into SamuROI is the conversion into a 3D numpy array. This is a convenient format that allows a whole range of computations to be applied to the data. Usually, a couple of pre-processing steps are applied to the raw fluorescence images. Pre-processing can be performed in Python on this numpy array. SamuROI comes with a set of standard pre-processing functions. These include image stabilization [via opencv  (Bradski, 2000) , stabilization consists of rigid and warpaffine transformations to align each image to a given reference frame of the video provided], background subtraction, bandstop filtering and transformation of the raw  SamuROI fluorescence data into a 1F/F dataset (see Supplementary After defining and curating the ROIs and performing the Methods for details on the underlying algorithms). Usage of these necessary post-processing steps, the user needs to save the data. functions requires the use of an interactive shell like Jupyter. With It is possible to document the analysis by saving the Jupyter a basic working knowledge of Python, users can also implement Notebooks underlying individual experiments. In addition, we their own custom pre-processing routines for the 3D numpy provide the option to export most of the relevant data stored array. in the SamuRoiData to hdf5 files. A pull-down menu in the  Next, a SamuRoiData object is created from the pre-processed GUI can be used directly to save the set of variables that is to data. The SamuRoiData object can be visualized with its be exported. At the moment this includes the threshold used associated GUI. This data object can be accessed and manipulated to construct the thresholding overlay mask, ROI location and from within this GUI or directly using python commands in identity with the corresponding calcium imaging traces and the the interactive shell or with stand-alone python scripts. The original 3D numpy dataset. User-specific post-processing results current version of the GUI can be used for ROI mask generation, like those related to event detection can be incorporated into the smoothing, detrending, and thresholding. hdf5 file, but this must be done outside of the GUI in Python.  We would now like to provide an overview of the current The hdf5 file is modeled on the structure of the SamuRoiData SamuROI GUI with all functional widgets. Our example data object, structured according to the masklist displayed in the displays a dendritic segment with adjacent spines in a layer TreeView widget. The analysis environment can be reconstructed 2 cell of the MEC. The cell was in whole-cell patch clamp from stored hdf5 files, which can be loaded into SamuROI as mode, the fluorescent Ca2C signal corresponds to a doublet of SamuRoiData objects. The hdf5 file structure allows the user backpropagating action potentials evoked by current injection. to selectively import parts of a previous analysis environment,  The GUI is built using the PyQt library and consists of four which makes it possible to easily reapply stored sets of ROI masks interactive widgets and a toolbar (Figures 2A-E). The central to a new dataset.  ImageView panel (Figure 2D) displays a morphological grayscale One key motivation for using the hdf5 file structure is that in image of the structure underlying the dynamic image series. large datasets, it often becomes necessary to identify individual A thresholding overlay mask defines the relevant pixels of the events in different segments using automated procedures. Here, morphology image, which are above a user-defined threshold we define an event as form of electrical neuronal activity (an (see Supplementary Methods for details on the underlying action potential or a synaptic response or a combination of both) algorithm). On top of the composite morphological grayscale that results in a temporary brightness change of the fluorescence and thresholding overlay mask image, a heatmap encodes the indicator that can be clearly differentiated from baseline noise. frame-specific fluorescence detected in each pixel. The threshold Usually, events occur in different spatially confined segments for the thresholding overlay mask can be set manually in the of the data (e.g., different cell bodies at the meso-scale). mask tab in the toolbar (Figure 2A) and a slider permits the Analyzed data, exported as hdf5 files, can be used for automated user to explore the frame-specific fluorescence detected in each batch analysis in Python or other analysis environments. Batch pixel frame by frame. After loading the dynamic image series processing of large datasets should best be performed on hdf5 files into the GUI, the user can define specific ROI masks for further of individual experiments exported from SamuROI. However, for analysis of location-specific changes of fluorescence over time. definition of the settings used for event detection and quality The SamuROI toolbar supports creation of four types of ROI control, the SamuROI GUI is built to facilitate visualization of masks: branches, polygons, circles and pixel groups. Further, event detection. As a starting point, SamuROI offers standard, predefined segmentations [e.g., ROI masks exported from ilastik built-in event detection functionality based on template matching  (Sommer et al., 2011)  or swc files denoting dendritic structures of a bi-exponential function. Briefly, this approach is based from Neutube  (Feng et al., 2015) ] can be loaded via the interactive on defining a template of a typical event signal. This template shell. The TreeView widget lists individually created or imported then slides along the fluorescence trace and is scaled to fit ROI masks (Figure 2C). While TreeView automatically generates the data at each point. This way, a point-by-point detection names for individual ROI masks, the user can change names criterion is generated based on the optimal scaling factor and interactively. Selecting an item from the list in TreeView will the quality of the fit. The user has to define the threshold display the corresponding trace of averaged intensity per frame above which the detection criterion defines an event (Clements in the TraceView widget (Figure 2E). Individual or all branch and Bekkers, 1997). While originally developed for analysing masks can be further subdivided into pixel-sized sub-segments electrophysiological data, this approach can also be applied in using the 'split' tabs in the toolbar (Figure 2A). Individual sub- imaging applications  (Tantirigama et al., 2017) . Time constants, segments can be selected as children of individual branchmasks in which define the fit parameters of representative 'bait' traces, the TreeView widget. The RasterView widget displays individual must be obtained from other software solutions; we recommend segments. The relative fluorescence of each segment is color the use of Stimfit  (Schmidt-Hieber, 2014) . Importing traces from coded and plotted against frame number (Figure 2B). hdf5 into Stimfit is relatively straightforward, which can then  Within the GUI, SamuROI offers different post-processors be used for curve fitting. Detected events are highlighted in the like detrending and smoothing tabs or a pull-down menu item Treeview, Rasterview and TraceView widgets. Once the event for event detection. Examples for how the interactive Jupyter detection settings (in our case time constants and detection shell can be used for additional post-processing of SamuRoiData criterion) have been optimized in the GUI, they can be performed objects are provided in the online documentation. in the Jupyter Notebook on larger datasets.    Application Cases\r\n  Subcellular Imaging One intended use for SamuROI is the generation and visualization of temporal and spatial profiles of neuronal activity related Ca2C signals from complex dendritic structures and spines (Figure 3). Specifically, this includes analysis of the spatial distribution of spontaneous synaptic events reflected by Ca2C 'hotspots' on dendrites or spines. See  Jia et al. (2010) ,  Kleindienst et al. (2011)  or  Takahashi et al. (2012)  for example research questions requiring this analysis approach. Another example is the identification and demarcation of spontaneous release from intracellular stores in dendrites. Related research questions can be found in  Larkum et al. (2003) ,  Miyazaki and Ross (2013)  and  Lee et al. (2016) .  In our example, we would like to illustrate how SamuROI can be used to visually identify and localize a spontaneously occurring Ca2C transient or 'hotspot' in a single spine on a long dendritic segment. Here, a layer 2 cell in the MEC has been electroporated with the Ca2C indicator OGB1. A morphological image was generated as a maximum projection along time of the motion- corrected 3D dataset. Then, the motioncorrected 3D data set is transformed into 1F/F data. SamuROI then transfers both, the morphology image and the 1F/F 3D numpy array, into a SamuRoiData object. In the online supplements, we provide a Jupyter Notebook that includes a step-by-step description of data import, the pre-processing steps and the generation of the SamuRoiData object and the corresponding GUI.  Manual drawing of ROIs delineating subcellular structures like dendrites and spines requires the investigator to manually trace the boundary between the structure and the background, which is a time-consuming and tedious task. In SamuROI, we implemented a functionality that speeds this process up significantly. Based on the morphology image, SamuROI generates a 'thresholding overlay mask.' The software implements a thresholding algorithm (see Supplementary Methods) that defines above background pixels incorporated into the further analysis. Compare the raw morphological image Figure 3A1 (top left) and the thresholded image Figure 3A2 (top right). SamuROI ignores the masked out black pixels in ROIs, which only contain background fluorescence. ROI masks, irrespective of whether they are generated as tubes using the SamuROI branch tool or incorporated from somewhere else, can therefore be larger than the structure of interest. This speeds up manual ROI generation significantly and also facilitates the import of ROI masks from for example swc files, as ROI masks can include regions where no pixels are analyzed. Small inter-experimental changes in ROI shape are automatically incorporated, so that the same ROI can be used for consecutive sweeps of the same structure. Using of the same ROI mask for consecutive sweeps is further facilitated by an alignment tab (Figure 2A2) that permits shifting of selected ROIs. Together with the example Jupyter Notebook in the online supplements we also offer an example swc file from the freeware software Neutube together with an instruction how to generate swcs in Neutube that can be used by SamuROI and incorporated directly as branch ROIs.  The key objective of our application example is the detection of spontaneously occurring local hotspots of activity on a large dendritic structure. For this task, it is necessary to subdivide dendritic branches into segments and visualize fluorescence changes in each individual segment. Again, manual ROI drawing tools that are typically implemented in analysis software would now require the user to manually draw a large number of evenly spaced ROIs. For this task, it is necessary to subdivide dendritic branches into segments and visualize fluorescence changes in each individual segment. In SamuROI, the split tool (Figure 2A3) automatically divides tubular branchmasks into identically spaced sub-segments oriented perpendicular to the longitudinal axis of the dendritic branch (Figures 3A3,4). The spacing of these segments is a user-defined number of pixels. This adaptability is important, as signal to noise improves when the number of pixels in a segment corresponds to the number of pixels active in a hotspot. In each segment, the thresholding overlay mask defines pixels that will be averaged. Further, a pixel's surface fraction, which resides within the ROI mask, determines its weight. Pixels in the interior of the ROI will have a weight of 1, boundary pixels will have a weight of less than 1.  After SamuROI has calculated the average for all segments, it is necessary for the experimenter to identify and localize hotspots of activity. The usual output of ROI-Data are fluorescence traces. Visually screening large numbers of fluorescence traces derived from individual dendritic segments is tedious and prohibits immediate recognition of temporal and spatial patterns. Therefore, the RasterView widget (Figure 3B) provides a linescan-based color-coded display of the intensity time-course of each segment in a branch.  This approach enables the investigator to rapidly visualize the spatial and temporal activity pattern and identify a hotspot of activity in our example. In addition, once the putative hotspot has been identified in the RasterView widget, we want to know the exact position in on our morphological image in the frame view widget and visualize the underlying fluorescence trace to evaluate qualitative parameters of the signal which get lost in a heatmap. SamuROI offers a solution to this problem that imaging signals need to be displayed in different formats synchronously for evaluation. Our software permits intuitive browsing of the data by synchronizing different widgets in the GUI: in our example, clicking a temporally and spatially defined hotspot in the RasterView widget (arrows in Figure 3B) highlights the corresponding ROI mask in the FrameView widget (Figure 3A4) and the segment in TreeView. It also triggers the display of the corresponding sweep in TraceView (Figure 3C). We are now able to locate the signal at the distal tip of the dendritic segment and estimate the time course and the signal to noise ratio looking at the trace.  By definition, our segment masks are stereotyped and may not capture the perimeter of a hotspot or spine correctly. SamuROI enables the visualization of the exact spatial extent of the hotspot we detected in our example. By selecting the time points of interest in the RasterView widget at the event peak, the 1F/F color-coded pixels are overlaid on the morphological image in FrameView (see inset in Figure 3C) and the corresponding frame is marked in the TraceView widget (Figure 3C, black arrow). SamuROI   It is now necessary to define the hotspot in greater detail.\r\n  For this purpose, we generated tools for ROI definition using freehand drawn polygons or individually selected pixel groups. In our example, the RasterView permits immediate identification of a hotspot and its localization in the FrameView widget. The intensity color-code in the FrameView widget demonstrates that the active pixels correspond to a dendritic spine (see the inset in Figure 3C). By drawing a freehand polygon around primarily active pixels in the FrameView widget, we manually generate a ROI mask that only incorporates the isolated hotspot (Figure 3D).  Using this example that illustrates the core functionality of SamuROI, we would now like to explain the options for further data processing offered by our hdf5 file based data format. From identification of a hotspot, one could save the adapted set of ROI masks (branch masks, segments and the newly generated polygon) and apply it to a different image series from the same structure. This way, it would be possible to identify and analyze all hotspots in a set of image series from the same structure. Additionally, one could use the detected signal as a 'bait' to generate a template for a typical signal and use this for automated event detection in this dataset. Once all image series are analyzed, the hdf5 files will not only contain all traces underlying labeled structures but also the spatial information related to these structures, which will be helpful when analysing spatial aspects of activity. One could for example analyze if hotspots tend to be spatially clustered or if they are distributed randomly.   Meso-scale Imaging\r\n  One of the goals of population imaging is to identify and describe structure in the activity of populations of cells. Specifically, singlecell Ca2C signals representing action potential firing can be spatially and temporally related to each other during spontaneous network activity as for example in  Namiki et al. (2013) , or following extracellular synaptic stimulation as in  Johenning and Holthoff (2007) . In vivo, these cellular activity patterns are often related to behavior, one of many examples can be seen in  Heys et al. (2014) .  It is common in this kind of data exploration to have no hypothesis regarding where activity will be located or how it will be temporally structured within a population of cells.  For this type of analysis the SamuROI GUI can be used for data visualization with generic ROI masks from a variety of software for interactive display of different groups of cells. In addition, the SamuROI GUI offers convenient functionality for manual curation of ROIs and for the testing of event detection parameters.  We would now like to give a specific example to highlight unique functionalities of SamuROI. In our example, we imaged immature spontaneous synchronized network events in a neonatal slice preparation of the olfactory cortex. In these network events, there is high synchrony between a subset of cells, which are hard to identify as single cells by established variancebased measurements relying on sparse firing  (Hjorth et al., 2015; see discussion for details) . Here, we present a workflow for measuring activity in densely packed cell populations that fire  SamuROI synchronously. We also show how our workflow can be used to Supplementary Material, we provide a short function that enables provide a read out of the number of cells that are silent for the SamuROI to add cell-specific ROI masks to the hdf5 files. total duration of the recording. Representative traces visualized in the GUI can be picked and  For Ca2C indication, constitutive GCaMP6f expression in exported to other software, such as Stimfit to generate curve post-mitotic excitatory neurons is achieved in the AI95/NexCre templates that permit automated event detection. The GUI can mouse line. The first step is to generate sets of ring-shaped then be used to test sensitivity and specificity of event detection ROI masks for GCaMP expressing cells that are based on pixel parameters in individual experiments before batch processing the classification segmentation using ilastik  (Sommer et al., 2011)  and hdf5 files in Python directly. This can be done using the same watershed segmentation using the scikit-image module in Python functions that have been used in the GUI. Batch analyzed data  (van der Walt et al., 2014) . In GCaMP-based datasets, the main will provide spatial and temporal information of detected events underlying morphological feature is the ring shape of GCaMP6f in the hdf5 files, which will enable the user to extract spatial and expression, with a fluorescent cytosolic rim and a dark central temporal correlations of network activity simultaneously. nucleus (Figure 4A), and we present a segmentation approach specified for this morphological pattern. The generated ROIs are Macro-scale Imaging illustrated in Figure 4B. Low magnification imaging of brain-activity induced changes  In the online supplements, we provide an example Jupyter in Ca2C indicator fluorescence (or, in principle any other Notebook for using these functions to generate single cell indicator of neuronal activity employing changes in brightness segmentation ROI masks and opening them in SamuROI. In as a readout) enables researchers to analyze the spatiotemporal the documentation we also outline how to generate ROI masks spread of activity patterns over different brain regions with low using ilastik. After automated ROI generation in ilastik, we spatial and high temporal resolution. Specific uses of macro-scale implemented a manual correction step for adding and deleting imaging include the spatial and temporal spread of spontaneous single cells. The user input required is essentially a mouse click on activity in brain slices  (Easton et al., 2014)  or interregional the dark nuclear center of a ring shaped cell. The final outcome synchrony in vivo  (Busche et al., 2015) . of our segmentation is a 2 dimensional array in which each cell is The generic functions of SamuROI can be used to facilitate denoted by a different number (i.e., every pixel belonging to cell interpretation of macro-scale datasets. In our example, we would 1 is denoted by a 1 in the image). This array is then imported into like to demonstrate how the spatiotemporal structure of a the SamuROI GUI. SamuROI works with these segmentations spontaneous synchronized network event is intuitively visualized and treats them just as though they were a set of individual ROIs. and related to different brain structures using SamuROI.  Basic GUI functionality of meso-scale population ROI masks GCaMP6f is expressed using the AI95/NexCre mouse line. is similar to that described above for micro-scale data. The GUI Figure 5A displays a sagittal slice of the parahippocampal displays the mean fluorescence of all pixels in each mask and formation, where neonatal spontaneous synchronized network displays this through RasterView and TraceView as can be seen events were imaged. A question we want to answer using in Figures 4C,D. RasterView reveals structured activity in cell SamuROI in this example is how the horizontal (lateral) spread of populations and allows event selection that leads to highlighting the signal in superficial layers of the parahippocampal formation of the cell of origin in both FrameView and TreeView, as is organized in time and space. The branch ROI tool we initially well as plotting in TraceView. Additionally, single or multiple developed for micro-scale imaging is especially well suited for cells can be selected in FrameView for simultaneous viewing this task, demonstrating how SamuROI can be applied for and comparison of activity in TraceView, which is shown in image analysis flexibly at different spatial scales. As branch ROIs Figure 4C. This way, it is possible to intuitively visualize aspects can have any user-defined width and direction, it is possible like synchrony, number of cells participating and the order of to generate a ROI incorporating the adjacent brain regions neuronal activation during events. One can pick cells displaying subiculum, presubiculum, parasubiculum and entorhinal cortex different activity patterns in the RasterView (e.g., the blue cell (Figure 5A2). The incorporation of deep and superficial layers showing a large number of small bursts and the green cell can be adjusted by modifying the width of the branch ROI mask. showing a small number of large bursts), directly visualize their Using the segmentation tool, we then divide these cortical regions location in FrameView and compare the underlying traces in into sub-regions at arbitrary spatial resolution (Figure 5A2). TraceView. In addition, it is possible to add more ROIs using A RasterView of the sub-regions then displays the temporal and the GUI. An example how this could be used in an experiment spatial dynamics of neuronal activity reflected by changes in to bridge subcellular micro- and meso-scale imaging would be fluorescence (Figure 5B) and the user can then localize individual simultaneous imaging of a meso-scale population and single signaling patterns like the leading edge of a wave (Figure 5B, dendritic branches of individual dye-filled cells. This example red arrow) or an oscillating structure (Figure 5B, green and would require the addition of branch segments to the cell ROI purple arrow). After clicking on the corresponding part of the masks, which can be easily accomplished in SamuROI. RasterView widget, the corresponding segment is localized in  The SamuROI GUI further permits standard post-processing the FrameView widget (Figure 5A2). The TraceView widget and event detection functionality of population imaging data sets. displays the corresponding traces (Figure 5C). Based on the Data export as hdf5 files currently needs user intervention from different spatiotemporal patterns extracted from the RasterView, the Jupyter Notebook, as the standard pull down menu does it is possible to draw freehand polygon-ROIs based on different not offer the export of cell-specific ROI masks. In our online patterns. This is facilitated by the time-locked intensity color code in the FrameView widget. In our example, this highlights the initiation of the signal in the parasubiculum.      DISCUSSION\r\n  When studying neuronal activity with imaging, the appropriate analytical unit depends on the scientific question and size scale. Depending on spatial resolution, these analytical units could be, for example, dendritic branches, spines, single cell bodies or cortical layers and they ideally represent a unit of neuronal or network computation. Researchers aim to extract fluorescence changes specific to these analytical units, based on which they visualize, detect and localize neuronal activity patterns. Technological progress challenges researchers with the opportunity to generate increasingly complex datasets in which the ideal spatial scale is often hard to define or predict in advance.  SamuROI is built to meet the rising demand for analysis freeware. It provides an intuitive and convenient workflow for data exploration and ROI creation at arbitrary spatial scales. SamuROI is a Python-based, open source analysis environment for image series of intensity changes of fluorescent indicators over time. The software permits both data browsing and deep analysis using Python by seamlessly integrating command-line interactions with a user-friendly GUI, achieved by using Jupyter Notebooks.  As such, the software has several core strengths:   Simplified identification of complex spatiotemporal\r\n  patterns by human observation that would otherwise get lost in highly complex datasets.  Time effective ROI management and manual curating of automatically generated ROIs from other software solutions.  Instantaneous switching between temporal and spatial aspects of the data via interactive point and click widgets. Facilitation of quality control in terms of the fluorescent signal, ROI segmentation and event detection that is presented to the user.    The tool is straightforward to install. The online documentation includes code templates to illustrate usage and enable \u2018out of the box\u2019 use with Jupyter Notebook. While Jupyter is the recommended platform for running the GUI it\r\n  is also possible to use SamuROI as a stand-alone application. The modularity of the pipeline permits each processing stage to be carried out independently, including pre-processing, data visualization, ROI definition, data export and event detection. Data are exported as hdf5 files, which contain all necessary information for further batch processing of the data. The package is carefully documented and open source to permit further collaborative development.  SamuROI is complementary to other existing imaging analysis software like Fiji  (Schindelin et al., 2012)  and SIMA  (Kaifosh, 2014) . These tools offer different data processing, visualization and exploration options than SamuROI. A unique feature of SamuROI is the document-view pattern based framework that permits online modification of objects in the SamuROI GUI in Python using an interactive shell like Jupyter and vice versa.  An example of integration of Fiji and SamuROI is the excellent file conversion functionality of Fiji, which enables the conversion of a larger number of file formats into Multi-tif files that can be read out by SamuROI. While Fiji offers both a neurite tracer and a ROI manager for fluorescent time series, to our knowledge there is no default way of combining the two. We found the visualization and manual curating options of ROIs generated with the Fiji ROI manager limited as there are no point and click widgets. These tools offer different visualization and exploration options to SamuROI, and can be easily used in parallel. While SIMA focuses on meso-scale population Ca2C imaging in vivo, SamuROI aims to provide an integrated analysis environment for imaging data at what we define as the microscale, meso-scale and macro-scale. In addition, SIMA offers the ROI Buddy, an excellent segmentation tool for manual curating of ROIs. However, we missed an intuitive display that permits visualization and browsing of fluorescence traces. However, SamuROI by no means aims to replace any of those tools, and we encourage using these tools in parallel. For example, one might prefer to use the frame alignment procedures and ROI Buddy segmentation in SIMA as a pre-processing step followed by further analysis and visualization/exploration of the data in SamuROI. This would be an easy way to incorporate activitybased pixel correlations  (Junek et al., 2009; Mukamel et al., 2009; Tomek et al., 2013; Kaifosh, 2014; Hjorth et al., 2015; Pnevmatikakis et al., 2016)  to the analytical pipeline and these can be further edited in SamuROI.    One of the most critical and, when performed manually, time\r\n  consuming steps of dynamic image series analysis is the definition of ROI masks. For micro-scale Ca2C imaging, we are not aware of an integrated software solution that permits both semiautomatic ROI mask generation and data browsing/analysis. On the other hand, for semiautomatic tracing of morphological data, many freeware software tools are already available for morphological segmentation of images. Software solutions like Neutube  (Feng et al., 2015) , Neuronstudio  (Wearne et al., 2005; Rodriguez et al., 2008)  or the simple neurite tracer plugin for Fiji  (Longair et al., 2011)  permit semi-automatic tracing of dendritic and axonal structures. SamuROI is built to interact with these, as any ROI pattern can easily be converted into an array of pixels that can be added to the attribute masks. In our online supplement, we provide examples that illustrate how ROI sets compatible with SamuROI can be generated from freeware programs validated for structure recognition. SamuROI can read SWC files (e.g., exported using Neutube  (Feng et al., 2015) ) and flatten these 3D dendritic tree structures into 2D branch masks. This greatly facilitates the generation of branch specific ROIs, and provides a good example how the excellent branch tracing functionality of Neutube can be combined with SamuROI.  In contrast to micro-scale imaging, there are many tools facilitating the detection of cell bodies in population Ca2C imaging on the meso-scale. A number of recently developed approaches define pixels belonging to active cells based on variance in brightness using activity-based pixel correlations  (Junek et al., 2009; Mukamel et al., 2009; Tomek et al., 2013; Kaifosh, 2014; Hjorth et al., 2015; Pnevmatikakis et al., 2016) . These variance-based approaches work well for identifying sparsely active cells, but cannot detect silent cells nor can they always distinguish between closely packed synchronously active cells that do not fulfill the prerequisite of statistical independence. A recently published approach directly addresses this issue for postnatal early synchronous network activity  (Hjorth et al., 2015) . Regardless of the method used to detect cells, SamuROI can provide a useful environment for visualization and quality management of the resulting ROIs. We also provide example functions that implement polygon ROI mask creation for inactive and synchronous cells using the machine-learning based structure recognition software ilastik  (Sommer et al., 2011) , together with python functions based on scikit-learn and the standard python library.     Outlook\r\n  SamuROI works well with existing tools and streamlines the analysis of dynamic image series such as those acquired using Ca2C indicators. SamuROI has many built in features covering SamuROI a complete pipeline of data processing and analysis. While many software packages for dynamic image series analysis exist, many necessary features missing from these packages have been combined into SamuROI. Since SamuROI permits the easy import of ROI masks generated (semi-) automatically with other software tools, we do not prioritize the implementation of new segmentation algorithms in future versions of the software. Our software has been designed in such a way that event detection algorithms different from the template based algorithms based on  (Clements and Bekkers, 1997)  can be easily implemented. SamuROI will be used as a versatile tool for data exploration and analysis, for identifying meaningful structure in complex datasets and for convenient ROI management. SamuROI together with sophisticated structure recognition software minimizes the need for human supervision in selecting pixel-defined structures of interest. This should allow scientists to focus their attention on data scanning for recognition of meaningful patterns in the data and quality control.    AUTHOR CONTRIBUTIONS\r\n   MR and SL wrote code. FJ, LM-V, DP, and SL contributed\r\n  example data. FJ, MR, and SL were involved in conceptualizing the software. FJ, LM-V, DP, SL, MR, SR, and DS designed and tested the software. FJ, SL, MR, SR, and DS prepared figures and wrote the manuscript.     FUNDING\r\n   This work was supported by the German Research Foundation (DFG), grant number JO1079/1-1, JO 1079/3-1, and SFB 665 to FJ, RU 1660, RU 1660/5-1 and IRTG 1740 to SR, Exc 257, SFB665 and SFB 958 to DS.\r\n     ACKNOWLEDGMENTS\r\n   We would like to thank Anna Vanessa Stempel and Robert Sachdev for critically reading the manuscript. In addition, we would like to thank Anke Schönherr, Susanne Rieckmann and Lisa Zuechner for excellent technical assistance.\r\n     SUPPLEMENTARY MATERIAL\r\n   The Supplementary Material for this article can be found online at: http://journal.frontiersin.org/article/10.3389/fninf. 2017.00044/full#supplementary-material\r\n     ",
    "sourceCodeLink": "https://github.com/samuroi/SamuROI",
    "publicationDate": "0",
    "authors": [
      "Martin Rueckl",
      "Stephen C. Lenzi",
      "Laura Moreno-Velasquez",
      "Daniel Parthier",
      "Dietmar Schmitz",
      "Sten Ruediger",
      "Friedrich W. Johenning"
    ],
    "status": "Success",
    "toolName": "SamuROI",
    "homepage": "http://samuroi.readthedocs.io/en/latest"
  },
  "16.pdf": {
    "forks": 2,
    "URLs": ["github.com/xiaofengsong/GFusion"],
    "contactInfo": [
      "hanping200701@163.com",
      "xfsong@nuaa.edu.cn"
    ],
    "subscribers": 1,
    "programmingLanguage": "Perl",
    "shortDescription": "GFusion is a software package to detect fusion genes using RNA-Seq data",
    "publicationTitle": "GFusion: an Efective Algorithm to Identify Fusion Genes from Cancer RNA-Seq Data",
    "title": "GFusion: an Efective Algorithm to Identify Fusion Genes from Cancer RNA-Seq Data",
    "publicationDOI": "10.1038/s41598-017-07070-6",
    "codeSize": 26,
    "publicationAbstract": "OPEN Fusion gene derived from genomic rearrangement plays a key role in cancer initiation. The discovery of novel gene fusions may be of significant importance in cancer diagnosis and treatment. Meanwhile, next generation sequencing technology provide a sensitive and ecfiient way to identify gene fusions in genomic levels. However, there are still many challenges and limitations remaining in the existing methods which only rely on unmapped reads or discordant alignment fragments. In this work we have developed GFusion, a novel method using RNA-Seq data, to identify the fusion genes. This pipeline performs multiple alignments and strict filtering algorithm to improve sensitivity and reduce the false positive rate. GFusion successfully detected 34 from 43 previously reported fusions in four cancer datasets. We also demonstrated the efectiveness of GFusion using 24 million 76 bp paired-end reads simulation data which contains 42 artificial fusion genes, among which GFusion successfully discovered 37 fusion genes. Compared with existing methods, GFusion presented higher sensitivity and lower false positive rate. The GFusion pipeline can be accessed freely for non-commercial purposes at: https://github.com/xiaofengsong/GFusion.",
    "dateUpdated": "2017-08-05T20:05:05Z",
    "institutions": [],
    "license": "No License",
    "dateCreated": "2016-02-19T03:24:05Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     SCientifiC REPORts |     10.1038/s41598-017-07070-6   GFusion: an Efective Algorithm to Identify Fusion Genes from Cancer RNA-Seq Data     Jian Zhao    Qi Chen    Jing Wu    Ping Han  hanping200701@163.com    Xiaofeng Song  xfsong@nuaa.edu.cn    7  6880    22  6  2017    13  2  2017     OPEN Fusion gene derived from genomic rearrangement plays a key role in cancer initiation. The discovery of novel gene fusions may be of significant importance in cancer diagnosis and treatment. Meanwhile, next generation sequencing technology provide a sensitive and ecfiient way to identify gene fusions in genomic levels. However, there are still many challenges and limitations remaining in the existing methods which only rely on unmapped reads or discordant alignment fragments. In this work we have developed GFusion, a novel method using RNA-Seq data, to identify the fusion genes. This pipeline performs multiple alignments and strict filtering algorithm to improve sensitivity and reduce the false positive rate. GFusion successfully detected 34 from 43 previously reported fusions in four cancer datasets. We also demonstrated the efectiveness of GFusion using 24 million 76 bp paired-end reads simulation data which contains 42 artificial fusion genes, among which GFusion successfully discovered 37 fusion genes. Compared with existing methods, GFusion presented higher sensitivity and lower false positive rate. The GFusion pipeline can be accessed freely for non-commercial purposes at: https://github.com/xiaofengsong/GFusion.       -\r\n  A fusion gene is a hybrid gene formed from two diefrent genes rejoining through chromosomal translocation, deletion or inversion1. Because of a close link to human cancer, fusion genes have attracted attentions of many researchers2. eTh rfist discovered fusion gene is BCR-ABL1 3-5, which is formed by a translocation event involving chromosome 9 and 22 and identified as the predominant factor predisposing to chronic myelogenous leukae mia (CML). Fusion BCR-ABL1 represents one class of fusion genes which impact cancer development through encoding chimeric proteins with carcinogenic biological function. In addition, some gene fusions can lead to activation of oncogenes (for example, R-spondin fusion proteins active the oncogenic Wnt/β-catenin signaling in colon cancer) or inactivation of tumor suppressor genes (for example, LACTB2-NCOA2 disrupts NCOA2 in colorectal cancer)6, 7.  ehT existence of fusion genes in cancer such as breast, lung, colon, prostate cancers and colorectal lymphoma has been confirmed in numerous researches 8-11. Demichelis and colleagues found that recurrent gene fusions between the androgen-regulated gene TMPRSS2 and the ETS (E26 transformation-specific) family genes ERG, ETV1 or ETV4 are expressed in most prostate cancers12. Tomlins and colleagues found that ERG or ETV1 was markedly overexpressed in 57% prostate cancer cases, whereas overexpression was never observed across benign prostate tissue samples13. Singh and colleagues found that some GBMs (glioblastoma multiforme) involve FGFR-TACC fusion and the fusion protein displays oncogenic activity when introduced into astrocytes in the mouse brain14. eTh discovery of the EML4-ALK fusion in non-small-cell lung cancer, SYT-SSX4 fusion in synovial sarcoma and many other fusion events in cancers indicated that gene fusions are widespread existed in tumor types15-17.  hTe fusion genes detections were traditionally relied on the FISH (fluorescence in situ hybridization) or RT-PCR techniques. With the development of next generation sequencing, many fusion genes have been identified based on RNA-seq data 18-20. Using the RNA-seq data, several computational tools have been developed to identify fusion genes, such as FusionSeq, FusionMap, Tophat-fusion, PRADA, SOAPfuse, FusionCatcher, JAFFA, ChimPipe21-28. FusionSeq is a novel approach which can detect candidate fusion transcripts by analyzing paired-end RNA-Seq data. It firstly clusters a large number of short 'tiles' from exon sequences from discordant read pairs and then constructs a 'fusion junction library' which is used to realign the potential fusion reads. uThs the running cost of FusionSeq is much higher in terms of running time and CPU usage because the function junction library is normally quite large. Both of FusionMap and Tophat-Fusion can apply on the single-end or paired-end read using similar strategy that is to split reads into shorter segments and select segments aligning against diferent genes. FusionMap then creates a pseudo fusion transcript library based on spanning fusion boundaries reads, and remaps full-length reads to this pseudo reference, while TopHat-Fusion uses a series of post-processing routines to filter out false fusions. To detect fusion genes, the current pipelines heavily depend on individual unmapped reads which harbor the fusion boundaries or discordant paired-end reads, in which each reads align against diefrent genes, leading to neglecting the mate reads of unmapped reads or reads that span fusion boundaries. uThs, the results contain lots of spurious fusion genes. PRADA is comprehensive soft ware platform which can solve gene expression levels, quality metrics, detection fusion transcripts, and so on. SOAPfuse employs a series of filters to nominate high-confidence fusion transcripts from the library of fusion events idenfied by an improved partial exhaustion algorithm. FusionCatcher searches for novel/known somatic fusion genes, translocations and chimeras in RNA-seq data (paired-end or single-end reads). JAFFA detects fusions with diefrent read-lengths, and it uses de novo assembly or raw reads directly to align to a reference transcriptome, rather than the genome. ChimPipe combines discordant paired-end reads and split-reads to detect any kind of chimeras. Liu et al. comprehensively evaluated 15 popular fusion transcript detection algorithms on three synthetic data sets and three real data sets respectively, and found that no single method dominantly performed the best for all data29. Shailesh et al. and collegues compared the performance of 12 well-known fusion detection pipelines30, and suggested that these 12 fusion detection tools have diefrent false positive rate, and none of the tools are inclusive.  To improve the sensitivity and specificity of fusion detection, and avoid the above limitations, we present a novel pipeline named GFusion, a powerful and eficient fusions detection method for both paired-end and single-end RNA-seq data, to predict fusion genes by comprehensive analyzing alignment split reads and mate reads. GFusion rfistly splits unmapped reads into three segments and aligns the rfist and last segments against reference genome using read aligner Bowtie31. Potential fusion boundaries are conrfimed by the split reads in which the segments are from diefrent genes. Finally, GFusion filters out false fusions through a series of filtering steps including analyzing the mate reads of split reads for paired-end read, detecting spanning fragments and constructing fusion reference and realigning fusion fragments (fusion reads for single-end data). We have demonstrated the eefctiveness of the GFusion on a normal breast RNA-Seq dataset as the control group and four cancer RNA-Seq datasets involving both paired-end and single-end read data. As a result, GFusion successfully detected 34 from 43 previously reported fusions in four cancer datasets. Further results illustrated that GFusion performed higher sensitivity and have lower false positive rate by comparing with other existing fusion detection pipelines.    Methods\r\n  GFusion is a Perl based software designed to identify fusion genes from single-end or paired-end RNA-Seq read data, which are aligned against the reference genome to find out the genomic locations, followed by integrated ifltering steps to determine the best gene fusion candidates (Fig.  1). GFuison involves several steps to eliminate false candidate fusions that are caused by misalignment, random pairing of transcript fragments or artificial errors. We identify fusion gene candidates from selected fragments harboring the fusion boundaries in their sequenced reads or insert segments. In the output file, the GFusion reports a list of information about the fusion result including detected fusion genes, genomic locations of breakpoints, fusion transcript model, as well as the number of supporting split fragments and spanning fragments (split reads for single-end data), that characterize fusion genes comprehensively at transcript expression level.  In this work, a fusion boundary was defined as the precise genomic location as the breakpoint of the fusion genes. Two segment sequences from diefrent genes are combined at a fusion boundary to be a fusion transcript. We defined a split read as the sequenced read which harbors a fusion boundary in the read itself, while a split fragment is the pair-end read where one read is the split read and the mate read has a special alignment related to the split read. In addition, we denfied a spanning fragment as a paired-end read that harbors a fusion boundary in the insert segment with two reads being aligned to diefrent genes (Fig.  2). eTh following analysis show the main steps of GFusion fusion detection procedure on paired-end RNA-Seq fastq datasets, some filters are omitted for single-end fastq datasets.  Initial alignment. GFusion detect fusion genes based on alignment locations of reads in the genome. To determine the precise genomic location of short sequences, GFusion involves multiple uses of basic aligners. eTh initial step of the process is to align all the reads to the reference genome (hg19 version in this work). Because of the presence of introns in the genome, we used Tophat32, a fast and eficient splice aligner, for initial alignment. Tophat uses Bowite to identify the reads that can be mapped to the genome as exon reads or known exon junction spanning reads through aligning a read in small pieces to a chromosome, producing Sequence Alignment/ Map (SAM)33 format output file and comprising every reads either mapped or unmapped against the reference genome. Some of the unmapped reads and poor quality mapping reads may be due to RNA extraction errors, sequencing bases bias and aligner algorithm defects. We discard poor quality reads by setting the mapping quality score to be higher than 30.  Creation of anchors. eTh next step of the GFusion is to split unmapped reads into three shorter segments, while its mate reads should be aligned to genome with high mapping quality. The first and last segments are defined as anchors_1 and anchor_2 separately with 20 bp or longer length (0.4 by default but no greater than 0.5 of the length of reads). Middle segment is defined to be a gap and the gap length equals read length minus anchor length. For example, a 50 bp read will be split into three segments: 20 bp anchors_1, 10 bp gap, and 20 bp anchors_2. The two anchors and a gap form a special pseudo paired-read reads that the anchors are the paired-end reads and gap is the insert segment. What's more, query name and the sequence quality values of the anchors are inherited from the parental reads with corresponding symbols to distinguish each anchor. GFusion identiefis the potential split reads based on the necessary conditions that fusion boundaries are located in the gaps and the anchors are originated from diefrent genes. In order to confirm the original genes of anchors, the split segments should be aligned against reference genome, shorter length of anchors may lead to multiple alignments, whereas the longer length of anchors makes it hard to detect the fusion boundary from narrow gaps34. eThrefore, a best length of anchors is suggested to be 0.4 of read length.  Aligning and locating anchors. hTe third step is to align the anchors independently to the reference genome by using Bowtie, an ultrafast, memory-ecfiient aligner for aligning un-spliced short reads to the genome. GFusion picked out the pair anchors, both of which are originated from same parental reads with common identiifers and can be aligned to genome with high mapping quality as described in the above. eThn, GFusion attempts to select potential split reads with two anchors uniquely aligning to diefrent genes and discards normal reads with the anchor pairs both matching the same genes. The paired-end reads, where the anchor_2 of potential split reads and mate reads locate in the same gene, will be considered as potential split fragment because fusion boundaries cut the paired-end reads into two segments from diefrent genes: one consists of mate reads, insert segment, anchor_2 and the other is anchor_1. eTh most powerful advantage of GFusion is that, unlike traditional fusion gene detection models such as Tophat-Fusion, FusionMap and other pipelines relying on unmapped split reads or discordant alignments, it comprehensively considers the mate reads and split reads alignment results in an eficient way to make the false positive rate far lower.  Locating fusion boundaries. To locate fusion boundaries, we proposed an evaluation criterion based on the human gene annotation (GTF file) and potential split reads which harbor the fusion boundaries in the gaps. Potential split reads with alignments where two anchors locate near the approximate exon boundaries is selected and exon junction boundaries are considered to be approximate fusion boundaries. uThs, the distance between fusion boundary and anchor alignment coordination should be less than gap length. We denfied Cor as an anchor alignment position (the first alignment base coordination) in reference genome. eTh anchors aligning to forward reference are denfied as strand+, while anchors aligning to reverse-complement reference are denfied as strand −. We also defined G1 as the start coordinate of an exon, and G2 as the end coordinate of an exon. eTh abbreviation for anchor length and gap length is expected to be al and gl separately. In addition, we added a parameter (default: p = 3), which is defined as the maximum allowing distance between anchor boundary site and its mapped exon boundary site. eTh most probable range of anchor location is restricted as follows: −p &lt; Cor − G1 &lt; gl + p , if anchor1strand − or anchor2strand+  al − p &lt; G2 − Cor &lt; al + gl + p, if anchor1strand + or anchor2strandConfirming fusion models. We defined four fusion models to denote the fusion gene strand direction based on supporting split fragment aligning orientations and gene transcribing orientations. f f is one fusion model that upstream gene and downstream genes are both forward transcribed and splicing in the forward orientation. Similarly fused genes both having reversed strand orientations are defined as rr model. Besides, fr is another fusion model for the fusion gene which is generated by upstream gene with forward orientation and downstream gene with reversed orientation, while fusion model rf confronts fr that fusion gene with rf model concatenates the reversed strand sequence of upstream gene to the forward strand sequence of downstream gene. Aggregating candidate fusions. For paired-end RNA-seq data, GFusion identifies candidate fusions based on the number of split fragments and spanning fragments as an evidence of gene fusion events. Spanning fragments are special discordant pair alignments harboring fusion boundaries in the insert segments with each read aligning to the diefrent genes. To search spanning fragments, firstly we selected the aligned reads with high mapping quality scores from initial alignment results. If the reads with common query name hit diefrent gene exons on the side of potential fusion boundaries, the read pairs are stored as potential spanning fragments. By default, GFusion only reports fusion candidates supported by at least one split fragment and one spanning fragment to remove false positive fusions.  Constructing fusion index and realigning. After identifying the potential fusions, we constructed the bowtie reference index using the potential fusion sequences and realign all the supporting fragments against the index for further filtering. This is the last filtering step to eliminate false positives. All sequences of fusion genes on the each side of fusion boundaries are collected and concatenated to produce the pseudo fusion reference indexes. For example, when the fusion model is fr, GFusion extracts upstream sequence from the first base to the fusion boundary base of upstream gene and extracts downstream sequence from the fusion boundary base to the last base of downstream gene using reference genome sequence file (hg19.fa). While the fusion model is rf, GFusion extracts upstream sequence from the fusion boundary base to the stopping base of downstream gene with reverse-complemented transforming and extracts downstream sequence from the fusion boundary base to ( 1 ) GFusion  Tophat-Fusion FusionMap  JAFFA the stop base of upstream gene. eTh upstream and downstream sequences are concatenated together to produce potential fusion reference sequences at fusion boundaries. Using fusion reference sequences, GFusion is able to build fusion bowtie index for supporting fragments realigning by Bowtie. Every candidate split fragments and spanning fragments are realigned against constructing index to recount the number of supporting fragment. As described in Step6, GFusion finally confirms fusion genes with fusion boundaries, fusion transcript model supported by at least one split fragment and one spanning fragment.  Tools selected for comparison. The fusion detection tools that we chose to benchmark together with GFusion were the following: Tophat-Fusion, FusionMap, JAFFA, and ChimPipe. Tophat-Fusion and FusionMap were chosen because they were extensively used by the community. In addition, they were also commonly used to conduct the comparison with other subsequent fusion detection tools. JAFFA and ChimPipe were chosen because they were two latest tools for fusion discovery and showed a better performance than other state-of-the-art tools. ehTrefore, these four tools were finally selected for comparison.  Tophat-Fusion and FusionMap parameters. In the Tophat-Fusion, the fusion-min-dist means that for intrachromosomal fusions, Tophat-Fusion tries to find fusions separated by at least this distance, and the num-fusion-reads means that fusions with at least this many supporting reads will be reported. In the FusionMap, α denotes the minimum end length of a seed read, β denotes the maximum hits of a read end, G denotes the non-canonical splice pattern penalty, MinimalHit denotes the minimal distinct fusion reads, and MinimalFusionSpan denotes the minimal distance (bp) between two fusion break points.    Results\r\n  We tested GFusion on three types of RNA-Seq dataset: ( 1 ) paired-end RNA-Seq datasets: three breast cancer cell lines BT474, SKBR3, MCF7 and a normal breast cell line described by Edgren et al. and can be available from the NCBI Sequence Read Archive [SRA:SRP003186]35; ( 2 ) single-end RNA-Seq datasets: the K562 CML cell line, containing 14 million 76 bp SE reads from work by Levin et al.36; ( 3 ) simulated datasets: consisting of approximately 24 million 76 bp paired-end reads and artificial fusion reads. We mapped all reads and short fragments to the human genome (hg19) with TopHat and Bowtie, and identified the genes involved in each fusion using the human gene annotations.  Testing on Paired-end Datasets. In three cancer breast cell lines, 37 fusion genes have been discovered in the previously research35, 37. GFusion found 34 fusion candidates containing 28 previously reported fusions showed in Table 1 (see details in Table S2). We also ran Tophat-Fusion, FusionMap, JAFFA, and ChimPipe on the same datasets to compare performance with GFusion. Tophat-Fusion found 66 fusion candidates containing 30 known fusion events and FusionMap reported 26 candidate fusion genes, 16 of which were previously known fusion genes. JAFFA found 37 fusion candidates containing 21 known fusion events, while ChimPipe reported 38 candidate fusion genes including 30 previously known fusion genes. It can be seen that Tophat-Fusion and ChimPipe achieves the highest sensitivity (81.08%) following by GFusion (75.67%). For the precision, GFusion has the highest value (82.35%) following by ChimPipe (78.94%), while the Tophat-Fusion has the lowest value (46.45%).  For the MCF7 cell lines, ChimPipe found 5 known fusion genes following by the 4 known fusions respectively detected by GFusion, Tophat-Fusion, and JAFFA. FusionMap only reported 3 known fusion transcripts (BCAS4-BCAS3, ARFGEF2-SULF2, RPS6KB1-VMP1), which were also found by other four tools. In addition, GFusion has the highest precision as it only reported two novel fusions (Table S2), while Tophat-Fusion reported other seven novel fusion genes so it's precision is the lowest. In the two novel fusion genes detected by GFusion, the novel fusion (PAPOLA-AK7) was also found by other four programs except the ChimPipe. Figure 3 illustrates BCAS4-BCAS3 fusion supporting fragments identified by GFusion. The facts that fusion reads spanned the fusion boundary between the BCAS4 and BCAS3 genes on chromosomes 20 and 17 proved that BCAS4 and BCAS3 gene fused and expressed in the MCF7 breast cancer cell line.  For the BT474 cell line, GFusion detected the 17 fusions out of 21 previously reported fusion transcripts, and predicted only one novel fusion transcript (Table S2). FusionMap found 9 reported fusion genes and predicted 3 novel fusion transcripts. Algthough TopHat-Fusion found 19 known fusions, it predicted 24 novel fusions. JAFFA detected 12 known and 6 novel fusions, while ChimPipe reported 18 known and 4 novel fusions. The above results shows GFusion achieves the highest precision, and its sensitivity difers by only about 1% from that of Tophat-Fusion. Furthermore, GFusion missed one known fusion (LAMP1-MCF2L) because this fusion was supported by only two low mapping quality split fragments and it was filtered out in our initial filtering step. Figure 4 shows MED1-ACSF2 fusion supporting fragments identified by GFusion, these reads clearly span the intra-chromosomal (chromosome 17) fusion boundary of the two genes. The upstream fusion gene MED1 is reversed transcribed, while downstream fusion gene ACSF2 is forward transcribed.  In the SKBR3 breast cancer cell line, GFusion, Tophat-Fusion and ChimPipe all found the same seven fusions out of ten previously reported fusion genes and predicted three (Table S2), vfie and one novel fusions respectively. While FusionMap missed six known fusions and found just four previously reported fusion transcripts and predicted four novel fusions. JAFFA reported five known fusions and five novel fusions. Among the fusion genes missed by GFusion, one known fusion, CSE1L-ENSG00000236127, was not found because ENSG00000236127 annotation has been removed from the recent Ensembl database. NFS1-PREX1 was filtered out because fusion supporting fragments had low mapping quality. However in the previously researches35, the evidences of fusion NFS1-PREX1 was not enough, as only a short segment of NFS1 was included in the fusion. Figure 5 shows the fusion supporting fragments for one of four fusion isoforms of TATDN1-GSDMB with strong alignment evidence that found by GFusion in SKBR3 cells. eTh upstream fusion gene TATDN1 and downstream fusion gene GSDMB are both reversed transcribed.  In fusion genes detection, the biggest challenge is to reduce the huge number of false positives resulted from error of sequencing and alignment. In order to decrease the number of false fusions, we have designed a series of strict filtering algorithms during fusion genes detection process. In order to estimate the false positive rate of three pipelines, we ran them on RNA-Seq datasets of normal breast cell line. GFusion and FusionMap both predicted just one novel fusion transcript. JAFFA and ChimPipe both predicted three novel fusion transcripts, and Tophat-Fusion predicted 4 novel fusions. These results indicate that GFusion had far fewer false positives than Tophat-Fusion, JAFFA, and ChimPipe.  In addition, it should be noted that the running time of GFusion was about 30 minutes through × 64 eight-core computer and six running threads and the majority of running time was spent on the read aligning. Using the same computer and threads, Tophat-fusion took an hour and 40 minutes, ChimPipe used an hour and 34 minutes, and FusionMap was the fast one for fusion analysis based on the fast alignment program GSNAP. JAFFA took vfie hours and 13 minutes with 12 threads.  In conclusion, GFusion can achieve high sensitivity and precision, and should be a reliable tool for fusion gene detection because of a series of filtering steps. Unlike Tophat-Fusion and FusionMap as well as other fusion detection tools, one of powerful features in GFusion is that it focuses more on the mapped mate reads and split reads. Testing on Single-end Dataset. Since recent advances in NGS platforms has resulted in vast increases in coverage and read length, gene fusion boundaries are now adequately represented by single-end reads. Fusion detection using single-end reads may be more powerful than the paired-end reads when the discordant read pairs or the short insert segment is used in detection algorithm.  We further applied GFusion on the published K562 CML cell line RNA-seq data containing 14 million 76 bp single-end reads. Two main high expression level gene fusions, BCR-ABL1 and NUP214-XKR3 already validated before were detected by GFusion19, 36. In the result, 281 split reads were found to bridge the fusion boundary of the fusion from upstream gene BCR at chr22:23632600 to downstream gene ABL1 at chr9:133729451 at the transcription level. As showed in Fig. 6, split reads are abundant for the fusion boundary between the chromosome 22 and 9, illustrating the fusion boundary precision achieved by GFusion even on single-end datasets.  In close proximity to BCR and ABL1, another gene fusion between NUP214 and XKR3 resided on chromosome 22 and 9 respectively was also detected. NUP214 have been known to be related in T-cell acute lymphoblastic leukemia38. One isoform of fusion NUP214 and XKR3 was identified by GFusion to be supported by 60 split reads connecting the end of exon 29 in NUP214 (chr9: 134074402) to the start of exon 2 in XKR3 (chr22:17288973). Another two additional isoform of fusion NUP214 and XKR3: exon 29 of NUP214 fused to exon 3 of XKR3, and exon 29 of NUP214 fused to exon 4 of XKR3 only supported by 2 and 8 split reads separately, were also detected by GFusion. eTh anchors orientation of the split reads detected by GFusion shows that fusion strand is forward to reverse. The fourth isoform of fusion NUP214-XKR3 involving exon 27 of NUP214 and exon 2 of XKR3 could also be found when we reduced the anchor length in GFusion. All of these detected fusion isoforms are in agreement with previous results36. In addition to BCR-ABL1 and NUP214-XKR3, GFusion identified all the other previously reported fusion genes, SNHG3-PICALM, PRIM1-NACA, NCKIPSD-CELSR3, SLC29A1-HSP90AB1. Among these six fusion genes, the two genes of fusion PRIM1-NACA, NCKIPSD-CELSR3, and SLC29A1-HSP90AB1 are close neighbors on the genome separately, and are likely to be fusion transcripts caused by read-through events39. eTh inner distances between two fused points on the genome are respectively 19 Kb in PRIM1-NACA, 21 Kb in NCKIPSD-CELSR3 and 15 Kb in SLC29A1-HSP90AB1.  To compare the performance of GFusion with FusionMap and Tophat-Fusion, we comprehensively analyzed their sensitivity, false positive rate as well as running time on the same dataset. As described in above results, GFusion detected the six fusion genes that are all previously reported by Levin et al.36 and predicted other 28 novel fusions (Table S3). Tophat-Fusion, by comparison, reported just three known fusion genes  True fusions Sensitivity  False fusions False positive rate(10−5) and 71 novel fusion genes with the parameters: fusion-min-dist = 10000, num-fusion-reads = 4, and missed three fusion PRIM1-NACA, NCKIPSD-CELSR3, and SLC29A1-HSP90AB1. Tophat-Fusion appears not to consider those read-through transcripts to be gene fusions. FusionMap detected five known fusions and predicted 221 novel fusion genes with the default parameters except: α = 25, β = 1, G = 2, MinimalHit = 2, MinimalFusionSpan = 10000 (Table S4)22. In addition, using the same computer as above-mentioned, GFusion took an hour that was five times faster than TopHat-Fusion. FusionMap is the fastest one which took only 10 minutes. Obviously, the problem concern of FusionMap is its sensitivity and lower false positive rate. GFusion is a superior and powerful tool for fusion gene detection using single-end reads that it shows greater sensitivity and has lower false discovery rate than Tophat-Fusion and FusionMap.  Testing on Simulated Dataset. To further compare the performance of GFusion with the existing methods, we generated a simulation dataset that consists of a normal background data and artificial fusions. We used paired-end RNA-Seq data of human embryonic stem cells as the normal background dataset in which it was not expected to harbor any fusions. eTh background data used in our work can be downloaded from NCBI Sequence Read Archive (SRA: SRR521477) with approximately 24 million 76 bp paired-end reads generated by WiCell Research Institute. We firstly bridged sequences from exons of two diferent genes and obtained 42 artificial fusion transcripts. Secondly, the fusion supporting reads were generated at random to represent the expression level of artificial gene fusions and their insert segment length distribution was similar to the normal paired-end reads. Finally, the simulate dataset was produced by mixing the fusion supporting reads with normal background reads.  Aeftr that, we ran GFusion, Tophat-Fusion, FusionMap, JAFFA, and ChimPipe on the simulated dataset, and counted the number of true and false supporting fusion reads to calculate sensitivity and false positive rate of three methods separately. It can be seen from Table 2 and Fig. 7A that GFusion had higher sensitivity and lower false positive rate than other four tools in the fusion detection on the simulated dataset. GFusion achieved a high sensitivity of 88% by identifying 37 out of 42 true fusion genes as well as just predicting seven false fusion genes with parameters: anchor length = 30, max intron length = 5000. Tophat-Fusion found 35 fusion genes with 83% sensitivity and reported 50 false fusion genes with parameters (fusion-min-dist = 5000, num-fusion-reads = 1). FusionMap reported 22 of 42 true fusion genes and 160 false fusions with parameters α = 30, β = 1, G = 2, MinimalHit = 1, MinimalFusionSpan = 5000. JAFFA detected 22 true fusion genes and 77 false fusions with the direct mode, and the hybrid mode was not used because this mode is very time-consuming. ChimPipe has the lowest sensitivity of 21% with the default parameters.  As shown in Table 2, the number of false positives detected by FusionMap were heavily more than that detected by other tools, especially comparing with GFusion. In addition, only 4 fusions were detected in common by all of the vfie methods as shown in Fig.  7B, and 12 fusions were detected at least four tools. 8 fusions were only found by GFusion, while only 1 fusions was found by other four tools but GFusion. This result also shows that these vfie tools are really diefrent from each other in the detection of fusion genes.  For fusion detection, the number of supporting reads is another important indicator to confirm the fusions from candidate fusion genes. We calculated the Positive Predictive Values (PPV) with the number of supporting reads to estimate the fusion genes detection ability for these three pipelines.  PPV =  TP TP + FP ( 2 ) where TP means the number of true fusions; FP means the number of false fusion gene.  GFusion reported the most number of true fusions and the fewest false fusions when supporting fragments number is greater than one. Considering the split fragments containing the fusion boundary in the reads (seed reads of FusionMap, spanning reads of Tophat-Fusion and JAFFA, nbSpanningReads of ChimPipe) (Fig. 8A), GFusion and JAFFA had higher PPV than other three tools. In addition, if we selected three split fragments as a minimum threshold to support the fusions, GFusion and JAFFA would filter out all the false fusion genes successfully. In contrast, Topaht-fusion, FussionMap and ChimPipe still predicted many false fusions. Furthermore, Tophat-fusion and ChimPipe could not filter out all the false fusion genes until the minimum supporting split fragments is set to be 17. By analyzing all the supporting split fragments and spanning fragments (rescued reads denoted in FusionMap, spanning pairs denoted in Tophat-Fusion, nbTotal denoted in ChimPipe) (Fig. 8B), the fusions reported by GFusion and Tophat-Fusion have more supporting fragments than FusionMap and ChimPipe. In addition, compared with Tophat-Fusion, GFusion has a higher PPV value which always kept stable greater than 80% as the supporting fragments increase, indicating that GFusion are more reliable in performance than Tophat-Fusion and FusionMap.  ehT distribution of supporting fragments for fusion genes can also be seen in Fig.  9, the number of false fusions supporting fragments reported by GFusion is relatively lower than the true fusions, as well as the ChimPipe. However in Tophat-fusion and FusionMap, the distribution of supporting fragments for true fusions is similar to the false fusions, and it is hard for these methods to reduce the false positive rate by setting the threshold of supporting fragments number. 1 0    Conclusions\r\n  GFusion is an eefctive and reliable Perl script tool for fusion gene detection on both single-end and paired-end RNA-Seq data. GFusion can achieve highly sensitive and fewer false positive rate using split-realign protocols: splitting reads, realigning anchors, locating fusion boundaries and constructing fusion transcript reference. In order to remove the false fusion genes and keep the higher sensitivity, GFusion designed several strict filtering steps including locating the reads position, confirming transcribing orientations, constructing fusion reference, and realigning candidate fusion fragments. Compared with discordant pairs in which each reads align against diferent genes or individual unmapped reads harboring the fusion boundaries, GFusion focuses more on the mapped mate reads and split reads. GFusion requires the anchor length more than 20 bp to minimize impact of short sequence multi-mapping.  We have tested the performance of the GFusion on published RNA-Seq datasets with known fusion genes involving paired-end and single-end read dataset as well as a simulated dataset. Both of these results indicated that GFusion performed higher sensitivity and far lower false positive rate compared with other existing fusion detection methods. 1 1    Acknowledgements\r\n  hTe study was supported by grants from the National Natural Science Foundation of China (No. 61571223, 61171191, X.S; No. 81270700, P.H), the Specialized Research Fund for the Doctoral Program of Higher Education (No. 20133218110016, X.S), the \u201cSix Talent Peak\u201d Project of Jiangsu Province under Grant No. SWYY-021(X.S).    Author Contributions\r\n  J.Z., Q.C. and J.W. implemented the study and wrote the manuscript. X.S. and P.H. conceived study and revised the manuscript. All authors read and approved the final manuscript.    Additional Information\r\n  Supplementary information accompanies this paper at doi:10.1038/s41598-017-07070-6 Competing Interests: eTh authors declare that they have no competing interests.  Publisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afiliations.  Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.    ",
    "sourceCodeLink": "https://github.com/xiaofengsong/GFusion",
    "publicationDate": "0",
    "authors": [
      "Jian Zhao",
      "Qi Chen",
      "Jing Wu",
      "Ping Han",
      "Xiaofeng Song"
    ],
    "status": "Success",
    "toolName": "GFusion",
    "homepage": ""
  },
  "67.pdf": {
    "forks": 0,
    "URLs": [
      "github.com/clzani/DEREP-NP;",
      "github.com/clzani/DEREP-NP",
      "github.com/clzani/DEREP-NP,"
    ],
    "contactInfo": [],
    "subscribers": 1,
    "programmingLanguage": "",
    "shortDescription": "Repository for files related to DEREP-NP dereplication database",
    "publicationTitle": "Database for Rapid Dereplication of Known Natural Products Using Data from MS and Fast NMR Experiments",
    "title": "Database for Rapid Dereplication of Known Natural Products Using Data from MS and Fast NMR Experiments",
    "publicationDOI": "10.1021/acs.jnatprod.6b01093",
    "codeSize": 256483,
    "publicationAbstract": "The discovery of novel and/or new bioactive natural products from biota sources is often confounded by the reisolation of known natural products. Dereplication strategies that involve the analysis of NMR and MS spectroscopic data to infer structural features present in purified natural products in combination with database searches of these substructures provide an efficient method to rapidly identify known natural products. Unfortunately this strategy has been hampered by the lack of publically available and comprehensive natural product databases and open source cheminformatics tools. A new platform, DEREP-NP, has been developed to help solve this problem. DEREP-NP uses the open source cheminformatics program DataWarrior to generate a database containing counts of 65 structural fragments present in 229 358 natural product structures derived from plants, animals, and microorganisms, published before 2013 and freely available in the nonproprietary Universal Natural Products Database (UNPD). By counting the number of times one or more of these structural features occurs in an unknown compound, as deduced from the analysis of its NMR (1H, HSQC, and/or HMBC) and/or MS data, matching structures carrying the same numeric combination of searched structural features can be retrieved from the database. Confirmation that the matching structure is the same compound can then be verified through literature comparison of spectroscopic data. This methodology can be applied to both purified natural products and fractions containing a small number of individual compounds that are often generated as screening libraries. The utility of DEREP-NP has been verified through the analysis of spectra derived from compounds (and fractions containing two or three compounds) isolated from plant, marine invertebrate, and fungal sources. DEREP-NP is freely available at https://github.com/clzani/DEREP-NP and will help to streamline the natural product discovery process.",
    "dateUpdated": "2017-07-17T15:52:25Z",
    "institutions": ["Centro de Pesquisa René Rachou-Fiocruz"],
    "license": "No License",
    "dateCreated": "2017-02-14T15:11:40Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     J. Nat. Prod.     10.1021/acs.jnatprod.6b01093   Database for Rapid Dereplication of Known Natural Products Using Data from MS and Fast NMR Experiments     Carlos L. Zani  0    Anthony R. Carroll    0  Natural Products Chemistry Laboratory, Centro de Pesquisa René Rachou-Fiocruz ,  Belo Horizonte, 30190-002, MG ,  Brazil     2017   80  1758  1766    24  11  2016     The discovery of novel and/or new bioactive natural products from biota sources is often confounded by the reisolation of known natural products. Dereplication strategies that involve the analysis of NMR and MS spectroscopic data to infer structural features present in purified natural products in combination with database searches of these substructures provide an efficient method to rapidly identify known natural products. Unfortunately this strategy has been hampered by the lack of publically available and comprehensive natural product databases and open source cheminformatics tools. A new platform, DEREP-NP, has been developed to help solve this problem. DEREP-NP uses the open source cheminformatics program DataWarrior to generate a database containing counts of 65 structural fragments present in 229 358 natural product structures derived from plants, animals, and microorganisms, published before 2013 and freely available in the nonproprietary Universal Natural Products Database (UNPD). By counting the number of times one or more of these structural features occurs in an unknown compound, as deduced from the analysis of its NMR (1H, HSQC, and/or HMBC) and/or MS data, matching structures carrying the same numeric combination of searched structural features can be retrieved from the database. Confirmation that the matching structure is the same compound can then be verified through literature comparison of spectroscopic data. This methodology can be applied to both purified natural products and fractions containing a small number of individual compounds that are often generated as screening libraries. The utility of DEREP-NP has been verified through the analysis of spectra derived from compounds (and fractions containing two or three compounds) isolated from plant, marine invertebrate, and fungal sources. DEREP-NP is freely available at https://github.com/clzani/DEREP-NP and will help to streamline the natural product discovery process.       -\r\n  A diverse compounds present in a wide range of crude biota extract may contain thousands of structurally concentrations.1 The separation of these compounds from each other (many of which often occur in low abundance) can be challenging.1 The majority of these compounds are involved in primary metabolic processes, but a subset, natural products (NPs), are not.2 NP research often aims to identify the role of purified NPs in biological processes, and to do this, extracts or semipurified fractions obtained through chromatographic separation of crude extracts are tested in a bioassay.1 Further purification of the bioactive fractions ultimately leads to the isolation of individual bioactive compounds.1 The identification of the complete 3D structure of these unknown bioactive NPs is not trivial and requires extensive analysis of data obtained from multiple spectroscopic experiments (mainly MS and 1D and 2D NMR). Often configurational isomers (either enantiomers or diastereomers) are identified, and these molecules provide an added challenge to correctly assign a structure.1 Even after peer review some structures are incorrectly assigned, and their true identity may subsequently be corrected either through reinterpretation of spectroscopic data or total synthesis.3 To date, well over 230 000 structurally characterized NPs have been reported from plants, animals, and microorganisms, and the number of new structurally characterized NPs published continues to increase by many thousands each year.4 As a consequence, known NPs are frequently reidentified during NP discovery research. The reisolation of known bioactive NPs when the discovery of novel or new ones is the intended outcome of research is an expensive and timeconsuming impediment. The concept of dereplication was introduced in 1978 as a methodology to avoid the rediscovery of known anticancer antibiotics from bioactive extracts derived from microorganisms when the discovery of novel anticancer compounds was intended.5 Dereplication is now defined as the elimination of known active substances from consideration when a bioactive mixture is being investigated.6 Since 1978 a diversity of dereplication strategies have been reported.7−12 These strategies generally rely upon the ability to match molecular features present in unknown bioactive NPs (either spectroscopic/spectrometric or structural) with data stored in spectroscopic (containing MS and/or 13C NMR data of NPs) and/or structural databases. A dereplication strategy is generally implemented after initial screening of extracts or semipurified fractions, and the most widely used involve the so-called hyphenated techniques, in which a separation device (a chromatograph) is coupled with spectrometers such as MS, UV, IR, and NMR and analysis of the spectra obtained provides structural information on the compounds present in mixtures.9,13−15 UV and IR provide the least discriminatory spectroscopic data and can really be used only for dereplication in combination with other spectroscopic data. Mass spectrometry is a powerful tool, but many compounds have identical molecular weights and fragmentation data can be ambiguous. An NMR spectrometer has been described as the universal chemical detector since it generates the most information-rich data to derive structural features present in a compound and molecules with the same molecular weight but different constitution or configuration possess different NMR fingerprints. The intrinsic low sensitivity of this technique has been circumvented by enormous advances in hardware, software, and pulse programs, and nowadays a very small amount of sample is required to generate high-quality NMR data. This frequently allows unambiguous identification of compounds eluting from an HPLC column in submilligram quantities.6,16 NMR spectroscopy therefore provides a significant advantage over MS to definitively assign molecular structures. Whatever dereplication approach is used the computational matches (or \u201chits\u201d) still need to be verified as identical structures through more thorough spectroscopic analysis, literature comparisons with published spectroscopic data, or chromatographic comparison with reference compounds.  Metabolomics is a related cheminformatics methodology that has been developed to study the total metabolic processes within organisms through the identification and quantification of metabolites.17 A significant distinction between metabolomics and dereplication is that metabolomics research usually aims to identify all metabolites involved in metabolic processes, whereas dereplication is a tool used to identify only known NPs. Metabolomics can however provide insights for NP research since the majority of metabolites present in an organism are involved in primary metabolic processes (the main focus of metabolomics research), but the metabolic pathways of primary metabolism often supply the precursors for NP biosynthesis.2 The accuracy of metabolomics analysis can suffer exactly the same limitations as hyphenated dereplication techniques since validation of the true identity of a molecule that matches a database entry requires more rigorous analysis than just a molecular ion match or a fragmentation pattern match, as these give no indication of the constitution or configuration within the molecule. A significant limitation of many metabolomics and hyphenated dereplication strategies is that they ignore configurational isomerism. To this end, databases of 1H NMR spectra for common primary metabolites have been generated, and these serve as a powerful tool for cross referencing MS data with NMR data to give an orthogonal set of corroborating data to unambiguously assign the true identity of a molecule. Unfortunately NMR data for the vast majority of published NPs are not available in databases and the majority of the metabolome of the world's biodiversity remains uncharacterized and thus unidentifiable.  There are free Web-based databases available that have been used for fast identification of known compounds. NAPROC,13 available at the University of Salamanca (Spain), contains 13C NMR data for about 20 000 natural products,18,19 while the CH-NMR-NP database contains 13C NMR chemical shifts for 30 000 natural products published between 2000 and 2014.20 However, besides containing only a limited number of compounds, matching structures in these databases requires researchers to acquire 13C NMR spectra for any isolated compound (an insensitive method). Since a database containing annotated NMR data for the 230 000 structurally characterized NPs does not currently exist, other approaches to utilize the powerful discriminating features of NMR data have been developed. In 2001, Bradshaw and co-workers developed a method to quickly match the planar structures of previously published NPs using MS and NMR data obtained for purified NPs.21 They showed the discriminant power of structural information such as molecular weight and the exact counts of the number of methyl, methylene, and methine groups occurring in each structure from a database containing about 126 000 natural product structures. Their approach was further improved by other groups,6,22 to include other structural features that also can be easily deduced from 1H, HSQC, and/ or HMBC NMR experiments. Unfortunately, the software and data described by these authors are either proprietary or commercial,23 restricting their widespread access.  More recently, Williams and co-workers created a subset of the ChemSpider database containing 22 million diverse compounds for the dereplication of natural products using minimal NMR data inputs.24 They concluded that their approach would give better results if the database could be focused on NPs only. This context prompted us to prepare a database that, besides containing the molecular structures, molecular formulas, and exact and nominal molecular weights of known natural products, also included the frequency at which specific substructures, identified from simple and fast NMR experiments (e.g., 1H NMR and edited HSQC), occur in each structure in the database. For molecules containing methyl groups, HMBC spectra can also be acquired quickly, and this often provides further substructures containing quaternary carbons such as ketones, esters, amides, double bonds, and amino-substituted and oxygenated carbons. The recent publication of the Universal Natural Products Database (UNPD), a compilation of 229 358 natural products from terrestrial and marine macro- and microorganisms that was made publicly available, has made this work possible.4 Furthermore, free and open source software necessary to process, store, retrieve, and analyze chemical structures and related data is now available.25−27 With these data and tools in hand, a database (DEREP-NP) was generated that provides the number of times that each of the selected 65 structural features (small substructures that can be deduced from NMR experiments) occurs in each NP present in UNPD. This dereplication methodology can be applied to purified natural products or fractions containing a small number of compounds. Fraction libraries that have been developed to contain a small number of individual NPs per fraction through judicious choice of HPLC separation protocols are routinely used in NP drug discovery, and this dereplication methodology is well suited to interrogate these sorts of libraries. The DEREP-NP database is freely available for download at https://github.com/clzani/DEREP-NP. It must be emphasized that DEREP-NP is a manual low-throughput dereplication tool that is not useful for ■ high-throughput metabolomics. It is, however, a powerful tool to rapidly identify \u201chits\u201d. Hits are defined as published NP structures that are present in DEREP-NP and that share the queried structural features with the purified unknown compound. Verification of the identity of the unknown compound with one of the \u201chits\u201d still requires an analysis of the necessary set of experimentally obtained spectroscopic data and comparison with published data of the \u201chits\u201d for unambiguous identification. Furthermore, if configurational isomers of a structure are present in DEREP-NP, a search will return \u201chits\u201d for all published configurational isomers, but again comparison with published data should clarify these stereochemical assignments.    RESULTS AND DISCUSSION\r\n  The origin and scope of the data compiled in UNPD (http://pkuxxj.pku.edu.cn/UNPD) were detailed by Gu and coworkers.4 The DEREP-NP database was prepared using the procedures outlined in the Experimental Section. It contains 1.92 GB of data and takes about 3 min to be processed and displayed by DataWarrior26 using the hardware described. By saving the database in the DataWarrior format (*.dwr), the file size is reduced to 361.6 MB and requires less than 40 s to process and present the data on the screen. Searches usually take less than a second to be completed. The DataWarrior interface is user-friendly and contains a help file that provides detailed instructions to use its functionalities.  DEREP-NP searches can be performed using the different formats available in DataWarrior: textual/numeric, with the options \u201cstarts with\u201d, \u201ccontains\u201d, and \u201cequals\u201d; the slider format is useful to search ranges of values. If partial structures can be derived from NMR data, they can be searched using the substructure filter format. All structures retrieved by a query are shown in a single window pane, making it easier for the user to inspect and decide which feature to query next or what signals to check in the NMR spectra, thus facilitating the iterative nature of the process. As the data file also contains 3D coordinates, the 3D viewer of the software can be used to facilitate the visualization of structures that may be difficult to inspect in a 2D representation.  The distribution of the MW of the compounds in the database is shown in Figure 1. Overall, the database contains 33 339 different MW values, with the most frequent value being 264.13615, representing 640 compounds with the molecular formula C15H20O4.  Interpreting edited HSQC NMR data in combination with analysis of 1H NMR integrals to distinguish CH's from CH3's provides the most discriminant feature since the numbers of CH3's, CH2's, and CH's can be rapidly identified. The addition of specific types of quaternary carbons deduced from HMBC NMR correlations provides an added level of discrimination. Although the number of structures with singular values for each of these features can be as high as 35 000 (Figure 2), if combined, the discriminant effect is increased and can lead, in many cases, to very few \u201chit\u201d structures, as demonstrated by Bradshaw et al.21 Indeed, the number of different CH3−CH2CH−Cq combinations in the database is 70 138, which is almost twice the number of distinct MW combinations. Thus, the combination of these four features also has a high discriminating power.  The most frequent combination in the database, for example, is for structures with 4 CH3's, 4 CH2's, 4 CH's, and 3 Cq's, which occurs only 454 times. If MW is included as a criterion, the number of structures retrieved can be drastically reduced. However, while some MW values return just one structure, others can retrieve as many as 152 (Table 1).  These results emphasize that although being valuable, the criteria used by Bradshaw et al.21 are not always enough to reduce the number of hits to a manageable number. This is the reason that Lang et al.6 and Bitzer et al.22 included more features that can be extracted from simple NMR experiments. For the same reason, DEREP-NP includes the ability to search for 65 features (Table 2), which can help the researcher to reduce the number of candidate structures to be analyzed and increase the chances for the rapid identification of the isolated compound under investigation if it is present in the database.  The papers from Bitzer et al.,22 Bradshaw et al.,21 and Lang et al.6 provide good advice and strategies to make the best use of computational databases using NMR data for dereplication. The small-scale fractionation procedure described by Lang and co-workers is especially noteworthy.6 It is important to emphasize that queries with incorrect input values will undoubtedly result in wrong structures being retrieved. Even if wrong \u201chits\u201d are retrieved, they will ultimately be eliminated because their published spectroscopic data will not match those obtained experimentally. To avoid this situation and if one is not sure about the frequency of a specific feature from the spectra, either this feature should not be used or the number of these features should be estimated using a larger value range. Although the latter approach increases the number of \u201chits\u201d, at least the correct structure, if present in the database, will be among those retrieved. It must be stressed that the absence of a feature can be as important as its presence to reduce the number of candidate structures. So, if a given output shows structures containing features that cannot be corroborated by NMR data, a zero value for that feature should be entered to indicate its absence and hence exclude those structures containing that feature. Dereplication searches should aim to extract the most information from the 1H NMR and editedHSQC experiments, as the number of possible structures can be dramatically reduced if one can use, for example, the number (or absence) of aromatic protons, the aromatic substitution pattern, number of oxygenated carbons (methoxy, carbinol, anomeric, and methylenedioxy), sp3-, sp2-, and sp-hybridized carbon atoms, terminal methylene groups, the multiplicity of the methyl signals, number of acidic OH and formyl groups, and so on (Table 2). For those with access to accurate mass spectrometric data the use of the text filter with the option \u201cstarting with\u201d to enter the MW values, starting with an integer value and then inserting, one by one, the dot and the other digits, can be beneficial. Some caution should be made here since many molecules produce adducts by electrospray MS, and therefore blindly using the accurate mass data without paying due attention to potential adducts being observed could lead to erroneous results.  To verify DEREP-NP outputs, we revisited the six examples given by Bitzer et al.,22 Bradshaw et al.,21 and Lang et al.,6 querying the same features in the same order used in their work. The results are summarized in Table 3. Since DEREP-NP includes 229 358 structures of natural products published in the literature up to 2013,4 the number of \u201chits\u201d is usually higher than those reported in previous studies with smaller or focused databases.  The following examples were selected from our own research. A compound was purified from the endophytic fungus Cochliobollus sp., and its HRMS indicated a molecular CH3 (all) CH3 (singlet) CH3 (doublet) CH3 (triplet)   CH3 (isopropyl)\r\n  CH3−Arb    CH3 (vinyl)\r\n  CH3 (acetyl) CH3−O (all) CH3O−Ar CH3−N (all) CO (all) CO (ester/  lactone) COOH CH2 (all) CH (all) CH2 (sp3) CH (sp3) CH2 (sp2) CH (sp2 all) CH2−O CH (sp2 olefinic) (all) CH2 (dioxy) CH (sp2 arom) CH2−N CH (arom-singlet) (all) OH (all) OH  (alcohol) OH  (phenol) OH (acidic)  CH (sp) CH−O (all) CH/CO (aldehyde) CH (anomeric) CH (peptide) CH−N (all) benzc 1-monosubst benz 1,2-disubst benz 1,3-disubst benz 1,4-disubst benz 1,2,3-trisubst benz 1,3,5-trisubst benz 1,2,4-trisubst benz 1,2,3,4  tetrasubst benz 1,2,3,5  tetrasubst benz 1,2,4,5  tetrasubst benz 1,2,3,4,5pentasubst Cqa (all) Cq (sp3) Cq (sp2 all) Cq (sp2 olefinic)    Cq (sp2 arom)\r\n  Cq (sp)   CH−X (arom)d\r\n  CH−X2 (arom)e CH3NCH2f    CH3NCHf\r\n  CH2NCH2f CH2NCHf CH3NCOf CH3OCORf CH3C−COf NH (all) NH2 (all)  NH (arom) aCq = quaternary carbon. bAr = aromatic ring. cbenz = benzene ring. d1JCH &lt; 200 Hz. e1JCH &gt; 200 Hz. fHMBC experiments. formula of C30H42O7. Analysis of the integrals obtained from a 1H NMR spectrum in combination with correlations observed in an edited HSQC spectrum (Figure 3) highlighted the presence of eight methyl groups, five methylenes, and seven methines. aNatural Products Database21 with 126 000 natural products. bPrivate database22 with 15 000 unique natural products. cAntiMarin database6 with 47 000 unique compounds.  A molecular formula search in DERREP-NP resulted in 90 matches, and restricting the search to structures with the abovementioned number of methyl, methylene and methine groups reduced the matches to just one compound, anhydrocochlioquinone A (1). Comparison of the complete set of NMR and mass spectra with those published for this compound confirmed its identity.28  For those researchers who lack ready access to mass spectrometry facilities, 1H/HSQC NMR data alone can provide sufficient evidence to obtain hits to structures present in the database. Extracts from a plant in the genus Corymbia (Myrtaceae) yielded NMR data from semipurified fractions that contained signals for eight methyl singlets, an additional six methyls, and a phenolic proton (Figure S2, Supporting Information). A search of DEREP-NP using the three criteria (8 CH3 (singlet), 14 CH3 (all), and 1 OH (phenol)) yielded nine hits. Since the HSQC data also suggested the molecule contained three methylenes, supplementing the above search with the addition of CH2 (all) = 3 reduced the hits to one compound, rhodomyrtosone C ( 2 ).29 Comparison of the 1H and HSQC NMR data with those reported in the literature confirmed the compound to be rhodomyrtosone C.  An extract from the leaves of a Triunia species (Proteaceae) yielded a fraction containing a compound with three methyls, four methines, three methylenes, and no aromatic protons (Figure S3, Supporting Information). A search of DEREP-NP using these four criteria yielded 458 hits. The 1H NMR spectrum indicated that two of the methyls are doublets, the HSQC spectrum (Figure S4, Supporting Information) suggested that one of the methyls is attached to a nitrogen and one of the methines is oxygenated, while the HMBC spectrum (Figure S5, Supporting Information) indicated the molecule also contains a ketone carbonyl adjacent to one of the methyl groups. Adding these criteria (CH3 (doublet) = 2, Chart 1  CH3N = 1, CH−O (all) = 1, and CO (all) = 1) to the search reduced the hits to one compound, 2,3-dihydrodarlingine ( 3 ), and comparison of its NMR data with the literature confirmed its structure.30  Many marine natural products are proton poor and can often contain isolated proton spin systems. However, even these groups of molecules can be identified with a limited number of search criteria. A blue-colored fraction obtained from purification of an extract from the marine ascidian Eudistoma glaucum gave a 1H NMR spectrum that contained a mixture of three compounds in a ratio of 1:2:12 (Figure S6, Supporting  Information). The negative-mode ESIMS exhibited a major [M − H]− ion at m/z 362 consistent with a MW of 363. The 1H and HSQC NMR data indicated the major compound to have two 1,4-disubstituted aromatic systems and five downfield protons not attached to carbons. A DEREP-NP search using benz-1,4-disubst = 2 and MW = 363 yielded 10 hits. Adding CH2 (all) = 0, CH3 (all) = 0, and CH (all) = 8 retrieved only the structure of rigidin ( 4 ),31 and comparison with literature data confirmed the major compound to have this identity.  There were not enough distinguishing features in the 1H NMR spectrum to allow for database searching of the two minor compounds in the mixture.  Another ascidian produced several fractions containing aromatic compounds with few 1H NMR signals. One compound was found to contain a 1,2,4-trisubstituted aromatic system and three aromatic singlets (Figure S7, Supporting Information). The molecule also contained five methoxy group proton singlets and two methylenes. A DEREP-NP search using benz-1,2,4-trisubst = 1, CH arom (singlet) = 3, CH2 (sp3) = 2, and CH3-O-Ar (methoxy) = 5 yielded eight hits, all of which were lamellarin-type alkaloids.32 Comparison of the 1H NMR data obtained with that reported in the literature for the eight hits established that the compound isolated was lamellarin T ( 5 ).33  There has been a trend over recent years to produce libraries of natural product fractions for biological screening. This approach provides advantages over screening crude extracts since the libraries can be tailored to comply with \u201clead-like\u201d properties such as logP, and each fraction is likely to contain only a small number of individual compounds.34,35 These fractions are therefore also well suited for dereplication using the above-described technique. A fraction from a bryozoan extract contained two compounds in a ratio of ∼3:1 (Figure 4).  MS analysis indicated the two compounds had MWs of 196 and 210. Both compounds contained aromatic NH, a 1,2disubstituted aromatic, and two additional aromatic protons.  The major compound contained an ethyl group and the minor compound contained a methyl singlet, which showed a correlation to a ketone carbon in the HMBC spectrum. A search of DEREP-NP with the above criteria for each compound yielded only two compounds, 1-ethyl-9H-betacarboline ( 6 ) and 1-(9H-beta-carbolin-1-yl)ethanone ( 7 ).36,37  The examples given above demonstrate the utility of the DEREP-NP database and search tools to identify known NPs rapidly using a limited set of spectroscopic data. This approach is limited to the evaluation of spectra of purified compounds or spectra containing a mixture of a small number of compounds.  Although many organisms can contain many thousands of individual small molecules in concentrations varying from &gt;1% to less than 0.0001% dry weight, the use of a small number of separation steps can generally yield fractions containing a small number of components in quantities visible by NMR spectroscopy, as demonstrated by Lang and co-workers.6 Since modern NMR spectrometers can be used to generate 1H and HSQC NMR data on submilligram quantities of material in minutes, this technique should be able to provide a quick assessment of the uniqueness of purified or semipurified natural product fractions and thus provide a useful starting point for any natural product discovery process. By limiting the search criteria to only definitive features (and spreading the net wide where ambiguous data occur, especially in crowded regions of NMR spectra) a manageable number of \u201chits\u201d can still be obtained. Once \u201chits\u201d have been obtained, however, the identity of the compound still requires validation though comparison with published spectroscopic data. The complexity of extracts obtained from diverse organisms still provides researchers with a challenge to distinguish known compounds from new and novel metabolites. This is particularly the case in the study of herbal medicines, where the bioactive ingredients are often unknown. The approach outlined in the study is not intended to be used to identify new and novel chemistry, but to reduce the time and resources spent on reidentifying known chemistry. Researchers in the field of metabolomics have tried to address this discovery bottleneck through the generation of MS- and NMR-based databases. These databases however have generally targeted primary metabolites of interest as biomarkers of disease and environmental disturbance or essential oils present in plants. When applied to natural product research, where compound diversity is high and generally selective for specific biota, the generation of spectroscopic databases for metabolomics research is a major challenge. As mentioned earlier, 13C NMR spectroscopic natural product databases have been generated, but these are of limited utility due to the small number of compounds present and a reliance on the acquisition of 13C NMR data for searching purposes. There are many limitations to this approach including standardization of a specific NMR solvent used to acquire all data and the challenge of reisolating all published NPs to generate data for inclusion into such a database. An alternative approach would be to generate NMR spectroscopic databases from the published literature and to make these freely available. Searching such databases would still be problematic if spectra were acquired in different solvents than those used to generate the database. On the other hand, structural features that can be derived from NMR analysis, but which do not rely on a specific search of chemical shift data, provide a more powerful tool to identify potential matches to known structures. The protocols outlined in this study have demonstrated that this approach can be an effective tool to identify published NPs, and by including 65 searchable structural (small substructures) features, this open access database is a powerful tool for dereplication. Its usefulness was compared with similar approaches previously described and found to yield similar results, but, since DEREPNP contains 229 358 natural product structures, its added benefit is that it includes a much larger number of compounds isolated from terrestrial and marine macro- and microorganisms published before 2013 than previously described databases. The methodology described above will encourage researchers to find structural alternatives when no verifiable match (through literature comparisons of spectroscopic data with database \u201chits\u201d) is obtained. The methodology therefore provides a powerful tool to aid and encourage new and novel bioactive natural product discovery. DEREP-NP is freely available and can be used on desktop computers running open source software. It can be used as is or improved by the users to fit their needs. To this end, DEREP-NP is deposited at https://github.com/clzani/DEREP-NP, where new versions will be included as the database evolves and interested researchers can upload their own versions and make requests for corrections and additions. ■      EXPERIMENTAL SECTION\r\n    General Experimental Procedures. The UNPD files containing\r\n  structures (SDF format) and associated data (csv file format) of 229 358 NP structures were downloaded from the Web site indicated by Gu et al.4 (http://pkuxxj.pku.edu.cn/UNPD). These files were processed using KNIME25 (see Supporting Information for details) in ■ order to merge and select only the desired information, namely, 2D and 3D structures, available names and CAS numbers, InChI, InChI Key, SMILES, canonical SMILES, molecular formula (MF), and exact molecular weight (MW) calculated from the monoisotopic masses using the mass of the most abundant isotope of each element. The 65 structural features (Table 2) were chosen because they can be easily recognized using 1H NMR and HSQC spectra and correlations from intense proton signals such as methyl singlets in HMBC spectra. These features were translated into SMARTS queries using the SMARTSEditor26 and processed in a KNIME workflow (Figure S8, Supporting Information) to determine the number of occurrences of each feature in each structure of the UNPD data set. An SDF file containing the original information (structure, MF, etc.) plus the count number of each of the NMR features was generated. This SDF file, named DEREP-NP, can be read by any chemistry-aware software. Adhering to the principle of bringing a freely accessible platform, we used DataWarrior, an open-source software for chemical data visualization and analysis.27 All procedures were run on a MacMini 6.2 computer equipped with an Intel Core i7 processor working at 2.6 GHz, 16 GB RAM and a GPU Intel HD Graphics 4000.  Since correlations between protons and carbons can be distinguished even when the resolution in the carbon dimension in HSQC and HMBC spectra is low, these spectra can be acquired successfully for dereplication purposes even with a moderately small number of experiments (or increments, usually less than 100). Furthermore, most protons have a relatively short spin−lattice relaxation time (T1), and therefore the delay between scans (or transients) can be reduced to less than a second (0.8 s is a good compromise). By applying both approaches, HSQC and HMBC spectra can successfully be obtained in less than 5 min even for compounds isolated in 1 mg quantities on a high-field NMR spectrometer.    ASSOCIATED CONTENT\r\n  *S Supporting Information The Supporting Information is available free of charge on the ACS Publications website at DOI: 10.1021/acs.jnatprod.6b01093.  1H and HSQC NMR spectra for compounds 2−5, tips for using the database as well the KNIME workflow and node settings used to count the substructures (PDF) DEREP-NP-v1 file (RAR)    AUTHOR INFORMATION\r\n  Notes The authors declare no competing financial interest. Updated versions of the DEREP-NP file in DataWarrior format (dwr) can be downloaded at https://github.com/clzani/DEREP-NP; Osiris DataWarrior can be downloaded for free at http://openmolecules.org/datawarrior/download.html ■    ACKNOWLEDGMENTS\r\n  We are grateful to G. A. Landrum for help with substructure counting using KNIME, to Oswaldo Cruz FoundationFIOCRUZ for financial support, and to CNPq for a Science without Borders Program Senior Training Fellowship for C.L.Z. in the laboratory of A.C. ■    REFERENCES\r\n  (1) Molinski, T. F. Org. Lett. 2014, 16, 3849−3855.    ",
    "sourceCodeLink": "https://github.com/clzani/DEREP-NP",
    "publicationDate": "0",
    "authors": [
      "Carlos L. Zani",
      "Anthony R. Carroll"
    ],
    "status": "Success",
    "toolName": "DEREP-NP",
    "homepage": ""
  },
  "83.pdf": {
    "forks": 0,
    "URLs": ["github.com/theislab/kbranches"],
    "contactInfo": ["fabian.theis@helmholtz-muenchen.de"],
    "subscribers": 5,
    "programmingLanguage": "R",
    "shortDescription": "Finding branching events and tips in single cell differentiation trajectories",
    "publicationTitle": "Model-based branching point detection in single-cell data by K-branches clustering",
    "title": "Model-based branching point detection in single-cell data by K-branches clustering",
    "publicationDOI": "10.1093/bioinformatics/btx325",
    "codeSize": 871,
    "publicationAbstract": "Motivation: The identification of heterogeneities in cell populations by utilizing single-cell technologies such as single-cell RNA-Seq, enables inference of cellular development and lineage trees. Several methods have been proposed for such inference from high-dimensional single-cell data. They typically assign each cell to a branch in a differentiation trajectory. However, they commonly assume specific geometries such as tree-like developmental hierarchies and lack statistically sound methods to decide on the number of branching events. Results: We present K-Branches, a solution to the above problem by locally fitting half-lines to single-cell data, introducing a clustering algorithm similar to K-Means. These halflines are proxies for branches in the differentiation trajectory of cells. We propose a modified version of the GAP statistic for model selection, in order to decide on the number of lines that best describe the data locally. In this manner, we identify the location and number of subgroups of cells that are associated with branching events and full differentiation, respectively. We evaluate the performance of our method on single-cell RNA-Seq data describing the differentiation of myeloid progenitors during hematopoiesis, single-cell qPCR data of mouse blastocyst development, single-cell qPCR data of human myeloid monocytic leukemia and artificial data. Availability and implementation: An R implementation of K-Branches is freely available at https://github.com/theislab/kbranches. Contact: fabian.theis@helmholtz-muenchen.de Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2016-12-15T15:01:43Z",
    "institutions": [
      "Technical University of Munich",
      "Helmholtz Zentrum Mu ̈ nchen"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-11-16T17:01:48Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx325   Model-based branching point detection in single-cell data by K-branches clustering     Nikolaos K. Chlis  1  2    F. Alexander Wolf  1    Fabian J. Theis  0  1  2    0  Department of Mathematics, Technical University of Munich ,  Garching 85748 ,  Germany    1  Institute of Computational Biology, Helmholtz Zentrum Mu ̈ nchen ,  Neuherberg 85764 ,  Germany    2  School of Life Sciences Weihenstephan, Technical University of Munich ,  Freising 85354 ,  Germany     2017   1  1  9   Motivation: The identification of heterogeneities in cell populations by utilizing single-cell technologies such as single-cell RNA-Seq, enables inference of cellular development and lineage trees. Several methods have been proposed for such inference from high-dimensional single-cell data. They typically assign each cell to a branch in a differentiation trajectory. However, they commonly assume specific geometries such as tree-like developmental hierarchies and lack statistically sound methods to decide on the number of branching events. Results: We present K-Branches, a solution to the above problem by locally fitting half-lines to single-cell data, introducing a clustering algorithm similar to K-Means. These halflines are proxies for branches in the differentiation trajectory of cells. We propose a modified version of the GAP statistic for model selection, in order to decide on the number of lines that best describe the data locally. In this manner, we identify the location and number of subgroups of cells that are associated with branching events and full differentiation, respectively. We evaluate the performance of our method on single-cell RNA-Seq data describing the differentiation of myeloid progenitors during hematopoiesis, single-cell qPCR data of mouse blastocyst development, single-cell qPCR data of human myeloid monocytic leukemia and artificial data. Availability and implementation: An R implementation of K-Branches is freely available at https://github.com/theislab/kbranches. Contact: fabian.theis@helmholtz-muenchen.de Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  Recent advances in single-cell technologies have led to the discovery and characterization of novel cell types in multicellular organisms. Studying diverse cell populations that differ in morphology and function can pinpoint distinct cell types in different stages of regulatory processes, such as cellular development. For example, singlecell methods have led to new discoveries related to hematopoietic stem cells  (Moignard et al., 2015; Paul et al., 2015) , as well as the immune system  (Jaitin et al., 2014; Mahata et al., 2014; Proserpio et al., 2016) .  The development of novel computational techniques for the analysis of single-cell data is an active research topic in the field of bioinformatics  (de Vargas Roditi and Claassen, 2015; Gr u¨n and van Oudenaarden, 2015; Stegle et al., 2015) . The key idea of the Waddington epigenetic landscape  (Waddington, 1942, 1957)  is that individual cells can be mapped from a high-dimensional space to a low-dimensional manifold of trajectories that reflect the continuous regulatory processes. As a result, a number of methods have been proposed that can reconstruct differentiation trajectories, given snapshot data of individual cells in different stages of the differentiation process, such as Monocle  (Trapnell et al., 2014) , Wishbone  (Setty et al., 2016) , Diffusion Pseudotime (DPT)  (Haghverdi et al., 2016) , SLICER  (Welch et al., 2016)  and TSCAN  (Ji and Ji, 2016) . Given a 'root' cell as a starting point, most of these methods can also calculate an ordering of the cells (pseudotime) based on the stage each cell is in the differentiation process. However, with the exception of DPT, while these methods are successful in assigning cells to discrete differentiation trajectories (branches) they do not tackle the problem of identifying the local dimensionality around each cell. That is, identifying branching regions of cells not yet strongly associated to any branch, intermediate regions along a branch and tip regions of fully differentiated cells. Moreover, all the above methods lack a sound statistical model to identify the existence and number of cell subgroups associated to branching events. Finally, while TSCAN employs model selection to decide on the number of cell-clusters, it does not aim to identify branching and tip regions.  In this study, we propose a data driven, model-based clustering method that identifies the exact number of 'branching regions', as well as the exact number of fully differentiated 'tip regions' in the lineage tree. The method then proceeds to assign each cell to a branching, intermediate or tip region. The proposed methodology does not aim to infer a pseudotemporal ordering of the cells and as such no 'root' cell needs to be defined. Moreover, since characterization of each cell is based on local information in the differentiation trajectory, the method can successfully identify cells belonging to the aforementioned regions of interest in trajectories of arbitrary geometry.    2 Materials and methods\r\n   2.1 Problem formulation\r\n  Given a center c and direction v, a halfline L is defined as the set of points satisfying L ¼ fc þ t v; t 0g; with l; c; v 2 RP. We aim to find K halflines L1; . . . ; LK with a common center c and K distinct direction vectors v1; . . . ; vK. In this case, each halfline Lk corresponds one cluster Ck. As a prerequisite to defining a cost function, note that the Euclidean distance of a given point x to a halfline Lk reads: dðx; LkÞ ¼ 8 &gt; &gt; &lt; &gt; &gt;: jjx  I vkvkT ! vkT vk ðx cjj; cÞ ; if ðx cÞT vk 0  : (1) if ðx cÞT vk &lt; 0 Additionally, one may also use other distance metrics  (Kiselev et al., 2017) .  The clustering method aims to assign each of the given data points (cells) into its closest halfline, while minimizing the total cost. In other words, the goal is to identify the center c, as well as the direction vectors v1; . . . ; vK of unit length that minimize the overall A clustering cost. To this end, we define the cost function J to describe the total dispersion, which corresponds to the sum of dispersions over the K clusters and reads:  J ¼ PkK¼1 Px2Ck dðx; LkÞ2 ¼ PK k¼1  Px2Ck jjx cjj2 þ Px2Ckþ  I vkvkT ! vkT vk ðx cÞ 2! ; (2) where Ck ¼ Ck [ Ckþ corresponds to all elements in cluster k and Ck ; Ckþ correspond to the sets of elements in cluster k with negative and positive dot product to all vectors in the direction of Lk, respectively.  The main idea of the proposed methodology is to perform local clustering in single-cell trajectories, by fitting K halflines (branches) that share a common center. Then, model selection is applied to identify the number of K branches best fitting the local neighborhood around each cell. Thus, the local structure of single-cell trajectories is identified and each cell is assigned to a tip, intermediate or branching region, as illustrated in Figure 1.   2.1.1 The K-Branches clustering method\r\n  In order to calculate the model parameters, after random initialization we follow an EM-like iterative optimization procedure similar to that of K-Means  (Hastie et al., 2009) . Namely, we iteratively (i) assign data points to their closest cluster and (ii) update the estimates of c and v1; . . . ; vK while minimizing J in each step, until convergence. Since the method might converge to a local optimum of the cost function, multiple executions using different initializations have to be carried out. The method is randomly initialized by assigning one random data point as the center c and K other random data points as the direction vectors xv1; . . . ; xvK. In the following subsections we present the update equations for the center and directions, respectively.    2.1.2 Estimating the center of the halflines\r\n  First, we optimize the cost function J with respect to the center of the halflines c. Therefore, we have to calculate the gradient rcJ, as follows: rcJ ¼ 2XK k¼1  X x2Ck ðc xÞ þ X x2Ckþ AkT ðc xÞ ; where the matrix Ak is defined as:  Ak ¼  I vkvkT !T vkT vk  I vkvkT !; vkT vk (3) (4) with vkT vk ¼ 1.  B  The equation rcJ ¼ 0 can be solved in closed form, and the optimal c reads: copt ¼  PkK¼1 Px2Ck xT þ Px2Ckþ xT Ak PK k¼1 jCk jI þ jCkþjAk 1 ; (5) where jCk6j refers to the size of the set Ck6. In the case K ¼ 1 the right part of Equation (5) simplifies to jCk jI þ jCkþjA1 1, which is not full rank and therefore not invertible when jCk j ¼ 0. Although the method for local clustering introduced in a subsequent section is also performed with K ¼ 1, it uses a fixed center c, rendering the above limitation irrelevant.    2.1.3 Estimating the directions of the halflines\r\n  To optimize the cost function J with respect to the direction vector of unit length vk, we have to calculate the gradient rvk J, as follows: rvk J ¼ rvk Px2Ckþ jj I vkvkT ðx  cÞjj2 ¼ rvk Px2Ckþ ðx cÞT ðx cÞ ðx cÞT vkvkT ðx cÞ : Assuming that Xb is as matrix whose ith row corresponds to ðcxoimpucÞtiTn;gi t¼he1f;ir.s.t. e;ijgCekþnjv,ecthtoernosfeXtbtiTnXgb .rvk J to zero is equivalent to  The pseudocode for the K-Branches algorithm is presented in Algorithm 1, while a comparison between K-Branches and K-Means is illustrated in Figure 2.    2.1.4 Medoid version of K-Branches\r\n  As in K-Means, the K-Branches method described above determines a 'centroid' Lkðc; vkÞ per cluster, which depends on arbitrary vectors c; vk 2 RP. We can easily modify this to use data points, as in K-Medoids  (Hastie et al., 2009; Theodoridis and Koutroumbas, 2008) . The goal of the Medoid version of K-Branches is to identify one data point as the center medoid xc and K data points as the direction medoids xv1; . . . ; xvK. That is, the model parameters now correspond to K þ 1 data points, instead of K þ 1 points in RP, where P the number of dimensions. Similar to K-Medoids, the proposed algorithm searches over all data points during each iteration in a greedy manner and identifies the data points that minimize the cost function J given by Equation (2). All medoids are reassigned during each iteration of the algorithm, until a local minimum for J is Algorithm 1 K-Branches clustering 1: Inputs: K: number of clusters, x1; . . . ; xN: data points 2: Random initialization of c; v1; . . . ; vK 3: for n in 1:N do \" N: number of all data points 4: assign xn to nearest Lk, according to Equation (1) 5: end for 6: repeat 7: update the center c \" according to Equation (5) 8: update the direction vectors v1; . . . ; vK 9: for n in 1:N do 10: assign xn to nearest Lk, according to Equation (1) 11: end for 12: until no change in cluster assignments (6)  Algorithm 2 K-Branches clustering, medoid version 1: Inputs: K: number of clusters, x1; . . . ; xN: data points 2: Define: M ¼ fic; iv1 ; . . . ; ivK g \" medoid indices f1; . . . ; Ng 3: Random initialization of fic; iv1 ; . . . ; ivK g \" to random indices 4: for n in 1:N do \" N: number of all data points 5: assign xn to nearest Lk, according to Equation (1) 6: end for 7: while total cost J decreases do \" Repeat until convergence 8: ic argmini62MðJðc ¼ xiÞÞ \" update the center 9: for k in 1:K do \" iterate over K directions 10: ivk argmini62MðJðvk ¼ xiÞÞ \" update the directions 11: end for 12: for n in 1:N do 13: assign xn to nearest Lk, according to Equation (1) 14: end for 15: end while reached and the total clustering cost cannot be further decreased. At this point, the algorithm converges to a solution where one of the data points is the center medoid xc of the halflines and K data points correspond to the direction medoids xv1; . . . ; xvK. The relationship between the original and the medoid version is similar to that of K-Means and K-Medoids. That is, the medoid version is more robust in selecting the center of the halflines with respect to non-global optima and usually even only one random initialization is sufficient in practice. In the original algorithm, calculating the parameters c; v1; . . . ; vK requires time proportional to the number of data points O(N). A speedup of the medoid version is possible by computing the distance matrix D only once, where Dij ¼ jjxi xjjj. Then, the distance of a data point xi to a halfline Lðxc; xv xcÞ can be computed in O(1) time from Equation (8). However, for every one of the N ðK þ 1Þ candidate medoids, the distance to every other data point is taken into consideration to calculate the overall clustering cost. As a result, O N2 time is required to update the medoids during every iteration.  dðxi; Lðxc; xv  2 xcÞÞ ¼ Dic  2 2 Dic þ Dcv 2Dcv  2 Div : (8) To summarize, in cases where robustness in the identification of the center of the halflines is crucial, the medoid version might be preferable. In applications where robust identification of the center of the halflines is not as crucial, especially in larger datasets, the original version of the algorithm could be preferable. Last, in cases where the center of the halflines is known (or held fixed), such as the case of local clustering presented later in the methods section, there is no advantage to using the medoid over the original version, since both are equally robust in identifying the directions of the halflines.     2.2 Identifying branching and tip regions through local clustering\r\n   2.2.1 Local clustering\r\n  In this section we derive a method for the identification of\u201d regions of interest\u201d in single-cell data, in particular, the identification of branching regions and tips of branches in lineage trees of differentiating single cells. The main idea is to center the previous model on each data point and adopt a local perspective by examining only the neighborhood of S nearest neighbors to the center. We will show that by fixing the center of the halflines on a given data point and fitting the previous model of K halflines using a neighborhood size of S data points, one can infer whether the center data point itself belongs to branching, intermediate or tip region, through appropriate model selection.    2.2.2 Selection of the neighborhood size S\r\n  The proposed method utilizes a number of S nearest neighbors to extract the neighborhood of the center data point that is being examined. The size of the neighborhood must be sufficiently large to reflect the local structure of the data, without capturing irrelevant global information. The proposed method is able to automatically suggest a value for S using a threshold on d ¼ N1 PiN¼1 Pj6¼ijjxi xjjj2, which ensures that the average cumulative squared distance d of each data point to all other data points in the dataset is kept at a constant value. Moreover, the accompanying software package provides the option of visualization and manual fine tuning of S through a graphical user interface. Supplementary Figure S1 demonstrates the effect of neighborhood size in the overall performance of the method on a toy model of differentiation  (Haghverdi et al., 2015) .    2.2.3 Neighborhood scaling\r\n  Another challenging aspect is related to datasets showing strong variation in the density of data points along the differentiation trajectories.  For example, in the dataset of Guo et al., 2010), there are sparse and dense regions. Variability of data point density might reflect an artifact of the data acquisition process, or could be a result of the underlying biological system. In the datasets examined so far, regions of very low density do not pose a threat to the performance of the method, since efficient selection of S will expand the neighborhood size accordingly. On the other hand, the fixed number of S neighbors may drastically shrink the size of the neighborhood in regions of very high density. To compensate for this effect, an appropriate heuristic rule was implemented. To be precise, for a given number of S neighbors, we calculate the median neighborhood radius q over all neighborhoods of size S. The neighborhood scaling scheme is as follows: prior to performing local clustering for the ith data point, its neighborhood radius qi (which corresponds to its distance to the furthest point in the neighborhood) is calculated and the condition qi q is assessed. If it is true, clustering is performed as usual. Otherwise, the neighborhood size (S) of the ith data point is increased until qi q holds.    2.2.4 Local model selection\r\n  The goal is to infer whether each data point belongs to a tip, intermediate or branching region of a differentiation trajectory, based on local clustering. That is, using a given data point as the fixed center c of the halflines, three different models are fit using K ¼ 1, 2 and 3 halflines. The aim of the model selection step in the problem at hand is to identify the clustering model, i.e. the value of K, that best fits the data of the local neighborhood centred around the data point in question. If one halfline best fits the neighborhood, then the central data point belongs to a branch tip. If two halflines provide the best fit, then the central data point belongs to an intermediate region. If three halflines best fit the local neighborhood, then the central data point belongs to a branching region. Although values of K &gt; 3 could in theory be considered for local clustering and model selection, we have observed that K ¼ 3 is sufficient in practice for the identification of branching regions. Therefore, the computational overhead of assessing additional values of K can be safely avoided.  The GAP statistic  (Tibshirani et al., 2001)  is a popular method for identifying the number of clusters that best fit some given data. It depends on the sum of pairwise distances of points in each cluster. If the Euclidean distance is used as the distance measure, it corresponds to the dispersion around the cluster means (clustering cost). The GAP statistic compares the decrease in the clustering cost of the original data with the decrease in clustering cost of data drawn from a null distribution where no natural cluster structure exists. In theory, the dispersion in the data sampled from the null distribution decreases monotonically as K increases, while the dispersion in the original data drops rapidly for the value of K that best fits the dataset. Thus, the GAP statistic is maximized when the best value of K is used for clustering. Assuming that the Euclidean distance is used as the distance measure, the total within-cluster-dispersion WK  (Tibshirani et al., 2001)  is:  WK ¼ where Ef log WK g ¼ B1 PbB¼1 log WK;b and the dispersions WK;b are calculated by applying Equation (9) after performing clustering on each of the b ¼ 1; . . . ; B bootstrap datasets (of the same size as the original dataset) drawn from the null reference distribution. (9) (10)  In the case of local K-Branches clustering, we introduce a modification of the GAP statistic that calculates the dispersion around halflines, as follows:  WfK ¼ where dðx; LkÞ is given by Equation (1). Moreover, in contrast to the original GAP we do not take the logarithm of the dispersion, since it has been reported to overestimate the number of clusters in some cases  (Mohajer et al., 2010) . Finally, the modified GAP statistic is given by:  1 XB GgAPK ¼ B b¼1 WfK;b  Wfk: (12) The dispersions WfK;b are calculated by applying Equation (11) after performing clustering on each of the b ¼ 1; . . . ; B bootstrap datasets (of the same size as the original dataset) drawn from the null reference distribution.  To summarize, given a data point as the center of the halflines, local clustering is performed. Then, if GAPK¼1 &gt; GAPK¼3, it belongs to a branch tip. Otherwise, if the data point does not belong to a tip and GgAPK¼2 GgAPK¼3 holds, it belongs to an intermediate region. Finally, if the data point does not belong to a tip and GgAPK¼2 &lt; GgAPK¼3, it belongs to a branching region. Both the original and modified versions of the GAP statistic are necessary for model selection and are complementary to each other. That is, GAP can identify tip cells (Fig. 3C) but is not suitable for separating intermediate from branching cells (Fig. 3D). On the other hand, GgAP can separate intermediate and branching cells (Fig. 3E), but it not suitable for identifying tip cells, since it would falsely identify a large number of branching cells as tip cells (Fig. 3F). The performance comparison of the different GAP statistics is illustrated in Figure 3. Moreover, the behavior of the GAP statistic when additional noise is added is illustrated in the Supplementary Figure S2. After all data points have been assigned to tip, intermediate and branching regions, an optional filtering of each cell's label (tip, branching, or intermediate) based of the values of a few (e.g. 5) nearest neighbors can be performed to aid in smoothing out any random false positives caused by the inherent stochasticity of the GAP statistic. As a final step, K-Means clustering is performed on the subset of the data belonging to tips, using the original GAP statistic for model selection. In this manner, the exact number of tips is identified and each data point that has been characterized as belonging to a tip region is uniquely assigned to a specific tip. The same process is applied to cells belonging in branching regions in order to identify the exact number of branching events and assign branching region cells to their corresponding branching event.    2.2.5 Dimension reduction precedes model selection\r\n  In this section we focus on the selection of the null reference distribution. Uniform sampling of features over a box aligned with the principal components of the data is suggested in  (Tibshirani et al., 2001) . Alternatively, uniform sampling over the range of every feature in the original dimensions of the data is suggested for simplicity. Although the K-Branches clustering method performs well in the A D B E C F original space, model selection does not. This follows from the 'curse of dimensionality'  (Hastie et al., 2009) , since it becomes exponentially hard to estimate the null distribution in high dimensions. As a result, dimensionality reduction is a necessity if model selection is to be performed. Diffusion maps  (Coifman et al., 2005)  are a nonlinear dimensionality reduction method which are known to successfully identify differentiation trajectories  (Haghverdi et al., 2015) , outperforming traditional dimensionality reduction methods such as principal component analysis (PCA)  (Hastie et al., 2009)  and Locally Linear Embedding (LLE)  (Roweis and Saul, 2000) . As a result, the dataset is first processed by diffusion maps and the first few diffusion components (DCs) are selected. Then, local clustering is performed for each data point in the space of the selected DCs. Finally, the reference distribution is calculated by uniform sampling over a box aligned with the same DCs, resulting in the computation of the GAP and GgAP statistics used for model selection.      3 Results\r\n   3.1 Datasets\r\n  The performance of local K-Branches is evaluated using three publicly available datasets, as well as one synthetic dataset. The first dataset corresponds to single-cell RNA-seq data describing the differentiation of myeloid progenitors during hematopoiesis  (Paul et al., 2015) ; Accession Number GSE72857) and consists of measurements of 2730 cells and 8716 genes. The second dataset consists of single-cell qPCR data related to mouse blastocyst development  (Guo et al., 2010) ; Accession Number J:140465) and includes measurements of 428 cells and 48 genes. The third dataset corresponds to a single-cell qPCR dataset of multiple time points where THP-1 human myeloid monocytic leukemia cells undergo differentiation into macrophages  (Kouno et al., 2013) ; Data available in the supplement of the original publication) and include measurements of 960 cells and 45 genes. The last dataset corresponds to an artificial dataset used as proof of concept and includes measurements of 2 synthetic genes and 244 cells that differentiate into three branches but the differentiation process includes a loop. Such a dataset could for example correspond to cellular reprogramming, or cells exiting the cell cycle, as also suggested by  (Welch et al., 2016) .    3.2 Comparison to other methods\r\n  The purpose of local K-Branches is to identify branching and tip regions, while current popular methods assign cells to distinct branches. Local K-Branches is compared with DPT  (Haghverdi et al., 2016)  which in addition to assigning cells to distinct branches, also identifies tip cells and undecided cells in branching regions. One difference between DPT and the proposed method is that DPT only identifies one cell of each branch as the tip, while the proposed method typically identifies a region of tip cells. Although TSCAN does not directly identify branching and tip regions, it does construct a minimum spanning tree that connects the cluster centers. As a result, one could consider as tip, intermediate and branching clusters those clusters that are connected to one, two and more than two clusters in the minimum spanning tree. Monocle is similar to TSCAN. However, it connects single cells instead of cell-clusters on the minimum spanning tree. Consequently, extending monocle to identify tip and branching regions in a similar manner is not straightforward or statistically motivated. As such, Monocle  (Trapnell et al., 2014)  and SLICER  (Welch et al., 2016)  are only indirectly compared with the proposed method, in terms of estimating correct branching in the data. The results of applying the above methods on all datasets are presented in Figure 4. The proposed method was either performed on the first two or three DCs, depending on the morphology of the dataset. On the other hand, DPT always takes all available DCs into account. All other methods perform dimensionality reduction as part of their pre-processing and they are only visualized using diffusion maps. Additionally, the performance of local K-Branches when LLE  (Roweis and Saul, 2000)  is used for dimensionality reduction is presented in the Supplementary Figure S3. Finally, in the datasets where ground truth for the identification of tip cells is available, quantitative comparison of local K-Branches, DPT and TSCAN was performed, assessing their capability to identify tips cells in terms of precision and recall. Precision calculates the fraction of cells identified as tip-cells that actually correspond to true tip-cells. Recall calculates the fraction of true tipcells selected by the method, over the total number of true tip-cells present in the dataset. Both scores range from zero to one, with one corresponding to a perfect score. Another quantitative comparison is performed on the basis of correct identification of the number of branching events present in the dataset. Quantitative results are summarized in Table 1.   3.2.1 Single-cell RNA-seq data of myeloid progenitors\r\n  When applied to the first two DCs of the single-cell RNA-Seq dataset of  (Paul et al., 2015) , the proposed method identifies three branch tips of fully differentiated cells, as well as one branching region. The regions identified by K-Branches are illustrated with respect to Fluorescence Activated Cell Sorting (FACS) labels in Figure 5. In order to perform a quantitative comparison, true tip-cells were considered cells belonging in the granulocyte/macrophage progenitor (GMP) and megakaryocyte/erythrocyte progenitor (MEP) gates of Figure 5. However, selecting tip cells in this manner is only approximately accurate. The results of DPT on the same data agree with the findings of local K-Branches. Two of the three tips identified by DPT are in the tip regions of local K-Branches, while the third tip of DPT is not inside but in the vicinity of the local KBranches tip region. When comparing the branching region, the undecided cells of DPT are either inside or in close proximity to the branching region identified by local K-Branches. However, considerably fewer cells are considered as undecided by DPT. Additionally, TSCAN finds no branching and identifies two tip regions. Finally, Monocle overestimates, while SLICER underestimates the overall branching. According to the results in Table 1, local K-Branches is the most precise method, while TSCAN achieves better recall but fails to identify the branching event.    3.2.2 Single-cell qPCR data of mouse blastocyst development\r\n  The proposed method was applied to the first three DCs of the single-cell qPCR data which contains two distinct branching events  (Guo et al., 2010) . Once more there is close agreement between the results of local K-Branches and DPT. Both methods identify four branch tips and the tip cells of DPT are in the tip regions of the proposed method. One key difference is that the proposed method automatically identified four branch tips and two branching regions, while DPT had to be manually executed twice on the data: First, three branches were identified, then DPT was performed on one of the branches, identifying the second branching region and new branch tips. On the other hand, TSCAN identifies two tips, one of which corresponds to a tip in the diffusion map trajectory, while it identifies no branching regions in the data. In order to perform quantitative comparisons, cells belonging to the 2- and 64-cell blastocysts were considered tip-cells. DPT is the most precise  aQuantifying tip recall is problematic since ground truth is based on thresholding of FACS markers and hence recalls too many cells.  bThe number of branching events was identified correctly. method achieving precision of 1, with local K-Branches being a close second with 0.96 precision. On the other hand, TSCAN achieves only 0.6 precision and fails to identify the branching events. However, it performs better in terms of recall, even though it does not identify any cells of the 2-cell stage tip (T1 of local K-Branches and DPT). Finally, Monocle identifes 5, while SLICER finds 3 clusters in the data.    3.2.3 Single-cell qPCR data of human monocytic leukemia cells\r\n  The third dataset contains measurements of 960 THP-1 human myeloid monocytic leukemia cells which undergo differentiation into macrophages and includes measurements along eight distinct timepoints  (Kouno et al., 2013) . In order to perform a quantitative comparison, we considered the cells belonging to the first and last timepoints as the two tip populations. In terms of pre-processing, one of the genes (KLF10) was removed, since it was only strongly expressed during the second timepoint and hindered the average performance of all methods as shown in the Supplementary Figure S4. Local K-Branches was performed on the first two DCs and identified two tips and no branching event. On the other hand, DPT and TSCAN identified three tips and a branching event. As such, local  K-Branches is the only method that successfully does not identify branching in the data. On the other hand, all three tip-cells of DPT lie in tip regions and it achieves the highest precision of 1, followed by local K-Branches with 0.77 and TSCAN with 0.47. Finally, TSCAN achieves the greatest recall score of 0.43, followed closely by local K-Branches with 0.4 while DPT only achieves recall of 0.012. Monocle finds 23, while TSCAN identifies 5 clusters in the data.    3.2.4 Artificial data of arbitrary geometry\r\n  The final dataset highlights an important advantage of the proposed methodology. Namely, the identification branch tips and branching regions in datasets of arbitrary geometry. In this case, the dataset was manually generated to consist of three branches with a loop among them and the first two DCs retain the same geometry as the original dataset. Even though it could be directly applied to the original two-dimensional data, the proposed method was performed on the first two DCs. This was done for two reasons: First, for real data of high dimensions clustering and model selection will be performed on the DCs and we assume that dimensionality reduction through diffusion maps will also retain the loop structure of real data. Second, by using the DCs there are direct comparison to the performance of DPT. Despite the challenging geometry of the dataset, the proposed method correctly identifies the three regions corresponding to the branch tips, as well as the three branching regions. On the other hand, DPT correctly identifies the three tip cells but fails in identifying the branching regions. To be precise, it identifies one branching region correctly, but then it fails to find the other two and considers one irrelevant part of the loop as a branching region. Monocle underestimates the number of branching events, probably since it always assumes that the differentiation trajectory corresponds to a tree-like structure. Finally, SLICER overestimates the overall branching in the data, while TSCAN identifies two tips mostly lying in an intermediate region. An illustration of the performance of K-Branches on the same dataset for different levels of added noise is presented in the Supplementary Figure S5.      4 Conclusion and discussion\r\n  In this study, a model based clustering approach was introduced for the identification of regions of interest in single-cell data. First, a novel clustering method called K-Branches was introduced, which clusters data points into a set of K halflines with a common center. Subsequently, this clustering method was applied locally to the neighborhood of each cell and a modified version of the GAP statistic was developed to perform model selection. The goal of model selection is to identify the local dimensionality of the data. That is, identify fully differentiated tip cells and cells belonging to branching regions. In this manner, all branching events, as well as all endpoints (tips) in differentiation trajectories can be identified. As demonstrated, this local view of the data allows the method to be successfully applied to challenging datasets that include sparsity and complex geometries.  The main idea of the proposed methodology is different from that of commonly used methods such as DPT, Monocle, Wishbone, SLICER or TSCAN. To be precise, these methods aim to assign each cell to a distinct branch in the differentiation process and also calculate pseudotime: an ordering of the cells, relevant to their distance from a starting root cell, which reflects how far they have progressed in the differentiation process. As such, K-Branches cannot be directly compared with most of these methods, perhaps with the exceptions of DPT and TSCAN. To be precise, DPT also identifies tip cells and branching regions of undecided cells, while TSCAN can be extended to search for tip, intermediate and branching clusters. The performance of the proposed method was compared with that of DPT and TSCAN in three single-cell datasets, as well as an artificial dataset. Local K-Branches achieved high precision and correctly identified the number (or absence) of branching events in all three single-cell datasets, while performing better than DPT in terms of recall. DPT was very precise and found the correct number of branching events in two of the three datasets, but since it only selects one cell per tip, it is poor in terms of recall. TSCAN was the least precise of all methods and did not identify the correct number of branching events in any dataset. However, it performed better than all other methods in terms of recall, in part since it selects a large number of cells. Moreover, in the dataset which consists of three branches with a loop in between, the local approach of the proposed methodology successfully identifies all tip and branching regions, while DPT only identifies the branch tips and TSCAN finds two tips in an intermediate region. Although this difference was observed on a synthetic dataset, real datasets containing loops could in theory correspond to cells exiting cell cycle, cells resulting in the same state through different differentiation trajectories, or cellular reprogramming  (Bendall et al., 2014) . One advantage of DPT is faster execution time since the entire dataset is typically processed in a few minutes. On the other hand, local K-Branches requires a few seconds per data point. However, in the case of local K-Branches each data point can be processed completely in parallel. TSCAN is also faster than local K-Branches but was less precise in the identification of tip-cells. To be fair, it was designed to solve a different problem and uses PCA for dimensionality reduction. PCA can be sufficient when the goal is to identify distinct cell-clusters, but has limited capabilities when it comes to learning continuous manifolds of differentiation trajectories which appear to be a necessity for the accurate identification of branching and tip regions. Finally, TSCAN utilizes a model-based approach to decide on the global number of clusters. In contrast, local K-Branches utilizes model selection to identify the dimensionality of the data in a local context.  In terms of future work, it would be interesting to extend the method to support explicit identification of the branches that lie between the branching and tip regions, which are currently only characterized as intermediate regions. Although clustering works in the original dimensions, model selection using the GAP statistic does not. As such, the proposed method utilizes diffusion maps for dimensionality reduction. Although LLE achieved similar results, it required tedious fine-tuning to produce satisfactory trajectories. Moreover, developing a different model selection method, other than the GAP statistic, that would allow the methodology to be directly applied in the original dimensions could be an additional topic of future work.    Acknowledgements\r\n  We would like to acknowledge L. Haghverdi for her helpful advice. We would like to thank M. Bu¨ ttner for her comments and support on drawing biological conclusions. Finally, we would like to thank P. Angerer and D. S. Fischer for their comments on the R package and article, respectively.    Funding\r\n  N.K.C. is supported by a DFG Fellowship through the Graduate School of Quantitative Biosciences Munich (QBM). F.A.W. acknowledges support by the 'Helmholtz Postdoc Programme', Initiative and Networking Fund of the Helmholtz Association. F.J.T. acknowledges financial support by the German Science Foundation (SFB 1243 and Graduate School QBM) as well as by the Bavarian government (BioSysNet).    ",
    "sourceCodeLink": "https://github.com/theislab/kbranches",
    "publicationDate": "0",
    "authors": [
      "Nikolaos K. Chlis",
      "F. Alexander Wolf",
      "Fabian J. Theis"
    ],
    "status": "Success",
    "toolName": "kbranches",
    "homepage": ""
  },
  "91.pdf": {
    "forks": 1,
    "URLs": ["github.com/BilkentCompGen/GateKeeper"],
    "contactInfo": [
      "mohammedalser@bilkent.edu.tr",
      "onur.mutlu@inf.ethz.ch",
      "calkan@cs.bilkent.edu.tr"
    ],
    "subscribers": 4,
    "programmingLanguage": "VHDL",
    "shortDescription": "GateKeeper: Fast Alignment Filter for DNA Short Read Mapping",
    "publicationTitle": "GateKeeper: a new hardware architecture for accelerating pre-alignment in DNA short read mapping",
    "title": "GateKeeper: a new hardware architecture for accelerating pre-alignment in DNA short read mapping",
    "publicationDOI": "10.1093/bioinformatics/btx342",
    "codeSize": 34114,
    "publicationAbstract": "Motivation: High throughput DNA sequencing (HTS) technologies generate an excessive number of small DNA segments -called short reads- that cause significant computational burden. To analyze the entire genome, each of the billions of short reads must be mapped to a reference genome based on the similarity between a read and 'candidate' locations in that reference genome. The similarity measurement, called alignment, formulated as an approximate string matching problem, is the computational bottleneck because: (i) it is implemented using quadratic-time dynamic programming algorithms and (ii) the majority of candidate locations in the reference genome do not align with a given read due to high dissimilarity. Calculating the alignment of such incorrect candidate locations consumes an overwhelming majority of a modern read mapper's execution time. Therefore, it is crucial to develop a fast and effective filter that can detect incorrect candidate locations and eliminate them before invoking computationally costly alignment algorithms. Results: We propose GateKeeper, a new hardware accelerator that functions as a pre-alignment step that quickly filters out most incorrect candidate locations. GateKeeper is the first design to accelerate pre-alignment using Field-Programmable Gate Arrays (FPGAs), which can perform prealignment much faster than software. When implemented on a single FPGA chip, GateKeeper maintains high accuracy (on average >96%) while providing, on average, 90-fold and 130-fold speedup over the state-of-the-art software pre-alignment techniques, Adjacency Filter and Shifted Hamming Distance (SHD), respectively. The addition of GateKeeper as a pre-alignment step can reduce the verification time of the mrFAST mapper by a factor of 10. Availability and implementation: https://github.com/BilkentCompGen/GateKeeper Contact: mohammedalser@bilkent.edu.tr or onur.mutlu@inf.ethz.ch or calkan@cs.bilkent.edu.tr Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-06-03T00:42:24Z",
    "institutions": [
      "Bilkent University",
      "Carnegie Mellon University",
      "ETH Zu ̈ rich",
      "TOBB University of Economics & Technology"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-03-29T22:29:38Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx342   GateKeeper: a new hardware architecture for accelerating pre-alignment in DNA short read mapping     Mohammed Alser  0    Hasan Hassan  2  3    Hongyi Xin  1    Oguz Ergin  3    Onur Mutlu  2    Can Alkan  0    0  Department of Computer Engineering, Bilkent University ,  Bilkent, Ankara 06800 ,  Turkey    1  Department of Computer Science, Carnegie Mellon University ,  Pittsburgh, PA 15213 ,  USA    2  Department of Computer Science, ETH Zu ̈ rich ,  8092 Zu ̈ rich ,  Switzerland    3  TOBB University of Economics &amp; Technology ,  Sogutozu, Ankara ,  Turkey     2017   1  1  9   Motivation: High throughput DNA sequencing (HTS) technologies generate an excessive number of small DNA segments -called short reads- that cause significant computational burden. To analyze the entire genome, each of the billions of short reads must be mapped to a reference genome based on the similarity between a read and 'candidate' locations in that reference genome. The similarity measurement, called alignment, formulated as an approximate string matching problem, is the computational bottleneck because: (i) it is implemented using quadratic-time dynamic programming algorithms and (ii) the majority of candidate locations in the reference genome do not align with a given read due to high dissimilarity. Calculating the alignment of such incorrect candidate locations consumes an overwhelming majority of a modern read mapper's execution time. Therefore, it is crucial to develop a fast and effective filter that can detect incorrect candidate locations and eliminate them before invoking computationally costly alignment algorithms. Results: We propose GateKeeper, a new hardware accelerator that functions as a pre-alignment step that quickly filters out most incorrect candidate locations. GateKeeper is the first design to accelerate pre-alignment using Field-Programmable Gate Arrays (FPGAs), which can perform prealignment much faster than software. When implemented on a single FPGA chip, GateKeeper maintains high accuracy (on average &gt;96%) while providing, on average, 90-fold and 130-fold speedup over the state-of-the-art software pre-alignment techniques, Adjacency Filter and Shifted Hamming Distance (SHD), respectively. The addition of GateKeeper as a pre-alignment step can reduce the verification time of the mrFAST mapper by a factor of 10. Availability and implementation: https://github.com/BilkentCompGen/GateKeeper Contact: mohammedalser@bilkent.edu.tr or onur.mutlu@inf.ethz.ch or calkan@cs.bilkent.edu.tr Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Bonnie Berger    1 Introduction\r\n  High throughput sequencing (HTS) technologies are capable of generating a tremendous amount of sequencing data. For example, the Illumina HiSeq4000 platform can generate more than 1.5 trillion base pairs (bp) in less than four days. This flood of sequenced data continues to overwhelm the processing capacity of existing algorithms and hardware (Canzar and Salzberg, 2015). The success of the medical and genetic applications of HTS technologies relies on the existence of sufficient computational resources, which can quickly analyze the overwhelming amounts of data that the sequencers generate. An HTS instrument produces short reads (typically 75-150 bp) sampled randomly from DNA. In the presence of a reference genome, the short reads are first mapped to the long reference sequence. During this process, called read mapping, each short read is mapped onto one or more possible locations in the reference genome based on the similarity between the short read and the reference sequence segment at that location. Optimal alignment of the read and the reference segment could be calculated using the SmithWaterman local alignment algorithm (Smith and Waterman, 1981). However, this approach is infeasible as it requires O(mn) running time, where m is the read length (100-150 bp for Illumina) and n is the reference length ( 3.2 billion bp for human genome), for each read in the dataset (hundreds of millions to billions). Therefore, read mapping algorithms apply heuristics to first find candidate map locations (seed locations) of subsequences of the reads using hash tables  (Alkan et al., 2009; David et al., 2011; Hach et al., 2010; Homer et al., 2009; Xin et al., 2013)  or BWT-FM indices  (Langmead and Salzberg, 2012; Langmead et al., 2009; Li and Durbin, 2009; Li et al., 2004) , and then align the read in full only to those seed locations. Although the strategies for finding seed locations vary among different read mapping algorithms, seed location identification is typically followed by a verification step, which compares the read to the reference segment at the seed location to check if the read aligns to that location in the genome with fewer differences than a threshold. The verification step is the dominant part of the whole execution time in current mappers (over 90% of the running time)  (Cheng et al., 2015; Xin et al., 2013) . It calculates edit distance using quadratic-time algorithms such as Levenshtein's edit distance (Levenshtein, 1966), Smith-Waterman (Smith and Waterman, 1981) and Needleman-Wunsch (Needleman and Wunsch, 1970). Edit distance is defined as the minimum number of edits (i.e. insertions, deletions, or substitutions) needed to make the read exactly match the reference segment (Levenshtein, 1966). If the edit distance score is greater than a user-defined edit distance threshold (usually less than 5% of the read length  (Ahmadi et al., 2012; Hatem et al., 2013; Xin et al., 2015) ), then the mapping is considered to be invalid (i.e. the read does not match the segment at seed location) and thus is rejected.  DEFINITION 1. Given a candidate read r, a reference segment f, and an edit distance threshold E, the pairwise alignment problem is to identify a set of matches of r in f, where the read aligns with an edit distance E.  Recent work found that an overwhelming majority (&gt;98%) of the seed locations exhibit more edits than the threshold  (Xin et al., 2013, 2015) . These particular seed locations impose a large computational burden as they waste 90% of the mapper's execution time in verifying these incorrect mappings  (Cheng et al., 2015; Xin et al., 2013) . To tackle these challenges and bridge the widening gap between the execution time of the mappers and the increasing amount of sequencing data, most existing works fall into two approaches: (i) Design hardware accelerators to accelerate the verification step  (Arram et al., 2013; Houtgast et al., 2015; Liu et al., 2012; Luo et al., 2013; Olson et al., 2012; Waidyasooriya et al., 2014) . (ii) Build software-based alignment filters before the verification step  (Cheng et al., 2015; Marco-Sola et al., 2012; Rasmussen et al., 2006; Ukkonen, 1992; Weese et al., 2009, 2012; Xin et al., 2013, 2015) . Such filters aim to minimize the number of candidate locations on which alignment is performed. They calculate a best guess estimate for the alignment score between a read and a seed location on the reference. If the lower bound exceeds a certain number of edits, indicating that the read and the segment at the seed location do not align, the seed location is eliminated such that no alignment is performed. Unfortunately, existing filtering techniques are either slow, such as Shifted Hamming distance (SHD)  (Xin et al., 2015) , or inaccurate in filtering, such as the Adjacency Filter  (Xin et al., 2013)  (implemented as part of FastHASH  (Xin et al., 2013) ) and mrsFAST-Ultra  (Hach et al., 2014) ). While mrsFAST-Ultra is able to detect only substitutions, FastHASH is unable to tolerate substitutions efficiently. We provide full descriptions of the key principles underlying each strategy in Supplementary Material, Section S1.2.  Our goal, in this work, is to minimize the mapper time spent on accurate alignment filtering. To this end, we introduce a new FPGAbased fast alignment filtering technique (called GateKeeper) that acts as a pre-alignment step in read mapping. To our knowledge, this is the first work that provides a new pre-alignment algorithm and architecture using reconfigurable hardware platforms. A fast filter designed on a specialized hardware platform can drastically expedite alignment by reducing the number of locations that must be verified via dynamic programming. This eliminates many unnecessary expensive computations, thereby greatly improving overall run time.  Our filtering technique improves and accelerates the state-of-theart SHD filtering algorithm  (Xin et al., 2015)  using new mechanisms and FPGAs. We build upon the SHD algorithm as it is the fastest and the most accurate filter  (Xin et al., 2015) . Our new filtering algorithm has two properties that make it suitable for an FPGA-based implementation: (i) it is highly parallel, (ii) it heavily relies on bitwise operations such as shift, XOR and AND. Due to the highly parallel and bitwise-processing-friendly architecture of modern FPGAs, our design achieves more than two orders of magnitude speedup compared to the best prior software-based filtering approaches (SHD and Adjacency Filter), as our comprehensive evaluation shows (Section 3). Our architecture discards the incorrect mappings from the candidate mapping pool in a streaming fashion - data is processed as it is transferred from the host system. Filtering the mappings in a streaming fashion gives the ability to integrate our filter with any mapper that performs alignment, such as Bowtie2 (Langmead and Salzberg, 2012) and BWA-MEM (Li, 2013). \u2022 \u2022 \u2022   Contributions. We make the following contributions:\r\n  We introduce the first hardware acceleration system for alignment filtering, called GateKeeper, which greatly reduces the need for alignment verification in DNA read mapping. To this end, we develop both a hardware-acceleration-friendly filtering algorithm and a highly parallel hardware accelerator design. We show that developing a hardware-based alignment filtering algorithm and architecture together is both feasible and effective by building our accelerator on a modern FPGA system.  We comprehensively evaluate GateKeeper and compare it to two state-of-the-art software-based alignment filtering algorithms. A key result is that our design for reads of length 100 bp on a single FPGA chip provides, on average, 90-fold and 130-fold speedup over the state-of-the-art filters, Adjacency Filter  (Xin et al., 2013)  and SHD  (Xin et al., 2015) , respectively. Experimental results on both simulated and real datasets demonstrate that GateKeeper has a low false positive rate (the rate of incorrect mappings that are accepted by the filter) of 4% on average.  We provide the design and implementation of a complete FPGA system and release its source code. To our knowledge, GateKeeper is the first open-source, freely available FPGA based alignment filter for genome analysis.     2 Gatekeeper architecture\r\n   2.1 Overview of our accelerator architecture\r\n  Based on the discussion provided in the Supplementary Material, Section 1.2, we introduce the first specialized FPGA-friendly hardware architecture for a new alignment filtering algorithm. The overall architecture, implementation details and flowchart representation of GateKeeper are discussed in the Supplementary Material, Section 1.3.1. Our current filter implementation relies on several optimization methods to create a robust and efficient filtering approach. At both the design and implementation stages, we satisfy several requirements: (i) Ensuring a lossless filtering algorithm by preserving all correct mappings. (ii) Supporting both Hamming distance and edit distance. The Hamming distance is a special case of the edit-distance. It is defined as the minimum number of substitutions required to change the read into the reference segment. The Hamming distance is computed in linear time. (iii) Examining the alignment between a read and a reference segment in a fast and efficient way (in terms of execution time and required resources).    2.2 Parallelization\r\n  GateKeeper is designed to utilize the large amounts of parallelism offered by FPGA architectures (Aluru and Jammula, 2014; Herbordt et al., 2007; Trimberger, 2015). The use of FPGAs can yield significant performance improvements, especially for massively parallel algorithms. FPGAs are the most commonly used form of reconfigurable hardware engines today, and their computational capabilities are greatly increasing every generation due to increased number of transistors on the FPGA chip. An FPGA chip can be programmed (i.e. configured) to include a very large number of hardware execution units that are custom-tailored to the problem at hand. We take advantage of the fact that alignment filtering of one read is inherently independent of filtering of another read. We therefore can examine many reads in a parallel fashion. In particular, instead of handling each read in a sequential manner, as CPU-based filters (e.g. SHD) do, we can process a large number of reads at the same time by integrating as many hardware filtering processing cores as possible (constrained by chip area) in the FPGA chip. Each processing core is a complete alignment filter and can handle a single read at a time. We use the term 'processing core' in this paper to refer to the entire operation of the filtering process involved in GateKeeper. Processing cores are part of our architecture and are unrelated to the term 'CPU cores' or 'threads'.    2.3 GateKeeper processing core\r\n  Our primary purpose is to enhance the state-of-the-art SHD alignment filter such that we can greatly accelerate pre-alignment by taking advantage of the capabilities and parallelism of FPGAs. To achieve our goal, we design an algorithm inspired by SHD to reduce both the utilized resources and the execution time. These optimizations enable us to integrate more processing cores within the FPGA chip and hence examine many alignments at the same time. We present three new methods that we use in each GateKeeper processing core to improve execution time. Our first method introduces a new algorithmic method for performing alignment very rapidly compared to the original SHD. This method provides: (1) fast detection for exact matching alignment and (2) handling of one or more basesubstitutions. Our second method supports calculating the edit distance with a new, very efficient hardware design. Our third method addresses the problem of hardware resource overheads introduced due to the use of FPGA as an acceleration platform. We provide the workflow of GateKeeper including the three optimization methods in the Supplementary Material, Figure S8. All features are implemented within the filtering processing core hardware and thus are performed highly efficiently. Next, we describe the three new methods. 2.3.1 Method 1: Fast approximate string matching We first discuss how to examine the alignment of reads against the reference sequence with a given Hamming distance threshold, and later extend our solution to support edit distance. Our first method aims to quickly detect the obviously-correct alignments that contain no edits or only few substitutions (i.e. less than the user-defined threshold). If the first method detects a correct alignment, then we can skip the other two methods but we still need the optimal alignment algorithms. A read is mappable if the Hamming distance between the read and its seed location does not exceed the given Hamming distance threshold. Hence, the first step is to identify all bp matches by calculating what we call a Hamming mask. The Hamming mask is a bit-vector of '0's and '1's representing the comparison of the read and the reference, where a '0' represents a bp match and a '1' represents a bp mismatch. We need to count only occurrences of '1' in the Hamming mask and examine whether their total number is equal to or less than the user-defined Hamming distance threshold. If so, the mapping is considered to be valid and the read passes the filter. Similarly, if the total number of '1' is greater than the Hamming distance threshold then we cannot be certain whether this is because of the high number of substitutions, or there exist insertions and/or deletions; hence, we need to follow the rest of our algorithm. Our filter can detect not only substitutions but also insertions and deletions in an efficient way, as we discuss next. 2.3.2 Method 2: Insertion and deletion (indel) detection Our indel detection algorithm is inspired by the original SHD algorithm presented in  (Xin et al., 2015) . If the substitution detection rejects an alignment, then GateKeeper checks if an insertion or deletion causes the violation (i.e. high number of edits). Figure 1 illustrates the effect of occurrence of edits on the alignment process. If there are one or more base-substitutions or the alignment is exact matching, the matching and mismatching regions can be accurately determined using Hamming distance. As the substitutions have no effect on the alignment of subsequent bases, the number of edits is equivalent to the number of '1's in the resulting Hamming mask. On the other hand, each insertion and deletion can shift multiple trailing bases and create multiple edits in the Hamming mask. Thus, pairwise comparison (bitwise XOR) between the bases of the read and the reference segment is not sufficient. Our indel detection method identifies whether the alignment locations of a read are valid, by shifting individual bases. We need to perform E incremental shifts to the right direction to detect any read that has E deletions, where E is the edit distance threshold. The right shift process guarantees to cancel the effect of deletion. Similarly, we need to perform E incremental shifts to the left direction to detect any read that has E insertions. As we do not have prior knowledge about whether there exist insertions, or deletions, or both, we need to test for every possible case in our algorithm. Thus, GateKeeper generates 2E Hamming masks regardless the source of the edit. Each mask is generated after incrementally shifting the candidate read against the reference and performing pairwise comparison (i.e. bitwise XOR operation). A segment of consecutive matches in the one-step right-shifted mask indicates that there is a single deletion that occurred in the read sequence.  Since deletions and insertions affect only the trailing bases, we need to have an additional Hamming mask that is generated with no shifts. This mask helps detect the matches that are located before the first indel. However, this mask is already generated as part of the first method of the algorithm (i.e. Fast Approximate String Matching). The last step is to merge all the 2E þ 1 Hamming masks using a bitwise AND operation. This step tells us where the relevant matching and mismatching regions reside in the presence of edits in the read compared to the reference segment. We provide an example of a candidate alignment with all masks that are generated by a single GateKeeper processing core in the Supplementary Material, Figure S9. Identical regions are identified in each shifted Hamming mask as streaks of continuous '0's. As we use a bitwise AND operation, a zero at any position in the 2E þ 1 Hamming masks leads to a '0' in the resulting final bit-vector at the same position. Hence, even if some Hamming masks show a mismatch at that position, a zero in some other masks leads to a match ('0') at the same position. This tends to underestimate the actual number of edits and eventually causes some incorrect mappings to pass. To fix this issue, we build a new hardware-based amending process. The amending process is first presented in the original SHD filter  (Xin et al., 2015)  that actually amends (or flips) short streaks of '0's (single or double zeros) in each mask into '1's such that they do not mask out '1's in other Hamming masks. Short streaks of '0's do not represent identical sections and thus they are useless. As a result, bit streams such as 101, 1001 are replaced with 111 and 1111, respectively. In SHD, the amending process is accomplished using a 4-bit packed shuffle (SIMD parallel table-lookup instruction), shift and OR operations. The number of computations needed is 4 packed shuffle, 4m bitwise OR, and three shift operations for each Hamming mask, which is (7 þ 4m)(2E þ 1) operations, where m is the read length. We find that this is very inefficient for FPGA implementation. To reduce the number of operations, we propose using dedicated hardware components in FPGA slices. More precisely, rather than shifting the read and then performing packed shuffle to replace patterns of 101 or 1001 to 111 or 1111 respectively, we perform only packed shuffle independently and concurrently for each bit of each Hamming mask. As illustrated in Figure 2, the proposed architecture for amendment operations contains one 5-input look-up table (LUT) dedicated for each output bit, except the first and last output bits. We provide full details of our amending architecture in the Supplementary Material (Section 1.3). Using this dedicated architecture, we are able to get rid of the four shifting operations and perform the amending process concurrently for all bits of any Hamming mask. Thus, the required number of operations is only (2E þ 1) instead of (7 þ 4m)(2E þ 1) for a total of (2E þ 1) Hamming masks. This saves a considerable amount of the filtering time, reducing it by 407 for a read that is 100 bp long. 2.3.3 Method 3: Minimizing hardware resource overheads The short reads are composed of a string of nucleotides from the DNA alphabet P¼ {A, C, G, T}. Since the reads are processed in an FPGA platform, the symbols have to be encoded in to a unique binary representation. We need 2 bits (log2jPj bits) to encode each symbol. Hence encoding a read sequence of length m results in a 2m-bit word. Encoding the reads into a binary representation introduces overhead to accommodate not only the encoded reads but also the Hamming masks as their lengths also double (i.e. 2m). The issue introduced by encoding the read can be even worse when we apply certain operations on these Hamming masks. For example, the number of LUTs required for performing the amending process on the Hamming masks will be doubled, mainly due to encoding the read. To reduce the complexity of the subsequent operations on the Hamming masks and save about half of the required amount of FPGA resources, we propose a new solution. We observe that comparing a pair of DNA nucleotides is similar to comparing their binary representations (e.g., comparing A to T is similar to comparing '00' to '11'). Hence, comparing each two bits from the binary representation of the read with their corresponding bits of the reference segment generates a single bit that represents one of two meanings; either match or mismatch between two bases. This is performed by encoding each two bits of the result of the pairwise comparison (i.e. bitwise XOR) into a single bit of '0' or '1' using OR operations in a parallel fashion, as explained in Figure 3. This makes the length of each Hamming mask equivalent to the length of the original read, without affecting the meaning of each bit of the mask. The modified Hamming masks are then merged together in 2E bitwise AND operations. Finally, we count the number of ones (i.e. edits) in the final bit-vector mask; if the count is less than the edit distance threshold, the filter accepts the mapping.    2.4 Novelty\r\n  GateKeeper is the only read mapping filter that takes advantage of the parallelism offered by FPGA architectures in order to expedite the alignment filtering process. GateKeeper supports both Hamming distance and edit distance in a fast and efficient way. Each GateKeeper processing core performs all operations defined in the GateKeeper algorithm (Supplementary Material, Section 1.3, Algorithm 1). Table 1 summarizes the relative benefits gained by each of the aforementioned optimization methods over the best previous filter, SHD (E is the user-defined edit distance threshold and m is the read length). When a read matches the reference exactly, or with few substitutions, GateKeeper requires only 2m bitwise XOR operations, providing substantial speedup compared to SHD, which performs a much greater number of operations. However, this is not the only benefit we gain from our first proposed method (i.e. Fast Approximate String Matching). As this method provides an accurate examination for alignments with only substitutions (i.e. no deletions or insertions), we can directly skip calculating their optimal alignment using the computationally expensive alignment algorithms (i.e. verification step). For more general cases such as deletions and insertions, GateKeeper still requires far fewer operations (as shown in    3.1 Theoretical speedup\r\n  We first examine the maximum speedup theoretically possible with our architecture, assuming the only constraint in the system is the FPGA logic. To this end, we calculate the number of mappings that our accelerator board can potentially examine in parallel using as many GateKeeper processing cores as possible. Table 2 shows the resource utilization of a single processing core for two read lengths of 100 and 300 bp, with different edit distance thresholds. We find that a single processing core for a read length of 300 bp shows 3-fold increase in the number of LUTs compared to its counterpart for a read length of 100 bp, for the same edit distance threshold. This observation is supported by theory: as we show in Table 1, the number of operations of GateKeeper is proportional to both read length and edit distance threshold. Based on the resource report in Table 2, we estimate that we can design GateKeeper, on the VC709 FPGA, to process up to 140 alignments of 100 bp reads and edit distance threshold of up to 5% in parallel in a single clock cycle. The number of alignments drops to 20 for a read length of 300 bp and E ¼ 15. The bottleneck in this idealized system is transferring a total of 28 000 (140 alignment 100 bp 2 bits for encoding) bits in a single clock cycle into the FPGA, which is not practical for any of the existing PCIe drivers that supply data to the FPGA. For instance, RIFFA  (Jacobsen et al., 2015)  transmits the mapping pairs into the FPGA in 'packages' of 128 bits per clock cycle at a clock speed of 250 MHz (i.e. 4 nanoseconds). We conclude that the theoretical speedup provided by GateKeeper is extremely large, but practical speedup, which we will examine next, is mainly limited by the data transfer rate into the accelerator.   Read length\r\n    Edit distance Slice LUT Slice register Block memory\r\n     3.2 Experimental speedup\r\n  Throughput and resource analysis. Filtering speed of GateKeeper is dependent on the total number of concurrent processing cores and the clock frequency. The number of processing cores is determined by the maximum data throughput and the available FPGA resources. The operating frequency of the accelerator is 250 MHz. At this frequency, we observe a data throughput of nearly 3.3 GB/s, which corresponds to 13.3 billion bases per second, nearly reaching the maximum throughput of 3.64 GB/s provided by the RIFFA communication channel that feeds data into the FPGA  (Jacobsen et al., 2015) . Table 3 lists the resource utilization of the entire design including the PCIe communication logic, for various read lengths and edit distance thresholds. For a read length of 100 bp, we find that we can align each read against up to 16 different reference segments in parallel, without violating the timing constraints (e.g. maximum operating frequency). This design occupies about 50% of the available FPGA resources (i.e. slice LUTs). We find that as read length increases, timing constraints of the design can be violated. By pipelining the design (i.e. shortening the critical path delay of each processing core by dividing it into stages or smaller tasks), we can meet the timing constraints and achieve more parallelism. However, pipelining the design comes with the expense of increased register utilization. For a read length of 300 bp, GateKeeper can process up to 8 alignments concurrently and use 91% of the available registers. As our design is FPGA-platform independent, FPGAs with higher logic density (such as Xilinx UltraScaleþ FPGAs) can be used, to achieve more parallelism and higher data throughput. Next, we evaluate the effect of varying the number of processing cores on the execution time of GateKeeper.  Speedup versus existing filters. We now evaluate the execution time of GateKeeper compared to the best existing filters. We use mrFAST  (Alkan et al., 2009)  mapper to retrieve all potential mappings (read-reference pairs) from two datasets. The first set (ERR240727_1) contains about 4 million real reads, each of length 100 bp, from the 1000 Genomes Project Phase I (Consortium, 2012). The second set contains about 100 thousand reads, each of length 300 bp, simulated from the human genome using the mason simulator (http://packages.seqan.de/mason/). Figure 4 shows the number of mappings that are processed by GateKeeper (with different numbers of processing cores), SHD, and the Adjacency Filter within 40 minutes. To ensure as fair a comparison as possible, we evaluate Gate Keeper using a single FPGA chip and run both SHD and the Adjacency Filter using a single CPU core. We believe our comparison is fair because we compare GateKeeper running on a part of a single FPGA chip to SHD/Adjacency-Filter running on a part of a single CPU (Section 1.5, Supplementary Material). Both SHD and the Adjacency Filter are software filters (i.e. cannot run on an FPGA) and they do not support multithreading. SHD supports a read length up to only 128 bp (due to SIMD registers size). Under different edit distance thresholds (up to 5% of the read length), GateKeeper provides consistently good performance.  On average, GateKeeper for 100 bp reads is 130x faster than SHD and 90 faster than the Adjacency Filter. For longer reads (i.e. 300 bp), GateKeeper is also, on average, 10 faster than the Adjacency Filter. As edit distance threshold increases, Gatekeeper's speedup over SHD and the Adjacency Filter also increases (e.g. up to 105 and 215 faster than the Adjacency Filter and SHD, respectively, when E ¼ 5 edits and read length ¼ 100 bp). This is because our architecture offers the ability to perform all computations in a parallel fashion (as we explained when we described our three new methods in the GateKeeper core). Note that the Adjacency Filter becomes faster than SHD as E increases, but at the expense of accuracy, as we will show soon. We conclude that GateKeeper greatly improves the performance of alignment filtering by at least one order of magnitude. GateKeeper also scales very well over a wide range of both edit distance thresholds and read lengths.    3.3 Filtering accuracy\r\n  An ideal filter should be both fast and accurate in rejecting the incorrect mappings. We evaluate the accuracy of GateKeeper by computing its true negative, false positive and false negative rates. We use the Needleman-Wunsch algorithm to benchmark the three filters as this algorithm has both zero false positive and zero false negative rates. To evaluate the accuracy of SHD regardless of the limitation of its SIMD implementation (i.e. limited read length), we implement SHD in C and refer to it as SHD-C. We also compare the accuracy of our filter with SHD and the Adjacency Filter using both simulated and real mapping pairs. We simulate reads from the human genome using the mason simulator. The configuration and parameters used in our experiment are provided in Supplementary Material (Section 1.4). We generate five sets, each of which contains 400 000 Illumina-like reads. Each set has an equal number of reads of length 64, 100, 150 and 300 bp. While two sets have a low number of different types of edits, the other three sets have a high number of substitutions, insertions and deletions. The purpose of simulating the low-edit reads is that we want most of the reads to have edits less than the allowed threshold. This enables us to quantify the false negatives (i.e. correct mappings that are rejected by the filter) of the three filters with different read lengths. On the other hand, we use the edit-rich reads to evaluate the robustness of the three filters to incorrect mappings. This enables us to quantify both the false positives and true negatives. While the false positive rate is the rate of incorrect mappings that are accepted by the filter, the true negative rate is the rate of incorrect mappings that are rejected by the filter. Figure 5(a) shows the result of this experiment. We also consider a more realistic scenario in which reads can have a combination of substitutions and indels. Instead of simulated reads, we use the first 30 million pairs produced by mrFAST when the dataset ERR240727_1 mapped to the human genome to evaluate both the false positive and true negative rates of the three filters, as shown in Figure 5(b).  Based on these results, we make five main observations. (i) Using the low-edit reads, we observe that the three filters never filter out correct mappings; hence, they provide a lossless filtering mechanism with a false negative rate of zero. (ii) We find that GateKeeper is very effective and superior to the Adjacency Filter at both substitution and indel detection. Figure 5(a) shows the average false positive and true negative rates of the three filters, respectively, using the three simulated edit-rich sets. We observe that both GateKeeper and SHD have the same false positive and true negative rates. (iii) On (a) (b) average, GateKeeper produces a false positive rate of 4%, which is much smaller (on average, 0.25 ) than that of the Adjacency Filter. (iv) GateKeeper rejects a significant fraction of incorrect mappings (e.g. 84% to 99.9% of the mappings, depending on the edit distance threshold used) and thus avoids expensive verification computations required by alignment algorithms. GateKeeper rejects up to 20% more incorrect mappings than the Adjacency Filter. (v) The Adjacency Filter is more robust in handling indels than in handling substitutions. This is expected as the presence of one or more substitutions in any seed is counted by the Adjacency Filter as a single mismatch. The effectiveness of the Adjacency Filter for substitutions and indels diminishes when E becomes larger than 3%. The detailed results for each of the three edit-rich sets are provided in the Supplementary Material (Section 1.4). We conclude that Gatekeeper's accuracy is as good as that of the best previous filter, SHD, and much better than that of the Adjacency Filter yet GateKeeper is much faster than both SHD and the Adjacency Filter (as we showed earlier). Hence, GateKeeper is extremely fast and accurate.    3.4 Verification\r\n  GateKeeper is a standalone filter and can be integrated with any existing reference-based mapper. GateKeeper does not replace the local/global alignment algorithms (e.g. Smith-Waterman (Smith and Waterman, 1981) and Needleman-Wunsch (Needleman and Wunsch, 1970)). GateKeeper should be followed by an alignment verification step, which precisely verifies the alignments that pass our filter and eliminates the false positives (as provided in the Supplementary Material, Fig. S9). The verification step is accurate and admits zero false positive rate. It also allows specifying a cost to each edit (i.e. a scoring system). Such integration is mapper-specific and will be explored in detail for various mappers in our future research. In this work, we mainly focus on and deeply evaluate the benefits and downsides of our filtering algorithm and architecture independently of any mapper it can be combined with. Nonetheless, we have a preliminary assessment on the overall benefits of integrating GateKeeper with the mrFAST mapper  (Alkan et al., 2009) . We select mrFAST for two main reasons. (i) It already includes the Adjacency Filter  (Xin et al., 2013)  as a pre-alignment step, so it constitutes a state-of-the-art baseline. (ii) It utilizes a banded Levenshtein edit distance algorithm  (Ukkonen, 1985)  that is parallelized using the Intel SSE instructions, and thus it utilizes the capabilities of state-of-the-art hardware. Table 4 summarizes the effect of pre-alignment on the overall mapping time, when all reads from ERR240727_1 (100 bp) and Set_5 (300 bp, mason-simulated deletion-rich reads) are mapped to the human genome with an edit distance threshold of 5%. We make three observations. (i) GateKeeper is at least 41 times faster than the banded dynamic programming alignment algorithm  (Ukkonen, 1985) . (ii) The verification time drops by a factor of 10 after replacing the Adjacency Filter with GateKeeper as the pre-alignment step. (iii) GateKeeper reduces the overall mapping time of mrFAST (mrFAST-2.6) by a factor of 1.3-3. Details are provided in the Supplementary Material, Section 1.6.     4 Future work\r\n  GateKeeper shows that there is a great benefit in designing an alignment filtering accelerator to handle the flood of sequenced data. Since a single-core GateKeeper has only a small footprint on the FPGA, we can combine our architecture with any of the FPGA-based accelerators for BWT-FM or hash-based mapping techniques on a single FPGA chip. With such a combination, the end result would be an efficient and fast multi-layer mapping system: alignments that pass GateKeeper can be further verified using a dynamic programing based alignment algorithm within the same chip. We leave this combination for future work. Another potential target of our research is to influence the design of more intelligent and attractive sequencing machines by integrating GateKeeper inside them, to perform real-time pre-alignment. This approach has two benefits. First, it can hide the complexity and details of the underlying hardware from users who are not necessarily fluent in FPGAs (e.g. biologists and mathematicians). Second, it allows a significant reduction in total genome analysis time by starting read mapping while still sequencing (Lindner et al., 2016). Our next efforts will also focus on investigating the sources of the false positives and explore the possibility of eliminating them to achieve a dynamic-programming-free alignment approach or a more accurate filter.    5 Summary\r\n  In this paper, we propose the first hardware accelerator architecture for pre-alignment in genome read mapping. In our experiments, GateKeeper can filter up to 4 trillion mappings within 40 mins using a single FPGA chip while preserving all correct ones. Comparison against the best two software-based alignment filters reveals the following: (i) Our filter provides, on average, 90-fold and 130-fold speedup compared to the Adjacency Filter and SHD, respectively. (ii) Our filter is as accurate as the SHD and 4 times more accurate than the Adjacency Filter. We conclude that GateKeeper is both a fast and an accurate filter that can improve the performance of existing and future read mappers. Our preliminary results show that the addition of GateKeeper as the pre-alignment step can reduce the filtering and verification time of the mrFAST mapper by a factor of 10.  Our design is open source and freely available online. To our knowledge, GateKeeper is the first open-source FPGA-based alignment filtering accelerator for genome analysis. As such, we hope that it catalyzes the development and adoption of such hardware accelerators in genome sequence analysis, which are becoming increasingly necessary to cope with the processing requirements of greatly increasing amounts of genomic data.    Funding\r\n  This study is supported by NIH Grant (R01 HG006004 to O. Mutlu and C. Alkan) and a Marie Curie Career Integration Grant (PCIG-2011-303772) to C. Alkan under the Seventh Framework Programme. M. Alser also acknowledges support from the Scientific and Technological Research Council of Turkey, under the TUBITAK 2215 program.  Conflict of Interest: none declared. Ahmadi,A. et al. (2012) Hobbes: optimized gram-based methods for efficient read alignment. Nucleic Acids Res., 40, e41-e41.  Alkan,C. et al. (2009) Personalized copy number and segmental duplication maps using next-generation sequencing. Nature Genet., 41, 1061-1067.  Aluru,S. and Jammula,N. (2014) A review of hardware acceleration for computational genomics. Des. Test IEEE, 31, 19-30.  Arram,J. et al. (2013) Reconfigurable acceleration of short read mapping. In: Field-Programmable Custom Computing Machines (FCCM), 2013 IEEE 21st Annual International Symposium on IEEE, pp. 210-217.  Canzar,S. and Salzberg,S.L. (2015) Short read mapping: an algorithmic tour.  In: Proceedings of the IEEE, pp, 1-23.  Cheng,H. et al. (2015) BitMapper: an efficient all-mapper based on bit-vector computing. BMC Bioinformatics, 16, 192-207.  Consortium,G.P. (2012) An integrated map of genetic variation from 1,092 human genomes. Nature, 491, 56-65.  David,M. et al. (2011) SHRiMP2: sensitive yet practical short read mapping.  Bioinformatics, 27, 1011-1012.  Hach,F. et al. (2010) mrsFAST: a cache-oblivious algorithm for short-read mapping. Nat. Methods, 7, 576-577.  Hach,F. et al. (2014) mrsFAST-Ultra: a compact, SNP-aware mapper for high performance sequencing applications, Nucleic Acids Res., gku370. Hatem,A. et al. (2013) Benchmarking short sequence mapping tools. BMC  Bioinformatics, 14, 184.  Herbordt,M.C. et al. (2007) Achieving high performance with FPGA-based computing. Computer, 40, 50.  Homer,N. et al. (2009) BFAST: an alignment tool for large scale genome resequencing. PloS One, 4, e7767.  Houtgast,E.J. et al. (2015) An FPGA-Based Systolic Array to Accelerate the BWA-MEM Genomic Mapping Algorithm. In: International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS). Samos Island, Greece.  Jacobsen,M. et al. (2015) RIFFA 2.1: a reusable integration framework for  FPGA accelerators. ACM Trans. Reconfigurable Technol. Syst., 8, 1-23. Langmead,B. and Salzberg,S.L. (2012) Fast gapped-read alignment with  Bowtie 2. Nat. Methods, 9, 357-359.  Langmead,B. et al. (2009) Ultrafast and memory-efficient alignment of short  DNA sequences to the human genome. Genome Biol., 10, R25. Levenshtein,V.I. (1966) Binary codes capable of correcting deletions, insertions, and reversals. Soviet Phys. Doklady, 10, 707-710.  Li,H. (2013) Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM, arXiv preprint arXiv:1303.3997.  Li,H. and Durbin,R. (2009) Fast and accurate short read alignment with  Burrows-Wheeler transform. Bioinformatics, 25, 1754-1760.  Li,M. et al. (2004) PatternHunter II: highly sensitive and fast homology search. J. Bioinf. Comput. Biol., 2, 417-439.  Lindner,M.S. et al. (2016) HiLive-real-time mapping of illumina reads while sequencing, Bioinformatics, btw659.  Liu,C.-M. et al. (2012) SOAP3: ultra-fast GPU-based parallel alignment tool for short reads. Bioinformatics, 28, 878-879.  Luo,R. et al. (2013) SOAP3-dp: fast, accurate and sensitive GPU-based short read aligner. PloS One, 8, e65632-e65632.  Marco-Sola,S. et al. (2012) The GEM mapper: fast, accurate and versatile alignment by filtration. Nat. Methods, 9, 1185-1188.  Needleman,S.B. and Wunsch,C.D. (1970) A general method applicable to the search for similarities in the amino acid sequence of two proteins. J. Mol.  Biol., 48, 443-453.  Olson,C.B. et al. (2012) Hardware acceleration of short read mapping. In: Field-Programmable Custom Computing Machines (FCCM), 2012 IEEE 20th Annual International Symposium on IEEE, pp. 161-168.  Rasmussen,K.R. et al. (2006) Efficient q-gram filters for finding all e-matches over a given length. J. Comput. Biol., 13, 296-308.  Smith,T.F. and Waterman,M.S. (1981) Identification of common molecular subsequences. J. Mol. Biol., 147, 195-197.  Trimberger,S.M. (2015) Three ages of FPGAs: a retrospective on the first thirty years of FPGA technology. Proc. IEEE, 103, 318-331.    ",
    "sourceCodeLink": "https://github.com/BilkentCompGen/GateKeeper",
    "publicationDate": "0",
    "authors": [
      "Mohammed Alser",
      "Hasan Hassan",
      "Hongyi Xin",
      "Oguz Ergin",
      "Onur Mutlu",
      "Can Alkan"
    ],
    "status": "Success",
    "toolName": "GateKeeper",
    "homepage": ""
  },
  "85.pdf": {
    "forks": 0,
    "URLs": [
      "ftp.ncbi.nlm.nih.gov/blast/executables/blast+/2.5.0/",
      "github.com/kyungtaekLIM/PSI-BLASTexB"
    ],
    "contactInfo": ["k-tomii@aist.go.jp"],
    "subscribers": 2,
    "programmingLanguage": "C++",
    "shortDescription": "PSI-BLAST with a new sequence weighting algorithm",
    "publicationTitle": "Simple adjustment of the sequence weight algorithm remarkably enhances PSI-BLAST performance",
    "title": "Simple adjustment of the sequence weight algorithm remarkably enhances PSI-BLAST performance",
    "publicationDOI": "10.1186/s12859-017-1686-9",
    "codeSize": 227198,
    "publicationAbstract": "Background: PSI-BLAST, an extremely popular tool for sequence similarity search, features the utilization of PositionSpecific Scoring Matrix (PSSM) constructed from a multiple sequence alignment (MSA). PSSM allows the detection of more distant homologs than a general amino acid substitution matrix does. An accurate estimation of the weights for sequences in an MSA is crucially important for PSSM construction. PSI-BLAST divides a given MSA into multiple blocks, for which sequence weights are calculated. When the block width becomes very narrow, the sequence weight calculation can be odd. Results: We demonstrate that PSI-BLAST indeed generates a significant fraction of blocks having width less than 5, thereby degrading the PSI-BLAST performance. We revised the code of PSI-BLAST to prevent the blocks from being narrower than a given minimum block width (MBW). We designate the modified application of PSI-BLAST as PSI-BLASTexB. When MBW is 25, PSI-BLASTexB notably outperforms PSI-BLAST consistently for three independent benchmark sets. The performance boost is even more drastic when an MSA, instead of a sequence, is used as a query. Conclusions: Our results demonstrate that the generation of narrow-width blocks during the sequence weight calculation is a critically important factor that restricts the PSI-BLAST search performance. By preventing narrow blocks, PSI-BLASTexB upgrades the PSI-BLAST performance remarkably. Binaries and source codes of PSI-BLASTexB (MBW = 25) are available at https://github.com/kyungtaekLIM/PSI-BLASTexB.",
    "dateUpdated": "2016-12-09T11:18:35Z",
    "institutions": ["National Institute of Advanced Industrial Science and Technology (AIST)"],
    "license": "GNU Lesser General Public License v3.0",
    "dateCreated": "2016-12-09T10:58:56Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Oda et al. BMC Bioinformatics     10.1186/s12859-017-1686-9   Simple adjustment of the sequence weight algorithm remarkably enhances PSI-BLAST performance     Toshiyuki Oda  0    Kyungtaek Lim  0    Kentaro Tomii  k-tomii@aist.go.jp  0  1    0  Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology (AIST) ,  2-4-7 Aomi, Koto-ku, Tokyo 135-0064 ,  Japan    1  Biotechnology Research Institute for Drug Discovery, National Institute of Advanced Industrial Science and Technology (AIST) ,  2-4-7 Aomi, Koto-ku, Tokyo 135-0064 ,  Japan     2017   18    15  5  2017    1  2  2017     Background: PSI-BLAST, an extremely popular tool for sequence similarity search, features the utilization of PositionSpecific Scoring Matrix (PSSM) constructed from a multiple sequence alignment (MSA). PSSM allows the detection of more distant homologs than a general amino acid substitution matrix does. An accurate estimation of the weights for sequences in an MSA is crucially important for PSSM construction. PSI-BLAST divides a given MSA into multiple blocks, for which sequence weights are calculated. When the block width becomes very narrow, the sequence weight calculation can be odd. Results: We demonstrate that PSI-BLAST indeed generates a significant fraction of blocks having width less than 5, thereby degrading the PSI-BLAST performance. We revised the code of PSI-BLAST to prevent the blocks from being narrower than a given minimum block width (MBW). We designate the modified application of PSI-BLAST as PSI-BLASTexB. When MBW is 25, PSI-BLASTexB notably outperforms PSI-BLAST consistently for three independent benchmark sets. The performance boost is even more drastic when an MSA, instead of a sequence, is used as a query. Conclusions: Our results demonstrate that the generation of narrow-width blocks during the sequence weight calculation is a critically important factor that restricts the PSI-BLAST search performance. By preventing narrow blocks, PSI-BLASTexB upgrades the PSI-BLAST performance remarkably. Binaries and source codes of PSI-BLASTexB (MBW = 25) are available at https://github.com/kyungtaekLIM/PSI-BLASTexB.    PSI-BLAST  Sequence similarity search  Sequence weighting  Position-specific scoring matrix       Background\r\n  Sequence similarity search is an initial choice for structural and functional inference of unknown biological sequences, for which BLAST [ 1 ] is widely used. BLAST uses an amino acid substitution matrix such as BLOSUM62 [ 2 ] to score similarities between amino acid pairs. Starting from the original BLAST, it has evolved in several aspects, such as gap treatment [ 3 ] and composition-based adjustment [ 4 ]. Using an iterative search, BLAST (precisely, PSI-BLAST [ 3 ]) can employ patterns of amino acids varying among homologs and among positions within homologs. It can therefore detect more distant homologs than the original BLAST does. The multiple sequence alignment (MSA) of closely related homologous sequences detected by BLAST is expected to contain such homolog-specific and positionspecific information. An MSA can be transformed into a position-specific scoring matrix (PSSM), which is a more sophisticated model for sequence similarity search than the substitution matrix because scores for amino acids are modeled for individual positions. Iterative search methods including PSI-BLAST [ 3 ] construct a PSSM from an MSA obtained from the previous search. Then such methods use the PSSM for another similarity search. It has been demonstrated that much more distant homologs can be detected by iterating these steps. Because of its usefulness and availability, many modifications have been proposed since PSI-BLAST was first published, including introduction of composition-based statistics, optimizing cache utilization, and revising pseudo-count strategy [ 4-6 ]. Overcoming the problem of \u201chomologous over-extension (HOE)\u201d also improves the PSI-BLAST accuracy [ 7, 8 ]. In this study, we describe that PSI-BLAST can be improved further by slightly changing the sequence weighting method.  Because sequences in public databases are highly biased into organisms that are medically and commercially important, and because they are easy to culture, it is crucially important to adjust amino acid observations in the MSA of homologous sequences before PSSM calculation. Sequence weight is a straightforward way for attaining such adjustment, where a sequence with more closely related counterparts in the MSA should be assigned a smaller weight. PSI-BLAST calculates the position-specific sequence weight (PSSW) using a procedure derived from the formula proposed [ 9 ] as W i ¼  Xl  j¼11= rj naj l where Wi stands for the weight of ith sequence in a MSA, rj denotes the number of unique amino acids found at the position j, l signifies the length of the alignment, and naj represents the number of amino acids a found at j. After sequences are weighted, the probability of a at j (Paj) is calculated as  Paj ¼ t ¼  Xn i¼1  W i t; 1 if uij ¼ a ; 0 if uij ≠ a where uij stands for the amino acid at j in the ith sequence, and n signifies the number of sequences in MSA.  This formula lacks the consideration of gaps. Simply put, gaps (including N-terminal and C-terminal gaps) can be treated as the 21st amino acid. An important problem of this approach is that the weights of gappy sequences in a gappy MSA will be underestimated. One can avoid this problem by considering an MSA subregion with few or no gaps for PSSW calculation. This is expected to be advantageous for dealing with MSAs constructed from local alignments that are likely to include many gaps. PSI-BLAST defines such blocks for individual positions. PSI-BLAST first selects a subset of sequences (a reduced MSA) in an MSA, such that no gap is included at a position of interest j. PSI-BLAST then collects starting and ending positions of all pairwise alignments between query and subjects in the reduced MSA to define the boundary of the block as the starting and ending positions closest to j [ 3 ]. This approach also has an important limitation: The block width can be extremely narrow, failing to reflect actual evolutionary information.  This study demonstrates that such narrow blocks are created during the PSSM construction of PSI-BLAST, which gives rise to inaccurate calculation of PSSW and PSSM, and which thereby drastically hampers the homology detection performance. We propose a simple method for better PSSW calculation, which boosts the PSI-BLAST performance.    Implementation\r\n   Narrow blocks result in wrong sequence weight calculation\r\n  To exemplify the effect of narrow blocks, we show two artificial MSAs presented in Fig. 1. The MSA in Fig. 1a (MSA-A) is a subset of AAA ATPase MSA in the Pfam database [ 10 ]. The MSA in Fig. 1b (MSA-B) is identical to MSA-A except for 10th and 11th sequences, which were derived from the 10th sequence in MSA-A by dividing it into two pieces with an overlap at position 19. The two MSAs were converted to PSSMs (Additional files 1 and 2, respectively) by PSI-BLAST search against a dummy database with \u201c-in_msa\u201d, \u201c-num_iterations 1\u201d and '-out_ascii_pssm' options.  We checked the inner variables of PSI-BLAST to mark blocks on MSA-A and MSA-B (Fig. 1). A block that covers the whole MSA was used for all positions in MSA-A because it lacks gaps, whereas three blocks were generated for MSA-B, where the block width (l) at position 19 is one (Fig. 1b, orange block). At position 19, the weights of the sequences not only of seq10a and seq10b but also of seq1-9 in MSA-B deviate drastically from those in MSA-A. Consequently, at position 19 of MSA-B, the weighted percentage of alanine, leucine, isoleucine, and serine were equally 25 (Additional file 2). Because when l is one, the number of sequences which have a at j is naj. The weighted probabilities of amino acids are 1/(rj *naj)* naj = 1/rj. In MSA-A, the weighted percentage of those 4 amino acids were 62, 15, 12, and 11, respectively (Additional file 1), demonstrating the limitation of PSI-BLAST PSSW calculation when the block width is tiny.    Block extended PSI-BLAST (PSI-BLASTexB)\r\n  A simple and direct solution of this problem is to prevent block widths from being narrower than a certain width by exceptionally allowing gaps in the blocks. These gaps might cause the underestimation of gappy sequences in an alignment as discussed above, which however would certainly be a better estimation than the weights calculated for blocks having width of several residues.  The PSI-BLAST source code in the BLAST+ package [ 11 ] was downloaded from the BLAST FTP site (ftp:// ftp.ncbi.nlm.nih.gov/blast/executables/blast+/2.5.0/). We revised the PSI-BLAST code and added lines after line 1415 of ncbi-blast-2.5.0 + −src/c++/src/algo/blast/core/ blast_psi_priv.c, as shown below. It implements the minimum block width (MBW), which is \u201c1\u201d in the original code. Blocks with widths &lt; MBW are extended front and rear by MBW-1 until the termini of the MSA. For example, when MBW is 13, the deviated weights of MSA-B (Fig. 1b, red block) became similar to the weights of MSA-A (Fig. 1a). The resulting PSSM of MSA-B with MBW13 is provided as Additional file 3. The source code was configured with \u201c-with-binrelease\u201d and \u201c-with-ncbi-public\u201d options and compiled by the make command with no options. We designate the modified PSI-BLAST as PSI-BLASTexB.    Benchmark dataset\r\n  The search performance was compared using SCOP20_training, SCOP20_validation and CATH20-SCOP datasets, as established in our previous study [ 12 ]. The SCOP20_training and SCOP20_validation datasets were derived from the non-redundant set of 7074 proteins (SCOP20), which was provided by the ASTRAL compendium [ 13 ]. The 7074 sequences were divided into two groups for parameter optimization (SCOP20_training) and performance evaluation (SCOP20_validation). CATH20-SCOP dataset was derived from the CATH database [ 14 ] excluding sequences in the SCOP database. The sequences in the datasets were filtered so that the sequences did not have &gt; 20% mutual sequence identity. Finally, our dataset included respectively 3537, 3537, and 1754 sequences. All datasets are available from http://csas.cbrc.jp/Ssearch/benchmark/.    PSSM construction\r\n  PSSMs for individual sequences in the benchmark datasets were constructed using PSI-BLAST and PSIBLASTexB against the Uniref50 dataset (Release 2015_10) [ 15 ] downloaded from the UniProt FTP site (ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/). In this study, PSSMs for iteration X were generated using the following command: psiblast -query &lt; QUERY &gt; −db &lt; UNIREF50 DB &gt; −out_pssm &lt; PSSM PATH &gt; −num_iterations &lt; X &gt; −num_alignments 1000000.  We also extracted an MSA consisting of hits from a PSI-BLAST search with \u201c-num_interation 1\u201d option, and used the MSA directly to another search against Uniref50 using the \u201c-in_msa\u201d option, which is an alternative method of running an iterative PSI-BLAST search with an MSA instead of a query (\u201c-query\u201d) or checkpoint PSSM (\u201c-in_pssm\u201d).    Performance evaluation\r\n  Similarity searches were conducted respectively against benchmark datasets using the constructed PSSMs and MSAs as queries using the \u201c-in_pssm\u201d and \u201c-in_msa\u201d options. We followed the rule set proposed by Julian Gough (http://www.supfam.org/SUPERFAMILY/ruleset.html) [1 16 ] to define true positive (TP) and false positive (FP) hits at the superfamily level. Superfamily definitions of the rule set differ from the original ones of SCOP. The rule set also excludes hits with a potential homologous relation from FPs.  To evaluate the performance, we introduced a receiver operating characteristic (ROC) curve plot, which has been used widely for performance evaluation [ 17, 18 ]. Hits from all queries were pooled and ranked by their Evalues. Then TP and FP hits until various E-value thresholds were counted and shown, with weighting of the TP and FP counts by 1/(number of all TPs in the dataset) for each query.  We also calculated the ROC5 score for hits with Evalues less than 1.0, which indicates the search performance of individual queries using the following equation: 1 ROC5 ¼ 5T  X5  i¼1ti:  Therein, T signifies the total TP count; ti denotes the TP count until the i-th FP appears [ 19 ].     Results and Discussion\r\n  We first investigated how many narrow-width blocks, which are potentially causing the problem of sensitivity reduction, are generated during PSI-BLAST searches. We therefore measured the distribution of block widths used for individual query positions by PSI-BLAST at the second to eighth iterations for three independent benchmark sets (Fig. 2). About 35%, 35%, and 25% of the blocks had widths of less than 5 amino acids (aa) at the eighth iteration of SCOP20_training, SCOP20_validation, and CATH20-SCOP datasets, respectively. This fact demonstrates that PSI-BLAST produces the narrowwidth blocks constantly.  Using the SCOP20_training dataset, we analyzed the PSI-BLASTexB performance with varying MBW values (5, 13, 25, and 41) at the fifth iteration. PSI-BLAST corresponds to PSI-BLASTexB with the MBW of one. As Fig. 3a shows, the performance of PSI-BLASTexB is much higher than that of PSI-BLAST across all MBW values. The performances are almost identical when MBW values are 13, 25, and 41, and are slightly low when MBW is 5, which suggests that 5 aa long blocks are insufficient to calculate the correct PSSW. The weighted TP count was highest when MBW was 25 at the false discovery rate (FDR) of 10%. Therefore, we use the value as the default in the following experiments.  The performance improvement was also clear for SCOP20_validation and CATH20-SCOP (Figs. 3b and c). However, the performance improvement for CATH20SCOP was slight compared with those of SCOP20_training and SCOP20_validation. That result is consistent with the result of the distributions of block widths. The fractions of narrow-width blocks in CATH20-SCOP are smaller than those of SCOP20_training and SCOP20_validation (Fig. 2), which is expected because our new method would be of no use if few narrow-width blocks existed.  To observe the relation between performance improvement and the block extension for each query, the incremental ROC5 scores (ROC5 score by PSIBLASTexB - ROC5 score by PSI-BLAST) are shown against the ratio of positions with one aa long blocks at the second iteration for each query (Fig. 4). When the ratio is larger than 0.1, in other words, when more than 10% of PSSM positions are derived from one aa long blocks, 92, 90, and 81 PSI-BLASTexB searches among 189, 195, and 196 achieve higher performance than PSIBLAST searches. Only for 10, 11, and 9 cases are PSIBLASTexB searches worse, respectively, than PSI-BLAST searches against SCOP20_training, SCOP20_validation, and CATH20-SCOP. In contrast, improvement of queries with the ratio less than 0.1 appears to be more random, although PSI-BLASTexB searches are also effective for many queries with the ratio less than 0.1. These results show how widening the widths of narrow blocks improves the search performance.  PSI-BLAST supports a search using an MSA as an input with \u201c-in_msa\u201d option. We constructed MSAs from the outputs of PSI-BLAST and PSI-BLASTexB to use them as queries for the next search (see Methods for details). As Fig. 5 shows, the performance of PSI-BLAST with \u201c-in_msa\u201d option is distinguishably lower than that of normal PSI-BLAST search with the corresponding number of iterations. From our understanding, when \u201c-in_msa\u201d is used, PSI-BLAST divides a sequence in an MSA into multiple pieces if large gaps exist within (10 aa in case of ver. 2.5.0). Therefore, more narrow-width blocks are generated with the \u201c-in_msa\u201d option. Block extension by PSI-BLASTexB effectively suppresses performance degradation using MSAs as queries (Fig. 5). Therefore, PSI-BLASTexB can facilitate the use of MSAs prepared in advance as queries, e.g. Pfam seed alignments [ 10 ], HMM-HMM alignments by HHblits [ 20 ], and progressive alignments by MAFFT [ 21 ] for distant homology detection.  We presume that troubles of at least two types can be sources of narrow-width blocks in an MSA, although such blocks might also arise from other sources. One is an HOE [ 8 ] related problem. We present an example of this phenomenon in Fig. 6. When multiple conserved regions (often domains) exist in a query, narrow-width blocks are likely to be included in the resulting MSA attributable to overlaps between extended nonhomologous residues flanking a conserved region and an adjacent conserved region. Mainly, this is a querydependent problem. Some solutions have been proposed [ 7, 8 ]. The other is an issue of the sequence library. As shown in Fig. 1b, fragmented sequences in libraries can produce narrow-width blocks caused by their overlaps. Therefore, dividing queries such that each query has only one conserved region or removing fragmented sequences from the library should be workarounds to reduce the number of narrow-width blocks. However, the practical applications of these procedures might require further consideration (e.g., how to determine \u201cconserved\u201d and \u201cfragmented\u201d). Consequently, our simple adjustment of the sequence weight algorithm is a more practical way of handling narrow-width blocks in a MSA produced by PSI-BLAST.    Conclusion\r\n  Because of sequence weighting scheme limitations, the PSI-BLAST performance has been penalized until now. We developed a customized PSI-BLAST, designated as PSI-BLASTexB, which solved such problems with extremely simple modification of the PSI-BLAST code. PSI-BLASTexB significantly outperformed PSI-BLAST. Therefore, it is expected to be useful not only for distant homology search, but also for many downstream methods that depend on PSI-BLAST with trivial effort.    Additional files\r\n  Additional file 1: The ascii pssm file made from MSA-A using PSI-BLAST. (ASCII 5 kb) Additional file 2: The ascii pssm file made from MSA-B using PSI-BLAST. (ASCII 5 kb) Additional file 3: The ascii pssm file made from MSA-B using PSI-BLASTexB with setting minimum block width as 13. (ASCII 5 kb) Additional file 4: Table S1. The number of searches which were not converged before each iteration of PSI-BLAST. (XLSX 9 kb) Abbreviations AA: Amino acid; FDR: False discovery rate; FP: False positive; HOE: Homologous over-extension; MBW: Minimum block width; MSA: Multiple sequence alignment; PSSM: Position-specific scoring matrix; PSSW: Position-specific sequence weight; ROC: Receiver operating characteristic; TP: True positive Acknowledgements We thank Dr. Yoshinori Fukasawa for helpful discussions.  Funding This work was partially supported by the Platform Project for Supporting in Drug Discovery and Life Science Research (Platform for Drug Discovery, Informatics, and Structural Life Science) from the Japan Agency for Medical Research and Development (AMED). The content is solely the responsibility of the authors and does not necessarily represent the official views of AMED. Availability of data and materials Project name: PSI-BLASTexB Project home page: https://github.com/kyungtaekLIM/PSI-BLASTexB Operating system(s): Linux Programming language: C &amp; C++ Other requirements: GCC ver. 4.6 or higher License: GNU LGPL Datasets used during the current study are available from http://csas.cbrc.jp/Ssearch/benchmark/.  Authors' contributions TO conceived and implemented the method. TO and KT designed the research. TO and KL analyzed the data. The benchmarking and repository maintenance were done by KL. The paper was written by TO, KL, and KT. All authors read and approved the final manuscript.  Competing interests The authors declare that they have no competing interests.  Consent for publication Not applicable.  Ethics approval and consent to participate Not applicable.    Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/kyungtaekLIM/PSI-BLASTexB",
    "publicationDate": "0",
    "authors": [
      "Toshiyuki Oda",
      "Kyungtaek Lim",
      "Kentaro Tomii"
    ],
    "status": "Success",
    "toolName": "PSI-BLASTexB",
    "homepage": ""
  },
  "42.pdf": {
    "forks": 1,
    "URLs": ["github.com/UdeM-LBIT/CoreTracker"],
    "contactInfo": ["mabrouk@iro.umontreal.ca"],
    "subscribers": 3,
    "programmingLanguage": "Python",
    "shortDescription": "CoreTracker, a codon reassignment tracker ",
    "publicationTitle": "CoreTracker: accurate codon reassignment prediction, applied to mitochondrial genomes",
    "title": "CoreTracker: accurate codon reassignment prediction, applied to mitochondrial genomes",
    "publicationDOI": "10.1093/bioinformatics/btx421",
    "codeSize": 106189,
    "publicationAbstract": "Motivation: Codon reassignments have been reported across all domains of life. With the increasing number of sequenced genomes, the development of systematic approaches for genetic code detection is essential for accurate downstream analyses. Three automated prediction tools exist so far: FACIL, GenDecoder and Bagheera; the last two respectively restricted to metazoan mitochondrial genomes and CUG reassignments in yeast nuclear genomes. These tools can only analyze a single genome at a time and are often not followed by a validation procedure, resulting in a high rate of false positives. Results: We present CoreTracker, a new algorithm for the inference of sense-to-sense codon reassignments. CoreTracker identifies potential codon reassignments in a set of related genomes, then uses statistical evaluations and a random forest classifier to predict those that are the most likely to be correct. Predicted reassignments are then validated through a phylogeny-aware step that evaluates the impact of the new genetic code on the protein alignment. Handling simultaneously a set of genomes in a phylogenetic framework, allows tracing back the evolution of each reassignment, which provides information on its underlying mechanism. Applied to metazoan and yeast genomes, CoreTracker significantly outperforms existing methods on both precision and sensitivity. Availability and implementation: CoreTracker is written in Python and available at https://github.com/UdeM-LBIT/CoreTracker. Contact: mabrouk@iro.umontreal.ca Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-01-26T20:04:37Z",
    "institutions": [
      "Universite ́ de Montre ́ al",
      "McGill University"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2015-04-02T15:14:59Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx421   CoreTracker: accurate codon reassignment prediction, applied to mitochondrial genomes     Emmanuel Noutahi  0    Virginie Calderon  0    Mathieu Blanchette  2    Franz B. Lang  1    Nadia El-Mabrouk  0    0  De ́ partement d'Informatique et de Recherche Ope ́ rationnelle (DIRO), Universite ́ de Montre ́ al ,  Montre ́ al, QC CP 6128 ,  Canada    1  De ́ partement de Biochimie, Centre Robert Cedergren, Universite ́ de Montre ́ al ,  Montre ́ al, QC CP 6128 ,  Canada    2  School of Computer Science, McGill University ,  McConnell Engineering Bldg., Montr e ́al, QC H3A 0E9 ,  Canada     2017   1  1  9   Motivation: Codon reassignments have been reported across all domains of life. With the increasing number of sequenced genomes, the development of systematic approaches for genetic code detection is essential for accurate downstream analyses. Three automated prediction tools exist so far: FACIL, GenDecoder and Bagheera; the last two respectively restricted to metazoan mitochondrial genomes and CUG reassignments in yeast nuclear genomes. These tools can only analyze a single genome at a time and are often not followed by a validation procedure, resulting in a high rate of false positives. Results: We present CoreTracker, a new algorithm for the inference of sense-to-sense codon reassignments. CoreTracker identifies potential codon reassignments in a set of related genomes, then uses statistical evaluations and a random forest classifier to predict those that are the most likely to be correct. Predicted reassignments are then validated through a phylogeny-aware step that evaluates the impact of the new genetic code on the protein alignment. Handling simultaneously a set of genomes in a phylogenetic framework, allows tracing back the evolution of each reassignment, which provides information on its underlying mechanism. Applied to metazoan and yeast genomes, CoreTracker significantly outperforms existing methods on both precision and sensitivity. Availability and implementation: CoreTracker is written in Python and available at https://github.com/UdeM-LBIT/CoreTracker. Contact: mabrouk@iro.umontreal.ca Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  The genetic code sets the rules for translating the genetic information from coding sequences (genes or mRNAs) into proteins. It was initially deciphered in the 1960s  (Nirenberg et al., 1963; So¨ ll et al., 1965) . Using a combination of in vitro and in vivo experiments conducted on E. coli, the complete set of 64 codons has been mapped to either amino acids or a translation termination signal. Long seen as universally conserved among all domains of life  (Crick, 1968) , the discovery that human and yeast mitochondria interpret the UGA stop codon as tryptophan  (Barrell et al., 1979; Fox, 1979)  has challenged the hypothesis of a 'frozen' universal code, revealing its evolvability. Since then, many instances of other codon reassignments have been reported across all three domains of life, in both organelle and nuclear genomes  (Keeling, 2016; Kollmar and M u¨hlhausen, 2017; Knight et al., 2001; Ling et al., 2015; Santos et al., 2004; Sengupta et al., 2007; Watanabe and Yokobori, 2011) .  The evolution of codon reassignment requires coordinated changes at the gene/RNA sequence level and the translation machinery in charge of recognizing and assigning a given codon to an amino acid. Two main, mutually non-exclusive mechanisms have been initially proposed for natural codon reassignment. According to the codon capture mechanism, the codon first disappears completely from the genome due to mutational pressure, followed by the complete disappearance of the corresponding tRNA or release factor  (Osawa and Jukes, 1989; Osawa et al., 1990) . Then, re-use of this codon by a different amino acid (AA) would emerge, in conjunction with the appearance of a corresponding tRNA and/or tRNA synthetase, with different specificities. This mechanism is present mainly in small mitochondrial genomes encoding small gene sets, for which the disappearance of a codon (leading to an unassigned codon), is feasible. The second mechanism, called ambiguous intermediate, does not require the disappearance of the codon during reassignment  (Schultz and Yarus, 1994) . Instead, it is ambiguously decoded either by a single tRNA recognized by different aminoacyl tRNA synthetases, or by two different tRNAs (or alternatively, a tRNA and a release factor). In the case of the CTG reassignment to serine in some yeast nuclear genomes, it has been hypothesized that a selective advantage could have arisen from a decoding ambiguity, gradually allowing for the reassignment of the codon  (Santos et al., 1999) . More recently, this hypothesis has been challenged by the discovery of CUG reassignment to alanine in Pachysolen tannophilus  (M u¨hlhausen and Kollmar, 2014a; M u¨hlhausen et al., 2016; Riley et al., 2016) , suggesting a different mechanism (tRNA loss driven codon reassignment) that could explain the polyphyly of the CUG codon usage in yeasts. Sengupta and Higgs have also proposed a classification through gain and loss scenarios  (Sengupta and Higgs, 2005)  which integrates the codon capture and ambiguous intermediate mechanisms, in addition to their unassigned codon and compensatory change scenarios.  Computational prediction of codon reassignments is straightforward when stop codons are involved, as proteins with either premature termination or multiple additional C-terminal domains will be predicted. Sense-to-sense codon identity changes are more difficult to infer, in distinction to ongoing mutations, and when an identity switch occurs among biochemically similar AAs. This motivates the development of appropriate bioinformatics methods. To our best knowledge, three tools based on comparative sequence analyses, have been developed to predict genetic codes. The GenDecoder web server  (Abascal et al., 2006a) , exclusively designed for metazoan mitochondrial genomes, infers codon reassignments by comparing translations of the standard protein-coding genes of a genome of interest to a set of pre-aligned reference profiles including 54 metazoans. Each codon of the input genome is assigned to the AA to which it is most frequently aligned. The second tool, FACIL  (Dutilh et al., 2011) , which is not specific to mitochondrial genomes, aligns the sequences of interest to PFAM protein domains, then uses Random Forests (RF) to infer the most probable AA-codon match. Finally, Bagheera  (M u¨hlhausen and Kollmar, 2014b)  is a web server for predicting CUG codons reassignment to serine in yeast nuclear genomes, based on the comparison of 38 cytoskeletal and motor proteins to a reference protein dataset. CUG reassignments are predicted by comparing CUG positions within the predicted genes to the reference dataset. The first two methods are restricted to predictions that apply to the codon capture mechanism, as they do not consider the possibility of ambiguous decoding of a codon to more than one AA  (Li et al., 2011; Swart et al., 2016; Yadavalli and Ibba, 2013) . Moreover, their predictions are not validated a posteriori by measuring the effect of predicted reassignments on the protein alignment quality. This lack of validation, plus a high sensitivity to substitutions between close AAs, usually leads to a high rate of false positives as we show in the result section. Bagheera, on the other hand, has a validation step based on tRNACAG identity prediction by comparing its sequence to a set of reference tRNAs. The main drawback of this method however is its limited scope as it concerns exclusively CUG codon reassignments to serine in yeast nuclear genomes.  Further, these methods are limited to the study of one genome at a time, completely ignoring its phylogenetic context (although Bagheera can perform an a posteriori phylogenetic grouping). In contrast, we argue that inferences based on the simultaneous study of multiple related genomes and their phylogenetic relatedness will lead to more sensitive predictions. Such an approach eliminates the dependency on an a priori reference set, thus allowing predictions in newly sequenced phylogenetic groups, and enabling the inclusion of non-standard proteins only shared by certain genomes. Furthermore, a phylogenetic framework can provide data for a better distinction between codon reassignments and ongoing mutations, and since codon reassignment is a progressive change, studying multiple genomes simultaneously will help identify footprints of ongoing reassignments. This innovative way of detecting codon reassignments can offer better insights toward the understanding of the underlying mechanisms of codon reassignments while systematically tracing back their evolutionary path.  Based on these ideas, we developed CoreTracker, a new algorithm exhaustively exploring sense-to-sense codon reassignments across any given group of genomes. It is the first automated approach for the prediction of codon reassignments that includes a phylogenetic framework and also extends to the context of ambiguous decoding of a codon to various amino acids.  Starting from a set of conserved positions in protein multiple alignments (derived by translating gene sequences with a given initial translation code) and a phylogenetic tree of the considered species, CoreTracker identifies candidate codons for reassignment, based on the recurring incidence of unexpected amino acids in conserved positions. Using a random forest classification approach, candidate codons are then evaluated according to a set of features related to various characteristics of the potential reassignment.  Although both use a random forest approach, CoreTracker and FACIL are significantly different. In contrast to FACIL, which is a complete parameter-free approach, CoreTracker has control over its level of precision and recall. In addition, CoreTracker integrates a correction using a similarity matrix accounting for frequent substitutions between close AAs, and a validation step, which evaluates the impact of a predicted reassignment on the alignment quality given a phylogenetic tree.  We applied CoreTracker to yeast and metazoan mitochondrial genomes and respectively predicted 54 and 85 codon reassignments. We were able to retrieve all known reassignment types and to extend them to newly analyzed genomes. On both datasets, CoreTracker achieved high precision and recall, outperforming FACIL and GenDecoder. We also compared CoreTracker to Bagheera and FACIL on a yeast nuclear dataset, on which CoreTracker also made accurate predictions for CUG codon reassignments and was even able to predict the CUG(Leu, Ala) in Pachysolen tannophilus missed by the other methods.    2 Materials and methods\r\n   2.1 Overview of CoreTracker\r\n  The algorithm steps are given below and illustrated on Figure 1 (see Supplementary Methods for more detailed information on each step). CoreTracker requires as input, groups of orthologous genes (nucleotide sequences) from the species of interest, and a corresponding phylogenetic tree. To identify orthologous positions, genes are first translated into proteins according to a preliminary, userdefined genetic code. They are then aligned and concatenated into a raw alignment, which is filtered by removing highly variable positions that may introduce noise. The obtained filtered alignment is used to identify potential codon reassignments. For each amino acid Xi an Xi-filtered alignment is obtained by keeping only columns in which Xi is the prevalent amino acid (Fig. 1A).  Denote by C Xi; Xj the reassignment of a codon C from Xi to a different amino acid Xj . A candidate genome G for such reassignment is identified by comparing the average and observed conservation of Xj; respectively computed from the filtered and Xj-filtered alignments. An amino acid Xj that is less conserved than average in G might reflect a different genetic code. In our example (Fig. 1B), average (l) and observed (lMet) conservation of methionine are significantly different in genomes G2 and G3. These genomes are candidates for the reassignment of isoleucine (which appear in the Met-filtered alignment) to methionine and are labeled as 'Met' on the tree (Fig. 1C). Fitch parsimony  (Fitch, 1971)  is then used on the species tree to trace back the history of each reassignment on the phylogeny. The analysis of the obtained history also helps recover the candidate genomes that would have been missed by steps A and B, due to an excess of filtering or a low amino acid usage.  On the other hand, an interesting reassignment from Xi to Xj in a genome G is selected according to codon usage. Codon usage is reported for all codons C encoding Xi in G (isoleucine in Fig. 1D) in two types of columns from the filtered alignment where Xi appears in G: I) columns prevalent in Xj (methionine in Fig. 1D), indicating a potential reassignment C Xi; Xj in G, and II) columns prevalent in Xi (isoleucine in Fig. 1D). In a 'perfect' dataset where C is fully reassigned from Xi to Xj, C would not be present anymore in Xi prevalent columns (type II columns). In practice, due to methodological issues (sequencing errors and alignment quality), ambiguous translation or mutations caused by true genetic divergence, the intersection between Xi and Xj prevalent columns is not expected to be empty. We use a generalized Fisher's exact test instead, to evaluate the difference in synonymous codon usage between the two columns. The P-values returned by this test are strong indicators for the identification of reassignments.  To quantify the reliability of each prediction, we implement a Random Forest (RF) approach (see Supplementary Methods). RF is a non-parametric classification algorithm  (Breiman, 2001)  which uses many classification trees in parallel. The features used here are described in Table 1. The RF was only trained on a set of 25 metazoan mitochondrial genomes (see Supplementary Table S1) extensively studied in the literature  (Knight et al., 2001; Sengupta et al., 2007; Swire et al., 2005) , and for which we can assume that almost all reassignments have correctly identified. In order to determine the most relevant features, we measure their importance according to the Gini impurity index  (Breiman et al., 1984)  (see Supplementary Fig. S1). From the ten selected features, the Fisher's exact test contributed the most to the predictive performance of the model. Surprisingly, the fraction of genes affected by the reassignment (Gene.fraction) and the distance to the closest reassigned node (Fitch) contributed the least.  Predictions made by the RF are then run through two validation steps that measure their impact on the proteome alignment (see Section 1.6 in Supplementary Information for more details). The first step validates the predictions per clade, while the second considers all genomes simultaneously. Both require a re-translation of gene sequences using the newly inferred genetic code. Since a reassignment shared by a whole clade is more likely to affect the same positions across all genomes in the clade, validating by clade ensures that randomly distributed sequence mutations are not inferred as codon reassignments. On the other hand, considering all genomes simultaneously reduces false predictions caused by clade-specific sequence mutations. For both validation steps, we expect an improvement of the similarity between sequences in the protein alignment for a genuine reassignment, of if the number of affected codons is significant (Fig. 1F). These validation tests are not relevant, however, for reassignments affecting too few positions in the genome.    2.2 Dataset of mitochondrial genomes\r\n  We annotated 104 genomes (see Supplementary Table S2) from a wild range of yeast mitochondrial genomes, using MFannot (http://megasun.bch.umontreal.ca/cgi-bin/mfannot/mfannotInterface.pl). The only sense-to-sense codon reassignments reported in the literature for these genomes involve codons CUN and AUA. The corresponding genetic code is number 3 in NCBI. As for the 40 metazoan mitochondrial genomes considered in our study (Supplementary Table S2), several genetic codes (2, 4, 5, 9 and 13 as referred in NCBI) are required for translation. As all these genetic codes are derived from the genetic code 4, we used it to translate the fourteen mtDNA-encoded protein (Cob, Cox1-2-3, Atp6-8-9 and Nad1-2-34-4L-5-6) from both the yeast and metazoan mitochondrial genomes and, when applicable, we correct frame-shifts.    2.3 Phylogenies of metazoan and yeast species\r\n  The yeast phylogeny was constructed using thirteen standard mtDNA-encoded derived protein sequences (Cob, Cox1-2-3, Atp6-9 and Nad1-2-3-4-4L-5-6). The alignment was done in two steps. Sequences were first pre-aligned with Muscle  (Edgar, 2004)  and then refined with hmmalign from the HMMER package  (Eddy, 2001) . Alignments were then concatenated, resulting in a dataset that includes 104 species and has 5812 amino acid positions. The phylogenetic analysis was performed with PhyloBayes  (Lartillot et al., 2009) , using the CAT/GTR model, six discrete categories, four independent chains, 6000 cycles and the -dc parameter to remove constant sites. The first 1000 cycles were discarded as burn-in.  The metazoan phylogeny is based on the phylogeny of Figure 4 from  Sengupta et al. (2007) . New genomes were added, while maintaining the relationships between groups  (Adoutte et al., 2000; Halanych, 2004) .    2.4 Comparison of CoreTracker, FACIL and GenDecoder predictions on mitochondrial dataset\r\n  We compared CoreTracker, FACIL and GenDecoder predictions on the metazoan dataset. As GenDecoder is restricted to metazoan, only CoreTracker and FACIL were compared on the yeast dataset. We ran CoreTracker with default parameters and without the HMM alignment refinement step. FACIL was iteratively run on each genome in the dataset. A python script was written to query and retrieve the genetic code of each genome from the GenDecoder webserver. Since GenDecoder uses only sequence comparison to predict the genetic code, we used parameters slightly more stringent than the default (metazoan reference dataset, filtering out columns with more than 20% gap and keeping only 'highly conserved S &lt; 1.0' sites according to the Shannon entropy) in order to increase precision. For all three programs, genetic code 4 was set as reference code. Non-determined predictions (marked by a '?' or '-' for GenDecoder or an 'X' for FACIL) were discarded. We kept GenDecoder's unreliable predictions (reported in lower cases) when comparing with CoreTracker and FACIL, as a sizeable proportion of its non-reassigned codons (305/2072) were reported as unreliable. This information on non-reassigned codons is hidden if we remove lower-case predictions, due to precision and recall being computed according to predicted codon reassignments only.  Due to the lack of a gold standard dataset for codon reassignments (even the NCBI annotations cannot be trusted), an initial step was to build a composite reference standard for comparison. For this purpose, literature reviews on codon reassignments in yeast and metazoan, information on species phylogenetic positions (for genomes with no reported predictions but located in a clade affected by a particular codon reassignment) and predictions shared by the three methods were considered to establish a list of true positives consisting of 90 codon reassignments of seven types in metazoans and 72 of six types in yeasts (see Supplementary Table S3). Contradictories and ambiguous cases were discarded as well as expected reassignments in genomes avoiding the codon. Since some of the genomes in the metazoan dataset were used in the training set for CoreTracker, we also assessed performance when those genomes were excluded.     3 Results\r\n  We applied CoreTracker, using default parameters, to 40 metazoan (including the 25 used as the RF training set) and 104 yeast mitochondrial genomes (see Supplementary Table S2 for a list of the genomes used, and Table S4 for predicted reassignments and each feature value). CoreTracker was also applied to 23 nuclear yeast genomes. Results on the nuclear genomes are reported and discussed in section 2.1 of the Supplementary Material (see Supplementary Table S5 and Fig. S3).   3.1 Predicted codon reassignments in metazoan mitochondrial genomes\r\n  In metazoan mitochondrial genomes, CoreTracker predicted and validated 85 potential sense-to-sense codon reassignments of eight different types (see Fig. 2), among them the seven described in the literature: AUA(Ile, Met), AGG-AGA(Arg, Ser), AGG-AGA(Arg, Gly), AAA(Lys, Asn) and AGG(Arg, Lys). The new prediction is AUA(Ile, Leu), which was predicted in five genomes (Euseius, Strigamia, Lumbricus, Epigonichthys and Ciona).  We were able to extend the known codon reassignments to 13 new genomes. For example, the AUA(Ile, Met) reassignment has been reported in the literature for 14 genomes  (see Fig. 4 of Sengupta et al., 2007) . Here, the prediction was extended to five more genomes located in monophyletic groups where the reassignment has already been detected.  Regarding AGR(Arg, Ser) and AGR(Arg, Gly) reassignments, most of them were correctly inferred and extended to the remaining genomes of the considered taxonomic group. Two additional genomes, Aphrocallistes vastus and Metaseiulus occidentalis, belonging to different monophyletic groups, were predicted to both reassign AGR codons to serine and glycine. CoreTracker also predicted AGA(Arg, Ser) in Strigamia maritima, while the synonymous arginine codon AGG was predicted to be reassigned to lysine. This pattern of reassignment has previously been observed in Limulus polyphemus and some other arthropods  (Abascal, et al., 2006b)  highlighting the fact that synonymous codons can be independently reassigned to different amino acids.  As for AAA(Lys, Asn), the four previously known reassignments were retrieved and extended to eight of the nine analyzed Platyhelminthes genomes. This reassignment was not predicted in Echinococcus equinus since its AAA codon usage is very low, and the codon is absent in asparagine predominant columns (column I). In this genome and also in Haplorchis taichui, AAA(Lys, Ser) was predicted but not validated (not shown) because too few codons were involved. A closer look at the alignment showed that in the positions affected by AAA(Lys, Ser), asparagine codons were found in other closely related Platyhelminthes genomes. This further supports AAA(Lys, Asn) reassignment in Platyhelminthes, under the hypothesis of an ancestral substitution from serine to asparagine in some positions.  The new identified reassignment type AUA(Ile, Leu), although supported by the validation steps, remains questionable for various reasons. Aside from the fact that the two amino acids are highly similar and frequently substituted each other, the reassignment concerns five genomes that are spread apart in the phylogeny and already predicted to have reassigned AUA to methionine. Furthermore, according to the validation steps, AUA(Ile, Met) seems to improve the alignment more than AUA(Ile, Leu) (P-value of 1.54e-10 versus 4.06e-04). A closer look at positions affected by AUA(Ile, Leu) in the alignment shows that leucine's CUA and UUA codons are used in related genomes, suggesting sequence substitutions at the nucleotide level rather than codon reassignment. In some of these positions, we also found valine and methionine in a few genes (Cox1, Cob and Nad3). These mutations mostly occur in transmembrane domains where substitutions between hydrophobic residues is tolerated  (Liu et al., 2002; McClellan and McCracken, 2001) , further supporting nucleotide substitutions over reassignment. However, a possible decoding of AUA codons as leucine cannot be ruled out completely without biochemical evidence.  Notice that few known AGR(Arg, Ser) and one AUA(Ile, Met) reassignments were missed. As shown in Figure 2, these missed predictions coincide with low codon usage in corresponding genomes. It is possible that an excess of filtering plus a very low usage of these codons in conserved positions conceal the reassignment signal.    3.2 Predicted codon reassignments in yeast mitochondrial genomes\r\n  In yeast mitochondrial genomes, CoreTracker predicted and validated 54 codons reassignments of seven types (see Fig. 3). These types include known CUA-CUU-CUG(Leu, Thr), AUA(Ile, Met) and CUA-CUU(Leu, Ala) reassignments and a new AUA (Ile, Leu) reassignment.  Among the 54 inferred reassignments, ambiguity only lies for the translation of the codon AUA to either methionine or leucine in the two Ashbya gossypii species. As in the metazoan dataset, AUA(Ile, Leu) is most likely a false positive.  In yeast mitochondrial genomes, CUN codon reassignments from leucine to threonine were previously detected in Saccharomycetaceae mitochondrial genomes  (Osawa et al., 1990; Su et al., 2011) . CoreTracker predicted this reassignment for 23 of the 104 analyzed genomes. The predicted least common ancestor of all genomes affected by this reassignment is shown on the phylogeny (Fig. 3). The corresponding monophyletic group involves 26 genomes. This subgroup also contains two genomes, Kluyveromyces lactis and Kluyveromyces marxianus, where CUN(Leu, Thr) was not predicted, due to their complete lack of CUN codons in the 13 mitochondrial genes we considered.  CUU and CUA codons have been reported to be reassigned to alanine in Ashbya gossypii ATCC10895  (Ling et al., 2014) . Our dataset includes two additional Ashbya genomes: Ashbya gossypii FDAG1 and Ashbya aceri. Both CUU and CUA codons are predicted to be reassigned to alanine with a high probability (0.99), in all Ashbya species. Since CUC and CUG codons are avoided in most yeast mitochondrial genomes, they were often not found after filtering the alignment and were therefore rarely predicted to be reassigned. In the literature, the AUA codon was reported reassigned from isoleucine to methionine in Saccharomyces and A. gossypii mitochondrial genomes  (Miranda et al., 2006) . Our predictions are in agreement with these results. According to the ancestral state reconstruction, such a reassignment appears to have occurred twice during the evolution of two Saccharomycetaceae monophyletic lineages (indicated by a green circle on Fig. 3). However, the low codon usage in Saccharomycetaceae genomes outside Saccharomyces and Ashbya clades (for example Lachancea), suggest that a single event may have affected the whole group leading eventually to AUA reassignment in certain subgroups.  Finally, as in metazoans, some expected reassignments are missed by CoreTracker due to a low codon usage in conserved coding regions. In particular, the avoidance of CUC, CUG and even CUU codons in most Saccharomycetaceae make the prediction and validation of their reassignments difficult.    3.3 Efficiency of CoreTracker compared with FACIL and\r\n    GenDecoder\r\n  We compared CoreTracker to GenDecoder and FACIL on metazoan and yeast mitochondrial datasets (Table 2). The latter algorithm is more comparable to CoreTracker, as it is not restricted to any particular phylum or type of genome. The three methods were compared on a manually curated composite reference dataset (see Section 2.4 in Methods), in terms of precision, recall and F-score (harmonic mean of precision and recall). This dataset contains 90 codon reassignments of seven types in metazoans and 72 of six types in yeasts (see Supplementary Table S3).  On the metazoan mitochondrial dataset, CoreTracker achieved the highest precision (89.4%) and F1-score (86.9%), while FACIL, the closest in terms of precision has a F1-score of only 69.9%. This high precision was also achieved when genomes from CoreTracker's training set were removed. Only GenDecoder was slightly more sensitive than CoreTracker (87.8% versus 84.4%), but it had a lower precision. By removing GenDecoder's unreliable predictions (reported in lower-cases), it's precision jumps from 54.9% to 89.3%, at a cost of a decrease in sensitivity (74.4%), making it the second best tool according to the F1-Score (81.2%) (see Section 2.4 for a discussion on keeping or removing GenDecoder's lower cases). It is noteworthy that CoreTracker can achieve better recall by decreasing  CoreTracker performs better than the other methods. For direct comparison, the numbers of true positives (TP), false positives (FP) and false negatives (FN) are also provided. the stringency of the alignment filtering parameters. As for the yeast mitochondrial dataset, CoreTracker significantly outperformed FACIL by displaying precision as high as 96.3% and a recall of 72.2%, compared to FACIL with a precision of only 25.6% and a recall of 47.2%. A closer look at the two programs' predictions on the yeast dataset showed that they both have difficulties predicting reassignments involving codons with extremely low usage, such as CUU(Leu, Thr) and CUG(Leu, Thr) in Saccharomycetaceae, which explains the low recall. However, CoreTracker was able to detect the CUG(Leu, Thr) reassignments with only one codon count in L. nothofagi that FACIL missed (see Discussion). FACIL also failed to predict most AUA(Ile, Met) reassignments, whereas CoreTracker was able to predict them in 9 genomes.  In addition, the alternative comparison of Supplementary Figure S2, show that GenDecoder and FACIL predicted several unlikely codon reassignments types. In particular, GenDecoder made 48 unheard-of codon reassignment prediction types, not shared with any of the two other methods, and never reported in the literature.  Although the three methods have difficulty to distinguish codon reassignments from substitutions between amino acids with similar properties due to neutral evolution, this effect is more noticeable in GenDecoder's and FACIL's prediction.     4 Discussion\r\n  CoreTracker is the first automated tool developed for codon reassignment prediction that uses a phylogenetic context. It is a generic method that allows exploring a large number of genomes from any taxonomic group, with distinct reassignment scenarios. Application to mitochondrial and nuclear genomes highlights its ability to efficiently predict known sense-to-sense reassignments.   4.1 Reassignments in metazoan and yeast mitochondrial genomes\r\n  In metazoan genomes, CoreTracker correctly predicted the five previously known and well characterized sense-to-sense reassignment types, and was able to extend them to newly analyzed genomes. Here, we discuss the unexpected AGR(Arg, Ser) and AGR(Arg, Gly) simultaneous predictions in the hexactinellid sponge Aphrocallistes vastus and in the arachnid Metaseiulus occidentalis. It has been previously suggested  (Sengupta et al., 2007)  that AGR(Arg, Gly) is caused by the gain of a new tRNAUGClyU after the reassignment of AGR(Arg, Ser). This tRNA then outcompetes tRNASGeCrU to decode AGR codons, leading eventually to AGR(Arg, Gly). However, it is also known that in several invertebrate groups, the G base of tRNASGeCrU is mutated into a U, resulting in tRNASUeCrU which is also able to decode AGR codons, sometimes with better affinity  (Abascal, et al., 2006b) .  Previous studies have revealed the presence of tRNASUeCrU in the mitochondrial genome of Aphrocallistes  (Haen et al., 2007; Rosengarten et al., 2008) . This tRNA has also been previously predicted in Metaseiulus occidentalis  (Jeyaprakash and Hoy, 2007) . The two genomes also have only one tRNAAUrCgG and a single tRNAUGClyC, supporting further the decoding of AGR codons as serine by tRNASUeCrU. Considering these results and the fact that only a single point mutation from G to A is required to transform glycine's GGR codons into AGR codons (which can occur due to sequencing errors or sequence mutation), it can be argued that the AGR(Arg, Gly) prediction in A.vastus and M.occidentalis are most likely false positives. However, in A. vastus, according to CoreTracker's output, while AGR codons are completely avoided in arginineconserved positions, AGA was found in 14 conserved serine positions compared to 16 conserved glycine positions and AGG in three glycine conserved positions. In most of those glycine positions (particularly abundant in Cox1 and Cob gene), GGN codons were found in other sponges, while the three tunicates known for AGR(Arg, Gly) used AGR codons. Moreover, AGG(Arg, Gly) was also predicted in A. vastus and M. occidentalis by GenDecoder. Further investigation through tRNA phylogenetic analysis and enzymatic study, which is beyond the scope of this paper, is thus needed.  In yeast mitochondrial genomes, CoreTracker confirmed and extended known CUA-CUU(Leu, Ala) in the Ashbya mitochondrial genome and CUN(Leu, Thr) in Saccharomycetaceae monophyletic group except K.lactis and K.marxianus in which CUN codons are absent. CUN reassignments have been extensively studied, and it has been shown that reassigned CUN codons are decoded by a tRNAUAG that emerges from the duplication of tRNAGHUisG then further diverges into two distinct identities to decode CUN codons as either threonine or alanine. It is believed that this reassignment is preceded by the disappearance of CUN codons, as illustrated on Figure 3 by their absence in Kluyveromyces and their low usage in other Saccharomycetaceae genomes. Although CoreTracker was able to predict most CUU and CUA reassignments, reassignments involving CUG and CUC were harder to predict, due to their extremely low usage. This low usage was expected however, since yeast mitochondrial genomes are AT-rich genomes with strong mutation pressure toward A and U. In S. cerevisiae YJM993 and L. nothofagi where CUG codons appear only once, CoreTracker predicted CUG (Leu, Thr) with strong support. This reassignment was also predicted by FACIL in S. cerevisiae YJM993 but missed in L. nothofagi. From CoreTracker's output, it can be observed that the leucine codon usage difference in leucine and threonine conserved columns is extremely high (P-value of 3.41e-10 in S. cerevisiae and 1.57e-07 in L. nothofagi). In fact UUA was almost the only codon used in leucine conserved positions, while CUA and the only CUG were found in highly conserved threonine positions. As synonymous codon usage is one of the most important feature of the RF model, this weight a lot. Furthermore, in both genomes, the CUG codon appeared in an extremely conserved threonine positions with only a few S. cerevisiae using CUA while all the remaining genomes used threonine's ACA or ACU codons. Therefore, the clade validation test for CUG was successful. The second validation test was also successful since it simultaneously consider all synonymous codons predicted reassigned to the same amino acid, thereby CUG was validated alongside the more frequent CUA and CUU codons. If this second validation test were to be performed separately for each codon, there would not be enough positions for the improvement in the alignment quality to be significant (P-value of 2.57e-01) for CUG (Leu, Thr), and the reassignment would have failed the validation test. Note that CoreTracker has a parameter to set the minimum occurrence of a codon required before prediction, which default value is one.  As for AUA (Ile, Met), it was inferred in both Ashbya and Saccharomyces. This reassignment was previously reported to be linked to the loss of the tRNAIle  CAU followed by a gain of function in the tRNACMAeUt which is then able to decode both AUG and AUA codons  (Sengupta et al., 2007) . Such hypothesis requires the avoidance of AUA codons at one point of the reassignment process. As shown on Figure 3, AUA codons are effectively either completely absent or avoided in other Saccharomycetaceae, suggesting that AUA(Ile, Met) in Ashbya and Saccharomyces is initiated by the loss of the gene encoding tRNAIle  CAU in an ancestral Saccharomycetaceae genome.    4.2 Limitations, flexibility and possible extensions\r\n  Although CoreTracker was able to detect some reassignments of weakly used codons and reassignments occurring in single genomes (such as AGG(Arg, Lys) in Strigamia on Fig. 2), codon usage remains a limitation of the method. In fact, in the validation step, measuring the impact of the new genetic code on the proteome is informative only if the new genetic code affects enough positions to significantly alter the alignment quality. To overcome this limitation, prior knowledge on functional domains may be used to attribute different weights to reassigned positions according to their location in the gene. In our study, we checked the location of the predicted reassignments according to PFAM domains. However, as almost all predicted reassignments were in such domains, this step did not offer any significant filtering advantage. Alternatively, using more proteins when available, as it is the case for nuclear genomes, can help reduce the effect of codon usage limitation. However, the trade-off will be the increase in running time needed to analyze this larger dataset.  As input protein sequences are obtained from the translation of annotated genes, CoreTracker solely predicts sense-to-sense reassignments. Although this can be seen as a limitation, reassignments involving a stop codon are easily predicted by most existing annotation tools. In fact, missing C-terminal domains or proteins with abnormally long or short length, compared to others in the same family, are strong indicator of stop-to-sense and sense-to-stop codon reassignments. By default, CoreTracker also removes from the input dataset, genes with frame-shifts. Since this might not be the desired action, we provide tools to help identify and correct frame-shifts.  In order to measure the performance of CoreTracker, we evaluated its precision and sensitivity compared to GenDecoder and FACIL on mitochondrial genomes. In contrast with CoreTracker, neither GenDecoder, nor FACIL use a phylogenetic framework. Instead, each genome is analyzed independently. Applying the three algorithms on metazoan and yeast mitochondrial genomes, CoreTracker performed better than both GenDecoder and FACIL. Indeed, although GenDecoder displayed a slightly higher sensitivity than CoreTracker, both GenDecoder and FACIL demonstrated lower precision. As GenDecoder and FACIL are based on a preestablished reference datasets that do not necessarily allow an appropriate taxon sampling, both algorithms are highly sensitive to amino acid substitutions, which explains in large part their lack of precision. CoreTracker addresses this issue by offering a wide range of control over its precision, and built-in steps that ensure accuracy. Aside from the possibility to perform appropriate taxa sampling, facilitated by the provision of dataset merging tools, the added validation steps help filtering out unreliable predictions. For instance, on the metazoan dataset, 97 predictions were originally made but only 85 were validated, increasing precision by 11.06%. As CoreTracker is user oriented, it outputs several additional information (both visual and statistical) on each prediction, and even on some nonpredicted reassignments in order to help users decide if a predicted (or non-predicted) reassignment should be further inquired into.  CoreTracker's efficiency on nuclear genomes was also assessed by comparing its predictions on a yeast nuclear dataset to Bagheera's and FACIL's. On this dataset, CoreTracker accurately predicted CUG codon reassignments, missing only CUG (Leu, Ser) in L.elongisporus (see Supplementary Fig. S3). More importantly, it was the only method able to detect the CUG(Leu, Ala) in Pachysolen tannophilus. This application to nuclear genomes demonstrates its consistent high accuracy and its wide range of applicability.  It is worth noticing that CoreTracker's predictions are strongly dependent upon the sequence alignment. However, as shown on Supplementary Figure S4, the method is robust even towards unrealistic high rate of errors in the alignment. Nevertheless, we provide a default alignment pipeline (see methods section) that use HMMs to refine alignments. Although CoreTracker does not require the full resolution of the input phylogenetic tree (as shown by the metazoan phylogenetic tree used) predictions depend on the considered phylogenetic tree. In particular, a reassignment affecting a whole clade will only be predicted if a genome outside this clade and missing the reassignment is included in the analysis.  One useful information that was not included in CoreTracker is the analysis of tRNAs, given that codon reassignments are often linked to changes in tRNAs. Analyzing tRNA is particularly useful when a set of reference tRNAs that have changed identity is available. In this case, it will be possible to predict reassignments by comparing the predicted tRNA of a query genome to the reference tRNA set, as done by Bagheera. This approach is suitable when predictions are restricted to a specific codon reassignment but less applicable when all potential codon reassignments are being considered as it is the case for CoreTracker. In this latter scenario, analysis of the full tRNA repertoire is necessary. This requires both a complete characterization of the tRNA identity determinants, and a reliable tRNA phylogenetic tree. Unfortunately, these information are often not available  (Giege´ and Frugier, 2003; Rogers and Griffiths-Jones, 2014)  or difficult to reliably obtain without human intervention. Furthermore, tRNAs are not the only components involved in codon reassignments, since mutations in aminoacyl tRNA synthetases can also potentially lead to codon reassignments. Therefore, despite being linked, there is no necessary a one-to-one correspondence between tRNA evolution and codon reassignments. Although tRNA analysis could not be automatically included in CoreTracker, it still represents an important validation test that should be done when possible.  Future extensions of CoreTracker will involve accounting for branch length variation in the ancestral reconstruction step and the random forest as well. Other changes at the mRNA level that could be confounded with codon reassignments, such as RNA editing, will also be taken into account, in order to make CoreTracker a truly universal tool for codon reassignment predictions. Possible ways to achieve distinction between RNA editing, codon reassignment and artifact due to amino acid substitution, include consideration of relative codon usage between genomes, codon substitution models and more importantly tRNA phylogenetic analysis.     Acknowledgements\r\n  We thank G. Thauvette for helpful discussions. We are grateful to J. Nosek for providing his comprehensive collection of yeast mitochondrial genomes and to three anonymous reviewers for their comments and suggestions that greatly improved the article.    Funding\r\n  This work was supported by \u201cFonds de recherche du Que´bec - Nature et technologies\u201d (FRQNT).  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/UdeM-LBIT/CoreTracker",
    "publicationDate": "0",
    "authors": [
      "Emmanuel Noutahi",
      "Virginie Calderon",
      "Mathieu Blanchette",
      "Franz B. Lang",
      "Nadia El-Mabrouk"
    ],
    "status": "Success",
    "toolName": "CoreTracker",
    "homepage": "http://udem-lbit.github.io/CoreTracker/"
  },
  "8.pdf": {
    "forks": 0,
    "URLs": ["github.com/tsudalab/MDTS"],
    "contactInfo": [],
    "subscribers": 5,
    "programmingLanguage": "Python",
    "shortDescription": "Materials Design by Monte Carlo Tree Search",
    "publicationTitle": "MDTS: automatic complex materials design using Monte Carlo tree search",
    "title": "MDTS: automatic complex materials design using Monte Carlo tree search",
    "publicationDOI": "10.1080/14686996.2017.1344083",
    "codeSize": 176,
    "publicationAbstract": "",
    "dateUpdated": "2017-07-25T13:00:50Z",
    "institutions": [],
    "license": "No License",
    "dateCreated": "2017-03-22T04:03:56Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Sci. Technol. Adv. Mater.   1468-6996    10.1080/14686996.2017.1344083   MDTS: automatic complex materials design using Monte Carlo tree search     Thaer M. Dieb    Shenghong Ju    Kazuki Yoshizoe    Zhufeng Hou    Junichiro Shiomi    Koji Tsuda    The Author    . Published by Informa UK Limited    trading as Taylor    Francis Group     2017   18  2017  498  503      -\r\n  View supplementary material Submit your article to this journal    View related articles\r\n    View Crossmark data\r\n  7 1 0 2 t s u g u A 9 2 0 3 : 0 1 t a ] y r a r b i L A L C U [ y b d e d a o l n w o D OPEN ACCESS MDTS: automatic complex materials design using Monte Carlo tree search aNational Institute for Materials Science, Tsukuba, Japan; bGraduate School of Frontier Sciences, The University of Tokyo, Kashiwa, Japan; cDepartment of Mechanical Engineering, The University of Tokyo, Tokyo, Japan; dRIKEN, Center for Advanced Intelligence Project, Tokyo, Japan   ABSTRACT\r\n  Complex materials design is often represented as a black-box combinatorial optimization problem. In this paper, we present a novel python library called MDTS (Materials Design using Tree Search). Our algorithm employs a Monte Carlo tree search approach, which has shown exceptional performance in computer Go game. Unlike evolutionary algorithms that require user intervention to set parameters appropriately, MDTS has no tuning parameters and works autonomously in various problems. In comparison to a Bayesian optimization package, our algorithm showed competitive search efficiency and superior scalability. We succeeded in designing large SiliconGermanium (Si-Ge) alloy structures that Bayesian optimization could not deal with due to excessive computational cost. MDTS is available at https://github.com/tsudalab/MDTS.    CLASSIFICATION\r\n  60 New topics/Others; 600 Others: Materials informatics   1. Introduction\r\n  Complex materials design is a key topic in materials science and engineering. The design of a complex materials' structure that meets certain criteria is often formulated as the problem of finding the optimal solution from a space of candidates [ 1,2 ]. A common problem in solid-state materials design is the structure determination of a substitutional alloys problem [ 3,4 ], where atoms or vacancies are assigned to positions in a crystal structure. For example, Ju et al. [ 4 ] recently solved the optimal assignments of Silicon (Si) and Germanium (Ge) to a certain crystal structure that achieves minimum and maximum thermal conductance.  To accelerate the materials design process, several experimental design algorithms have been used to find the optimal structure with as few experiments as possible (Figure 1). Experimental design is an iterative process for selecting the next candidates for experiments, where the outcome of the experiments are exploited for making further choices. In many cases, simulators are substituted to experiments, e.g. first-principle calculations. In earlier studies, quantitative structureproperty relationship (QSAR) models were mainly used [ 5 ]. Recently, Bayesian optimization [ 6 ], a technique to select promising candidates using Bayesian learning, has been proven as an effective tool in materials design [ 1,2,4,7-9 ]. The difference between Bayesian optimization methods and traditional QSAR models is that the uncertainty of prediction is quantified as predictive variance: the candidates are scored by an acquisition function that takes into account both predicted merit and uncertainty. Bayesian optimization is very effective in finding optimal structures but has problems with scalability, as the acquisition function has to be applied to all candidates. Evolutionary algorithms such as genetic algorithms [ 10,11 ] are more scalable, but have many parameters, such as crossover and mutation rates, that must be tuned properly to obtain the best 7 1 0 2 t s u g u A 9 2 0 3 : 0 1 t a ] y r a r b i L A L C U [ y b d e d a o l n w o D performance. In most cases, in materials design, the amount of data available a priori is very limited, so tuning parameters using data may not be possible.  In this paper, we propose a novel python library called Materials Design using Tree Search (MDTS). MDTS solves structure determination of substitutional alloys with composition constraints using a Monte Carlo tree search [ 12 ], a guided-random best-first search method that showed significant success in computer Go [ 12,13 ]. Our library is highly scalable and does not have any tuning parameter.  In experiments, we applied MDTS and an efficient Bayesian optimization implementation [ 7 ] to a Si-Ge alloy interface design between two Si leads [ 4 ]. The local force field (bonding characteristics) in the structure can change due to substitution. However, in this demonstration case, we did not consider structure relaxation because the force constants of Si and Ge are known to be transferable [ 14 ]. On the other hand, there are ways to include the change in the local force constants and the current method can be simply used to incorporate such an effect [ 15 ]. The total computational time is decomposed into design time and simulation time. The former represents the selection of the next candidates and the latter simulator time. In terms of the number of calculations to find the optimal solution, Bayesian optimization was better due to its high prediction ability. However, MDTS was comparable or better in terms of total computational time, because Bayesian optimization takes exponential design time with respect to the number of atoms. MDTS is a practical tool that material scientists can easily deploy in their own problems and has the potential to become a standard choice.    2. Method\r\n  Consider a black-box function, f (x), where x is a vector of discrete variables x ∈ {0, 1, k − 1}N . We aim to find the optimal solution x∗ that maximizes f (x) subject to composition constraints  N =1  I(x = j) = nj, j = 0, . . . , k − 1 (1) where I is the indicator function that returns one if the given condition is satisfied and zero otherwise. The constant ni indicates the number of variables with value i. Notice that jk=−01 nj = N . In an atom assignment problem, x corresponds to atom types and f (x) is a target property evaluated, for example, through firstprinciples calculations.  Monte Carlo tree search (MCTS) employs a search tree, where nodes at the th level correspond to value assignment to x (Figure 2). A path from the root to a node at level corresponds to a partial solution with respect to x1, . . . , x . In the first round of MCTS, only the root node exists and then the search tree is gradually constructed. To obtain a full solution x, a complete path to a leaf node at the N th level is necessary. One interesting feature of MCTS is that only a shallow tree is built and the complete paths are obtained via random playouts [ 12 ]. A 'playout' creates a solution by starting from a node and determining the remaining variables randomly. The random playout allows us to explore a large candidates space without learning from data. Once a solution has been obtained by a playout, the black-box function f (x) is evaluated and recorded. By combining tree expansion, backtracking and playouts, a large candidate space can be searched systematically. When a predetermined number of calculations is reached, the best solution so far is returned as the final result.  Each node i contains three variables: the visit count vi represents the number of visits in the search process; fi denotes the immediate merit of node i evaluated by playout; and the cumulative merit wi is defined as the sum of all direct merit for all descendant nodes including itself. The Upper Confidence Bound (UCB) score [ 12 ] of a node is an index indicating how promising it is to explore the subtree under the node. It is defined based on the cumulative merit and the number of visits as follows:  wi ui = vi + C 2 ln vparent vi where C is the constant to balance exploration and exploitation and vparent is the visit count of the parent node. Whenever a new node is added, the variables are initialized as  vi = wi = fi = 0, ui = ∞  Each round of MCTS consists of: selection, expansion, simulation and back propagation (Figure 2). In the selection step, the tree is traversed from the root to a leaf by choosing the child with the maximum UCB score at each branch. If there is a tie, the winning child is chosen randomly. Let i denotes the identified leaf, the level of the node i, x1, . . . , x the partial solution corresponding to the path from the root to i. In the expansion step, children nodes are added under the node i. If the number of atoms j reaches the limit (2) (3) 7 1 0 2 t s u g u A 9 2 0 3 : 0 1 t a ] y r a r b i L A L C U [ y b d e d a o l n w o D already, i.e. l=1 I(xl = j) = nj the jth child is not added. In the simulation step, a playout is performed from each of the added children. Notice that the random assignments are made such that the composition constraints are satisfied. With the solutions obtained, a simulator is applied to evaluate f (x) and store the value as the immediate merit of the corresponding nodes. Finally, in the back propagation step, the visit count of each ancestor node of i is incremented by one and the cumulative value is also updated to keep consistency.  The value of C crucially affects the performance of MDTS. According to the analysis by Kocsis and Szepesvári [ 16 ], to guarantee the convergence to the optimal solution, C should be proportional to the range between zmax and zmin, i.e. the maximum and minimum immediate merit observed in downstream nodes. Adjusting C, either statically or dynamically, is a standard technique for applying MCTS (as shown in [ 12 ]). Following a similar idea, MDTS controls C adaptively at each node as follows:  C = √2J 4 (zmax − zmin) (4) where J is a meta-parameter initially set to one and increased whenever the algorithm encounters a 'deadend' leaf, to allow more exploration. At a dead-end leaf, the number of possible structures narrows to one. This happens when the numbers of k − 1 atoms reaches the limit. J is updated as J ← J + max{ TT−t , 0.1}, where T is the total number of candidates to be evaluated and t is the number of candidates for which the black-box function is evaluated. See supplemental material for the algorithm.    3. Experiments and results\r\n  In this section, we compare MDTS to a Bayesian optimization package called COMBO [ 7 ] in a binary atom assignment problem (notice that MDTS is able to handle multiple atom types assignment problems). The performance of MDTS depends on the variable ordering in x. The following three options were tried: direct (left-to-right), reversed (right-to-left) and random.  MDTS and COMBO were applied to design optimal Si-Ge alloy (Si:Ge=1:1) interfacial structures (Figure 3) with both minimum and maximum thermal conductance [ 4 ]. Materials with both minimum (e.g. thermoelectric materials) and maximum (e.g. CPU cooling) interfacial thermal conductance have potential applications. As shown in Figure 3, the system consists of an interface region between two Si leads with infinite thickness. In the interface, there are N positions filled either by Si or Ge. The number of atoms of each type is constrained to N /2. The number of possible structures grows rapidly as the number of atoms N increases. For example, at 14, 20 and 26 atoms, the number of possible structures is 3432, 184,756 and 10,400,600, 7 1 0 2 t s u g u A 9 2 0 3 : 0 1 t a ] y r a r b i L A L C U [ y b d e d a o l n w o D respectively. The thermal conductance was computed using the atomistic Green's function implemented in the ATK-Classical Simulator of Atomistix ToolKit (ATK) [17,18]. SiGe Tersoff [19,20] potential was used to describe the atom interactions. The size of the supercell in the transverse direction (perpendicular to the direction of heat conduction) is 1 unit cell, i.e. 5.43 Å × 5.43 Å, and periodic boundary conditions were used. See Ref. [ 4 ] for further details.  Since the process of simulation-based structure optimization involves an experimental design algorithm and a simulation algorithm, the total computational time is divided into two parts: design time and simulation time. The design time per structure against the 7 1 0 2 t s u g u A 9 2 0 3 : 0 1 t a ] y r a r b i L A L C U [ y b d e d a o l n w o D number of atoms is shown in Figure 4(a). Bayesian optimization shows an exponential increase in design time, because it needs to compute a score for every candidate structure. On the other hand, the design in MDTS takes only a tree traversal, whose computational cost is scarcely affected by the number of atoms. Figure 4(b) shows the fraction of optimal structure discovery over 100 runs (i.e. success rate) for both minimum and maximum thermal conductance against the number of thermal conductance calculations at N = 16. Bayesian optimization required a smaller number of calculations to achieve the same level of success rate due to its sophisticated prediction algorithm. Nevertheless, the performance of MDTS was better than random search, indicating its substantial capability of learning from data. Among the three variable orderings of MDTS, the reversed order was best. Random order performance was lowest in this particular case, likely because the existence of neighbourhood relations may be crucial for the optimal thermal conductance. Despite better learning capability, the advantage of Bayesian optimization in total computational time is rapidly wiped out, as N increases, because of the exponentially increasing design time. At N = 22, the speed of thermal conductance minimization and maximization of MDTS and Bayesian optimization is comparable as shown in Figure 4(c). At N = 26, however, Bayesian optimization becomes significantly slower: it takes about 15 times more time than the N = 22 case. This result shows that MDTS should be chosen over Bayesian optimization unless the problem size is sufficiently small.    4. Conclusion\r\n  In this paper, we presented MDTS: a materials design library based on Monte Carlo tree search. MDTS is an open source project and interested researchers can join in the development of MDTS. The balance between design time and simulation time is an important factor in automatic materials design. Efficient design methods including MDTS are most useful when the simulation time is short. The long design time of a more inefficient machine-learning based approach can appear less problematic when the simulation time is longer. In future work, it would be necessary to pursue an adaptive approach that can balance optimality and design time in a variable manner. Additionally, we plan to make MDTS more customizable for diverse materials design problems with possibly different kinds of constraints.    Acknowledgements\r\n  We would like to thank David A. duVerle for fruitful discussions. We also would like to thank anonymous referees for their comments and suggestions to improve the manuscript.    Disclosure statement\r\n  Authors declare no conflict of interest.    Funding\r\n  This work was supported by the 'Materials research by Information Integration' Initiative (MI2I) project and CREST [grant number JPMJCR16Q5] from Japan Science and Technology Agency (JST). It was also supported by Grant-in-Aid for Scientific Research on Innovative Areas 'Nano Informatics' [grant number 25106005] from the Japan Society for the Promotion of Science (JSPS).      ",
    "sourceCodeLink": "https://github.com/tsudalab/MDTS",
    "publicationDate": "0",
    "authors": [
      "Thaer M. Dieb",
      "Shenghong Ju",
      "Kazuki Yoshizoe",
      "Zhufeng Hou",
      "Junichiro Shiomi",
      "Koji Tsuda",
      "The Author",
      ". Published by Informa UK Limited",
      "trading as Taylor",
      "Francis Group"
    ],
    "status": "Success",
    "toolName": "MDTS",
    "homepage": ""
  },
  "69.pdf": {
    "forks": 0,
    "URLs": [
      "moleculardistancemaps.github.io/MoDMaps3D/",
      "github.com/moleculardistancemaps/MoDMaps3D/"
    ],
    "contactInfo": ["lila@uwaterloo.ca"],
    "subscribers": 1,
    "programmingLanguage": "JavaScript",
    "shortDescription": "Build your Molecular Distance Map and visualize it in 3D!",
    "publicationTitle": "MoDMaps3D: an interactive webtool for the quantification and 3D visualization of interrelationships in a dataset of DNA sequences",
    "title": "MoDMaps3D: an interactive webtool for the quantification and 3D visualization of interrelationships in a dataset of DNA sequences",
    "publicationDOI": "10.1093/bioinformatics/btx367",
    "codeSize": 571368,
    "publicationAbstract": "Summary: MoDMaps3D (Molecular Distance Maps 3D) is an alignment-free, fast, computationally lightweight webtool for computing and visualizing the interrelationships within any dataset of DNA sequences, based on pairwise comparisons between their oligomer compositions. MoDMaps3D is a general-purpose interactive webtool that is free of any requirements on sequence composition, position of the sequences in their respective genomes, presence or absence of similarity or homology, sequence length, or even sequence origin (biological or computer-generated). Availability and implementation: MoDMaps3D is open source, cross-platform compatible, and is available under the MIT license at http://moleculardistancemaps.github.io/MoDMaps3D/. The source code is available at https://github.com/moleculardistancemaps/MoDMaps3D/.Contact: lila@uwaterloo.ca Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-03-20T17:51:22Z",
    "institutions": [
      "University of Western Ontario",
      "University of Waterloo"
    ],
    "license": "MIT License",
    "dateCreated": "2017-02-23T03:46:20Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx367   MoDMaps3D: an interactive webtool for the quantification and 3D visualization of interrelationships in a dataset of DNA sequences     Rallis Karamichalis  0    Lila Kari  0  1    0  Department of Computer Science, University of Western Ontario ,  London, ON N6A 5B7 ,  Canada    1  School of Computing Science, University of Waterloo ,  Waterloo, ON N2L 3G1 ,  Canada     2017   1  1  3   Summary: MoDMaps3D (Molecular Distance Maps 3D) is an alignment-free, fast, computationally lightweight webtool for computing and visualizing the interrelationships within any dataset of DNA sequences, based on pairwise comparisons between their oligomer compositions. MoDMaps3D is a general-purpose interactive webtool that is free of any requirements on sequence composition, position of the sequences in their respective genomes, presence or absence of similarity or homology, sequence length, or even sequence origin (biological or computer-generated). Availability and implementation: MoDMaps3D is open source, cross-platform compatible, and is available under the MIT license at http://moleculardistancemaps.github.io/MoDMaps3D/. The source code is available at https://github.com/moleculardistancemaps/MoDMaps3D/.Contact: lila@uwaterloo.ca Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: John Hancock    1 Introduction\r\n  Phylogenetic trees have been the traditional means to represent evolutionary history and species classification, but there is a growing realization, see  Gusfield (2014) , that some type of graphs or networks rather than trees are often needed, e.g. to take into account phenomena such as recombination, hybridization, horizontal gene transfer, and convergent evolution. At the same time, alignment-free methods, see  Bonham-Carter et al. (2014) , have been proposed to complement conventional morphological or sequence-alignmentbased methods for phylogenetic analysis. Combining features of both these approaches, we propose MoDMaps3D (Molecular Distance Maps 3D, hereafter MoDMaps), an alignment-free webtool for computing and displaying sequence and species' relatedness. MoDMaps uses approximated information distance (AID) to quantify the pairwise differences in oligomer composition for all input genomic sequences, and visualizes the interrelationships thus obtained as an interactive map in three-dimensional Euclidean space.  As oligomer composition of genomic sequences has been shown to be a source of taxonomic information, see e.g.  Deschavanne et al. (1999) , MoDMaps could complement other alignment-based or alignment-free methods for species identification and classification. The advantage of the distance computation module of the MoDMaps webtool is that it is general-purpose and can be applied to any dataset of genomic sequences. In particular, this module is free of any requirements on: sequence composition, position of the sequences in their respective genomes, presence or absence of similarity or homology, and sequence length (same or different, several kbp-long or complete genomes). The advantages of the visualization module are that the output map is easy to explore, as well as easy to interpret visually. In particular, the spatial distances between a sequence-representing point and all the other points in the map are meaningful, in terms of their interrelatedness.  Note that, although this webtool can compute pairwise distances and visualize the resulting relationships among any DNA genomic sequences (and, implicitly, among their originating organisms and species), it is alignment-free and, as such, it does not invoke the concept of phylogeny. Nevertheless, all prebuilt ModMaps are remarkably consistent with known taxonomies, which confirms that oligomer composition can be a source of taxonomic information.    2 Materials and methods\r\n  Creating a MoDMap involves three computational modules: Chaos Game Representation (CGR), Approximated Information Distance, and Multidimensional Scaling (MDS).  The CGR of a DNA sequence was defined in  Jeffrey (1990)  as a graphical representation of a DNA sequence, where the patterns correspond to the frequencies of k-mers in the sequence. CGR was proposed by  Deschavanne et al. (1999) -and later confirmed-as a candidate for the role of genomic signature, defined by  Karlin and Burge (1995)  as any quantitative characteristic of a DNA sequence that is pervasive along the genome, while being dissimilar for genomic sequences originating from organisms of different species. The CGR of a sequence s is stored as a 2k 2k matrix, where each entry represents the number of occurrences of one of the possible k-mers in the sequence s.  The AID between two DNA sequences s and t, introduced in  Li et al. (2004)  and slightly modified in  Karamichalis et al. (2015) , is defined as: dkðs; tÞ ¼ jMkðsÞnMkðtÞj þ jMkðtÞnMkðsÞj ; 0 jMkðfs; tgÞj dkðs; tÞ 1; where for a sequence s, the set Mk(s) comprises all distinct k-mers that occur in s, and Mkðfs; tgÞ comprises all distinct k-mers that occur in s or t. For two sets X and Y, the set XnY comprises all elements that belong to X but not to Y, while jXj is the number of elements of X. Informally, dk(s,t) is the ratio of the number of k-mers that occur in s but not in t or vice versa, to the total number of k-mers that occur in s or in t. In MoDMaps, Mk(s) is computed as the number of non-zero entries in the CGR matrix of the sequence s, while Mkðfs; tgÞ is the number of non-zero entries of the sum of the CGRs of the sequences s and t.  MDS is an information visualization technique, see  Kruskal (1964) , that takes as input a distance matrix of pairwise distances among a set of items, and outputs a spatial representation of the items in a common Euclidean space. Each item is represented as a point, and the spatial distance between any two points approximates the distance between the items in the distance matrix.  Given an input set of n DNA sequences, MoDMaps first computes the CGR of each DNA sequence. Secondly, it computes all pairwise approximation information distances between CGRs, and stores the distance values in a distance matrix. The third step is to use this distance matrix as input for MDS, which then outputs a visualization of the input DNA sequences as points in a 3D space. The overall time complexity of the algorithm is OðnmÞ þ Oðn2Þ þ Oðn3Þ, for the computation of CGR, distance matrix, and MDS, respectively, where m is the maximum length of a sequence in the dataset.    3 Software description\r\n  genomic sequence details, visualize the structural composition of a selected DNA sequence, and search and highlight subsets of DNA sequences of interest.  MoDMaps can be used in different ways: to explore a prebuilt map, to build a map ab initio for a new set of sequences, or add new DNA sequences to an existing map. All pre-built maps use k ¼ 9, as this value empirically produced the best results while at the same time being computationally inexpensive. For ab initio or extended maps, the user can choose other values of k, from k ¼ 3 (for short, or for dissimilar sequences) to k ¼ 12 (for whole genomes, or for highly similar sequences). MoDMaps also provides the option to compute separately the AID between any pair of sequences, entered as NCBI numbers. See Supplementary Material for additional information.  MoDMaps is written in Javascript, and uses jQuery (a free open-source cross-platform Javascript library), Bootstrap (a free open-source collection of user interface elements), and Three.js (a cross-browser JavaScript library using WebGL for displaying animated 3D computer graphics). In addition the Parallel.js library is used for parallel computation when applicable.    Acknowledgements\r\n  We thank K. Hill, A. Poon, D. Smith and S. Solis-Reyes for discussions.    Funding\r\n  This research was supported by NSERC (Natural Sciences and Engineering Research Council of Canada) Discovery Grants R2824A01 (L.K.). Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/moleculardistancemaps/MoDMaps3D",
    "publicationDate": "0",
    "authors": [
      "Rallis Karamichalis",
      "Lila Kari"
    ],
    "status": "Success",
    "toolName": "MoDMaps3D",
    "homepage": ""
  },
  "77.pdf": {
    "forks": 15,
    "URLs": [
      "bgruening.github.io/galaxyrna-workbench/",
      "galaxyproject.github.io/trainingmaterial",
      "toolshed.g2.bx.psu.edu",
      "bioconda.github.io",
      "github.com/bgruening/galaxy-rna-workbench",
      "bgruening.github.io/docker-galaxy-stable/"
    ],
    "contactInfo": ["gruening@informatik.uni-freiburg.de"],
    "subscribers": 10,
    "programmingLanguage": "HTML",
    "shortDescription": "Galaxy RNA workbench",
    "publicationTitle": "The RNA workbench: best practices for RNA and high-throughput sequencing bioinformatics in Galaxy",
    "title": "The RNA workbench: best practices for RNA and high-throughput sequencing bioinformatics in Galaxy",
    "publicationDOI": "10.1093/nar/gkx409",
    "codeSize": 18082,
    "publicationAbstract": "",
    "dateUpdated": "2017-10-11T23:27:40Z",
    "institutions": [
      "University of Freiburg",
      "Max-Delbr u ̈ck Center for Molecular Medicine",
      "University of Leipzig",
      "University of Rostock",
      "Erasmus University Medical Center",
      "Humboldt University",
      "University of Vienna",
      "Max Planck Institute for Mathematics in the Sciences",
      "Santa Fe Institute"
    ],
    "license": "MIT License",
    "dateCreated": "2015-01-28T12:18:39Z",
    "numIssues": 3,
    "downloads": 0,
    "fulltext": "     10.1093/nar/gkx409   The RNA workbench: best practices for RNA and high-throughput sequencing bioinformatics in Galaxy     Bjo¨ rn A. Gru¨ ning  gruening@informatik.uni-freiburg.de  2  4    Jo¨ rg Fallmann  3    Dilmurat Yusuf  1    Sebastian Will  9    Anika Erxleben  2    Florian Eggenhofer  2    Torsten Houwaart  2    B e´r e´nice Batut  2    Pavankumar Videm  2    Andrea Bagnacani  5    Markus Wolfien  5    Steffen C. Lott  8    Youri Hoogstrate  6    Wolfgang R. Hess  8    Olaf Wolkenhauer  5    Steve Hoffmann  3    Altuna Akalin  1    Uwe Ohler  1  7    Peter    F. St    0  BIOSS Centre for Biological Signaling Studies, University of Freiburg ,  Sch a ̈nzlestr. 18, D-79104 Freiburg ,  Germany    1  Berlin Institute for Medical Systems Biology, Max-Delbr u ̈ck Center for Molecular Medicine ,  Robert-R o ̈ssle-Str. 10, D-13125, Berlin ,  Germany    2  Bioinformatics Group, Department of Computer Science, University of Freiburg ,  Georges-Koehler-Allee 106, D-79110 Freiburg ,  Germany    3  Bioinformatics Group, Department of Computer Science, and Interdisciplinary Center for Bioinformatics, University of Leipzig ,  H a ̈rtelstr. 16-18, D-04107 Leipzig ,  Germany    4  Center for Biological Systems Analysis (ZBSA), University of Freiburg ,  Habsburgerstr. 49, D-79104 Freiburg ,  Germany    5  Department of Systems Biology and Bioinformatics, University of Rostock ,  Ulmenstr. 69, D-18051 Rostock ,  Germany    6  Department of Urology, Erasmus University Medical Center ,  Wytemaweg 80, 3015 CN Rotterdam ,  Netherlands    7  Departments of Biology and Computer Science, Humboldt University ,  Unter den Linden 6, D-10099 Berlin    8  Genetics and Experimental Bioinformatics, Faculty of Biology, University of Freiburg ,  Sch a ̈nzlestr. 1, D-79104 Freiburg ,  Germany    9  Institute for Theoretical Chemistry, University of Vienna ,  W a ̈hringerstrasse 17, A-1090 Vienna ,  Austria    10  Max Planck Institute for Mathematics in the Sciences ,  Inselstrasse 22, D-04103 Leipzig ,  Germany    11  Santa Fe Institute ,  1399 Hyde Park Rd., Santa Fe, NM 87501 ,  USA     2017   45    31  5  2017    02  3  2017    13  4  2017            -\r\n  RNA-based regulation has become a major research topic in molecular biology. The analysis of epigenetic and expression data is therefore incomplete if RNAbased regulation is not taken into account. Thus, it is increasingly important but not yet standard to combine RNA-centric data and analysis tools with other types of experimental data such as RNA-seq or ChIP-seq. Here, we present the RNA workbench, a comprehensive set of analysis tools and consolidated workflows that enable the researcher to combine these two worlds. Based on the Galaxy framework the workbench guarantees simple access, easy extension, flexible adaption to personal and security needs, and sophisticated analyses that are independent of command-line knowledge. Currently, it includes more than 50 bioinformatics tools that are dedicated to different research areas of RNA biology including RNA structure analysis, RNA alignment, RNA annotation, RNA-protein interaction, ribosome profiling, RNA-seq analysis and RNA target prediction. The workbench is developed and maintained by experts in RNA bioinformatics and the Galaxy framework. Together with the growing community evolving around this workbench, we are committed to keep the workbench up-to-date for future standards and needs, providing researchers with a reliable and robust framework for RNA data analysis.    Availability: The RNA workbench is available at https: //github.com/bgruening/galaxy-rna-workbench.\r\n    INTRODUCTION\r\n  Since recent advances in high-throughput sequencing (HTS) emphasized the importance and versatile role of (non-coding) RNAs, there is high demand for integrated computational analyses investigating RNA-mediated regulation. Previously existing workbenches (such as miARmaSeq ( 1 ) RAP ( 2 ) and the UEA Small RNA Workbench ( 3 )) were focused on providing tools for the analysis of RNA deep sequencing data and do not contain RNA centric tools.  We addressed these needs by developing the RNA workbench. Based on the Galaxy framework ( 4 ) it combines a comprehensive set of tools for the analysis of RNA structures, RNA alignments, RNA-RNA and RNA-protein interactions, RNA sequencing, ribosome profiling, genome annotation and many more. So far, we integrated more than 50 RNA-related tools, including suites like the ViennaRNA package, covering this broad variety of use-cases (a complete list of tools can be found on GitHub). Every available tool works as a single building-block that can be connected with other tools to create computational pipelines. Datasets can be incorporated in a similar manner, facilitating an intersection of diverse data sources such as DNA methylation with RNA-seq experiments. Input and output datasets can be defined by the user, and can be as diverse as the adapted set of tools. Established data types for sequence and/or structure information are accepted as input. Output data types follow the same principle, can be converted to different formats, or ultimately used to draw plots and create figures. The workbench provides tools for visualizations of RNA structure datasets, such as dot-bracket strings, and RNA 2D or 3D structures. The workbench also covers a broad range of RNA secondary structure prediction and analysis tools such as RNAfold ( 5 ) or LocARNA ( 6,7 ).    GOALS OF THE RNA WORKBENCH\r\n  The main driving force behind the development of the RNA workbench is the goal to establish a central, redistributable workbench for scientists and programmers working with RNA-related data, and build a sustainable community around it. This platform is unique in combining available tools, workflows and training material, as well as providing easy access for experimentalists. Simultaneously, it serves as a central hub for programmers, which can easily integrate and deploy their existing or novel tools and workflows. The RNA workbench is based on three pillars: (i) a comprehensive set of RNA-bioinformatics tools, (ii) easy and stable dissemination via Galaxy and Docker and (iii) a set of predefined workflows and associated descriptions /training material. The latter is needed for two reasons: first, it facilitates the use of the RNA workbench for researchers with limited bioinformatics experience, and second, it allows to integrate the workbench in the daily lab work by combining RNArelated analysis tasks with workflows for RNA-seq analysis.   Building on the shoulders of giants\r\n  In order to achieve long-term sustainability, we provide the essentials of our work on BioConda (https://bioconda.github.io) and BioContainers (8 ( 8) (http://biocontainers.pro) for reproducible deployments of tools into Galaxy. Using easy-to-distribute packages for all tool dependencies also enables automatic continuous integration tests for all developed tools and the workbench. After a tool passes the tests and gets accepted it will be made available via an automatic deployment into the Galaxy ToolShed (https://toolshed.g2.bx.psu.edu) (9) 9 ). From the ToolShed, Galaxy administrators can easily install desired tools and workflows.    Easily accessible and reproducible analysis platform\r\n  For the fast dissemination of the RNA workbench, as well as for an easy integration with other HTS analysis tasks, we implemented the RNA workbench within the Galaxy framework. A major advantage of relying on Galaxy as the core framework is that it is possible to leverage its scalability, which enables the RNA workbench to run on single CPU installations as well as on large multi-node high performance computing environments. Furthermore, Galaxy provides researchers with means to reproduce their own workflow analyses, enabling them to rerun entire pipelines, or publish and share them with others. The RNA workbench is containerized, i.e., administrators can deploy it via Docker. That makes it possible to have all tool installation dependencies already resolved, while still keeping maintenance tasks to a minimum. The provided layer of virtualization also allows the handling of user-defined input data in a secure and compartmentalized way, a key requirement for researchers working on sensitive data (e.g. patient data in clinics). Running the containerized RNA workbench simply requires installing Docker and starting the Galaxy RNA workbench image. Furthermore, containerizing Galaxy enables a customized Galaxy instance with a selected subset of tools dedicated to specific data analysis tasks, while keeping deployment and installation simple.     RNA-BIOINFORMATICS TOOLS\r\n  In its current state, the RNA workbench includes more than 50 tools covering all aspects of RNA research. In a community effort, these tools will be kept up-to-date and adapted to future needs. New tools and new ways to visualize data provided to the user will also be integrated. A current overview of tools available in the RNA workbench can be found at http://bgruening.github.io/galaxyrna-workbench/.  In the following, we will highlight a few of the integrated tools.  The ViennaRNA package ( 5 ) consists of a suite of tools centered around the prediction of secondary structures of RNAs based on the thermodynamic Turner energy model. Thus, it covers prediction of optimal and suboptimal structures from single sequences as well as alignments, prediction of ensemble base pair probabilities, accessibility of sequences, and RNA-RNA interaction prediction. Importantly, predictions can be flexibly controlled by hard and soft structure constraints; the latter enables the inclusion of structure probing data.  AREsite2 ( 10 ) is a resource for the investigation of AU, GU and U-rich elements (ARE, GRE, URE) in human and model organisms. It provides information on genomic location, genomic context, RNA secondary structure context and conservation of annotated motifs in the whole gene body including introns. It is integrated into the RNA workbench via its REST interface, which provides search results directly in Galaxy for further analysis.  LocARNA ( 6,7 ) provides a comparative analysis of multiple (unaligned) RNAs by simultaneous folding and alignment, implementing a fast variant of the Sankoff algorithm. Beyond pairwise and multiple alignments, it computes reliabilities of alignment columns and provides very fast analysis by simultaneous folding and matching. Finally, LocARNA supports anchor and structure constraints, which improve its applicability in practice.  doRiNA ( 11 ) is a database of RNA interactions in posttranscriptional regulation. The combined action of RNAbinding proteins (RBPs) and microRNAs (miRNAs) is believed to form the backbone of post-transcriptional regulation. doRiNA is implemented as data source tool inside the RNA workbench. This means that the Galaxy user is redirected to the post-transcriptional interaction database and can make selections using the optimized doRiNA interface. Once the selection is done, the data is streamed directly to Galaxy and can be freely analyzed with other tools.  The Infernal ( 12 ) tool suite can construct probabilistic models, also called covariance models (CM), that represent the sequence and structure of an RNA family from a multiple sequence alignment with consensus secondary structure. The covariance model can be used to find more members of this RNA family via homology search.  PARalyzer ( 13 ) generates a high resolution map of interaction sites between RNA-binding proteins and their targets. The algorithm utilizes the deep sequencing reads generated by the PAR-CLIP (PhotoactivatableRibonucleoside-Enhanced Crosslinking and Immunoprecipitation) protocol. The use of photoactivatable nucleotides in the PAR-CLIP protocol results in more efficient crosslinking between the RNA-binding protein and its target relative to other CLIP methods; in addition a nucleotide substitution occurs at the site of crosslinking, providing for single-nucleotide resolution binding information. PARalyzer utilizes this nucleotide substitution in a kernel density estimate classifier to generate the high resolution set of protein-RNA interaction sites.  FuMa ( 14 ) can generate an integration report on predicted fusion genes from most RNA-seq fusion gene detection software. It automatically orders the result based on the frequencies of the fusion genes such that frequently predicted fusion genes can be extracted.    WORKFLOWS\r\n  One of the core concepts of the RNA workbench is the definition of standard workflows as a minimal set of building blocks around which a researcher can compose and tailor specific pipelines. For example, a researcher wants to analyze the effects of an RNA-binding protein (RBP) in regard to expression levels in wild-type compared to knockout or knockdown of the RBP of interest. In this case, one needs to combine the detection of differentially expressed genes in the two conditions with the information of publicly available CLIP-data, as provided for example by the doRiNA ( 11 ) database, to differentiate between direct and indirect targets. Workflows for the analysis of differentially expressed genes are part of the RNA workbench, as well as an interface to doRiNA, such that it becomes an easy task to design a new workflow combining these analysis steps.  In Galaxy, workflows are typically created in two different ways: (i) from an existing history, which stores all tools applied in a previous analysis together with all pertinent parameters, or (ii) from scratch, using a graphical editor via drag-and-drop of tools from the tool panel into the workflow editor. Within workflows, tools can be freely combined to ensure a maximum of flexibility in their usage and connectivity between different analysis steps, e.g. RNA structure analysis tools and RNA-seq data analysis. Various format converters embedded in Galaxy allow combining diverse analysis outputs. Easy sharing of workflows with other Galaxy users guarantees highly reproducible and transparent research. In other words, the workflows ensure that all analysis steps, tools and parameters of an experiment are documented and visible to researchers, readers and reviewers. Workflows can also be submitted to the Galaxy ToolShed or myexperiment.org ( 15 ) for further distribution. The RNA workbench currently includes publicly available standard workflows for RNA data analysis, e.g. for RNAseq. These workflows contain all required steps such as quality control, mapping, differential expression analysis, and visualization of results. Provided workflows can easily be extended or modified, e.g. to use other read mappers available in Galaxy.  In the following, we will describe two sample workflows, one closely related to the detection of ncRNAs, which is a common task in RNA-related research. The other worklfow is related to the analysis of RNA-seq data and is often needed as a subworkflow for more complex analysis tasks. These workflows are well annotated and described in the RNA workbench and extended by interactive Galaxy tours.   Analysis of (unaligned) non-coding RNAs\r\n  An important task is to test for the existence of a functional structure in a non-coding RNA. However, the secondary structure of structured non-coding RNAs is not significantly more stable compared to random sequences (  16 ). Thus, putative functional structures can only be detected using information about conservation. Our workflow for non-coding RNAs performs the typical analysis steps required to detect conserved secondary structures, given a set of unaligned RNA sequences. It computes a sequence and a structure-based alignment by MAFFT ( 17 ) and LocARNA, respectively, and analyzes them with RNAcode ( 18 ) and RNAz ( 19 ) with appropriate parameter settings. RNAz and RNAcode both work on a given alignment. RNAz tests whether a consensus secondary structure is significantly conserved, whereas RNAcode differentiates coding from non-coding RNAs. Together these tools provide information, whether the RNAs are related and conserve a common secondary structure. In addition, a covariance model is built from the LocARNA alignment and subsequently used to search the given sequence database for RNAs with similar sequence- and structure-conservation. This workflow resembles the core of RNAlien ( 20 ), which is based on the same tools and is integrated into the RNA workbench. Going beyond the presented workflow, RNAlien automatically gathers sequences via homology search starting from a single sequence and constructs RNA family models in an iterative process.  To give an other example, in the context of ORFs detection, RNA-seq analysis, the identification of non-coding RNAs with RNAcode and RNAz and the detection of transcription start sites can be used to determine new, short transcripts that are expressed and do not exhibit secondary structure conservation (i.e. are likely not functional ncRNAs). Subsequent analysis of Ribo-seq data can then provide additional evidence for a new transcript that may code for a small protein. For all these tasks, partial workflows and required tools are already integrated in our RNA workbench, which implies that it is easy to set up a new workflow for a more complex task.    RNA-seq analysis: trimming, mapping and read count\r\n  As mentioned before, the analysis of RNA-centric data like CLIP-seq requires the combination with other type of data, and very often RNA-seq. For that reason, we provide a standard RNA-seq workflow that can easily be combined with other workflows. The RNA-seq workflow (as shown in Figure 1) takes a list of RNA-seq datasets as input and successively executes a series of analysis steps - adapter &amp; quality trimming, mapping to a reference genome and read count per annotated gene. The input allows two conditions, e.g. treatment versus control and it also accepts single-end and paired-end reads for each condition. At the trimming step, the workflow employs Trim Galore! ( 21,22 ) to perform adapter trimming. Then, TopHat2 ( 23 ) is used to map the trimmed reads against the reference sequences, which should be provided by the user. As last step, the worklfow executes HTSeq-count ( 24 ) to generate read counts per annotated gene for each condition and for each sequencing type. A reference annotation in Gene Transfer Format (GTF), e.g. provided by Ensembl ( 25 ) is required at this step. The nfial read counts can be used for the downstream assessment of differential expression using tools like DESeq2 ( 26 ). The current workflow can serve as a template that can be modified by the user according to different needs, for instance, replacement of tools or modification of the wrapping strategy.     IMPLEMENTATION\r\n  The workbench is implemented as portable virtualized container based on Galaxy. The Galaxy framework allows for reproducible and transparent scientific research which makes it easy to access, deploy and scale--conceptualized as a web service. The foundation of the workbench container is a generic Galaxy Docker instance (http://bgruening.github.io/docker-galaxy-stable/). On-top of this, pre-configured Galaxy tools can be automatically installed from the Galaxy ToolShed using the Galaxy API BioBlend (2 27 ). In Galaxy, tool dependencies are automatically resolved via BioConda, which is the bioinformatics channel for the Conda package manager. BioConda facilitates software packaging and enables installation at a user level, keeping track of different versions of the same software in virtual environments. These features are in line with the scope of Galaxy; maintaining large numbers of dependencies in a reproducible way. Therefore, all available tools within the RNA workbench are also distributed as BioConda packages and BioContainers, which are persistent, frozen, containerized versions of Conda packages. The RNA workbench ships with a variety of tools, tours, documentation, workflows and data that have been added as additional layers on top of the generic Docker instance. During development, the software has been tested extensively in a continuous integration setup (CI) at different levels: Galaxy itself, tool integration in Galaxy (IUC, galaxytools channels), dependencies (BioConda) and at the workbench level. Together with a strict version management on all levels, this contributes to a high degree of error-control and reproducibility. The RNA workbench started in January 2015 - with constant development over 2 years, and extensive testing in local and public Galaxy instances, such as the Freiburg Galaxy instance, the MDC instance in Berlin and Erasmus MC's Galaxian. More than 500 users accessed the RNA tools during the last two years and the virtualized Docker instance was already downloaded &gt;500 times. Moreover, due to an open and transparent development process, there is a growing community that contributes to our workbench, which guarantees the sustainability of the RNA workbench project and maintenance of the underlying Docker/rkt images.    USING THE RNA WORKBENCH\r\n  Installation: The RNA workbench can be installed under OSX and Windows using the graphical tool Kitematic (https://kitematic.com), or with the following Linux command:  docker run -d -p 8080:80 bgruening/galaxy-rnaworkbench  This installation is production-ready and can be configured to use external computer clusters or cloud environments. Due to the very modular system, it is also possible to install all or only a few tools of the RNA workbench on available Galaxy servers. Just get in contact with your local Galaxy administrator. When using the RNA workbench Docker image, the user has full administration rights, which enables customization independent of potential user restrictions.   Training\r\n  For self-empowering the user, documentation and training of the RNA workbench are important. We included an extensive set of documentation in traditional formats, e.g. tool descriptions and 'README' files.  We also provide training sessions around HTS data analyses and RNA-seq data analysis. The training materials ranging from the introduction to Galaxy, to usage and maintenance of Galaxy and the RNA workbench are freely accessible for self-paced studies at the Galaxyproject Github repository (http://galaxyproject.github.io/trainingmaterial). This training material is constantly improved and extended in an international community effort, including ELIXIR and EMBL. For HTS data analyses we provide training as a specific introduction to the topic with selfexplanatory presentation slides, a hands-on training documentation describing the analysis workflow, all necessary input files ready-to-use via Zenodo, a Galaxy Interactive Tour, and a tailor-made Galaxy Docker image for the corresponding data analysis.  To provide an even more intense training experience within the RNA workbench, we also included interactive training such as the Galaxy Interactive Tours. Such tours guide users through an entire analysis in an interactive and explorative way. It combines advantages from training videos and detailed protocols. Production of training videos is very time-consuming and tend to become outdated very soon, due to tool version changes or renewed workflows. In contrast to conventional screencasts, a Galaxy Interactive Tour can be easily updated and improved to guide the Galaxy user step-by-step, e.g. through a whole HTS analysis starting from uploading the data to using complex analysis tools. Exemplary, the RNA workbench currently integrates two Galaxy Interactive Tours. The first one introduces a new user to the Galaxy interface and its usage with an RNAseq example dataset. The second one illustrates secondary structure prediction of RNA molecules using parts of the ViennaRNA package. To show how Galaxy Interactive Tours can interactively guide users through the necessary steps of HTS analyses, the tours are also provided as online screencasts.    Visualization\r\n  Following data reduction as a key element of explorative research, there is a need for meaningful figures and visualizations that summarize results. The RNA workbench includes standard interactive plotting tools to draw bar charts and scatter plots from all kinds of tabular data and allows for connections to Integrated Genome Browser ( 29 ) and UCSC ( 30 ) like any other Galaxy instance. On top of this, we included three visualizations specific to RNA research. An interactive DotPlot visualization for secondary structures in EPS format (Figure 2b), a 2D visualization for the common dot-bracket format (Figure 2a) and a 3D visualization capable of visualizing PDB, SDF and MOL files containing three-dimensional coordinates (Figure 2c).     COMMUNITY\r\n  The RNA workbench project is an open source project that strives to create a community interested in accessible and reproducible RNA-related research. Knowing that real sustainability can only come true with a strong community we are aiming at more open participation, reward, and inclusion. We are working together with Galaxy, BioConda, BioContainers and BioJS and coordinating efforts to not reinvent the wheel but joining forces to create the new generation of bioinformatics infrastructure together. In the RNA workbench community, we practice the organizations on GitHub, IRC, and Gitter and welcome everyone to contribute on every level to improve the entire stack from documentation to tools and scientific workflows. Support will be provided through the same channels.    DISCUSSION\r\n  In this work, we present the RNA workbench, maintained and developed by a constantly growing community. The presented workbench is unique as it allows to easily combine RNA-centric analysis with other types of experiments. It provides a set of tools, each one being available as BioConda package as well as a Docker/rkt container (BioContainers). Based on the Galaxy Docker project, the proposed web server is more than the sum of its parts. It offers a comprehensive virtualized RNA workbench that can be deployed on every standard Linux, Windows and OSX computer, but can at the same time employ high-performanceor cloud-computing infrastructure.  Major advantages of our approach to deliver a dockerized workbench for RNA centric analysis are the ease of installation, the high number of pre-included tools, the flexibility in regard to extension with other tools and workflows and the high reproducibility and transparency of worklfows. All tools that are available on the Galaxy Toolshed can be installed along with their automatically resolved dependencies with a single click in the Galaxy interface. Best practice pipelines for the analysis of RNA-seq data are provided with the Docker image and can easily be modiifed, extended or combined with other analysis pipelines via Galaxy's workflow editor GUI.  The RNA workbench was designed as a community project, and as such it is easy for users to contribute to the workbench with workflows, new tools and training material, keeping the workbench up-to-date and valuable for research. Moreover, all components such as tools, workflows, visualizations, interactive tours and training material can be easily integrated into any available Galaxy instance for teaching, learning or exploratory purposes.  The main difference to existing solutions such as miARma-Seq ( 1 ), RAP ( 2 ) and the UEA Small RNA Workbench ( 3 ) is that our RNA workbench combines the realm of RNA-centric analysis on sequence and structure level with modern high-throughput sequence analysis. In this regard we provide well established tools for RNA structure prediction, analysis and visualization together with read mappers and expression analysis tools for HTS analysis.    ACKNOWLEDGEMENTS\r\n  We thank the de.NBI and ELIXIR projects for supporting bioinformatics infrastructure. Thanks also to the Galaxy community, especially to the Freiburg Galaxy Team, for developing, maintaining and supporting this great framework. We also like to acknowledge the BioConda and BioContainers community for setting new standards in reproducible software deployments. Thanks also to the BioJS community for great discussions about scientific visualizations and how we can make them more accessible. Moreover, the authors acknowledge the support of many upstream developers that helped us to integrate their tools into the RNA workbench and accepted patches.    FUNDING\r\n  Collaborative Research Center 992 Medical Epigenetics [DFG grant SFB 992/1 2012]; German Federal Ministry of Education and Research [BMBF grants 031 A538A/A538C RBC, 031L0101B/031L0101C de.NBI-epi, 031L0106 de.STAIR (de.NBI)]; Center for Translational Molecular Medicine (CTMM), TraIT project [05T-401 to Y.H.]. Funding for open access charge: German Government.  Conflict of interest statement. None declared.    ",
    "sourceCodeLink": "https://github.com/bgruening/galaxy-rna-workbench",
    "publicationDate": "0",
    "authors": [
      "Bjo¨ rn A. Gru¨ ning",
      "Jo¨ rg Fallmann",
      "Dilmurat Yusuf",
      "Sebastian Will",
      "Anika Erxleben",
      "Florian Eggenhofer",
      "Torsten Houwaart",
      "B e´r e´nice Batut",
      "Pavankumar Videm",
      "Andrea Bagnacani",
      "Markus Wolfien",
      "Steffen C. Lott",
      "Youri Hoogstrate",
      "Wolfgang R. Hess",
      "Olaf Wolkenhauer",
      "Steve Hoffmann",
      "Altuna Akalin",
      "Uwe Ohler",
      "Peter",
      "F. St"
    ],
    "status": "Success",
    "toolName": "galaxy-rna-workbench",
    "homepage": "http://bgruening.github.io/galaxy-rna-workbench/"
  },
  "34.pdf": {
    "institutions": [
      "University of Auckland",
      "ETH Zürich",
      "and Organismal Biology, Iowa State University",
      "University of Oxford",
      "Senior author",
      "Swiss Institute of Bioinformatics (SIB)"
    ],
    "URLs": [
      "www.github.com",
      "taming-the-beast.github.io/",
      "tamingthe-beast.github.io/"
    ],
    "contactInfo": ["tanja.stadler@bsse.ethz.ch"],
    "fulltext": "     Syst. Biol.     10.1093/sysbio/syx060   Taming the BEAST-A Community Teaching Material Resource for BEAST 2     JOËLLE BARIDO-SOTTANI  1  6    VERONIKA BOŠKOVÁ  1  6    LOUIS DU PLESSIS  1  4    DENISE KÜHNERT  1  3  6    CARSTEN MAGNUS  1  6    VENELIN MITOV  1  6    NICOLA F. MÜLLER  1  6    JU¯ LIJA PECˇ ERSKA  1  6    DAVID A. RASMUSSEN  1  6    CHI ZHANG  1  6    ALEXEI J. DRUMMOND  0    TRACY A. HEATH  2    OLIVER G. PYBUS  4    TIMOTHY G. VAUGHAN  0    TANJA STADLER  tanja.stadler@bsse.ethz.ch  1  5  6    0  Centre for Computational Evolution, University of Auckland ,  New Zealand    1  Department of Biosystems Science and Engineering, ETH Zürich ,  Mattenstrasse 26, 4058 Basel ,  Switzerland    2  Department of Ecology ,  Evolution ,  and Organismal Biology, Iowa State University ,  2200 Osborn Dr., Ames, IA 50011   USA    3  Department of Environmental Sciences, ETH Zürich ,  Universitätsstrasse 16, 8092 Zürich ,  Switzerland    4  Department of Zoology, University of Oxford ,  Peter Medawar Building South Parks Road Oxford, OX1 3SY ,  UK    5  Senior author    6  Swiss Institute of Bioinformatics (SIB) ,  Quartier Sorge - Batiment Genopode, 1015 Lausanne ,  Switzerland     2017   0  0  1  5    25  6  2017    21  12  2016     -Phylogenetics and phylodynamics are central topics in modern evolutionary biology. Phylogenetic methods reconstruct the evolutionary relationships among organisms, whereas phylodynamic approaches reveal the underlying diversification processes that lead to the observed relationships. These two fields have many practical applications in disciplines as diverse as epidemiology, developmental biology, palaeontology, ecology, and linguistics. The combination of increasingly large genetic data sets and increases in computing power is facilitating the development of more sophisticated phylogenetic and phylodynamic methods. Big data sets allow us to answer complex questions. However, since the required analyses are highly specific to the particular data set and question, a black-box method is not sufficient anymore. Instead, biologists are required to be actively involved with modeling decisions during data analysis. The modular design of the Bayesian phylogenetic software package BEAST 2 enables, and in fact enforces, this involvement. At the same time, the modular design enables computational biology groups to develop new methods at a rapid rate. A thorough understanding of the models and algorithms used by inference software is a critical prerequisite for successful hypothesis formulation and assessment. In particular, there is a need for more readily available resources aimed at helping interested scientists equip themselves with the skills to confidently use cutting-edge phylogenetic analysis software. These resources will also benefit researchers who do not have access to similar courses or training at their home institutions. Here, we introduce the \u201cTaming the Beast\u201d (https://taming-the-beast.github.io/) resource, which was developed as part of a workshop series bearing the same name, to facilitate the usage of the Bayesian phylogenetic software package BEAST 2. [Bayesian inference; MCMC; phylodynamics; phylogenetics.]       BEAST 2 IN A NUTSHELL\r\n  BEAST 2  (Bouckaert et al., 2014)  is an open source cross-platform software package for analysing genetic sequences in a Bayesian phylogenetic framework. It occupies the same niche, and thus incorporates many of the same models, as other popular Bayesian evolutionary analyses platforms, including BEAST  (Drummond and Rambaut, 2007)  (which we refer to here as BEAST 1 in order to distinguish it from BEAST 2), MrBayes  (Huelsenbeck and Ronquist, 2001) , and RevBayes ( Höhna et al., 2016 ). Although BEAST 2 is a complete redesign of the BEAST 1 software package, it retains a similar user interface and many core model components, including relaxed molecular clock models  (Drummond et al., 2006) , Bayesian skyline models for nonparametric coalescent analyses  (Drummond et al., 2005; Heled and Drummond, 2008) , multispecies coalescent inference with *BEAST  (Drummond and Heled, 2010) , and phylogeographical models  (Lemey et al., 2009, 2010) . Like in BEAST 1, an analysis is set up using input XML files. For most standard analyses, these files can be easily created using a graphical user interface (BEAUti 2).  The key difference in design philosophy between BEAST 1 and BEAST 2 is a greater emphasis in the latter on extensibility, resulting in a modular program built around a set of core components. This allows third-party developers to implement new methods as packages that can be added without rebuilding or redeploying BEAST 2. Through such packages, BEAST 2 provides a growing collection of new models not available in BEAST 1, such as flexible birthdeath tree-priors (S (Stadler et al., 2013; Gavryushkina et al., 2014; Kühnert et al., 2016)  and structured coalescent models (V (Vaughan et al., 2014; De Maio et al., 2015) , as well as updates to existing models, such as StarBEAST 2 (Ogilvie and Drummond, 2016). A list of available models in BEAST 1 and BEAST 2 can be found at http://beast2.org/beast-features/. (Users should bear in mind that BEAST 2 is modular by design, and thus some third-party packages may not be listed.)  This modular design requires the BEAST 2 user to make active modeling choices, and it is no longer possible to simply perform a \u201cdefault\u201d analysis. This active involvement opens the door for analyses tailored specifically to particular data sets and questions, greatly increasing the power of the package. However, it also markedly increases the complexity and makes it easier to inadvertently introduce errors or use inappropriate models. This added complexity could also be daunting to novice users and may result in them preferring simpler, but less powerful, software packages. We will now briefly highlight the key steps required from the BEAST 2 user when running a data analysis.  At its core, BEAST 2 estimates rooted phylogenies (T ) from genetic sequencing data (D), with branch lengths in units of calendar time (i.e., the phylogenies are time-trees). It concurrently estimates evolutionary parameters ( ), such as the substitution rate, and parameters describing population dynamics ( ), such as speciation/extinction or transmission/recovery rates. For inference, BEAST 2 uses a Markov chain Monte Carlo (MCMC) algorithm to sample from the posterior distribution,  Pr[T , , |D] =  Pr[D|T , ]Pr[T | ]Pr[ ]Pr[ ] .  Pr[D] (1)  The output of an analysis is a log-file containing a sample of the states (T , , ) visited by the MCMC algorithm. After a so-called burn-in phase, each value (T , , ) is visited by the chain at a frequency proportional to its posterior probability, so the output of BEAST 2 (after eliminating the burn-in) is a set of samples from the posterior distribution. A recent book  (Drummond and Bouckaert, 2015)  describes the general theory and design behind BEAST 2.  For the user to carry out a successful and correct analysis, several steps need to be performed carefully to analyze the data and answer the research question of interest. The researcher must specify a multileveled (i.e., hierarchical) model with several interacting components, including: (i) a suitable model describing the evolution of the sequence data on a time-tree, including the substitution and molecular-clock models (Pr[D|T , ]); (ii) a phylodynamic model describing the growth of the tree over time (Pr[T , ]); and (iii) sensible prior distributions for each of the parameters of the evolutionary models (Pr[ ] and Pr[ ]).  In addition to the model components, the researcher must also specify and fine-tune MCMC operators that propose new states for the model parameters (T , , ). By choosing appropriate proposal algorithms, an MCMC analysis is more likely to sample the posterior distribution efficiently. Finally, once the MCMC chain has sampled a sufficient number of states, the researcher must assess whether the chain has converged and recovered a meaningful signal from the data.  Consequently, the user is challenged with a myriad of choices on the road to a successful analysis. Although many potential pitfalls exist, a simple but solid understanding of the theory behind Bayesian phylogenetic inference can help guide new users through an analysis to reach sound conclusions. \u201cTAMING THE BEAST\" FOR T THE USER COMMUNITY In June 201 6, we organized a \u201cTaming the BEAST\" workshop in Engelberg, Switzerland, aimed at fostering interaction between BEAST 2 users and developers. The workshop was organized by graduate students and postdoctoral researchers in the Computational Evolution group at ETH Zürich (https://www.bsse.ethz.ch/cevo, with generous financial support from ETH Zürich) and was a mix of lectures by invited speakers (A.J.D., T.A.H., O.G.P., T.G.V., and T.S. were invited speakers.) and hands-on tutorials run by the organisers. (J.B.-S., V.B., L.d.P., D.K., C.M., V.M., N.F.M., J.P., D.A.R., and C.Z. organized the tutorial sessions.) Participants had the opportunity to learn how to use BEAST 2 with help from the developers and to discuss questions specific to their research with other experienced scientists. For the developers, such a workshop provides direct feedback from users on easeof-use, identifying specific issues and discovering the needs and wishes of the community for future software and methods development.  The workshop was met with great enthusiasm from researchers already using or planning to use BEAST 2, ranging from students to established PIs. (Although originally envisioned for graduate students only, many postdoctoral researchers, some lecturers, and a few professors applied for the workshop as well. Due to the limited capacity and resources, out of 75 applications, we selected 36 participants from 14 countries and 28 universities.) The positive feedback from the participants (see Fig. 1), the overwhelming support from the community and the demand for further workshops has provided motivation to initiate a series of \u201cTaming the BEAST\" workshops. At the time of writing, a second successful edition of \u201cTaming the Beast\u201d was run on Waiheke island (New Zealand) in February 2017 and a third edition will take place in July 2017 in London. Further editions are planned for 2018 in Switzerland, and for 2019 and 2020 in locations that are yet to be determined. (We secured funding from ETH Zürich to support the workshop series in 2017-2020.) Each workshop is intended as a global event, allowing users and developers from around the world to meet and share knowledge.  To ensure these resources are available to the community, we have set up a website (https://tamingthe-beast.github.io/) with the same name as the workshop series to serve as a platform for collating a comprehensive and cohesive set of BEAST 2 tutorials (see Fig. 2). By providing a set of well-curated tutorials, \u201cTaming the BEAST\" offers researchers the resources necessary to learn how to perform analyses in BEAST 2. In addition to tutorials provided by the BEAST 2 developers, this resource page also contains all of the materials (lecture slides, tutorials, data, and example outputs) used during the first two \u201cTaming the BEAST\" workshops in Switzerland and New Zealand. These materials will be updated and extended for future editions of the workshop. Tutorials are released under excellent/positively surprised very good/very satisfied  good/satisfied poor/unsatisfied very poor/very unsatisfied unacceptable/disappointed ovfesraaltlislefavcetlion o t e expectationsm  r hefluptfuurlneersessefoarch conAfidSeTnc2e(bweitfhore) BE conBfEidAeSnTce2w(aithfter) a license that gives anyone the right to freely use (and modify) tutorials for courses or workshops, as long as appropriate credit is given and the updated material is licensed in the same fashion. (By default we use a Creative Commons Attribution 4.0 license, however the exact license to be used is determined by the tutorial's authors.) We hope that these open resources will encourage other research groups/universities to host and organize their own \u201cTaming the BEAST\" workshops. As a community resource, the \u201cTaming the BEAST\" website will maintain a list of workshops, and tutorial developers are available to provide support to organizers.    CONTRIBUTING TO TAMING THE BEAST\r\n  In keeping with the BEAST 2 design philosophy, we designed the website to have a modular, extensible architecture. Each tutorial is stored in its own GitHub (http://www.github.com) repository, where it is bundled with all of the supporting data and scripts needed to run the tutorial, as well as example output files. This makes it possible for anyone with a GitHub account to raise issues and suggest edits or extensions to tutorials. Similarly, it is also possible for external contributors to submit new tutorials to the website. We provide a template tutorial and comprehensive documentation to help potential contributors get started.  By providing a \u201cTaming the Beast\u201d platform that allows issues to be raised and content to be edited, we hope that the community will play an active role in curating tutorials. We further envision these resources will continue to grow as the community contributes more tutorials. For instance, the developers of a new BEAST 2 package will be able to add a tutorial for their package to the \u201cTaming the BEAST\" site, where it will be accessible in a central location, along with other BEAST 2 tutorials, making it easier for users to become familiar with their package.  Because tutorials are stored in GitHub repositories that track change history, all contributors can receive proper credit for their work. Furthermore, authors of new tutorials can retain ownership of their tutorials after publication. In addition, GitHub tracks traffic to tutorials over time and makes it easy for users to interact with authors, giving authors a measure of their work's impact within the community. Finally, because of the distributed nature of the website, it is robust to changes in any single repository, making it easy to update or add individual tutorials.    SUMMARY\r\n  The tutorials on the \u201cTaming the Beast\u201d website allow users to learn about the entire BEAST 2 analysis pipeline, with most tutorials focusing on a particular model component or a single BEAST 2 package. The website provides immediate access to the materials that guide users in the application of a range of models to their own data. In addition, there are tutorials on postprocessing, interpreting results, as well as troubleshooting. We will ensure the maintenance of the website and incorporation of new tutorials through two to three responsible people from the Computational Evolution group at ETH Zürich as well as collaborating groups acting as website administrators. The administrators of the website can be reached via tamingthebeast@bsse.ethz.ch.  We hope that the \u201cTaming the BEAST\u201d platform will allow new BEAST 2 users to accelerate their learning process and to successfully \u201ctame\u201d the BEAST. At the same time, we hope that it will serve as a central repository of teaching materials that will allow BEAST 2 developers and users to exchange knowledge about how to effectively teach the use of BEAST 2. Finally, this platform will hopefully further encourage developers to share their own materials with the wider community.    ACKNOWLEDGMENTS\r\n  First and foremost we would like to express our immense gratitude to the community for the overwhelmingly positive response both before the first workshop (in the form of letters of support and interest) and after the workshop (in helping us turn it into a series of recurring workshops). We would also like to thank the BEAST 2 core developers for supporting our initiatives and helping us to run the workshop smoothly, in particular Walter Xie and Remco Bouckaert who tested tutorials and implemented last minute bugfixes. We further acknowledge generous support from ETH Zürich through the Swiss University Conference (SUK) program. The website architecture is based on Trevor Bedford's lab website. Many thanks to Trevor for making his code publicly available! O.G.P. wishes to thank Andrew Rambaut for his contributions to lecture slides. Further, we would like to thank the speakers of the second workshop, Simon Ho, David Bryant, Remco Bouckaert, Huw Ogilvie, and David Duchêne, as well as Carmella Lee for organizing the logistics of the second workshop. Finally, we would like to thank David Bryant and an anonymous reviewer for valuable comments on the article.    AUTHOR\u2019S CONTRIBUTIONS\r\n  J.B.-S., V.B., L.d.P., V.M., and J.P. wrote and submitted the SUK application for starting the \u201cTaming the BEAST\" workshop series, with substantial support of C.M. and D.A.R. The first workshop was organized by the whole Computational Evolution group (led by J.B.-S., V.B., and L.d.P.). J.B.-S., V.B., L.d.P., D.K., C.M., V.M., N.F.M., J.P., D.A.R., C.Z., A.J.D., T.A.H., O.G.P., T.G.V., and T.S. wrote the tutorials and/or lecture slides for teaching. L.d.P. created the figures, set up the web resource and GitHub repositories and is the corresponding person regarding these online resources. L.d.P., J.B.-S., V.B., and T.S. wrote the article.    ",
    "publicationTitle": "Taming the BEAST-A Community Teaching Material Resource for BEAST 2",
    "title": "Taming the BEAST-A Community Teaching Material Resource for BEAST 2",
    "publicationDOI": "10.1093/sysbio/syx060",
    "publicationDate": "0",
    "publicationAbstract": "-Phylogenetics and phylodynamics are central topics in modern evolutionary biology. Phylogenetic methods reconstruct the evolutionary relationships among organisms, whereas phylodynamic approaches reveal the underlying diversification processes that lead to the observed relationships. These two fields have many practical applications in disciplines as diverse as epidemiology, developmental biology, palaeontology, ecology, and linguistics. The combination of increasingly large genetic data sets and increases in computing power is facilitating the development of more sophisticated phylogenetic and phylodynamic methods. Big data sets allow us to answer complex questions. However, since the required analyses are highly specific to the particular data set and question, a black-box method is not sufficient anymore. Instead, biologists are required to be actively involved with modeling decisions during data analysis. The modular design of the Bayesian phylogenetic software package BEAST 2 enables, and in fact enforces, this involvement. At the same time, the modular design enables computational biology groups to develop new methods at a rapid rate. A thorough understanding of the models and algorithms used by inference software is a critical prerequisite for successful hypothesis formulation and assessment. In particular, there is a need for more readily available resources aimed at helping interested scientists equip themselves with the skills to confidently use cutting-edge phylogenetic analysis software. These resources will also benefit researchers who do not have access to similar courses or training at their home institutions. Here, we introduce the \u201cTaming the Beast\u201d (https://taming-the-beast.github.io/) resource, which was developed as part of a workshop series bearing the same name, to facilitate the usage of the Bayesian phylogenetic software package BEAST 2. [Bayesian inference; MCMC; phylodynamics; phylogenetics.]",
    "authors": [
      "JOËLLE BARIDO-SOTTANI",
      "VERONIKA BOŠKOVÁ",
      "LOUIS DU PLESSIS",
      "DENISE KÜHNERT",
      "CARSTEN MAGNUS",
      "VENELIN MITOV",
      "NICOLA F. MÜLLER",
      "JU¯ LIJA PECˇ ERSKA",
      "DAVID A. RASMUSSEN",
      "CHI ZHANG",
      "ALEXEI J. DRUMMOND",
      "TRACY A. HEATH",
      "OLIVER G. PYBUS",
      "TIMOTHY G. VAUGHAN",
      "TANJA STADLER"
    ],
    "status": "Success",
    "toolName": "www"
  },
  "30.pdf": {
    "forks": 12,
    "URLs": ["github.com/OpenGene/CfdnaPattern"],
    "contactInfo": [],
    "subscribers": 12,
    "programmingLanguage": "Python",
    "shortDescription": "Pattern Recognition for Cell-free DNA",
    "publicationTitle": "A Study of Cell-free DNA Fragmentation Pattern and Its Application in DNA Sample Type Classification",
    "title": "A Study of Cell-free DNA Fragmentation Pattern and Its Application in DNA Sample Type Classification",
    "publicationDOI": "10.1109/TCBB.2017.2723388",
    "codeSize": 1210,
    "publicationAbstract": "-Plasma cell-free DNA (cfDNA) has certain fragmentation patterns, which can bring non-random base content curves of the sequencing data's beginning cycles. We studied the patterns and found that we could determine whether a sample is cfDNA or not by just looking into the first 10 cycles of its base content curves. We analysed 3189 FastQ files, including 1442 cfDNA, 1234 genomic DNA, 507 FFPE tumour DNA and 6 urinary cfDNA. By deep analysing these data, we find the patterns are stable enough to distinguish cfDNA from other kinds of DNA samples. Based on this finding, we build classification models to recognise cfDNA samples by their sequencing data. Pattern recognition models are then trained with different classification algorithms like k-nearest neighbours (KNN), random forest and support vector machine (SVM). The result of 1000 iteration .632+ bootstrapping shows that all these classifiers can give an average accuracy higher than 98%, indicating that the cfDNA patterns are unique and can make the dataset highly separable. The best result is obtained using random forest classifier with a 99.89% average accuracy ( = 0:00068). A tool called CfdnaPattern (http://github.com/OpenGene/CfdnaPattern) has been developed to train the model and to predict whether a sample is cfDNA or not.",
    "dateUpdated": "2017-09-27T06:10:33Z",
    "institutions": [],
    "license": "https://github.com/OpenGene/CfdnaPattern/blob/master/LICENSE",
    "dateCreated": "2016-07-28T03:09:46Z",
    "numIssues": 2,
    "downloads": 0,
    "fulltext": "     TCBB.     10.1109/TCBB.2017.2723388   A Study of Cell-free DNA Fragmentation Pattern and Its Application in DNA Sample Type Classification    2017   2723388   -Plasma cell-free DNA (cfDNA) has certain fragmentation patterns, which can bring non-random base content curves of the sequencing data's beginning cycles. We studied the patterns and found that we could determine whether a sample is cfDNA or not by just looking into the first 10 cycles of its base content curves. We analysed 3189 FastQ files, including 1442 cfDNA, 1234 genomic DNA, 507 FFPE tumour DNA and 6 urinary cfDNA. By deep analysing these data, we find the patterns are stable enough to distinguish cfDNA from other kinds of DNA samples. Based on this finding, we build classification models to recognise cfDNA samples by their sequencing data. Pattern recognition models are then trained with different classification algorithms like k-nearest neighbours (KNN), random forest and support vector machine (SVM). The result of 1000 iteration .632+ bootstrapping shows that all these classifiers can give an average accuracy higher than 98%, indicating that the cfDNA patterns are unique and can make the dataset highly separable. The best result is obtained using random forest classifier with a 99.89% average accuracy ( = 0:00068). A tool called CfdnaPattern (http://github.com/OpenGene/CfdnaPattern) has been developed to train the model and to predict whether a sample is cfDNA or not.    Cell free DNA  Liquid Biopsy  Fragmentation  Pattern Recognition       INTRODUCTION\r\n  T DNA (cfDNA), was first found in the human plasma  HE extracellular DNA fragment, which is called cell-free by Mandel and Metais [1], and then found in other body fluids, like urine [2], pleural effusion [3] and cerebrospinal fluid [4]. For healthy people, cfDNA is mainly released from cell apoptosis [5], and partly released from necrosis [6] and active cell release [5]. But for tumour patients, it is known that tumour cells can release large amount of DNA carrying lots of mutation information from tumour cells, which is called circulating tumour DNA (ctDNA) .  As techniques like next generation sequencing (NGS) become cheaper and better, genetic testing using cfDNA samples goes popular and has been applied into clinical environments. After the boom of non-invasive prenatal testing (NIPT) [7] using cell-free fetal DNA, tumour genetics testing, which mainly relies on the analysis of ctDNA [8], [9], is considered as a much bigger opportunity. According to previous studies [ 29 ], the variations detected in tumour patient's cfDNA usually have highly consistency with the variations detected from the same patient's tumour tissue samples. CtDNA testing is usually non-invasive, highly available, truly dynamic, and is able to profile tumour heterogeneity. These features make ctDNA testing more feasible than tissue testing for tumour genetics diagnosis, which plays a key role for personalised tumour treatment, tumour monitoring and screening. A new term, liquid biopsy, was created for tumour genetics testing using cell-free DNA, and was presented as the top of 10 breakthrough technologies in 2015, by MIT Technology Review Press.  NGS is considered as the most common and powerful technique to discover DNA alterations in liquid biopsy. Lots of studies reported the methods of detecting genetic variations from cfDNA sequencing data, while some other studies reported hundreds of cases using mutation information found in cfDNA sequencing to guide tumour treatment. However, only few studies paid attention to the data and sequence characteristics of cfDNA, such as length distribution [10], break positions, and especially the fragmentation patterns [11].  For cfDNA length distributions, previous studies reported there is a dominant peak around 166bp, and two small peaks around 350bp and 510bp. These three peaks are roughly the size of DNA with mono-, di-, and trinucleosome. Shorter than 166bp, there are small peaks near 152bp, 143bp, 133bp, 122bp, 112bp and 102bp, which show a 10 bases periodicity, same as one turn of a double helix DNA in nucleosome packaging. These cfDNA size distributions obviously reflect the nucleosome guided fragmentation patterns [10].  The cfDNA fragmentation patterns were first reported by Chandrananda at one nucleotide resolution in 2014 [11]. When he investigated the sequencing coverage bias of cfDNA from pregnant women, he found some high frequency 10-nucleotide motifs on either side of cfDNA fragments. Specially, he found the first two bases of the cfDNA at cleavage site could determine most of the other 8 bases. His further study in 2015 indicated these fragmentation patterns were related to the non-random biological cleavage over chromosomes. The 10 positions on either side of the DNA cleavage site show consistent patterns with preference of specific nucleotides for nucleosomal cores and linker regions [12]. A further study [13] stated that the nonrandom cfDNA pattern could reflect its chromatin features, such as epigenetic landscapes and gene expressions. In a latest study [ 14 ] using deep sequencing of cfDNA from healthy people and tumour patients, a dense and genome wide map of nucleosome occupancy was constructed to distinguish the original cell types of cfDNA . These works show that cfDNA fragmentation patterns have the potential to be a novel biomarker for disease diagnosis.  This paper also focuses on the study and application of cfDNA fragmentation patterns. We will present our analysis result of cfDNA fragmentation patterns from more than one thousand samples, including cfDNA, genomic DNA and FFPE samples. Same as previous studies, we found the fragmentation patterns were non-random, and we confirmed they were unique and stable. These features give the fragmentation patterns ability to identify if a sequenced sample is cfDNA, or not cfDNA. Using pattern recognition technologies, we build a cfDNA classifier just using the fragmentation patterns extracted from the sequence data, and our evaluation result showed it could achieve about 99.89% accuracy by average ( = 0:00068).  We developed an open-source tool CfdnaPattern for training this cfDNA classification model. Interestingly, we found our tool could be used to scan regular sequencing FastQ files to detect if there exists the possibilities that cfDNA samples are marked or treated as non-cfDNA samples, or vice versa. This feature makes CfdnaPattern useful for regular data auditing to detect the messing up of cfDNA and other not-cfDNA samples. 2    METHODS\r\n  The data used to train and validate the models are a part of recent sequencing output from HaploX Biotechnology. 3189 FastQ files, including 1442 cfDNA, 1234 genomic DNA (gDNA), 507 FFPE tumour DNA and 6 urinary cfDNA, are gathered into this dataset. Some cfDNA and gDNA samples are paired, which means a pair of cfDNA and gDNA are both extracted from same single tube of blood. After centrifugation of the blood sample, cfDNA is extracted from plasma, while gDNA is extracted from blood cells. Most samples are sequenced after DNA target capture using gene panels, and few samples are sequenced in a whole exome or whole genome wide. 2.1   Experiment description\r\n  The collected blood samples were processed within 2 hours, and were centrifuged at 1600 g for 10 minutes at 4 C to separate the blood cells and plasma. Next, plasma was transferred into a new 10 mL tubes and centrifuged at 16000 g for 10 minutes at 4 C to remove residual cells. Then they were stored at -80 C.  cfDNA was extracted with the Serum / Plasma Circulating DNA Kit (Tiangen) according to the manufacturer's protocol from 2 mL plasma [ 15 ]. gDNA was extracted with the TIANamp Blood DNA Kit (Tiangen) according to the manufacture's protocol from 2 mL blood cell fraction [ 16 ]. FFPE DNA was extracted with the GeneRead DNA FFPE Kit (Qiagen) according to the manufacturer?s protocol from 10 mm thick sections of FFPE tissue blocks . GeneRead FFPE purification process introduced in the UNG enzymatic treatment could dramatically eliminate the artifactual G&gt;T or G&gt;A mutations [ 17 ], [ 18 ].  The concentration of purified DNA was determined by a Qubit dsDNA HS assay (Invitrogen), Qubit-quantified genomic DNA (6 ug; non-amplified) and FFPE DNA (2 ug; non-amplified) was digested to 100-250bp fragments using the NEBNext dsDNA Fragmentase (NEB, M0348L) [ 19 ], and the cfDNA was directly used for library construction. Libraries were constructed using enzymatic reagents from KAPA Library Preparation kits (KAPA Biosciences) according to protocols [ 20 ]. NimbleGen SeqCap EZ Choice(Roche) was used for hybridization-based enrichment according to the manufacturer's protocol [ 21 ]. 2.2    Feature selection\r\n  Initially the fragment length and fragmentation patterns of cfDNA are both considered as features to be extracted from cfDNA sequencing data. The cfDNA fragment length has certain distribution, such its peak is usually near 166bp [ 31 ].  This characteristic makes cfDNA length distribution a very good feature for modelling cfDNA classifiers. Fig.1 shows a typical curve of cfDNA fragment length distribution.  However, the calculation of DNA template length distribution requires pair-end sequencing, so this feature will be not available for single-end sequencing data. Secondly the DNA length distribution can be affected by the sequence trimming operation, which is a commonly applied before in data preprocessing stage. Furthermore, we usually need to do the time-consuming alignment process before we can calculate the insert size, this will take more computation resource and can be slow. For above reasons, we gave up the idea of using cfDNA fragment length distribution as a feature of the classifier model.  To make our cfDNA classification model to be more universal for different sequencing methods, we switched to using only DNA fragmentation patterns as the features. According to previous studies [11], the cfDNA fragmentation patterns locate in the first 10 base pairs. We did statistics of the selected 1442 cfDNA FastQ files, and the results also support the same conclusion. The AT CG ratio of the first 10 cycles are not flat, but highly homological across different samples. For each FastQ file, we counted the AT CG base numbers in first 10 cycles, calculated their percentages, and then stored these AT CG base content ratios of 10 cycles in a 40-element vector to be the feature. Fig.2 shows the mean ratio curves of AT CG bases and the quartile values at every cycle.  To confirm whether this 40-element feature is good to classify different kinds of DNA data, we first conducted principle component analysis (PCA) [ 30 ] to see if the dataset is separable by this feature. Fig. 3 shows the PCA result with (1) bases, have vector We can  (2) 8 &gt;&gt; Acfdna = arccos &gt; &gt; &gt; &lt;  FS :Fcfdna jFS j  jFcfdnaj &gt; &gt; &gt; &gt; &gt; :  Agdna = arccos  FS :Fgdna jFS j  jFgdnaj  We then visualised all samples' (Acfdna; Agdna) with different cycle number settings to see whether this angle value can be used to separate cfDNA from other DNA samples. Fig. 4 shows the data plotting result with cycle = 5, 6, 8, 10, from which we can find that this feature is able to separate the dataset if cycle 6. only top 2 principle components are kept. From Fig. 3, we can find this feature is good enough to separate cfDNA and non-cfDNA data.  By looking to the base content curves of thousands of cfDNA, we found the base content ratio fluctuation is a relative stable value. For each base b, let Rnb denotes its base content ratio at cycle n, we can define its base content fluctuation at this cycle as:  Fnb = 8 1; &gt; &lt;  0; &gt;: 1;  Rnb &lt; Rn+1  b Rnb = Rn+1  b Rnb &gt; Rn+1  b if a  Since we 4  there are count the (N 1)  AT CG four different first N cycles, we will  dimension fluctuation (F1A; F1T ; F1C ; F1G; :::; FNA 1; FNT 1; FNC 1; FNG 1). then calculate the vector Fcfdna as the mean base content ratio fluctuation of cfDNA, and Fgdna as the corresponding value of gDNA . For a sample S, we can calculate the angle of its base content ratio fluctuation FS to Fcfdna and Fgdna by:  Fig. 5. The benchmark results of cross validation with 1000 iteration bootstrapping using different algorithms. Y-axis values are the sorted accuracy score, 1.0 means 100% accuracy. From this figure, we can find that random forest (treeN umber = 20) gives the best performance (average accuracy = 99.89%), KNN (K = 8; weights = unif orm) and linear SVM (norm = L2; loss = hinge) give comparable performance, while nonlinear SVM (kernel = rbf; degree = 3) and GNB give the worst. LibD3C classifier achieved 99.1% of weighted accuracy with 5  By looking to the result of PCA analysis and base content fold cross validation, but its result is not shown here since it requires ratio fluctuation, we confirm this 40-element feature is a nontrivial efforts to support bootstrapping and .632+ rules. good candidate to classify cfDNA samples and other DNA samples. Then we can train classification models based on this feature. 2.3    Model training and validating\r\n  To train this cfDNA classification model, we tried different supervised learning algorithms including k-nearest neighbours (KNN), SVM with both linear and radial basis function (RBF) kernels, Gaussian Naive Bayes [ 23 ] and some ensemble algorithms like random forest [ 24 ] and libD3C [ 25 ].  We implemented bootstrapping [ 26 ] and applied .632+ rules [ 27 ] to evaluate the model errors. 1000 iterations of bootstrapping were performed, within each iteration the dataset was split into training set and validating set using random sampling with replacement. Model error in each iteration was evaluated using .632+ rule, and the mean error and mean accuracy were immediately obtained after all 1000 iterations were done. We can then simply compare the performance of different algorithms using the mean accuracy obtained from bootstrapping evaluation. To directly visualise the performance of different algorithms, we sorted the scores and plot them in a same figure, which is shown in Fig.5. 2.4    Software Implementation\r\n  This project is developed in Python and the core classification functions, including training and predicting utilise the well documented machine learning library scikit-learn [ 22 ]. Feature extraction is the most time-consuming process of the whole pipeline. We both tried using all or just a part of the reads in FastQ files to calculate the 40-element feature vector, and found they only produced very little difference. So we only count 10,000 reads by default to make this program ultra-fast. Typically it only takes a few minutes to do both feature extraction, training and validating on this 2362 files. Once the feature extraction process is done, its result is cached as a JSON file. Loading this JSON file to do training and validating again only takes a few seconds.  The CfdnaPattern tool is available at github (https://github.com/OpenGene/CfdnaPattern) with good maintenance. We have uploaded our pre-trained model for convenient use to classify cfDNA and not-cfDNA sample data. But be aware that the patterns of cfDNA sequencing data can be slightly different for whole genome sequencing, whole exome sequencing and other target sequencing. And the gene panels used to do target capturing can also affect the cfDNA fragmentation patterns. So it is preferred for users to train their own models if they have enough data. But commonly the pre-trained model is good and accurate enough to handle most classification tasks.     3 RESULTS\r\n  We first used this tool to test other plasma cfDNA and genomic DNA samples outside the training and validating datasets, and it gave near 100% accuracy for classifying hundreds of samples. After we confirmed the performance of our model, we started to use this tool to scan our every new FastQ file, and found ever one special case that CfdnaPattern prediction result was not consistent with the sample registry tables. In this case, a sample registered as cfDNA was predicted as not-cfDNA, and its paired genomic DNA was predicted as cfDNA. After careful checking the experiment processes, we confirmed the prediction was correct, and the cfDNA and genomic DNA samples were actually swapped by using incorrect adapters. This story indicates that CfdnaPattern tool can be used for regular sequencing data auditing, to prevent the happening of messing up of DNA types by incorrect experiment or wrong regents. The ability to check correctness of experiment and data analysis processes is extremely important for providing a high quality sequencing and analysing service, especially for those applications in clinical environments.  Then we tested cfDNA data from other cell-free fluids rather than plasma. First we sequenced 5 urinary cfDNA samples with target capturing, and we found their patterns are similar, but different from the patterns of plasma cfDNA. Then we downloaded some cerebrospinal fluid cfDNA sequencing data from NCBI SRA (accession numbers: SRR2496749 SRR2496739, SRR2496735, SRR2496731, SRR2496709, SRR2496702, SRR2496699, SRR2496693, SRR1656605, SRR1654347) [ 28 ], along with some plasma cfDNA data (accession numbers: SRR2496737, SRR2496711, SRR3706309, SRR3706280, SRR3706298), some germline genomic DNA data (accession numbers: SRR2496740, SRR2496716, SRR2496710) and some tumour DNA data (accession numbers: SRR2496722, SRR2496689, SRR2496713). We tried to predict if our model can differentiate the cfDNA samples and not-cfDNA correctly. The result showed that CfdnaPattern tool could correctly classified plasma cfDNA as cfDNA, genomic and tumour DNA as not-cfDNA. Visualised figures showed the patterns of cerebrospinal fluid cfDNA samples are varying and different from the patterns of plasma cfDNA, but the result showed that most of them still can be classified as cfDNA sample. We evaluated 44 items downloaded from NCBI SRA, and the result reported only 1 item wrongly predicted, which gave a prediction accuracy of 97.73%. Fig.6 plots some cerebrospinal fluid cfDNA and plasma pattern together.  To analyse this cfDNA fragmentation pattern more deeply, we split the original sequencing data by different criterions. The first immediate thought is to split aligned cfDNA sequencing data by chromosomes and try to find whether different chromosomes have different patterns. We got a result that the patterns of the 22 autosomes and 2 sex chromosomes were similar, and all these chromosome data could be recognised as cfDNA. We also separated the reads into forward reads and reverse reads to see if the patterns could be different but still got a negative result. Since mitochondria are not inside the nucleus and are not protected by nucleosome, we supposed that the cfDNA came from mitochondria might not have the same patterns as autosomes. We did find the patterns of mitochondria are a bit different, but the difference was much less than we expected. This result indicates that the chromosomes and mitochondria share major mechanisms of cleaving and producing cfDNA. Fig.7 shows a comparison of the patterns of a whole genome sequencing data and the its subset of mitochondrial DNA.  To figure out if DNA fragments with different lengths can have different patterns, we split the original sequencing data by sequencing DNA template length. In our experiment, we separated the pair-end sequencing reads by every 10 bp step, which means reads of 150-159 bp and 160-169 bp will be categorised into different subsets. We studied the patterns of every subsets and tried to recognise them as cfDNA using our tool. The result showed different length subsets shared the same patterns, and most of them could be successfully recognised as cfDNA, even for those in short length range of 60-69 bp, or long length range of 290-299 bp. After the cfDNA degradation phenomenon is known [13], a reasonable speculation is: for those short cfDNA fragments in fresh blood, are they mainly produced by such degradation of long cfDNA fragments when they are circulating? It seems that our result doesn't support this speculation. Since if it is true, the short cfDNA fragments should lose their fragmentation patterns. 4    DISCUSSION\r\n  fluid cfDNA patterns are caused by different cells, or caused by different experiment processing methods.  As one of the most important liquid biopsy technologies, cfDNA sequencing will play a more important role in applications like cancer diagnosis, monitoring and screening. For such applications, the correctness of experiments and data analysis pipelines should be guaranteed. This tool proposed in this paper is helpful for fast analysis of cfDNA sequencing data to check sample type identity. And in future, we will extend its functions to give more support to cfDNA sequencing quality control and data auditing, which are indispensable processes to provide steady sequencing service.    ACKNOWLEDGMENTS\r\n  This study was financed partially by National Science Foundation of China (NSFC Project No.61472411), Technology Development and Creative Design Program of Nanshan Shenzhen (Project No. KC2015JSJS0028A), and Special Funds for Future Industries of Shenzhen (Project No.  JSGG20160229123927512).  In summary, the fragmentation patterns of cell-free DNA were carefully studied, and a classification model was built based on these patterns to discriminate cfDNA or noncfDNA sequencing data. A tool called CfdnaPattern is de- REFERENCES veloped for training and benchmarking the cfDNA classifiers using different algorithms, and the chosen classifier [1] Mandel P, Metais P. (1948). Les acides nucleiques du plasma sanachieved an average of 99.89% accuracy ( = 0:00068) in [2] ogtueiznactuheIz,Sl'ehrodmyumkeO[i,nPFortaenpcohv]a. CG,RShSeealenpcoevs SVo, cABleiochlFinila1R4,2M:24o1ly-2a4k3a. the cross validation process. Y, Anan'ev V, Bazin I, Garin A, Narimanov M, Melkonyan H,  Deep analysis of different chromosomes gave similar Umansky S, Lichtenstein AV. ( 2000). Genetic analysis of DNA patterns for autosomes and sex chromosomes. Comparing eDxNcrAetesdeqinueunrciense:fraonmewceallpsprdoyaicnhg foinr daenteoctrignagnisspmec.ifiCclingenCohme mic to the patterns of whole genome sequencing, the patterns 46:1078-1084. of mitochondrial DNA were found closer than supposed, [3] Sriram KB, Relan V, Clarke BE, Duhig EE, Windsor MN, Matar KS, suggesting that all chromosomes and mitochondria may teitfyalc.y(t2o0l1o2g)i.caPllleyurnaelgflatuivide cmelal-lifgreneanDtNpAleuinratelgerfiftuysiionndsexintcoluiddienngshare same mechanisms to produce cfDNA. The cfDNA mesotheliomas. BMC Cancer 12:428. fragmentation patterns of long or short cell-free DNA frag- [4] Liimatainen SP, Jylhv J, Raitanen J, Peltola JT, Hurme MA. (2013). ments also have similar patterns, which indicates the short The concentration of cell-free DNA in focal epilepsy. Epilepsy Res DNA fragments are mainly from the release from cells, not [5] 1S0tr5o(u3)n:2M92,-8Lyautey J, Lederrey C, Olson-Sand A, Anker P. (2001). produced by the degradation of longer fragments when they About the possible origin and mechanism of circulating DNA: are circulating. Apoptosis and active DNA release. Clin Chim Acta 313(1-2):139cfDSNinAc,ewtehecagnenimomagicinDe NthAat hthase ddiaftfaerepnattteprantstewrnisll fbreoma [6] J4Ra2Dh. r, eSt, aHl e(n2t0z0e1)H.D,ENnAglifsrcahgmS,enHtsaridnt tDhe, Fbalocokdelmplaaysemr aFOof, cHaensccehr combination of cfDNA and gDNA patterns if large amount patients: quantitations and evidence for their origin from apoptotic of gDNA get mixed into cfDNA sample. This idea gives and necrotic cells. Cancer Res 61(4):1659-65 a possible way to check if hemolysis happened during the [7] CNhoinuinRvWasKiv,eCphraennaKtaClAd,iaGganoosYi,s LoafufeVtaYlMc h,rZohmeonsgomWa,leatnael.u(p2l0o0id8)y. blood storage or shipping stages. For pair-end sequencing, by massively parallel genomic sequencing of DNA in maternal the read length distribution can be also used as an important plasma. Proc Natl Acad Sci U S A 105: 20458-20463. feature for such classification tasks. We will try to implement [8] Diehl F, Schmidt K, Choti MA, et al (2008). Circulating mutant DNA this hemolysis checking function once we collect enough [9] tAotaamssaesnsiutuk mJ,oKrodpyenckamyCic,sS.kNoautpMyeSd, e1t4a:9l.8(52-091920).. Apoptotic cell-free data from samples with hemolysis for training our models. DNA promotes inflammation in haemodialysis patients. Nephrol This feature will be also included in CfdnaPattern tool once Dial Transplant 27:902-905. it is finished. [10]AFnaanlyHsiCs, oBflutmheensfiezled dYiJs,tCrihbuittkiaornasUo,fHfuetdaglinansdL,mQautaekrneaSlRc.e(l2l0-f1r0e)e.  Some other kinds of cfDNA also worth a study. We DNA by paired-end sequencing. Clin Chem 56(8):1279-86 analysed the pattern from urinary cfDNA and cerebrospinal [11] Chandrananda D, Thorne NP, Ganesamoorthy D, Bruno DL, fluid cfDNA, we found urinary cfDNA were with certain Benjamini Y, et al.. Investigating and Correcting Plasma DNA patterns while we didn't find any stable patterns for cere- SOeNquEe9n(c1i)n: ge8C6o9v9e3r.age Bias to Enhance Aneuploidy Discovery. PLoS brospinal fluid cfDNA. Other kinds of cfDNA, like pleural [12] Chandrananda D, Thorne NP, Bahlo M. (2015). High-resolution fluid cfDNA may also have certain patterns, but remain to characterization of sequence signatures due to non-random cleavbe examined. Since the data of cerebrospinal fluid cfDNA [13]aIgveaonfocveellt-farle.e(2D0N15A)..NBMonC-raMneddomGefnraogmmicesn8ta:2ti9o.n patterns in circuare downloaded from NCBI SRA, we currently cannot deter- lating cell-free DNA reflect epigenetic regulation. BMC Genomics mine whether the difference and instability of cerebrospinal 16(Suppl 13):S1. 7    ",
    "sourceCodeLink": "https://github.com/OpenGene/CfdnaPattern",
    "publicationDate": "0",
    "authors": [],
    "status": "Success",
    "toolName": "CfdnaPattern",
    "homepage": ""
  },
  "81.pdf": {
    "forks": 4,
    "URLs": ["github.com/CompSynBioLab-KoreaUniv/FunGAP"],
    "contactInfo": ["igchoi@korea.ac.kr"],
    "subscribers": 3,
    "programmingLanguage": "Python",
    "shortDescription": "FunGAP: fungal Genome Annotation Pipeline",
    "publicationTitle": "FunGAP: Fungal Genome Annotation Pipeline using evidence-based gene model evaluation",
    "title": "FunGAP: Fungal Genome Annotation Pipeline using evidence-based gene model evaluation",
    "publicationDOI": "10.1093/bioinformatics/btx353",
    "codeSize": 88193,
    "publicationAbstract": "Motivation: Successful genome analysis depends on the quality of gene prediction. Although fungal genome sequencing and assembly have become trivial, its annotation procedure has not been standardized yet. Results: FunGAP predicts protein-coding genes in a fungal genome assembly. To attain highquality gene models, this program runs multiple gene predictors, evaluates all predicted genes, and assembles gene models that are highly supported by homology to known sequences. To do this, we built a scoring function to estimate the congruency of each gene model based on known protein or domain homology. Availability and implementation: FunGAP is written in Python script and is available in GitHub (https://github.com/CompSynBioLab-KoreaUniv/FunGAP). This software is freely available only for noncommercial users. Contact: igchoi@korea.ac.kr Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-10-20T06:46:27Z",
    "institutions": [
      "Korea University",
      "Berkeley",
      "US Department of Energy Joint Genome Institute"
    ],
    "license": "No License",
    "dateCreated": "2016-12-06T10:06:02Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx353   FunGAP: Fungal Genome Annotation Pipeline using evidence-based gene model evaluation     Byoungnam Min  0    Igor V. Grigoriev  1  2    In-Geol Choi  0    0  Department of Biotechnology, College of Life Sciences and Biotechnology, Korea University ,  Seoul 02841 ,  Korea    1  Department of Plant and Microbial Biology, University of California, Berkeley ,  Berkeley, CA 94720 ,  USA    2  US Department of Energy Joint Genome Institute ,  Walnut Creek, CA 94598 ,  USA     2017   1  1  2   Motivation: Successful genome analysis depends on the quality of gene prediction. Although fungal genome sequencing and assembly have become trivial, its annotation procedure has not been standardized yet. Results: FunGAP predicts protein-coding genes in a fungal genome assembly. To attain highquality gene models, this program runs multiple gene predictors, evaluates all predicted genes, and assembles gene models that are highly supported by homology to known sequences. To do this, we built a scoring function to estimate the congruency of each gene model based on known protein or domain homology. Availability and implementation: FunGAP is written in Python script and is available in GitHub (https://github.com/CompSynBioLab-KoreaUniv/FunGAP). This software is freely available only for noncommercial users. Contact: igchoi@korea.ac.kr Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Inanc Birol    1 Introduction\r\n  Gene prediction is the most fundamental process in eukaryotic genome annotation. Several eukaryotic gene prediction programs, such as Augustus  (Stanke et al., 2006) , Maker  (Cantarel et al., 2008) , GeneMark  (Lukashin and Borodovsky, 1998)  and Braker1  (Hoff et al., 2016) , are available. Because these programs often output different predictions for the same genomic region, evaluation of predicted gene models is essential when using multiple gene predictors.  We developed a fully automated fungal Genome Annotation Pipeline designated 'FunGAP', which employs three publicly available programs, Augustus, Braker and Maker, to generate all plausible predicted gene models. FunGAP is a powerful tool for evaluating and filtering preliminary gene models that are highly supported by homology to known sequences. We benchmarked the pipeline with five representative fungal genomes: Saccharomyces cerevisiae, Neurospora crassa, Schizophyllum commune, Rhizopus oryzae and Gonapodya prolifera, which represent different branches of the fungal tree of life. The outputs were compared to those of other eukaryotic gene prediction programs.    2 Software description\r\n   2.1 Input files\r\n  FunGAP takes two inputs: genome assembly in FASTA format and Illumina-generated mRNA sequencing reads in FASTQ format (two paired-end files). Users also need to provide their own protein database in FASTA for homology searching, which can be facilitated by download_sister_orgs.py script packaged with FunGAP.    2.2 Annotation procedure\r\n  FunGAP comprises three major steps: (i) preprocessing, (ii) gene prediction and (iii) evaluation and filtration. In preprocessing, the repeats in the input assembly are masked, and mRNA sequencing reads are assembled. FunGAP uses Augustus, Braker and Maker for  5913 5913 10 785 10 785 13 194 13 194 17 459 17 459 13 831 13 831 gene prediction. The evidence scores for all predicted gene models are calculated based on the alignment of translated protein sequences with Pfam  (Finn et al., 2016) , Benchmarking Universal Single-Copy Orthologs (BUSCO)  (Simao et al., 2015)  and BLAST  (Boratyn et al., 2013) . The tools embedded in FunGAP are listed in Supplementary Table S1. The detailed pipeline description is explained in the Supplementary Notes.  The most prominent feature of FunGAP is its evaluation and filtration function. FunGAP produces 'non-overlapping' coding sequences by evaluating all gene models and retaining only bestscored models. The evaluation is performed by three tools: BLASTp, BUSCO and InterProScan  (Jones et al., 2014) . The assumption is that gene models with higher similarity to known sequences are more likely to be actual genes. Every gene model is translated and is assigned an evidence score by summing alignment bit scores for Pfam, BUSCO, and BLAST against the user-provided protein database. In the BLAST alignment, matching sequence length coverages (matched length/query length and matched length/target length) are multiplied by the BLAST bit score because longer gene models have more chances of getting higher bit scores. The evaluation score can be represented by the following equation:  Evidence score ¼ ðBXLAPSfTamscoscroeres:coverageÞ þ BUSCO score þ To aggregate the related gene models, FunGAP finds a set of gene models with at least one base pair overlap and tagged as a 'gene block' (Supplementary Fig. S2). In a gene block, the best combination of gene models with the highest evidence score sum is selected as the final gene models. Short coding sequence overlap (&lt;10% of coding sequence length) is allowed.    2.3 Output\r\n  FunGAP organizes outcomes into the various public formats, such as GFF3 and FASTA.     3 Results\r\n  We compared FunGAP annotation results with those of three known programs (Augustus, Maker, and Braker) and representative predictions (RefSeq in NCBI). The tested fungal genomes include S. cerevisiae, N. crassa, S. commune, R. oryzae and G. prolifera (Supplementary Table S2). Evaluation criteria established for checking the quality of gene predictions include BLAST hits against the provided protein database and SwissProt, all Pfam domains and gene models containing them, complete and missing BUSCOs, all transcriptome alignments and those with &gt;90% coverage, and the number of genes with the same exon-intron structure to the reference genes. As a whole, FunGAP showed more reliable predictions than those shown by the sole usage of other programs. The benchmark results are summarized in Table 1 and Supplementary Tables S3-S7. For all the five genomes, integrating three programs using our evaluation and filtration method made best matches to the Note: Predicted genes (up) and reference matches (down) are shown. We counted the matches when predicted genes have the same exon-intron coordinates with the corresponding reference genes. references. This indicates that a reliable algorithm was used to combine multiple programs.  Benchmarking was performed on a server with 48 cores [Intel(R) Xeon(R) CPU E5-2670 v3] and 64 GB of RAM. For multithread jobs, 40 cores were used. The total running time ranged from 9 to 36 h (Supplementary Fig. S4). FunGAP requires 14 GB hard disk space for installation, which the majority is from InterProScan (11 GB).    Funding\r\n  This work was supported by the Cooperative Research Program for Agriculture Science and Technology Development (Project No. PJ01044003), Rural Development Administration, Republic of Korea; and School of Life Sciences and Biotechnology for BK21 PLUS, Korea University.  Conflict of Interest: B.M. and I.C. receive royalties from commercial users of FunGAP. Code patent is pending approval.    ",
    "sourceCodeLink": "https://github.com/CompSynBioLab-KoreaUniv/FunGAP",
    "publicationDate": "0",
    "authors": [
      "Byoungnam Min",
      "Igor V. Grigoriev",
      "In-Geol Choi"
    ],
    "status": "Success",
    "toolName": "FunGAP",
    "homepage": ""
  },
  "57.pdf": {
    "forks": 0,
    "URLs": [
      "www.youtube.com/watch?",
      "github.com/samocooper/nucli",
      "uk.mathworks.com/matlabcentral/fileexchange/61479-samocooper-nuclitrack-matlab"
    ],
    "contactInfo": ["sam@socooper.com"],
    "subscribers": 2,
    "programmingLanguage": "Python",
    "shortDescription": "A python developed graphical user interface for tracking cell nuclei",
    "publicationTitle": "NucliTrack: an integrated nuclei tracking application",
    "title": "NucliTrack: an integrated nuclei tracking application",
    "publicationDOI": "10.1093/bioinformatics/btx404",
    "codeSize": 5715,
    "publicationAbstract": "Summary: Live imaging studies give unparalleled insight into dynamic single cell behaviours and fate decisions. However, the challenge of reliably tracking single cells over long periods of time limits both the throughput and ease with which such studies can be performed. Here, we present NucliTrack, a cross platform solution for automatically segmenting, tracking and extracting features from fluorescently labelled nuclei. NucliTrack performs similarly to other state-of-the-art cell tracking algorithms, but NucliTrack's interactive, graphical interface makes it significantly more user friendly. Availability and implementation: NucliTrack is available as a free, cross platform application and open source Python package. Installation details and documentation are at: http://nuclitrack.read thedocs.io/en/latest/ A video guide can be viewed online: https://www.youtube.com/watch? v¼J6e0D9F-qSU Source code is available through Github: https://github.com/samocooper/nucli track. A Matlab toolbox is also available at: https://uk.mathworks.com/matlabcentral/fileexchange/61479-samocooper-nuclitrack-matlab. Contact: sam@socooper.com Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-10-17T17:47:27Z",
    "institutions": [
      "Imperial College",
      "The Institute of Cancer Research"
    ],
    "license": "No License",
    "dateCreated": "2017-02-03T11:03:09Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx404   NucliTrack: an integrated nuclei tracking application     Sam Cooper  0  1    Alexis R. Barr  1    Robert Glen  0    Chris Bakal  1    0  Department of Computational Systems Medicine, Imperial College ,  South Kensington Campus, London SW7 2AZ ,  UK    1  Division of Cancer Biology, The Institute of Cancer Research ,  London SW3 6JB ,  UK     2017   1  1  3   Summary: Live imaging studies give unparalleled insight into dynamic single cell behaviours and fate decisions. However, the challenge of reliably tracking single cells over long periods of time limits both the throughput and ease with which such studies can be performed. Here, we present NucliTrack, a cross platform solution for automatically segmenting, tracking and extracting features from fluorescently labelled nuclei. NucliTrack performs similarly to other state-of-the-art cell tracking algorithms, but NucliTrack's interactive, graphical interface makes it significantly more user friendly. Availability and implementation: NucliTrack is available as a free, cross platform application and open source Python package. Installation details and documentation are at: http://nuclitrack.read thedocs.io/en/latest/ A video guide can be viewed online: https://www.youtube.com/watch? v¼J6e0D9F-qSU Source code is available through Github: https://github.com/samocooper/nucli track. A Matlab toolbox is also available at: https://uk.mathworks.com/matlabcentral/fileexchange/61479-samocooper-nuclitrack-matlab. Contact: sam@socooper.com Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Robert Murphy    1 Introduction\r\n  Live imaging studies are now allowing us to explore the previously shrouded world of mammalian cell signalling dynamics. For example, imaging populations of live single cells over several days has revealed how signalling dynamics can control key cell fate decisions  (Cooper and Bakal, 2017) . However, hindering the ease, timeperiods and throughput with which live single-cell studies can be performed, are challenges in automatically tracking fluorescently labelled objects (cells, nuclei or organelles) accurately over time periods often exceeding several days, sometimes in highly motile cells  (Cabantous et al., 2005) .  To date, the majority of cell tracking software relies on 'frameto-frame linking' of segmented nuclei or cells  (Meijering et al., 2012) . In such methods, errors in segmentation, where, for example, either multiple nuclei are labelled as a single segment, or a nucleus is missed over several timeframes resulting in a tracking gap, can severely compromise tracking results. To handle such difficulties, global optimization methods have now been developed  (Milan et al., 2016) . Yet, implementations of global optimization approaches to tracking in cell biology research have generally been limited to standalone packages, meaning that segmentation, track inspection and track correction have to be performed in other software which is non-trivial for scientists with little programming expertise  (Hilsenbeck et al., 2016) . Therefore, we have developed NucliTrack an integrated Python package that allows efficient segmentation, feature extraction and tracking of fluorescently labelled nuclei or cells through an intuitive graphical interface (Fig. 1). Additionally, the interface also includes tools for inspecting, correcting and exporting data on tracked nuclei.    2 Implementation\r\n  The identification and segmentation of fluorescently labelled objects in NucliTrack is based on marker controlled watershed segmentation, built with the scikit image Python library  (van der Walt et al., 2014) . We utilized a novel Python application development library 'Kivy' to create an interface that allows users to interactively vary segmentation parameters  (Virbel et al., 2011) . Once effective parameters are chosen, segmentation is then performed on the entire imaging sequence.  In many types of image analysis workflows aimed at quantifying cellular phenotypes, the segmentation step can result in errors, which must be handled by tracking algorithms. Moreover, cells undergoing cell division must also be detected. To address this, we use a probabilistic approach described by Magnusson et al. to perform tracking  (Magnusson et al., 2015) . This requires training data to be selected that contains examples of any erroneous segmentation, as well as normal, mitotic and post-mitotic cells. Training is performed within the user interface prior to tracking. The tracking algorithm then seeks to globally optimize the set of all tracks, by iteratively adding the highest scoring track given the current set of tracks. Tracks are scored with a cost function which penalizes large movements, gaps between frames, and passage through segments which are unlikely to be nuclei, whilst highly scoring tracks that minimize movement between frames, include few gaps and pass through segments likely to be nuclei. A dynamic programming technique is then used to find the highest scoring track that is added to the set of all tracks. Importantly, in identifying the highest scoring track, swaps with neighbouring tracks may be performed, thus minimizing the likelihood that a local, rather than a global, optimum is found.  After successful tracking of objects, tracks can be edited within the user interface. Here, three windows showing: (i) colour-coded tracked segments; (ii) the original video; and (iii) feature values over the full length of a selected track (in the form of a line plot over time), allow the user to easily analyze and inspect tracks for errors, or anomalous results. Moreover, using a set of tools with keyboard hotkeys the user can efficiently navigate the video, manually correct errors in tracking, and add flags to mark specific events, before exporting results. Throughout NucliTrack we have also incorporated data management steps, such that intermediate steps are saved in the HDF5 file format, through the Python h5py package  (Collette, 2013) . Parameters are stored in a separate HDF5 file meaning that the segmentation and training stages can be skipped when analyzing subsequent movies. A function also exists in the Python package that accommodates for batch processing of videos using the HDF5 parameter file.  To characterize the performance of the global optimization approach to automated cell tracking we tested NucliTrack against a benchmark set of fluorescently labelled nuclei videos  (Maska et al., 2014; Svoboda et al., 2009) . Here NucliTrack performed similarly to other global optimization approaches (Supplementary Methods). The performance of NucliTrack was limited by the more basic segmentation procedures we adopt, however we believe having an intuitive and interactive workflow is significantly more important for users with less technical expertise. Labeled images, segmented in alternative programs may also be imported in the file loading step.  NucliTrack is an easy to use package for tracking and extracting data from fluorescently labelled cells or nuclei, in significantly higher throughput than manual tracking can achieve. Importantly, by allowing users to quickly inspect and correct tracking data, near 100% accuracy can be achieved, which is critical in quantifying cell fate decisions. For example, NucliTrack allowed us to study how DNA damage influences the proliferation-quiescence decision in thousands of cells, tracked for hundreds of frames, over multiple days  (Barr et al., 2017) . Finally, by coupling established packages for data processing in Python, with more recent solutions to data management (h5py), graphical user interface development (kivy) and image processing (skimage), this software also provides an example of how user friendly, scientific software may be efficiently developed in an open source environment.    Funding\r\n  S.C. is supported by the NIHR Imperial BRC, and the STRATiGRAD graduate training program. C.B. and A.R.B. are funded by a BBSRC Strategic LoLa grant [BB/M00354X/1], C.B. is also funded by a Cancer Research UK Programme Foundation Award [C37275/1A20146].  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/samocooper/NucliTrack",
    "publicationDate": "0",
    "authors": [
      "Sam Cooper",
      "Alexis R. Barr",
      "Robert Glen",
      "Chris Bakal"
    ],
    "status": "Success",
    "toolName": "nuclitrack",
    "homepage": ""
  },
  "26.pdf": {
    "forks": 1,
    "URLs": ["github.com/uio-cels/NucDiff"],
    "contactInfo": ["lex.nederbragt@ibv.uio.no"],
    "subscribers": 5,
    "programmingLanguage": "Python",
    "shortDescription": "In-depth characterization and annotation of differences between two sets of DNA sequences",
    "publicationTitle": "NucDiff: in-depth characterization and annotation of differences between two sets of DNA sequences",
    "title": "NucDiff: in-depth characterization and annotation of differences between two sets of DNA sequences",
    "publicationDOI": "10.1186/s12859-017-1748-z",
    "codeSize": 3279,
    "publicationAbstract": "Background: Comparing sets of sequences is a situation frequently encountered in bioinformatics, examples being comparing an assembly to a reference genome, or two genomes to each other. The purpose of the comparison is usually to find where the two sets differ, e.g. to find where a subsequence is repeated or deleted, or where insertions have been introduced. Such comparisons can be done using whole-genome alignments. Several tools for making such alignments exist, but none of them 1) provides detailed information about the types and locations of all differences between the two sets of sequences, 2) enables visualisation of alignment results at different levels of detail, and 3) carefully takes genomic repeats into consideration. Results: We here present NucDiff, a tool aimed at locating and categorizing differences between two sets of closely related DNA sequences. NucDiff is able to deal with very fragmented genomes, repeated sequences, and various local differences and structural rearrangements. NucDiff determines differences by a rigorous analysis of alignment results obtained by the NUCmer, delta-filter and show-snps programs in the MUMmer sequence alignment package. All differences found are categorized according to a carefully defined classification scheme covering all possible differences between two sequences. Information about the differences is made available as GFF3 files, thus enabling visualisation using genome browsers as well as usage of the results as a component in an analysis pipeline. NucDiff was tested with varying parameters for the alignment step and compared with existing alternatives, called QUAST and dnadiff. Conclusions: We have developed a whole genome alignment difference classification scheme together with the program NucDiff for finding such differences. The proposed classification scheme is comprehensive and can be used by other tools. NucDiff performs comparably to QUAST and dnadiff but gives much more detailed results that can easily be visualized. NucDiff is freely available on https://github.com/uio-cels/NucDiff under the MPL license.",
    "dateUpdated": "2017-09-04T16:10:05Z",
    "institutions": [
      "University of Oslo",
      "Oslo University Hospital",
      "Norwegian Veterinary Institute"
    ],
    "license": "Mozilla Public License 2.0",
    "dateCreated": "2016-09-13T11:55:54Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     Khelik et al. BMC Bioinformatics     10.1186/s12859-017-1748-z   NucDiff: in-depth characterization and annotation of differences between two sets of DNA sequences     Ksenia Khelik  0    Karin Lagesen  0  3    Geir Kjetil Sandve  0    Torbjørn Rognes  0  2    Alexander Johan Nederbragt  lex.nederbragt@ibv.uio.no  0  1    0  Biomedical Informatics Research Group, Department of Informatics, University of Oslo ,  PO Box 1080, 0316 Oslo ,  Norway    1  Centre for Ecological and Evolutionary Synthesis, Department of Biosciences, University of Oslo ,  PO Box 1066 Blindern, 0316 Oslo ,  Norway    2  Department of Microbiology, Oslo University Hospital ,  Rikshospitalet, PO Box 4950 Nydalen, 0424 Oslo ,  Norway    3  Norwegian Veterinary Institute ,  PO Box 750 Sentrum, 0106 Oslo ,  Norway     2017   18  2  15    4  7  2017    23  9  2016     Background: Comparing sets of sequences is a situation frequently encountered in bioinformatics, examples being comparing an assembly to a reference genome, or two genomes to each other. The purpose of the comparison is usually to find where the two sets differ, e.g. to find where a subsequence is repeated or deleted, or where insertions have been introduced. Such comparisons can be done using whole-genome alignments. Several tools for making such alignments exist, but none of them 1) provides detailed information about the types and locations of all differences between the two sets of sequences, 2) enables visualisation of alignment results at different levels of detail, and 3) carefully takes genomic repeats into consideration. Results: We here present NucDiff, a tool aimed at locating and categorizing differences between two sets of closely related DNA sequences. NucDiff is able to deal with very fragmented genomes, repeated sequences, and various local differences and structural rearrangements. NucDiff determines differences by a rigorous analysis of alignment results obtained by the NUCmer, delta-filter and show-snps programs in the MUMmer sequence alignment package. All differences found are categorized according to a carefully defined classification scheme covering all possible differences between two sequences. Information about the differences is made available as GFF3 files, thus enabling visualisation using genome browsers as well as usage of the results as a component in an analysis pipeline. NucDiff was tested with varying parameters for the alignment step and compared with existing alternatives, called QUAST and dnadiff. Conclusions: We have developed a whole genome alignment difference classification scheme together with the program NucDiff for finding such differences. The proposed classification scheme is comprehensive and can be used by other tools. NucDiff performs comparably to QUAST and dnadiff but gives much more detailed results that can easily be visualized. NucDiff is freely available on https://github.com/uio-cels/NucDiff under the MPL license.    Whole-genome alignment  Comparative analysis  Whole-genome assembly  Annotation of differences       Background\r\n  Advances in whole genome sequencing strategies and assembly approaches have brought on a need for methods for comparing sets of sequences to each other. Common questions asked are how assemblies of the same read set obtained with different assembly programs differ from each other, or how genomes from different strains of the same bacterial species differ from each other. Whole genome alignment (WGA) methods are often used for performing such analyses and have long been studied in bioinformatics. WGA \u201cis, in general, the prediction of homologous pairs of positions between two or more sequences\u201d [ 1 ]. WGA is mainly used for identifying conserved sequences between genomes, e.g. genes, regulatory regions, non-coding RNA sequences, and other functional elements [ 2, 3 ], thus aiding, for instance, genome (functional) annotation, detecting large scale evolutionary changes between genomes, and phylogenetic inference [ 1, 2 ]. This field has been under continuous development since the 1970s, and many methods and tools for WGA have been created. Reviews of existing methods and tools can be found in [ 1, 4, 5 ].  For the purpose of detecting differences between sequence sets, tools that can be used to perform WGA analysis should come with certain features. First, they should be able to deal with very fragmented genomes, structural rearrangements, genome sequence duplications, and various differences that are often related to repeated regions. Second, the comparative analysis results should provide information about the types of differences and their locations. This information should be stored in ways suitable for further analysis. Such comparison information may, for example, be used for scaffolding purposes, for reference-assisted genome assembly, assembly error detection, and comparison of different assemblies. Third, they should enable visualisations of alignment results at different levels of detail. Global scale visualisation can be used for examining duplications, structural rearrangements, and uncovered regions, while local scale visualisation can provide information about small differences, such as substitutions, insertions and deletions (collectively called 'indels').  Three different tools are available today that partially satisfy these criteria: MAUVE [ 6 ], QUAST [ 7 ] and dnadiff [ 8 ]. MAUVE performs multiple genome alignment, identifies conserved genomic regions, rearrangements and inversions in these regions, and the exact sequence breakpoints of such rearrangements across multiple genomes as well as nucleotide substitutions and small indels [ 6 ]. It also enables analysis of results through interactive visualisation and stores information in separate files. However, only information about small differences (substitutions, indels) is easily accessible without running accessory programs.  QUAST is a tool for quality assessment of genome assemblies, which outputs different metrics on assembly quality in the presence of a reference genome. It gives information about the locations of structural and long local differences, specifying the types of structural differences only. QUAST enables visualisation in an accompanying genome browser called Icarus. However, QUAST lacks visualisation of small local differences, only providing summary statistics for them.  Dnadiff is a wrapper for the NUCmer alignment program from MUMmer [ 9 ] that quantifies the differences and provides alignment statistics and other high-level metrics [ 8 ]. Similar to QUAST, dnadiff can be used for quality assessment of assemblies and comparison of genomes, but it does not provide any visualization of the detected differences.  Here we present the tool NucDiff, which uses the NUCmer, delta-filter and show-snps programs from MUMmer for sequence comparison. NUCmer aligns sequences and outputs information about aligned sequence regions. Rigorous analysis of the relative positions of these regions enables detection of various types of differences, including rearrangements and inversions, and in some cases also to ascertain their connection with repeated regions. NucDiff identifies the differences between two sets of closely related sequences and classifies the differences into several subtypes. The precise locations of all differences using coordinates systems with respect to both input sequences are output as GFF3 (Generic Feature Format version 3, [ 10 ]) files. These precise locations enables both visualisation and further analysis. The information provided by NucDiff can thus significantly help clarify how two sets of sequences differ.    Implementation\r\n  NucDiff determines the various types of differences between two sets of sequences, usually referred to as a reference genome and a query, by parsing alignment results produced by the NUCmer, delta-filter and showsnps programs from the MUMmer sequence alignment package [ 9 ]. NUCmer performs DNA sequence alignment, while delta-filter filters the alignment results according to specified criteria. With the settings used by NucDiff by default, delta-filter also selects the longest consistent alignments for the query sequences. NUCmer alignment results contain information about fragments of sequences that match, which we here refer to as query and reference fragments. NUCmer output contains the exact coordinates of all fragments in relation to their source sequences, directions of query fragments relative to corresponding reference fragments, and percent similarity of the alignment. The show-snps results contain information about all inserted, deleted and substituted bases in the query fragments compared to the corresponding reference fragments.  If we represent the output fragments as blocks on the query and reference sequences, then a possible NUCmer alignment result may look as illustrated in Fig. 1.  During the alignment process, NUCmer searches for maximal exact matches of a given minimum length, then clusters these matches to form larger inexact alignment regions, and finally extends alignments outwards from each of the matches to join the clusters into a single high scoring pairwise alignment [ 11 ]. If the query sequences contain long (by default, more than 200 bp) insertions, deletions, substitutions, or any structural rearrangements, the alignment will be broken and subsequently consist of separate fragments with the ends coinciding with the locations of these differences. NucDiff classifies the alignment fragments by analysing the placement of all pairs of neighbouring query fragments (A-B, B-C, etc. in Fig. 1), their placement on the reference sequences (A*-B*, B*-C*, etc. in Fig. 1), and their orientations (5\u2032 to 3\u2032, or 3\u2032 to 5\u2032). The obtained differences together with the differences from show-snps form the set of all differences between query and reference sequences.  The NucDiff workflow is shown in Fig. 2. An overview of all types of differences that NucDiff is able to detect is presented in the Types of differences section. A description of the steps involved in their detection is given in the Stepwise detection of differences section.   Types of differences\r\n  We classify all types of differences into 3 main groups: global, local and structural (Fig. 3). These differences are here denoted as changes in the query when compared to the reference.   Global differences\r\n  Global differences affect the whole query sequence. This group consists of only one type, called unaligned sequence. unaligned sequence - a query sequence that has no matches of length equal to or longer than a given number of bases (65 by default) with the reference genome.    Local differences\r\n  Local differences involve various types of insertions, deletions and substitutions. NucDiff distinguishes between six types of insertions (the insertion subgroup in Fig. 3): simple insertion - an insertion of bases in the query sequence that were not present anywhere on the reference genome. duplication - an insertion in the query sequence of an extra copy of some reference sequence not adjacent to this region, creating an interspersed repeat, or increasing the copy number of an interspersed repeat tandem duplication - an insertion of an extra copy of some reference sequence region adjacent to this region in the query sequence inserted gap - an insertion of unknown bases (N's) in the query sequence in a region which is continuous (without a gap) in the reference, or which results in an elongation of a region of unknown bases in the reference. unaligned beginning - unaligned bases in the beginning of a query sequence unaligned end - unaligned bases at the end of query sequence  There are several types of deletions (the deletion subgroup in Fig. 3): simple deletion - a deletion of some bases, present in the reference sequence, from a query sequence collapsed repeat - a deletion of one copy of an interspersed repeat from the reference sequence in a query sequence collapsed tandem repeat - a deletion of one or more tandem repeat units from the reference sequence in a query sequence  And, last, there are two types of substitutions (the substitution subgroup in Fig. 3): substitution - a substitution of some reference sequence region with another sequence of the exact same length not present anywhere in the reference genome (note that this sequence is not categorised as unaligned sequence because it is within a fragment that overlaps between query and reference). SNPs can be considered as a subcategory of substitutions. gap - a substitution where a reference subsequence is replaced by an unknown sequence (N's) of the same length. If the query has an enlarged gap, it will be classified as a combination of a gap and an inserted gap, while a shortened gap is classified as a gap and a simple deletion.    Structural differences\r\n  NucDiff detects several structural differences. These can be grouped into intra- and inter-chromosomal differences, and some of these contain groups of types: translocation - a group of different types of inter-chromosomal structural rearrangements which occur when two regions located on different reference sequences are placed nearby in the same query sequence. The detailed description of all translocation types is given in the Structural difference detection between aligned fragments section. relocation - a group of different types of intrachromosomal structural rearrangements which occur when two regions located in different parts of the same reference sequence are placed nearby in the same query sequence. The detailed description of all relocation types is given in the Structural difference detection between aligned fragments section. reshuffling - an intra-chromosomal structural rearrangement which occurs when several neighbouring reference sequence regions are placed in a different order in a query sequence. inversion - an intra-chromosomal structural rearrangement which occurs when a query sequence region is the reverse complement of a reference sequence region.  The translocation type belongs to the inter-chromosomal subgroup, while relocation, reshuffling and inversion types belong to the intra-chromosomal subgroup (see Fig. 3). Examples of structural differences are given in Fig. 4.     Stepwise detection of differences\r\n  The steps in this section refer to Fig. 2.   Global difference detection\r\n  NucDiff starts the detection of differences by finding unaligned sequence differences. NUCmer does not output any information about sequences without mapped subsequences longer or equal to a predefined length. Therefore, to find unaligned sequences, NucDiff looks for query sequences with names not mentioned in the NUCmer output. By default, all query sequences shorter than 65 bp will be treated as unaligned sequences. This threshold may be changed using the NUCmer minimum cluster length option.    Local difference detection inside aligned fragments\r\n  Four types of simple differences may be detected inside the query fragments: simple insertion, simple deletion, simple substitution and gap. The lengths of the differences of these types are limited by how far NUCmer will attempt to extend poorly scoring regions before giving up and are up to 200 bases by default (this threshold may be changed using the NUCmer minimum length of a maximal exact match parameter). Information about the positions of all local differences, except gaps, is found in the show-snps output file. NucDiff parses this file to find simple insertions, simple deletions, and substitutions. To find gaps, NucDiff searches for N's in the query fragment sequences and outputs their locations.    Local difference detection between aligned fragments\r\n  NucDiff starts with examining the reason for alignment fragmentation by looking at fragmentation caused by local differences. First, it filters nested fragments in the query and reference sequences. A query nested fragment occurs when two (nearly) identical reference sequence regions have been merged together into one fragment in the query sequence. A reference nested fragment occurs when one reference sequence region is duplicated in the query sequence. Nested fragments provide important information about duplications and collapsed repeats. However, they can cause rather complicated interactions between aligned fragments, which can be difficult to resolve programmatically. Thus, the nested fragments are discarded, and all duplications and collapsed repeats are detected as simple insertions and deletions at later stages of the analysis. Then, NucDiff identifies bases in both ends of the query sequences that were not mapped to the reference sequences. Such bases will be output as unaligned beginning and unaligned end differences.  NucDiff next searches for pairs of neighbouring fragments that were not joined together by NUCmer during the alignment process due to the presence of simple differences, rather than structural differences. Such pairs of fragments should satisfy the following criteria: The pair of query fragments as well as the corresponding pair of reference fragments may overlap, be adjacent to each other, or be separated by an inserted region not mapped anywhere on the reference genome.  The two query fragments should have the same direction. Their two corresponding reference fragments should also have the same direction, but it may be opposite to the direction of the query fragments.  If the query fragments have the same direction as their corresponding reference fragments, then the reference fragments should be placed in the same order as the query fragments ([Additional file 1: Figure S1a]).  If the query fragments have the reverse direction of their corresponding reference fragments, then the reference fragments should be in reverse order ([Additional file 1: Figure S1a]).  The distance between corresponding reference fragments should not be more than a user-defined distance, by default 10,000 bases.  If all these criteria are fulfilled, NucDiff determines the differences based on the placement of the query and reference fragments relative to each other. Examples of all possible placement cases and the corresponding differences are shown in [Additional file 1: Table S1].  After detecting differences between the current pair of neighbouring fragments, NucDiff merges the pair of reference fragments as well as the pair of query fragments together, creating new continuous reference and query fragments, and then searches for the next pair.    Structural difference detection between aligned fragments\r\n  Fragments not merged during the previous step were kept separate by NUCmer due to structural rearrangements between the query and reference sequences. First, NucDiff searches for translocations, which is one type of inter-chromosomal differences, by searching for a pair of neighbouring query fragments that correspond to fragments located on different reference sequences. We distinguish between 5 types of translocations depending on the placement of the query fragments relative to each other (see also examples in Fig. 4a-c): simple translocation - a translocation where two query fragments are placed adjacent to each other. translocation with insertion - a translocation where two query fragments have a stretch of bases (not N's) inserted between them, not mapped anywhere on the reference genome. The inserted region is treated as a simple insertion difference. translocation with inserted gap - a translocation where two query fragments have a stretch of unknown bases (N's) inserted between them. The inserted region is treated as an inserted gap difference. translocation with insertion and inserted gap - a translocation where two query fragments have a stretch of bases (A, C, G, T or N's) inserted between them, not mapped anywhere on the reference genome. The inserted region is treated as both a simple insertion and an inserted gap. translocation with overlap - a translocation with a partial overlap between the two query fragments.  In the next step, NucDiff searches for relocations, which is one type of intra-chromosomal differences, by looking for pairs of neighbouring query fragments that were mapped to fragments located on the same reference sequence (e.g. the same chromosome) but separated from each other by at least 10,000 bases, by default. In addition, these fragments should not belong to the group of query fragments placed nearby each other (with the distance between each pair less than 10,000 bases) on the reference sequence in the wrong order, as that would be considered as a reshuffling (see further down). If these two conditions are fulfilled, then there is a relocation. There are 5 types of relocations (see also examples in Fig. 4d-f ): simple relocation - a relocation where two query fragments are placed adjacent to each other. relocation with insertion - a relocation where two query fragments have a stretch of bases (not N's) inserted between them, not mapped anywhere on the reference genome. The inserted region is treated as a simple insertion difference. relocation with inserted gap - a relocation where two query fragments have a stretch of unknown bases (N's) inserted between them. The inserted region is treated as an inserted gap difference. relocation with insertion and inserted gap - a relocation where two query fragments have a stretch of bases (both ATGC's and N's) inserted between them, not mapped anywhere on the reference genome. The inserted region is treated as both a simple insertion and an inserted gap. relocation with overlap - a relocation with a partial overlap between the two query fragments.  For circular genomes, there is one special case that causes alignment fragmentation: when the start of the query sequence does not coincide with the start of the reference sequence ([Additional file 1: Figure S2]). It satisfies all the criteria for relocations but is not treated as a difference, although it is included in the output.  In the case of translocations and relocations, the query and the corresponding reference fragments may be placed in any direction and order relative to each other. The translocated fragment may contain none, two or more relocated fragments inside. Before the detection of the types of relocations and translocations, NucDiff searches for the pairs of relocated or translocated query fragments that have an overlap between corresponding reference fragments. If such a pair is found, NucDiff truncates the rightmost fragment, so the overlap disappears. In this case information about the repeated nature of the insertion events will be lost.  Third, NucDiff searches for a group of nearby query fragments whose corresponding reference fragments are located on the same reference sequence (chromosome) but in a different order. The distance between two neighbouring reference fragments should not be more than 10,000 bases. If a group satisfying these conditions is found, then there is a reshuffling difference in the query. There may be simple insertion and simple deletion differences between reshuffled fragments. To find them, NucDiff first truncates fragments so that all overlaps between query or reference fragments are removed. It then searches for unmapped bases between neighbouring query fragments to find simple insertions and then searches for unmapped bases between neighbouring reference fragments to find simple deletions.  Finally, NucDiff searches for the last type of intrachromosomal structural difference, inversions. If a query sequence has several mapped fragments and one or more of them, but not all, have directions opposite to the directions of the corresponding reference fragments, then such fragments are inversions. Some examples of possible alignments of query sequences in cases with reshuffling and inversion are shown in Fig. 4g-h.  Reshufflings and inversions may be present inside translocated and relocated fragments. During reshuffling detection, the directions of reshuffled fragments are not taken into account. Their directions are checked during the inversion detection step. Simple insertions and simple deletions found during this step may be connected to repeated regions, but this connection will not be detected.     Datasets\r\n  We created ten simulated reference and query DNA sequences. The genomes were constructed from random DNA sequences, and different types of controlled genome modifications were subsequently applied to these sequences (e.g. relocation of different fragments, or deletions, or duplications of fragments). The detailed description of implemented genome modifications can be found in [Additional file 1: Table S2].  In addition, we used data produced for the GAGE-B article [ 12 ] for the demonstrations of the comparison of several assemblies. The assemblies from the ABySS [ 13 ], CABOG [ 14 ], MaSuRCA [ 15 ], SGA [ 16 ], SOAPdenovo [ 17 ] (shown as SOAP in the figures), SPAdes [ 18 ] and Velvet [ 19 ] assemblers for Vibrio cholerae based on HiSeq reads were used. These assemblies together with the V. cholerae reference genome were downloaded from the GAGE-B website [ 20 ].  For the demonstration of the comparison of genomes from different strains of the same species, 22 Escherichia coli K12 reference genomes were downloaded from the NCBI database [ 21 ]. Their accession numbers can be found in [Additional file 1: Table S3]. In the sections with the demonstrations, we also used annotations for the V. cholerae reference genome and E. coli K12 MG1655. They were downloaded from the NCBI database [ 22, 23 ], respectively.     Results\r\n   The NucDiff tool\r\n  We have created a tool, called NucDiff, which is primarily aimed at locating and categorizing differences between any two sets of closely related nucleotide sequences. It is able to handle very fragmented genomes and various structural rearrangements. These features make NucDiff suitable for comparing, for instance, different assemblies with each other, or an assembly with a reference genome. NucDiff first runs the NUCmer, delta-filter and show-snps programs from MUMmer and parses the alignment results to detect differences. These differences are subsequently categorized according to a carefully defined classification scheme of all possible differences between two sequences.  A unique feature of NucDiff is that it provides detailed information about the exact genomic locations of the differences in the form of four GFF3 files: two files with information for small and medium local differences that do not cause alignment fragmentation, two others for structural differences and local differences that cause alignment fragmentation. All locations of the differences are output in query - and reference-based coordinates, separately. Each GFF3 entry is additionally annotated with the location of the difference in the opposite coordinate system as well. A detailed description of the format of these GFF files can be found in the GitHub repository of NucDiff. NucDiff also finds the coordinates of mapped blocks (the query sequences split at the points of translocation, relocation, inversions, and/or reshuffling) and then stores them in the GFF3 files, one based on query coordinates and another with reference-based coordinates. Uploading these GFF3 files into a genome browser such as the Integrated Genome Viewer (IGV) [ 24, 25 ] enables visualisation of the differences as well as the coverage of a reference genome by query sequences, making it possible to see all uncovered reference bases or if any reference regions are covered multiple times.  In addition, NucDiff generates a summary file containing information about the number of differences of each type. The detailed level of reporting enables users to create their own custom summary from the NucDiff output (e.g. taking into account the length of differences, joining several types of differences together, and so on) if desired.    Effect of different MUMmer parameters\r\n  The alignment results parsed by NucDiff depend on the values of the input parameters for two MUMmer programs, NUCmer and delta-filter. NUCmer performs DNA sequence alignment, while delta-filter filters the alignment results according to specified criteria. Running these programs with different input parameters may result in alternative sets of matches, since the choice of parameters affects the sensitivity of the detection of matching sequence fragments as well as the stringency of the subsequent filtering. To analyse the influence of the different parameters on the alignment and on the subsequent NucDiff results, we compared the results of running NucDiff on the simulated genomes described in the Datasets section with different NUCmer and delta-filter input parameters values. The specific values for each test can be found in [Additional file 1: Table S4]. We also ran one test to enable comparison of QUAST and NucDiff as described in Comparison with QUAST section, since QUAST uses the same underlying tools as NucDiff.  The locations and types of simulated differences were compared with the results obtained from NucDiff, and the number of correctly detected differences was calculated for each test (see [Additional file 1] for details). The results with the total average number of correctly detected expected differences for each type are presented in Table 1. The detailed results for each implemented modification case (see in [Additional file 1: Table S2]) and for each parameter configuration set can be found in [Additional file 2].  We did not expect NucDiff to be able to detect all simulated differences of most types. This is confirmed in the results presented in Table 1, where NucDiff misses many differences of several types, no matter what parameter settings were used. A small deviation from the simulated results was expected since the fixed 30 bp limit for lengths of duplications in reference and query sequences and relocated blocks is much lower than the variable NUCmer and delta-filter thresholds. Another reason for the result deviation is that some difference locations were shifted a few bp due to accidental base similarity at the region borders. In such cases, the differences were considered wrongly resolved in spite of correctly detected types. These reasons are applicable to all difference types with the observed deviation to a greater or lesser extent. All other reasons are related to the chosen NUCmer and delta-filter parameter settings and NucDiff limitations and are discussed below.  The detailed results from [Additional file 2] indicate that increasing the alignment extension distance (−b parameter) led to the loss of information about repeat related local differences and inverted, relocated and substituted fragments. With a greater -b parameter value, NUCmer more successfully expands low scoring regions. It enables detection of more differences inside fragments and a reduction of the number of aligned fragments. However, at the same time, it does not allow tracking of possible locations of query regions involved in differences in the reference sequences. This leads to loss of information about the repeated, inverted and substituted nature of the regions. Changing the maximal exact match length (−l parameter) did not influence significantly on the obtained results within the considered simulations. Increasing the parameter value for minimum alignment identity (−i parameter) (see columns l65 and QUAST-like in Table 1) led to an increased number of wrongly discarded valid mapped short fragments as well as query sequences containing even a small number of short and medium length differences.  Increasing the values for the minimum cluster length (−c parameter) increases the number of discarded correct query sequences and discarded valid mapped fragments. This leads to 1) the undesirable loss of information about the inverted, relocated and translocated nature of some fragments and 2) the misrepresentation of correct query sequences as being unaligned.  Additional result deviations can be explained by the specifics and limitations of the approach implemented in NucDiff independent on the parameter values used. First, due to some simplifications during the NucDiff structural difference detection step, NucDiff does not allow detection of both relocations/translocations and duplications at the same time in cases when simple relocations/translocations are followed by duplications (see [Additional file 1: Table S2], relocation case 2 and translocation case 1). In such cases, the differences are detected either as a combination of a simple relocation/translocation and a simple insertion or as a combination of a simple insertion and a duplication depending on the length of a relocated or translocated fragment.  Second, another problem with duplication detection occurs in situations when reference fragments are duplicated and inserted into query sequences somewhere far away from their original locations (see [Additional file 1: Table S2], insertions, case 2). The duplications are detected by NUCmer but are filtered out by the delta-filter program as being aligned fragments with smaller length*identity weighted LIS [longest increasing subset]. This option is set by the -q parameter and is always used in NucDiff. As a result, NucDiff detects such duplications as simple insertions.  Third, in cases with a combination of a gap and an inserted gap, the order of the gap and the inserted gap varies depending on whether a subsequence of N's caused alignment fragmentation or not. Since in the simulated results a gap is always followed by an inserted gap, the number of correctly detected gaps was slightly lower than the expected number for all parameter settings. However, this behavior influences only the numbers in Table 1 but not the quality of the obtained results.    Comparison with QUAST\r\n  Both NucDiff and QUAST use the NUCmer package in their pipeline. However, QUAST only provides information about the locations of regions where the reference sequences were split during the alignment process and specifies the general reasons for the alignment fragmentations (e.g. local misassembly, relocation and so on). As with NucDiff, we calculated the number of correctly detected simulated differences. Since QUAST only separates the differences into broad categories, it is not possible to make direct one-to-one comparisons. We therefore grouped the simulated differences into types as described in [Additional file 1: Table S5]. A simulated difference is considered correctly detected if it overlaps with a QUAST difference that belongs to the same general category. In cases with repeat related types, a difference is considered correctly detected when one of the repeated fragments involved in the simulated difference overlaps with the QUAST difference. The obtained average total number for each type of difference is shown in Table 1. The detailed results for each simulated case (see in [Additional file 1: Table S2]) can be found in the [Additional file 2].  As expected, the results presented in Table 1 show that QUAST, as well as NucDiff, was not able to detect all simulated differences in most groups. The small deviation of QUAST results in all problematic groups can also be explained by the introduced 30 bp limit for lengths of duplications in reference and query sequences and relocated blocks and shifted locations of some differences. However, there are some additional reasons specific to QUAST.  First, QUAST does not output any information about the locations of small differences obtained after parsing the results given by the show-snps package, only providing information about their total number. This is reflected in a large deviation between the numbers of simulated and detected insertions, deletions, substitutions, gaps, and inserted gaps. Second, QUAST is unable to distinguish differences of several types at the identical locations. For example, duplications and reshufflings were not reported as stand-alone differences when they were located together with relocations or translocations. The same is also true for insertions and deletions when they were introduced between inverted and reshuffled blocks. Third, the comparison of the QUAST results with the NucDiff results obtained with the QUASTlike parameters settings suggests that QUAST has its own internal length threshold for filtering mapped fragments. This value is somewhat higher than the NUCmer -c parameter value used. This led to a reduced number of correctly detected relocation and translocation events.  During comparison of the QUAST results with the NucDiff results obtained with the QUAST-like settings, we noticed that QUAST was able to detect more duplication and translocation events. This can be explained by less strict requirements for correspondence between the simulated and obtained types for QUAST. For example, in situations where NucDiff detected simple translocations and duplications as translocation with insertions and simple insertions, respectively (see translocation case 1 in [Aditional file 1: Table S2]), the differences were considered wrongly resolved by NucDiff and correctly resolved by QUAST. The same problem is also applicable to simple relocations. However, since fewer relocations were detected by QUAST because of its filtering approach, the significant divergence between numbers is not apparent in Table 1.    Comparison with dnadiff\r\n  The NucDiff, dnadiff and QUAST tools provide a quantification of the differences between two sets of genomes. In this section, we compare the numbers output by these tools. Due to the way these tools report their results, it is very difficult to make a fair comparison between them. All tools were run on the same simulated genome described in Datasets section. NUCmer, whose output was used by NucDiff and dnadiff, was run with the QUAST-like parameter settings (see [Additional file 1: Table S4]). Since dnadiff only provides the number of differences and not their locations, we cannot know for sure whether the differences are actually in the same places as reported by the other tools. To perform the comparison, we created a set of categories suitable for comparison and grouped the differences reported into these categories (see [Additional file 1: Table S6] for grouping). The results are presented in Table 2.  The results showed that the obtained counts for NucDiff and dnadiff are largely similar, while QUAST has a tendency to detect fewer differences than NucDiff and dnadiff in almost all categories. A large deviation between the results from QUAST and the other tools was observed in the nonTandem and Relocations groups. In both cases, it can be explained by how the comparison is performed and not necessarily by the performance of the tool.    Comparison of several assemblies of the same read set to the same reference genome\r\n  We downloaded assemblies of the same V. cholerae read set as described in the Datasets section, and compared In the nonTandem group, the values shown for the simulated differences (Truth) and QUAST are the number of events, while in the other columns the values are the sum of the number of bases involved in the differences for short and medium local differences (found by the show-snps program) and the number of events of long local differences (those causing alignment fragmentation). In the Inversions group, the numbers of simulated inversions and inversions found by NucDiff were multiplied by two to enable a fair comparison, because QUAST and dnadiff report the number of fragment ends, while NucDiff reports the number of fragments. In the Substitutions group, the values shown are the number of bases, while in the other rows the values are the number of events. The reshuffling differences are contained in the nonTandem group for QUAST, but placed in the Relocations group in all other cases them to a V. cholerae reference using NucDiff with default parameter settings (see in [Additional file 1: Table S4]). The number of detected differences is presented in [Additional file 1: Figure S3]. The total number of scaffolds and differences is shown in Fig. 5. The resulting GFF3 files with mapped blocks and differences (shown with reference-based coordinates) were displayed using the IGV genome browser, and an example of assembly comparison is shown in [Additional file 1: Figure S4]. As is evident, we were able not only to compare quantitative metrics (i.e. the number of each type of difference, the number of uncovered reference bases, etc.) but also to analyse the placement of contigs/scaffolds and differences relative to each other and the exact location of the different types of detected differences.  Based on the obtained results, it is possible to conclude for the given examples that SGA gives the most fragmented assembly compared with other assemblers, while MaSuRCA gives the solution with the highest number of errors (differences are considered as errors in this case, since we are comparing to a good quality reference genome), mainly suffering from substitution errors (2839 out of 3106 differences). SOAPdenovo has rather high numbers of errors in all categories, confirming the result from GAGE-B using QUAST, which states that SOAPdenovo has \u201ca larger number of errors than most other methods\u201d [ 12 ]. It is also possible to see the large fragmentation in the SGA assembly and the large total number of differences in the MaSuRCA assembly by visualisation in IGV in [Additional file 1: Figure S4a and c].  According to GAGE-B results, MaSuRCA has produced the assembly with the best N50 size. However, in our experience, MaSuRCA did not distinguish itself when compared to other assemblers. All assemblers have managed to resolve some regions where most of the other assemblers failed to get continuous solutions. In addition, we noticed that there are some differences that were produced by all assemblers in the same places. For example, we detected two deletions, one of length 1255 bp, overlapping with two open reading frames of a transposase ([Additional file 1: Figure S5a]) and a second of length 1367 bp, overlapping with two genes of unknown function ([Additional file 1: Figure S5b]), and many short insertions, deletions and substitutions through the genome. We suspect that such errors may actually be true variations between the sequenced genome and the reference genome rather than errors in the assemblies in many cases. For example, in the case of the transposase, this may have inserted itself in the strain sequenced for the reference genome, while it was absent from the DNA of the strain sequenced for GAGE-B.    Comparison of genomes from different strains of the same species\r\n  With NucDiff, it is also possible to compare genomes of different strains of the same species to show genomic differences between them. We have compared the genomes of 21 different strains of E. coli K12 available in the NCBI database to the E. coli K12 MG1655 as the reference genome. We have calculated the total number of differences of each type at every base of each query reference. The result was saved in the bedGraph format and uploaded into the IGV genome browser together with the E. coli K12 MG1655 annotation (see Fig. 6).  The results show that the differences are not distributed randomly, they tend to be clustered in some locations. For example, in 15 out of 21 genomes there is a deletion of bases, starting from base 257,908 and ending with base 258,674 in U00096.3 (a circle in Fig. 6). These bases correspond to a mobile element with almost the same starting coordinate and ending in position 258,675.     Discussion\r\n  In this paper, we have described a tool, called NucDiff, which detects and describes the differences between any two sets of closely related DNA sequences according to our comprehensive classification scheme. The tool has several properties that make it very useful for doing comparative analysis of assemblies and reference genomes. 1) It is able to work with very fragmented genome assemblies and genomes with various structural rearrangements. We have demonstrated this with the V. cholerae assemblies and E. coli K12 reference genomes. Moreover, NucDiff is in many cases able to detect differences that are associated with repeated regions (for example, in case of duplication, tandem duplication, collapsed repeat and tandem collapsed repeat differences). However, it is not able to detect such associations for simple insertions and simple deletions found between mapped blocks. 2) The tool gives information about the locations and types of differences. This information is stored in the widely used GFF3 format with both querybased and reference-based coordinates, which can be used with existing genome browsers for visualizing the differences. The NucDiff output also enables users to incorporate the tool in a large variety of applications where detecting differences is either the final goal or as a component in an analysis pipeline. 3) The tool enables visualisation of alignment results, as a result of outputting differences in the GFF3 format. We have shown two different applications of visualisation to further inspect the results. First, by uploading the files with locations of differences and mapped blocks for all compared assemblies at once into a genome browser. This approach enables comparison of all differences at any level of detail across all datasets to look for patterns. Second, by uploading the files with the summarized counts of all difference types in all query genomes for each reference sequence base. This visualisation gives a better overview of the comparison analysis results when the number of compared genomes is high, revealing common and distinct patterns in the structures of genome sequences. However, such an approach leads to the loss of some information about differences (e.g. which query genome(s) have the specific differences and the difference locations in these genome(s)).  The benchmarking results showed that the NucDiff output depends on the NUCmer and delta-filter parameters values. The values mainly influence the types of differences and not the total number, revealing or hiding information about the repeated, inverted, substituted, or relocated nature of the short- and medium-sized differences. The locations of regions containing differences remain the same in most cases. As for NucDiff result quality, we have noticed systematic loss of information about the repeated nature of some differences in specific cases. This was due to the limitations of the approach implemented in NucDiff.  Together with NUCmer, MUMmer provides another alignment program called PROmer. Unlike NUCmer, PROmer can be used for highly divergent sequences that show little DNA sequence conservation. Since both tools output the alignments results in the same format, it is possible to run NucDiff with PROmer output file as an input parameter, thus enabling detection of differences between two highly divergent sequences.  There are similarities between NucDiff and QUAST, a software tool for comparing assemblies to reference genomes. Both use NUCmer as a part of their analysis pipeline to align the input sequences. However, QUAST assesses genome quality mainly based on contiguity and gene complement completeness, producing various reports, plots and tables. QUAST will output quality metrics (e.g. number of misassemblies, indels and so on) only when a reference genome is available. In this case, it reports information about similar reference and query sequences, unmapped query sequences, and the locations of the regions where the reference and query sequences were split during the alignment process, giving general explanations for the fragmentation. It does not output the locations of small indels and substitutions obtained after parsing results given by the show-snps package. It provides only the raw show-snps output and summary statistics for these types of differences. Our experiments showed that QUAST tends to count several differences located at the same position as one difference. Comparing to QUAST, our tool is also able to give more detailed information about the locations of all differences as well as a more detailed classification of them. In addition, NucDiff allows the users to upload the results to different genome browsers, while QUAST output can be directly visualised only in its own genome browser, Icarus, that does not handle uploading of additional tracks.  We have also compared NucDiff with dnadiff. Both tools parse the NUCmer output and produce detailed information about the differences between two sets of sequences. Their results are very similar, but, in contrast to NucDiff, dnadiff does not allow visualization of differences and is not able to quantify them at the same level of detail.  Our results from analyses of different real assemblies have revealed a complication related to assembly comparison. It is not always enough to only use the quality and contiguity summary metrics when choosing the \u201cbest\u201d assembly. The ability to visualize results and manually inspect the regions where the differences are located may dramatically influence this choice.    Conclusions\r\n  We present the tool NucDiff for the comparison of two sets of closely related sequences. NucDiff outputs information about the types and locations of the differences between the sequences. Special attention has been paid to detection of differences involving repeated regions. All differences are categorized according to a proposed detailed classification scheme. The output from NucDiff enables the user to visualise the results using a genome browser, and we demonstrate two different applications of such visualisations. The ability to 1) give detailed information about the differences, 2) handle small local differences as well as structural rearrangements, and 3) visualise the comparison results makes NucDiff convenient for whole-genome sequence comparison or as an intermediate step in an analysis pipeline.    Additional files\r\n  Additional file 1: Figure S1. Reference fragments placement order depending on query fragment orientations during detection of local differences. Figure S2. Circular genome alignment alternatives. Figure S3. Number of differences in each category obtained by NucDiff with the default parameter settings for all assemblers. Figure S4. Comparison of multiple assemblies against one reference using NucDiff. Figure S5. Examples of detection of long deletions located in all assemblies at the same place in the reference sequence. Table S1. Alignment fragmentation cases caused by simple differences. Table S2. Genome modifications implemented during the simulation process. Table S3. List of E. coli genomes used in the Comparison of genomes from different strains of the same species section. Table S4. Parameter values used for each parameter settings. Table S5. Correspondence between the QUAST difference types and the simulated difference types. Table S6. Correspondence between the QUAST, dnadiff and NucDiff difference types and the expected difference types. (PDF 989 kb) Additional file 2: Detailed results for Table 1. (TXT 472 kb) Abbreviation WGA: Whole-genome alignment Acknowledgements The authors wish to thank the Centre for Ecological and Evolutionary Synthesis (CEES) for access to the computational infrastructure ('cod' servers) that enabled the bioinformatics analysis for this project.  Funding KK was funded by the Computational Life Science initiative (CLSi) at the University of Oslo. The funding body played no role in the design or conclusions of this study.  Availability of data and materials  Project name: NucDiff Project home page: https://github.com/uio-cels/NucDiff Operating system(s): Unix-like system such as Ubuntu Linux and MacOS X.  Programming language: Python Other requirements: Python 2.7, MUMmer 3.23 License: Mozilla Public License (MPL), version 2.0 Any restrictions to use by non-academics: No Additional data: the used genomes and annotations are available through the links given in [ 20-23 ] Authors' contributions KK designed and implemented NucDiff. AJN and KK developed the proposed classification of the differences. TR, AJN, GKS, and KL suggested the demonstration examples and other experiments performed. KK performed all the experiments. KK, TR and AJN wrote the manuscript. KL and GKS revised the manuscript. All authors read and approved the final manuscript.  Ethics approval and consent to participate Not applicable.  Consent for publication Not applicable.  Competing interests The authors declare that they have no competing interests.    ",
    "sourceCodeLink": "https://github.com/uio-cels/NucDiff",
    "publicationDate": "0",
    "authors": [
      "Ksenia Khelik",
      "Karin Lagesen",
      "Geir Kjetil Sandve",
      "Torbjørn Rognes",
      "Alexander Johan Nederbragt"
    ],
    "status": "Success",
    "toolName": "NucDiff",
    "homepage": ""
  },
  "39.pdf": {
    "forks": 0,
    "URLs": ["github.com/tbuder/CellTrans"],
    "contactInfo": ["thomas.buder@tu-dresden.de"],
    "subscribers": 1,
    "programmingLanguage": "R",
    "shortDescription": "CellTrans is an automated tool to quantify stochastic cell state transitions between distinct cell states from FACS and flow cytometry experiments, to estimate equilibrium cell state proportions and the needed time equilibrium.",
    "publicationTitle": "CellTrans: An R Package to Quantify Stochastic Cell State Transitions",
    "title": "CellTrans: An R Package to Quantify Stochastic Cell State Transitions",
    "publicationDOI": "None",
    "codeSize": 545,
    "publicationAbstract": "Many normal and cancerous cell lines exhibit a stable composition of cells in distinct states which can, e.g., be defined on the basis of cell surface markers. There is evidence that such an equilibrium is associated with stochastic transitions between distinct states. Quantifying these transitions has the potential to better understand cell lineage compositions. We introduce CellTrans, an R package to quantify stochastic cell state transitions from cell state proportion data from fluorescence-activated cell sorting and flow cytometry experiments. The R package is based on a mathematical model in which cell state alterations occur due to stochastic transitions between distinct cell states whose rates only depend on the current state of a cell. CellTrans is an automated tool for estimating the underlying transition probabilities from appropriately prepared data. We point out potential analytical challenges in the quantification of these cell transitions and explain how CellTrans handles them. The applicability of CellTrans is demonstrated on publicly available data on the evolution of cell state compositions in cancer cell lines. We show that CellTrans can be used to (1) infer the transition probabilities between different cell states, (2) predict cell line compositions at a certain time, (3) predict equilibrium cell state compositions, and (4) estimate the time needed to reach this equilibrium. We provide an implementation of CellTrans in R, freely available via GitHub (https://github.com/tbuder/CellTrans).",
    "dateUpdated": "2016-12-22T11:29:48Z",
    "institutions": [
      "Hochschule für Technik und Wirtschaft Dresden",
      "Medizinische Fakultät Carl Gustav Carus",
      "Nationales Centrum für Tumorerkrankungen (NCT) Dresden",
      "Technische Universität Dresden"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-12-22T11:29:07Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics and Biology Insights\r\nVolume 11: 1-14\r\n© The Author(s) 2017\r\nReprints and permissions:\r\nsagepub.co.uk/journalsPermissions.nav\r\nhDttOpsI:/1/d0o.i1.o1rg7/71/01.1177779/131272719732721127274112241      CellTrans: An R Package to Quantify Stochastic Cell State Transitions     Thomas Buder  thomas.buder@tu-dresden.de  0  1  4    Andreas Deutsch  0  1    Michael Seifert  1  2  3    Anja Voss-Böhme  0  1  4    0  Fakultät Informatik/Mathematik, Hochschule für Technik und Wirtschaft Dresden ,  Dresden ,  Germany    1  INTERDIS-2. AD acknowledges support by Deutsche Krebshilfe and by DFG-SFB-TRR79 Access Publication Funds of the TU Dresden. de Cl ARATIon o F ConF l ICTIng I nT eReSTS: The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article. Co RRe SPond Ing Au THo R: Thomas Buder, Fakultät Informatik/Mathematik, Hochschule für Technik und Wirtschaft Dresden ,  Friedrich-List-Platz 1, 01069 Dresden    2  Institut für Medizinische Informatik und Biometrie, Medizinische Fakultät Carl Gustav Carus ,  Dresden ,  Germany    3  Nationales Centrum für Tumorerkrankungen (NCT) Dresden ,  Dresden ,  Germany    4  Zentrum für Informationsdienste und Hochleistungsrechnen (ZIH), Technische Universität Dresden ,  Dresden ,  Germany     2017   11  1  14   Many normal and cancerous cell lines exhibit a stable composition of cells in distinct states which can, e.g., be defined on the basis of cell surface markers. There is evidence that such an equilibrium is associated with stochastic transitions between distinct states. Quantifying these transitions has the potential to better understand cell lineage compositions. We introduce CellTrans, an R package to quantify stochastic cell state transitions from cell state proportion data from fluorescence-activated cell sorting and flow cytometry experiments. The R package is based on a mathematical model in which cell state alterations occur due to stochastic transitions between distinct cell states whose rates only depend on the current state of a cell. CellTrans is an automated tool for estimating the underlying transition probabilities from appropriately prepared data. We point out potential analytical challenges in the quantification of these cell transitions and explain how CellTrans handles them. The applicability of CellTrans is demonstrated on publicly available data on the evolution of cell state compositions in cancer cell lines. We show that CellTrans can be used to (1) infer the transition probabilities between different cell states, (2) predict cell line compositions at a certain time, (3) predict equilibrium cell state compositions, and (4) estimate the time needed to reach this equilibrium. We provide an implementation of CellTrans in R, freely available via GitHub (https://github.com/tbuder/CellTrans).       -\r\n  Keywo Rd S: Equilibrium cell state proportions, cell state transitions, Markov model, fluorescence-activated cell sorting (FACS), flow cytom etry experiments Re Ce IVed : January 31, 2017. ACCe PTed : April 22, 2017.  Pee R Re VIe:w Five peer reviewers contributed to the peer review report. Reviewers' reports totaled 711 words, excluding any confidential comments to the academic editor.  Ty Pe: Original Research Fund Ing: The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: TB and AV-B acknowledge support by \u201cSächsisches Staatsministerium für Wissenschaft und Kunst\u201d (SMWK) project    Introduction\r\n  Homeostasis with respect to the proportions of cells in different states is crucial for the functioning of multicellular organisms, and its regulation enables organisms to stay in a healthy state. Different types of tissues and organs need to maintain a stable composition of different cell types regardless of external conditions, injuries, and changing environmental conditions to function normally.1 Hence, finding mechanisms of homeostasis regulation is a key aspect in understanding the emergence of diseases, such as cancer, that leads to disturbance and loss of cell state homeostasis.2  Remarkably, diseases such as cancer disturb healthy homeostatic states but can lead themselves to a characteristic composition with respect to the proportions of distinctive neoplastic cell states.3 The establishment and maintenance of such a characteristic composition has been experimentally shown using fluorescence-activated cell sorting (FACS) and flow cytometry experiments for many types of cancer, e.g., breast cancer4 and colon cancer.5-7 In these experiments, it has been observed that subpopulations of cells purified for a *MS and AV-B contributed equally to supervising this work. given cell state return to the composition of cell state proportions of the original tumor over time.  The mechanisms for the maintenance of these characteristic compositions are only poorly understood. Cell state proportions could be maintained by regulated cell state-specific proliferation rates, e.g., due to intercellular signaling.4 However, in many cases, this possibility can be experimentally excluded by showing that the proliferation rates of all involved cell types are equal and constant over time. There is evidence that cell types stochastically transition between different states and that the transition rates do not depend on the current tissue composition or on intercellular signaling,4 i.e., the chances to transition into other cell states only depend on the current state of the cell. Quantifying the probabilities for transitions from one cell state to another would allow to predict the evolution of cell state proportions. Such a quantification can potentially help to understand the differences in homeostasis regulation between healthy and diseased tissues.  One approach to model cell state transitions uses ordinary differential equations (ODEs).Typically, the dynamics between different cell states is described by formulating ODEs incorporating parameters which describe detailed cell properties such as symmetrical/asymmetrical division rates and transition rates between cell states.5,7  Another possibility to model the evolution of cell state proportions is discrete-time Markov models. Discrete-time Markov models are particular stochastic processes which can be understood as sequences of random variables indexed by discrete time points, where the next state only depends on the current state of the process but is independent of earlier states.8 For instance, in Gupta et al,4 a Markov model describing the evolution of cell state proportions has been introduced and applied to breast cancer cell lines. However, a detailed discussion of how the transition probabilities are derived from the experimental data and of potential analytical challenges is missing. The quantification of cell state transitions by estimating transition probabilities would allow to better understand characteristics of cell state proportions in both healthy and disease-related tissues. To our knowledge, there is no tool available that allows to automatically estimate cell state transition rates from FACS and flow cytometry experiments.  We develop such a general tool to estimate the transition probabilities between different cell states from appropriately prepared data. The underlying model is based on a discrete-time Markov model and allows to quantify cell state transitions from data on the temporal evolution of cell state proportions. We use a discrete-time Markov model because it serves as a minimal model for the evolution of cell state proportions. In contrast, ODE models often require additional parameters which must be measured experimentally5 or obtained by fitting.7 Moreover, Markov models have already been successfully used to analyze dynamic cell compositions.4,9-11 Here, we generalize this approach and develop an automated tool for the analysis of cell state transitions. We demonstrate which analytical problems can occur in the estimation and in which way these problems are automatically solved by our tool. Furthermore, we provide a publicly available R package called CellTrans which can be directly used by experimentalists to analyze cell state proportion data from FACS and flow cytometry cell line experiments.  We illustrate potential applications of CellTrans by analyzing publicly available data on the evolution of cell state compositions in different cell lines. We show that the quantification of cell state transitions allows to predict the cell state composition at any time point of interest. In particular, our model is able to predict the long-term equilibrium composition of cell types. Furthermore, our model can reveal frequent and rare cell state transitions. Moreover, CellTrans can be used to estimate the time needed until perturbations of the characteristic cell state compositions level out. Such predictions have the potential to support experimentalists in planning the duration of FACS and flow cytometry cell line experiments.    Materials and Methods\r\n   Reference experiment\r\n  CellTrans is able to analyze data recording changes of cell state proportions over time. The identification of individual cell states from mixed cell populations is mainly based on cell type-specific gene markers which allow to experimentally separate the different cell types, for instance, by FACS techniques.12 We assume that cell state proportions change in time due to stochastic transitions dependent only on the current state of the cell. A further prerequisite for the application of our model is the equality of proliferation and death rates of all cell types.  According to the number of different cell types distinguished in the experiments, an arbitrarily large, but finite integer m is fixed, defining the number of distinct cell states in the model. Typically, all distinct cell states are purified and a large number of cells are separately cultured for each cell type. This experimental setup leads to m experiments whose evolution of cell state proportions are simultaneously monitored at t different time points n1,n2 ,,nt . The time points are multiples of a unit time step of length τ depending on the timescale of the experiment, e.g. τ is 1 hour, 1 day, or 1 week. The time points of measurement are not necessarily integer multiples of τ . The data on cell state proportions at each time point n j , j = 1,, t are the basis of the analysis with CellTrans as described in the next section.  Note that CellTrans also allows to analyze experiments with nonpure initial cell state proportions. Importantly, the number of experiments has to be the same as the number of defined cell states m . Furthermore, the vectors describing the initial cell state proportions have to be linearly independent, i.e., they cannot be represented as linear combinations of each other (see also the next section).    Detailed description and analysis of CellTrans\r\n  We denote the cell states distinguished in the experiments by 1,\u2026,m . The experimental data on cell state proportions obtained for each time point n j , j = 1,, t need to be arranged in matrices:  W (nj ) = {wk(nlj )}  k,l =1,\u2026,m (nj ) describes the proportion of cell state k in the l th Here, wkl experiment at time n j . These matrices can be stored into text files and read into the CellTrans R package. Figure 1 illustrates the experimental setup and the construction of the data matrices.  We assume that each cell transitions from state i to state j during a time step of length τ with probability pij ,i, j = 1, 2,,m . Note that the choice of the time step length has no substantial influence on the predictions which we discuss later. Furthermore, it is assumed that the transition probabilities are constant over time and do only depend on the state of the cell.4 These considerations lead to a discrete-time Markov process with transition matrix P = ( pij )i, j =1,,m for the random evolution of the state of individual cells. Our goal is to estimate P , i.e., all transition probabilities between the cell states, from the experimental data stored in the matrices  W (n_1) ,,W (nt ) . The interaction graph of the underlying Markov model together with the transition probabilities is illustrated in Figure 5A.    Construction of data matrices\r\n  Let w(0) = {wk( =  {wk(10) ,, wk(0m)} denote the initial cell state propork tions in the kth experiment which is a row vector of length m with non-negative entries summing to one. As explained before, m of those initial cell state proportions w1(0) , w2(0) ,, wm(0) are needed. The initial experimental matrix W (0) is row-wise constructed from these cell state proportions, i.e.,  W (0) =     w1(0) wm(0) −   w1(10) −  10 ) −  =     −   wm(01) \u2026 \u2026 w1(m0)    wm(0m)   An analytical requirement for the applicability of CellTrans is the existence of the inverse of W (0) . This can be assured by appropriate initial cell state proportions, e.g. by choosing purified initial cell cultures. Because this experimental setup is very common in FACS and flow cytometry experiments, it is implemented as default option in CellTrans. However, also an individually designed initial experimental matrix can be used.  In the experiments, the cell state proportions at each time point n1,n2 ,,nt have to be assessed. Let wk(nj ) denote the cell state proportions of experiment k = 1, 2,,m after time n j with j = 1, 2,, t . For each of these time points, a cell state proportion matrix after time n j is obtained by constructing the matrix as described above for W (0) :    W (nj ) =     w1(nj ) w2(nj )   wm(nj ) −    −    −    −    Because each row describes the cell state proportions in the corresponding experiment, all rows sum up to one with nonnegative entries. In total, t of such matrices need to be constructed from the experiments, one for each time point of measurement.   Derivation of transition matrices Pnj\r\n  For each time point n j with j = 1, 2,, t , CellTrans derives a transition matrix Pnj as follows. This derivation is based on the theory of Markov models.8 We use that the distribution of a Markov chain after n j time steps can be obtained by multiplying the initial distribution with the transition matrix raised to the power of n j ; hence,  W (0)P nj = W (nj ) Here, W (0) denotes the initial experimental matrix and W (nj ) the cell state proportion matrix after time n j . Because we are interested in the underlying transition matrix P , we solve this equation for P by multiplying with the inverse of W (0) and computing the n j th matrix root, i.e.,  P nj = (W (0) )−1 ⋅W (nj ) and Pn j  1 =  (W (0) )−1 ⋅W (nj ) nj   (1) for j = 1, 2,, t . Here, we compute the so-called principal matrix root because matrix roots are not necessarily unique. For an overview of the existence and computation of matrix roots see the work by Higham and Lin.13 Note that the existence of (W (0) )−1 is assured by the appropriate choice of the initial cell state proportions described earlier. The matrix Pnj is the estimated transition matrix derived from time point n j . Although this derivation looks straightforward, there are potential analytical problems which are described in the following section.     Regularizing matrix roots to stochastic matrix roots\r\n  Importantly, equation (1) should yield a transition matrix of a Markov chain, i.e., a stochastic matrix with non-negative entries and row sums equal to one. However, the root of a stochastic matrix is not necessarily stochastic again.13 CellTrans verifies whether the matrix roots are stochastic or not. If not, the matrix roots are regularized to be stochastic with the quasioptimization of the root matrix (QOM) algorithm which is sketched in the following section and described in detail in Kreinin and Sidelnikova.14  The QOM algorithm performs a row-wise Euclidean distance minimization by transforming each row of the matrix into a valid row of a transition matrix, i.e., a vector containing non-negative entries which sum to one. The result is a uniquely determined stochastic matrix which closely approximates the original matrix. In the work by Kreinin and Sidelnikova,14 the effect of QOM regularization on nonstochastic matrix roots is numerically investigated. The authors calculated the infinity matrix norm and also the mean absolute deviation of the difference between the QOM result and the original transition matrix for 32 examples of matrix regulation. This numerical comparison demonstrates a low approximation error of the QOM regularization.  Computation of the transition matrix P CellTrans estimates the transition matrix P by averaging the transition matrices Pnj for each time point j = 1, 2,, t , i.e., P = 1 t t ∑j=1 Pnj (2)  This transition matrix is the final estimation of CellTrans quantifying the transition probabilities between all cell states. The overall workflow of CellTrans is summarized in Figure 1.  Note that the dynamics of the Markov chain model can also be described by a master equation, i.e., a set of first-order differential equations. The master equation reads dt ai (t ) = ∑ ( p ji a j (t ) − pij ai (t )) i = 1,,m d  j ≠i with initial conditions ai (0) = ci .15 Here, ai (t ) describes the proportion of cells in state i at time t , and P = ( pij )i, j =1,,m represents the estimated transition matrix from CellTrans. Hence, these differential equations describe the temporal evolution of the cell state proportions. We will use this master equation later to compare the results of CellTrans to those of ODE models.    Important functions in CellTrans\r\n  Here, we introduce the most important functions which are implemented in CellTrans. In the following sections, we will demonstrate the usage of these functions in several case studies. readExperimentalData(). This function reads all necessary data. First, it opens a dialog box which asks for the number of cell types, the names of the cell types, the time step length τ , and the time points of measurement. Then, the files containing the cell state proportion matrices are read. First, the initial experimental setup matrix W (0) can be chosen as identity matrix (for pure initial cell populations) or an individual initial matrix can be used. Then, the experimental cell proportion matrices are read for each time point of measurement. It is recommendable to save the input into a variable for further analysis, e.g. input &lt;-readExperimentalData(). celltransitions(input). This function derives and prints the estimated transition probabilities and the predicted equilibrium distribution. The variable input contains the read data from the function readExperimentalData(). celltrans_plot(input), celltrans_plotPDF(input). These functions allow to create plots of the predictions of CellTrans and the experimental data. The variable input contains the read data from readExperimentalData(). timeToEquilibrium(input,initialDistribution,tol). This function estimates the time f rom any initial cell state proportions until the equilibrium proportions are reached. The variable input contains the read data f rom the function readExperimentalData(). The variable initialDistribution is a vector of length m which describes the initial cell state proportions, e.g. c(0.25,0.25,0.25,0.25) for equal proportions of m = 4 cell types. The third parameter tol gives a tolerance deviation between the cell state proportions of the equilibrium distribution and those of the predicted cell state proportions, as the exact equilibrium distribution is not reached, in general. For the parameter tol, we recommend values between 0.01 and 0.02 .  For a comprehensive introduction demonstrating the application of these functions, see the detailed vignette provided with the R package (Additional file 1-supplementary material). CellTrans can be applied to analyze cell state proportion data from FACS and flow cytometry experiments with respect to several questions. The applications are based on the estimation (3)    Applications of CellTrans\r\n  ESTIMATION OF Transition probabilities Cell state compositions Equilibrium compositions  Time to equilibrium of the transition probabilities of the underlying model as described above: 1. The estimated probabilities quantify the frequencies of state transitions and can be used to detect frequent, rare, or almost never occurring transitions. Such a prediction allows to hypothesize about biological mechanisms which are responsible for the observed transition structure, e.g. an underlying transition hierarchy. 2. Another application is the prediction of cell line compositions at any time point. Such an estimate can be used to predict cell line compositions even beyond the time periods of experiments which we will demonstrate in the \u201cResults\u201d section. 3. CellTrans can be used to estimate the equilibrium cell state proportions. This information can support experimentalists to decide whether experimentally observed cell line compositions already reached equilibrium. 4. CellTrans allows the prediction of the time needed to reach equilibrium proportions from any initial cell state composition. The choice of the time period of FACS experiments and the time points at which cell state compositions are measured is often difficult. Here, the estimate of the time needed to reach equilibrium can be useful.     Results\r\n  In this section, we apply CellTrans to publicly available data on the evolution of cell state proportions obtained from FACS and flow cytometry experiments. We point out possible conclusions that can be drawn from the application of CellTrans. The used data are provided in Additional file 2 (supplementary material) so that the results of this section can be reproduced.   Dynamics between cancer cell subpopulations in colon cancer\r\n  colon cells.6 In the work by Yang et al,6 the dynamics between CSCs and nonstem cancer cells (NSCCs) has been experimentally investigated. In detail, purified NSCCs and CSCs sorted from the SW620 cell line by FACS were cultured for 26 days, and the composition of these cultures was measured every second day in both experiments. Here, we analyze the data from these experiments with CellTrans and compare the resulting predictions with those obtained from an ODE model which has been analyzed in the work by Wang et al.5 Application of CellTrans. The experimental setup in the work by Yang et  al 6 can be formulated within the framework of CellTrans as follows. There are m = 2 subpopulations considered in SW620 human colon cancer cells which are called CSC and NSCC. Note that the assumption that both of these cell types do not differ in their proliferation and death rates is justified.5 The initial experimental matrix representing purified initial cell cultures is as follows:   1 0  W (0) =    0 1    where the first row corresponds to the experiment with sorted CSCs and the second row with sorted NSCCs. Hence, we choose the identity matrix in CellTrans as initial experimental matrix.  There are in total 12 time points of cell state proportion measurements which are given by 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, and 24 days.Thus, it is sensible to choose a time step length τ of 1 day. The cell state proportion matrices for each of these time points can be derived from the experimental data provided in Tables S2 and S3 in the work by Wang et al5 which leads to 12 corresponding cell state proportion matrices. These matrices are provided in .TXT files in Additional file 1 (supplementary material). For example, the cell state proportion matrix after 8 days is as follows:  0.7150 0.2850  W ( 8 ) =   0.5523 0.4477   Background. There is evidence that CD133+ cells represent a cancer stem cell (CSC) subpopulation within SW620 human That is, the experiment starting with 100% CSCs evolved to 71.5% CSCs and 28.5% NSCCs, and the experiment starting with pure NSCCs evolved to 55.23% CSCs and 44.77% NSCCs after 8 days, respectively.  The function CellTransitions(input) allows to derive the transition probabilities between the cell states and the predicted equilibrium distribution. CellTrans derives a transition matrix for each of the 12 cell state proportion matrices. Note that none of these matrices require regularization by the QOM algorithm because the matrix roots are already stochastic matrices. For example, the transition matrix derived from the experimental data after 8 days W ( 8 ) is as follows:  Finally, the transition matrix P is obtained by computing the average of the 12 transition matrices from each time point, i.e., P = 1 12  0.9455 0.0545  12 ∑j=1 Pnj =  0.1030 0.8970  which is an estimate of the transition probabilities between NSCC and CSC per day. For example, a CSC converts with probability 0.0545 to an NSCC or an NSCC converts with probability 0.1030 to a CSC within a day. The predicted long-term cell state proportions can be obtained by CellTrans from the steady state of the transition matrix P and are also provided by the function CellTransitions(input) which yields the following:  0.654  w* =   0.346   That is, CellTrans predicts a proportion of 65.4% CSCs and of 34.6% NSCCs after sufficient time. The time from 100% CSCs to the equilibrium composition can be estimated with the command timetoEquilibrium(input,c(1,0),0.01) which yields 21 days. Similarly, the time from 100% NSCCs to the equilibrium composition can be estimated with the command timetoEquilibrium(input,c(0,1),0.01) which yields 25 days. Therefore, the time of the experiment was sufficient to reach the equilibrium in this case.  Model comparison. In the work by Wang et al,5 the dynamics between CSCs and NSCCs has been modeled by an ODE model with 8 parameters. Some of these parameters have been collected from in situ experiments (see Table 1). The predictions from our model show a much better accordance to the data than the predictions of the ODE model (root mean square deviation (RMSD) CellTrans vs ODE: 0.03737 vs 0.09874 for the experiment starting with pure CSC cultures, RMSD CellTrans vs ODE: 0.05326 vs 0.10484 for the experiment starting with pure NSCC cultures). A comparison between the original data from the work by Wang et al5 and the predictions of the derived Markov chain is shown in Figure 2A illustrating that the estimated Markov model well describes the experimental data.  We can use the master equation (3) to equivalently describe the cell state dynamics with an ODE system d CSC(t ) = −0.0545CSC(t ) + 0.103NSCC(t ) dt d NSCC(t ) = 0.0545CSC(t ) − 0.1030NSCC(t ) dt with appropriate initial conditions CSC(0) = 1, NSCC(0) = 0 for the experiment with pure CSCs in the beginning. The solution  CSC(t ) = 0.346e−0.1575t + 0.654 allows to obtain the steady state by letting t → ∞ . Summary. This case study demonstrates that the Markov model underlying CellTrans is potentially able to make better predictions than more complex ODE models with parameter calibration. The experimental data on the evolution of cell state proportions are sufficient to interpolate the data. Moreover, the equilibrium proportion is reliably predicted.    Dynamic switch between adhesive and suspended cell types in colon cancer\r\n  Background. In the work by Geng et al,7 the dynamic switch between 2 different adhesion phenotypes in colorectal cancer cells has been analyzed. The involved cell states in this study are adherent and suspended cells. Both of these cell types have been separated and replated. Subsequently, both initially pure cell cultures have been monitored for 8 hours, and the cell state proportions have been measured after 0.5 and 1 hours and subsequently in 2-hour intervals.  Application of CellTrans. The study can be integrated in the CellTrans framework in the following way. There are m = 2 cell states: adherent and suspended. Although there is no explicit statement regarding the proliferation rates in the work by Geng et  al,7 we assume equal rates because the authors model the evolution of the cell state proportions without considering effects of proliferation. The initial experimental matrix is given by the identity matrix because the experimental initial cell state proportions are pure cell cultures of both phenotypes. Here, the first row corresponds to the experiment with 100% adherent cells and the second row with 100% suspended cells.  Subsequently, the cell state proportion matrices for t = 6 time points (at 0.5, 1, 2, 4, 6, and 8 hours) can be deduced from the experiments. Here, the time step length τ is chosen to be 1 hour. The matrices are provided in Additional file 1 (supplementary material).  Subsequently, CellTrans derives transition matrices for each of these cell state proportion matrices. This leads to 6 transition matrices P0.5 , P1, P2 , P4 , P6 , and P8 . All of these matrices do not require regularization in this case because all of them are already stochastic matrices. Finally, the transition matrix P is obtained as average:  The predicted cell state proportions based on the estimated transition matrix and the experimental data are plotted in Figure 2B.  The steady state of the derived transition matrix P is given as follows: That is, the long-term cell state proportions of adherent and suspended cells are predicted as 67.68% and 32.32%, respectively. This prediction is in good accordance with the corresponding experimentally observed equilibrium proportions described in the work by Geng et al.7 Model comparison. Geng et  al7 formulated a mathematical ODE model to describe the dynamics for the adherent and the suspended cells to reestablish the equilibrium ratio. Note that this approach requires to fit the analytical solution of the model to the experimental data. The fit in the work by Geng et  al7 yields the solution adh(t ) = 0.3e−0.5t + 0.7 .  In contrast, we can derive an alternative ODE system with the same structure using the master equation (3) based on the predictions of CellTrans, i.e., d adh(t ) = −0.1388adh(t ) + 0.2908susp(t ) dt d susp(t ) = 0.1388adh(t ) − 0.2908susp(t ) dt with initial conditions adh(0) = 1, susp(0) = 0 . The solution is close to the solution in the work by Geng et al7 and allows to predict the equilibrium distribution by letting t → ∞ . This equilibrium solution well corresponds to the experimental findings.  The predictions of CellTrans and the ad hoc ODE in the work by Geng et al7 are in good accordance with the original data (RMSD CellTrans vs ODE: 0.0219 vs 0.0278 for the experiment starting with pure adherent cultures and RMSD CellTrans vs ODE: 0.02709 vs. 0.02407 for the experiment starting with pure suspended cultures) (see also Figure 2B). Summary. It remains unclear in which way the \u201cbest-fit\u201d solution in the introduced ODE approach in the work by Geng et  al7 has been obtained. Instead of fitting parameters, our approach offers a transparent estimation of the underlying transition matrix yielding good predictions. Moreover, especially from the point of view of an experimentalist, the automated estimation by CellTrans does not require a deeper engagement with mathematical modeling.    Proportions of stem-like, basal, and luminal phenotypes in breast cancer\r\n  Background. The dynamics of phenotypic proportions in human breast cancer cell lines is studied by Gupta et  al.4 In detail, the authors used FACS analysis to isolate three mammary epithelial cell states (stem-like, basal, and luminal) from the SUM159 and SUM149 breast cancer cell lines. Pure subpopulations of the three cell states have been cultured for 6 days, and cell state proportions have been measured at the end of the experiment.  Application of CellTrans. We apply CellTrans to both cell lines, SUM149 and SUM159. The proliferation rates of the involved cell types are equal.4 The initial experimental cell proportions in both cases can be described as follows: where the first line corresponds to the experiment with sorted stem-like cells, the second row to sorted basal cells, and the third row to sorted luminal cells.  The proportions of cell states have been obtained at a single time point after 6 days. The time step length τ is 1 day in this case. For SUM149, the following cell state proportion matrix can be obtained from the work by Gupta et al4: where the first row contains the cell state proportions of the experiment with sorted stem-like cells, the second row with sorted basal cells, and the third row with sorted luminal cells in the beginning, respectively.  Using the function celltransitions(input), CellTrans derives the following transition matrix from this time point: which yields the final transition probabilities because there is only one time point of measurement in this case. For example, the second row indicates that basal cells transition to stem-like cells within a day with a probability of 1.45%, do not change their state with a probability of 88.87%, and convert to the luminal state with a probability of 9.68%. A similar derivation leads to the transition probabilities for SUM159 which exhibits different transition dynamics.  The predicted equilibrium distribution of our model for SUM149 is  w* = (0.014 0.019 0.967) and for the SUM159 cell line  w* = (0.0235 0.9734 0.0031) (first entry stem-like, second basal, and third luminal) (see Figure 3A to F). These plots can be created in CellTrans with the command celltrans_plot(input). The predictions are in good accordance with the original tumor compositions. In detail, the SUM149 tumor sample is composed of 3.9% stem, 3.3% basal, and 92.8% luminal cells. The proportions within the SUM159 cell line is 1.9% stem, 97.3% basal, and 0.62% luminal cells.4  The time from 100% stem-like cells to the predicted equilibrium composition can be estimated by CellTrans with the command timetoEquilibrium(input,c(1,0,0),0.01) which yields 31 days. In contrast, the command timeToEquilibrium(i nput,c(0,0,1),0.01) gives an estimation of 8 days to reach the equilibrium from 100% luminal cells. These predictions reflect that purified luminal cells are much closer to the equilibrium composition than purified stem-like cells.  Comparison with previously used Markov model. Gupta et  al4 introduced a Markov model of cell state transitions to explain the observed equilibrium. The predictions are based on a single time point, and no regularization of the matrix root is required. CellTrans is able to recover the transition matrices for both cell lines SUM149 and SUM159 presented in the work by Gupta et al4 (Table 1). Figure 3A to F illustrates the predicted evolution of cell fractions for both cell lines.  The master equation (3) can also be applied to derive an equivalent ODE system: d stem(t ) = −0.4186stem(t ) + 0.0145bas(t ) + 0.0058lum(t ) dt d bas(t ) = 0.1102stem(t ) − 0.1113bas(t ) + 0.0006lum(t ) dt d lum(t ) = 0.3084stem(t ) + 0.0968bas(t ) − 0.0064lum(t ) dt  The solution for the initial conditions stem(0) = 1, bas(0) = 0, lum(0) = 0 reads stem(t ) = 0.0141+ 0.9771e−0.4274t + 0.0088e−0.1089t bas(t ) = 0.0191− 0.3394e−0.4274t + 0.3203e−0.1089t lum(t ) = 0.9668 − 0.6377e−0.4274t − 0.3291e−0.1089t Summary. Several applications of our model are demonstrated here. First, CellTrans is able to analyze arbitrarily many cell states, not only 2. Second, the original tumor composition, which is not included in the analysis, is precisely predicted. Third, the estimation of the time to the equilibrium composition potentially helps experimentalists in planning the time periods of cell line experiments.    Epithelial-mesenchymal transition in breast cancer\r\n  Background. To investigate the epithelial-mesenchymal transition (EMT) and its implication on the development and progression of breast cancer, Mani et  al16 induced an EMT in nontumorigenic, immortalized human mammary epithelial cells (HMLEs). Subsequently, they used flow cytometry analysis to sort the cells based on the expression of CD44 and CD24, 2 cell surface markers whose expression in the CD44+/ CD24− configuration is associated with both human breast CSCs and normal mammary epithelial stem cells. One of their aims was to determine whether the CD44+/CD24− cells isolated from monolayer cultures of HMLE cells could generate CD44−/CD24+ cells in vitro.  To examine this question, the authors cultured purified cell phenotypes into monolayer cultures and assayed for the appearance of other cell phenotypes during time. The results of these experiments are summarized in Table S1.16 Application of CellTrans. This experimental setup can be formulated within the CellTrans framework as follows. There are m = 2 cell states which are named CD44+/CD24− and CD44−/CD24+. The initial experimental matrix is given by the identity matrix. This matrix is constructed such that the first row represents the pure CD44+/CD24− initial cell culture, and the second row corresponds to pure CD44−/CD24+ cell culture.  The cell state proportions have been experimentally determined for t = 4 time points (at 2, 4, 6, and 8 days). Here, the time step length τ is 1 day. The cell state proportions matrices W (2) ,W ( 4 ) ,W ( 6 ) , and W ( 8 ) can be constructed from the data in Mani et al,16 e.g.   0.70 0.30  W ( 4 ) =   0.01 0.99   where the first row represents the experiments starting with pure cultures of cell phenotype CD44+/CD24− and the second row with cell phenotype CD44−/CD24+, respectively. The files containing these matrices are provided in Additional file 1 (supplementary material).  CellTrans estimates transition matrices for each of the cell state proportion matrices. This approach leads to 4 matrices P2 , P4 , P6 , and P8 . In this case, all of the derived matrices do not require regularization because all matrices are already stochastic matrices. The transition matrix P is obtained by averaging over the derived transition matrices which yields the following:  P = P2 + P4 + P6 + P8 =  0.8868 0.1132   4  0.0007 0.9993  The predicted cell state proportions of this transition matrix with the initial states from the experiments and the experimental data are plotted in Figure 3G.  The solution of the master equation (3) for the experiment with pure CD44+/CD24− cells in the beginning is as follows: CD44+ / CD24− (t ) = 0.0006e−0.1133t + 0.9994 CD44− / CD24+ (t ) = −0.0006e−0.1133t + 0.0006 Summary. This case study demonstrates two potential applications of CellTrans. First, CellTrans can reveal rare cell transitions which are indicated by the estimated probability to convert from CD44−/CD24+ to CD44+/CD24− of 0.0007 per day. Hence, transitions from CD44−/CD24+ to CD44+/CD24cells almost never occur suggesting a potential cell transition hierarchy.  Second, the experiments only cover the first 8 days after culturing, but the time period until the equilibrium proportions are reached is not clear from the beginning. Here, the predicted equilibrium distribution is given as follows:   0.0063 w* =   0.9937    which corresponds to an equilibrium proportion of 0.63% of CD44+/CD24− and 99.37% of CD44−/CD24+. CellTrans can estimate the expected time until this equilibrium is reached. The command timetoEquilibrium(input,c(1,0),0.01) estimates the expected time to the equilibrium starting with a pure CD44+/CD24− cell line composition with a tolerance deviation of 1%. With these parameters, CellTrans predicts a time of 39 days until the equilibrium proportions are reached.   Influence of the choice of the time step length τ\r\n  To demonstrate that the predictions and results of CellTrans are independent of the choice of the time step length τ , we varied τ in the case studies with data from the works by Wang et al5 and Geng et al.7 We predicted the equilibrium distribution, the expected time to equilibrium, and the predicted cell state composition after an arbitrary time for different time step lengths. The results are summarized in Additional file 3 (supplementary material) and indicate that the predictions are not influenced by τ .     Validation of the predicted equilibrium of CellTrans\r\n  One important application of CellTrans is the prediction of the equilibrium of cell state proportions and the time needed to reach this equilibrium. In some experiments, the equilibrium proportions are not reached at the end of the experiment, e.g. in the investigation of the EMT in breast cancer introduced above.16 Here, we show that CellTrans is able to make predictions beyond the duration of the experiments. In detail, CellTrans is able to reliably predict both the equilibrium cell state proportions and the time needed to reach this proportion. To demonstrate this, we performed a validation analysis based on the 2 case studies dealing with colon cancer cells.6,7 We used only a subset of the available data points to create predictions of the cell state proportions over time with CellTrans. We excluded late data points to mimic an experimental situation in which the equilibrium is not reached yet. The results of this validation study are illustrated in Figure 3H to I. It turns out that only a few data points are sufficient to reliably predict the equilibrium cell state proportions. Moreover, the predicted time to reach equilibrium proportions is very robust with respect to the choice of available data points. This investigation indicates that CellTrans is able to make predictions even beyond the time period of experiments and might therefore also support the planning of experimental time periods.    Influence of nonpure initial cell state compositions\r\n  As our case studies demonstrate, most FACS and flow cytometry experiments are based on pure initial cell state compositions. However, CellTrans can also be used to analyze experimental data with nonpure initial compositions. Here, we demonstrate this possibility and investigate whether the predictions and results are influenced by such an initial composition. For this, we reused data from the works by Yang et al6 and Geng et al7 but excluded several of the first data points such that the initial cell state compositions are nonpure. We then used CellTrans to estimate a transition matrix based on these remaining data and compare the corresponding predictions with the original estimates derived from all available data. It turns out that both the evolution of cell line compositions and the equilibria are reliably predicted with nonpure initial cell state compositions. The results of this analysis are illustrated in Figure 4.    A simulation study demonstrating matrix regularization\r\n  The presented case studies so far do not require matrix regularization; ie, the matrix roots that CellTrans calculate are already stochastic matrices. To demonstrate that such a regularization might be necessary, we introduce a simulation study and explain how the QOM algorithm performs the necessary regularization.  In the simulation study, we assume the existence of m = 3 cell states. We simulate 3 experiments starting with pure cultures of each cell state. Hence, the experimental initial matrix is as follows:  As time points, we choose t = 2τ , 4τ , 6τ . Here, τ is an arbitrary time step length.  We created an arbitrary transition matrix to describe the transition probabilities between the 3 cell states:  The Markov chain associated with this transition matrix has the steady-state distribution:  * = (0.1872 0.3632 0.4496) wsim  Then, we generated experimental d2ata4 after 6times 2τ , 4τ , and 6τ from the matrix powers P , P , and P . We also added a normally distributed noise with mean 0 and a standard deviation of 0.01 to reflect unprecise measurements. This led to the following cell state proportion matrices: which is close to the originally generated Psim . Furthermore, the steady state of the process associated with P is as follows: w* = (0.191 0.3644 0.4446)  The effect of the QOM regularization on the prediction of CellTrans in this case is visualized in Figure 5B to D.     Discussion\r\n  Characteristic equilibrium proportions of distinct cell states are commonly observed in vivo and in vitro. Normal and cancerous cell lines exhibit and are able to maintain such an equilibrium.4,5,7,16 Understanding the mechanisms which are responsible for this observation is important to develop appropriate therapies against cancer. There is evidence that stochastic transitions between different cell states can lead to such an equilibrium. To infer the underlying transition dynamics and to quantify these transitions is a key to understand and control the origin of such an equilibrium.  We introduce CellTrans, an automated framework to deduce the transition probabilities between different cell states from FACS and flow cytometry experiments, in which no differences in the proliferative properties between cell states are observed. The key assumption of the underlying mathematical model is that cells stochastically transition between different states and that the rates for these transitions only depend on the current state of the cell.  We point out that the transition probabilities can be derived on the basis of Markov chain theory by determining matrix roots from appropriately arranged data matrices and regularizing them to stochastic matrices if necessary. We discuss which mathematical challenges can occur and demonstrate how these challenges are handled by CellTrans. We use the QOM algorithm 14 to achieve matrix regularization and provide a simulation study which demonstrates that regularization might be necessary and how it can be achieved by the QOM algorithm.  To ensure a reliable estimation of cell state transitions, the cell state proportion data should be obtained on the basis of a large number of cells. This ensures the validity of the estimation based on the law of large numbers.15 We suggest to use at least 100 cells in each cell culture experiment to obtain the data for CellTrans.  Our stochastic approach on the basis of a Markov model can be translated into a system of first-order ODEs with the help of the master equation (3). The predictions of these ODEs are equivalent to those of the underlying Markov model of CellTrans. We demonstrate that CellTrans is able to predict the evolution of cell state proportions even more precisely compared with more complicated ODE models which use in situ parameter estimation, as, e.g. in the work by Wang et al.5  The analyzed case studies in this work demonstrate that CellTrans can be used to compare different cell types with respect to their ability to convert to other cell types, their frequency within equilibrium proportions, and their position within an existing cell transition hierarchy. We showed that the main application of CellTrans is the quantitative inference of transitions between distinctive cell types. The resulting transition probabilities allow to estimate which transitions occur frequently, rarely, or even almost never. Therefore, CellTrans might be able to reveal a hierarchy with respect to the importance of specific transitions for maintaining the observed equilibrium distributions.  The steady state derived by CellTrans can be interpreted as prediction of the equilibrium cell state proportions. Moreover, CellTrans is able to predict the duration from any initial experimental setup to such an equilibrium (see Figure 3H to I).  As demonstrated in Figure 3A to F, even patients classified to have the same type of tumor exhibit different equilibrium distributions. Hence, the dynamics of the cell state transitions is patient specific. CellTrans is able to predict these patientspecific transitions and can therefore be used to reveal differences within the same tissue in different patients. These predictions might be a step toward individual therapies.  Although the presented case studies in this work deal with disease-related cell line experiments, CellTrans can also be used to analyze nondiseased cell lines, such as immunostained progenitor cells, as long as the underlying model prerequisites are fulfilled. Hence, CellTrans can be used to analyze all cell line experiments in which cell state transitions only depend on the current state of the cell, and cell proliferation and death rates are equal.  We focus here on the case in which cell states exhibit similar proliferation and death rates. In principle, it would be possible to apply a similar approach also if this prerequisite is not fulfilled. This would first require to formulate an extended and more complicated mathematical model which is a challenging task for future work.  Our case studies demonstrate that CellTrans is a valuable tool to model and quantify cell state transitions.Experimentalists only have to validate the model assumptions. Then, the whole process of mathematical modeling and estimation is automatized. CellTrans allows versatile experiments by being able to analyze also cell state data originating from nonpure initial cell state distributions. In summary, CellTrans is an automated tool that facilitates the analysis and interpretation of cell state proportion data from these experiments on the basis of a Markov model for cell state transitions.    Acknowledgements\r\n  The authors thank the Center for Information Services and High Performance Computing at TU Dresden for providing an excellent infrastructure. We also thank Anne Dirkse and Anna Golebiewska (both NORLUX Neuro-Oncology Laboratory, Luxembourg) for helpful discussions.    Additional Files (supplementary material)\r\n  Additional file 1 (pdf )-Vignette: A quick guide to CellTrans. A comprehensive example demonstrating how CellTrans can be used. Additional f ile 2 (zip)-Cell state proportion matrices of the case studies. Additional file containing text files with cell state proportion matrices of the introduced case studies. These data can directly be used for the analysis with CellTrans.  Additional f ile 3 (pdf )-The influence of the time step length. Predictions of CellTrans for varying time step lengths τ for case studies by Yang et al6 and Geng et al.7    Author Contributions\r\n  TB wrote the manuscript, designed, and analyzed the mathematical model and wrote the R package. AD contributed to study design and writing the manuscript. MS and AV-B contributed to study design, writing the manuscript, and supervised the study.  ReF eR en Ces    ",
    "sourceCodeLink": "https://github.com/tbuder/CellTrans",
    "publicationDate": "0",
    "authors": [
      "Thomas Buder",
      "Andreas Deutsch",
      "Michael Seifert",
      "Anja Voss-Böhme"
    ],
    "status": "Success",
    "toolName": "CellTrans",
    "homepage": ""
  },
  "44.pdf": {
    "forks": 0,
    "URLs": ["github.com/oarnaiz/TrUC"],
    "contactInfo": ["linda.sperling@i2bc.paris-saclay.fr"],
    "subscribers": 1,
    "programmingLanguage": "Perl",
    "shortDescription": "Transcription Units by Coverage",
    "publicationTitle": "Improved methods and resources for paramecium genomics: transcription units, gene annotation and gene expression",
    "title": "Improved methods and resources for paramecium genomics: transcription units, gene annotation and gene expression",
    "publicationDOI": "10.1186/s12864-017-3887-z",
    "codeSize": 37071,
    "publicationAbstract": "Background: The 15 sibling species of the Paramecium aurelia cryptic species complex emerged after a whole genome duplication that occurred tens of millions of years ago. Given extensive knowledge of the genetics and epigenetics of Paramecium acquired over the last century, this species complex offers a uniquely powerful system to investigate the consequences of whole genome duplication in a unicellular eukaryote as well as the genetic and epigenetic mechanisms that drive speciation. High quality Paramecium gene models are important for research using this system. The major aim of the work reported here was to build an improved gene annotation pipeline for the Paramecium lineage. Results: We generated oriented RNA-Seq transcriptome data across the sexual process of autogamy for the model species Paramecium tetraurelia. We determined, for the first time in a ciliate, candidate P. tetraurelia transcription start sites using an adapted Cap-Seq protocol. We developed TrUC, multi-threaded Perl software that in conjunction with TopHat mapping of RNA-Seq data to a reference genome, predicts transcription units for the annotation pipeline. We used EuGene software to combine annotation evidence. The high quality gene structural annotations obtained for P. tetraurelia were used as evidence to improve published annotations for 3 other Paramecium species. The RNA-Seq data were also used for differential gene expression analysis, providing a gene expression atlas that is more sensitive than the previously established microarray resource. Conclusions: We have developed a gene annotation pipeline tailored for the compact genomes and tiny introns of Paramecium species. A novel component of this pipeline, TrUC, predicts transcription units using Cap-Seq and oriented RNA-Seq data. TrUC could prove useful beyond Paramecium, especially in the case of high gene density. Accurate predictions of 3\u2032 and 5\u2032 UTR will be particularly valuable for studies of gene expression (e.g. nucleosome positioning, identification of cis regulatory motifs). The P. tetraurelia improved transcriptome resource, gene annotations for P. tetraurelia, P. biaurelia, P. sexaurelia and P. caudatum, and Paramecium-trained EuGene configuration are available through ParameciumDB (http://paramecium.i2bc.paris-saclay.fr). TrUC software is freely distributed under a GNU GPL v3 licence (https://github.com/oarnaiz/TrUC).",
    "dateUpdated": "2017-02-17T13:39:25Z",
    "institutions": [
      "Université Paris-Saclay",
      "Université Paris Diderot",
      "CNRS"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2017-02-15T14:03:14Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Arnaiz et al. BMC Genomics     10.1186/s12864-017-3887-z   Improved methods and resources for paramecium genomics: transcription units, gene annotation and gene expression     Olivier Arnaiz  2  3    Erwin Van Dijk  2  3    Mireille Bétermier  2  3    Maoussi Lhuillier-Akakpo  0  1    Augustin de Vanssay  1    Sandra Duharcourt  1    Erika Sallet  4    Jérôme Gouzy  4    Linda Sperling  linda.sperling@i2bc.paris-saclay.fr  2  3    0  Current address: IRCM, CEA, INSERM UMR 967, Université Paris Diderot, Université Paris-Saclay ,  92265 Fontenay-aux-Roses CEDEX ,  France    1  Institut Jacques Monod ,  CNRS, UMR 7592 ,  Université Paris Diderot ,  Sorbonne Paris Cité, F-75205 Paris ,  France    2  Institute for Integrative Biology of the Cell (I2BC), CNRS, CEA, Univ. Paris-Sud, Université Paris-Saclay ,  91198 Gif-sur-Yvette CEDEX ,  France    3  Institute for Integrative Biology of the Cell (I2BC), CNRS, CEA, Univ. Paris-Sud, Université Paris-Saclay ,  91198 Gif-sur-Yvette CEDEX ,  France    4  LIPM, Université de Toulouse, INRA, CNRS ,  Castanet-Tolosan ,  France     2017   18    21  6  2017    20  2  2017     Background: The 15 sibling species of the Paramecium aurelia cryptic species complex emerged after a whole genome duplication that occurred tens of millions of years ago. Given extensive knowledge of the genetics and epigenetics of Paramecium acquired over the last century, this species complex offers a uniquely powerful system to investigate the consequences of whole genome duplication in a unicellular eukaryote as well as the genetic and epigenetic mechanisms that drive speciation. High quality Paramecium gene models are important for research using this system. The major aim of the work reported here was to build an improved gene annotation pipeline for the Paramecium lineage. Results: We generated oriented RNA-Seq transcriptome data across the sexual process of autogamy for the model species Paramecium tetraurelia. We determined, for the first time in a ciliate, candidate P. tetraurelia transcription start sites using an adapted Cap-Seq protocol. We developed TrUC, multi-threaded Perl software that in conjunction with TopHat mapping of RNA-Seq data to a reference genome, predicts transcription units for the annotation pipeline. We used EuGene software to combine annotation evidence. The high quality gene structural annotations obtained for P. tetraurelia were used as evidence to improve published annotations for 3 other Paramecium species. The RNA-Seq data were also used for differential gene expression analysis, providing a gene expression atlas that is more sensitive than the previously established microarray resource. Conclusions: We have developed a gene annotation pipeline tailored for the compact genomes and tiny introns of Paramecium species. A novel component of this pipeline, TrUC, predicts transcription units using Cap-Seq and oriented RNA-Seq data. TrUC could prove useful beyond Paramecium, especially in the case of high gene density. Accurate predictions of 3\u2032 and 5\u2032 UTR will be particularly valuable for studies of gene expression (e.g. nucleosome positioning, identification of cis regulatory motifs). The P. tetraurelia improved transcriptome resource, gene annotations for P. tetraurelia, P. biaurelia, P. sexaurelia and P. caudatum, and Paramecium-trained EuGene configuration are available through ParameciumDB (http://paramecium.i2bc.paris-saclay.fr). TrUC software is freely distributed under a GNU GPL v3 licence (https://github.com/oarnaiz/TrUC).    Ciliate  Cap-Seq  TSS  RNA-Seq  Gene annotation  Autogamy  Differential gene expression       Background\r\n  Ciliates are unique among unicellular eukaryotes in making a germ/soma distinction. The germline and somatic functions of chromosomes are respectively ensured by a germline micronucleus (MIC) which undergoes meiosis and fertilization and a somatic macronucleus (MAC) that contains a version of the germline genome stripped of parasitic sequences and optimized for gene expression. The MAC is lost at each sexual cycle and a new one differentiates from a copy of the zygotic nucleus, by reproducible DNA elimination under the control of meiosis-specific RNA interference pathways [ 1 ].  Genetics of the ciliate Paramecium was pioneered nearly a century ago [ 2 ] and this complex unicellular eukaryote has since served as model for a variety of biological processes commonly found in animals, from excitable membranes and swimming behavior [ 3 ] to programmed DNA elimination and its epigenetic control during somatic differentiation [ 4 ]. The early genetics studies led to the realization that Paramecium aurelia is a complex of morphologically identical but reproductively isolated sibling species, renamed primaurelia, biaurelia, triaurelia, tetraurelia, etc. [ 5 ]. Paramecium tetraurelia became the most widely used species for genetics and physiology, because of its convenient growth properties.  P. tetraurelia somatic genome sequencing revealed that the present diploid genome was shaped by a series of at least 3 whole genome duplications (WGDs), each WGD being slowly resolved by gene loss over evolutionary time [ 6 ]. It was suggested that the P. aurelia species complex emerged concomitantly with the most recent WGD [ 6 ], a hypothesis confirmed by sequencing two other aurelia genomes and P. caudatum as outgroup [ 7, 8 ]. Custom microarrays were designed to obtain gene expression data for the nearly 40, 000 P. tetraurelia protein-coding genes [ 9 ]. The data were used to show that gene expression level is a major determinant of gene dosage and protein evolution [ 10 ].  The Paramecium aurelia species complex is now recognized as an outstanding system to study the consequences of WGD in a unicellular eukaryote [ 11 ] and should also prove powerful for investigation of genetic and epigenetic mechanisms that drive speciation. In this context, the MAC genomes of many species are being sequenced. It thus became necessary to develop a pipeline optimized for the Paramecium lineage, able to make accurate gene predictions for AT-rich, compact (&gt;80% coding) eukaryotic genomes with unusually small introns (20-30 nt). To this purpose, we generated oriented P. tetraurelia RNA-Seq and Cap-Seq data, as input for software we developed to predict transcription units (TrUC). The transcription unit predictions and other evidence were combined to produce gene annotations using EuGene software [ 12 ] trained for Paramecium. Annotations obtained for P. tetraurelia were used as evidence to improve the annotation of other Paramecium species. The P. tetraurelia RNA-Seq samples were also analyzed for differential gene expression during the sexual cycle of autogamy, generating an improved transcriptome resource.    Results and Discussion\r\n   Transcription units\r\n  Genome-guided transcription unit construction was pioneered by Denoeud et al. [ 13 ] (G-Mo.R-Se software) using short-read mapping. The widely used Cufflinks software [ 14 ] then adopted fragment alignment as part of its assembly strategy. To take into account alternative splicing, Cufflinks finds the minimal number of paths through the mapped fragments. We decided to develop our own transcription unit prediction software rather than use Cufflinks, because in our hands, Cufflinks did not always predict transcription units despite good fragment coverage in regions presenting strong evidence of protein-coding genes, probably because of overlapping UTRs in the very compact Paramecium genome. Our approach is conceptually similar to G-Mo.R-Se but takes into account improvements in library construction and sequencing, especially orientation information. We added detection of transcription termination sites (TTS) using the polyA signal and the optional use of Cap-Seq data to predict transcription start sites (TSS). The TrUC (TRanscription Units by Coverage) pipeline is schematized in Fig. 1. TrUC predicts TSS and TTS using the consensus position of Cap-Seq and polyA coverage, respectively. TrUC uses oriented data to predict oriented transcription units, however the software can predict introns from un-oriented RNA-Seq data, using the GT..  AG splice site consensus to determine orientation. TrUC does not consider alternative splicing since exon skipping is not found in Paramecium [ 15 ]. Like Cufflinks, TrUC can identify non-coding transcripts as it does not look at translation. TrUC multi-threaded Perl software is available from https://github.com/oarnaiz/TrUC.  Since not all genes are expressed at all stages of the life cycle, use of different life-cycle time points can help annotation by increasing the number of genes that are covered by RNA-Seq data. For P. tetraurelia transcription unit prediction, we sequenced polyA+ RNA from a time-course through the sexual process of autogamy as well as from vegetative cells. Combining all the RNASeq and Cap-Seq data (Additional file 1: Table S1), TrUC predicts 37, 847 transcription units greater than 300 nt in size, 12,389 TSS and 5967 transcription termination sites (TTS). We found 85% of the previously annotated P. tetraurelia genes [ 6 ] (hereafter called \u201cv1\u201d annotation) covered by a predicted transcription unit. The average size of the predicted transcription units, 1229 nt, is close to the average size of the v1 genes (Table 1; Additional file 2: Figure S1) indicating that most of the predicted transcription units correspond to one gene. However, some transcription unit predictions may be split, owing to reduced RNA-Seq coverage within the unit. Alternatively, since genes in the compact P. tetraurelia genome 6 5 e s ,3 0 1 8 th ile 2 7 3 .9 3 4 2 .6 3 3 0 f ,93 4 ,23 8 ,85 48 .73 .06 .68 ,59 85 9 1 ,17 2 .31 ,32 .3 0 .In F3 2 9 7 1 .2 8 4 6 7 2 1 8 3 .2 6 2 3 2 .3 3 2 3 2 y F V 2 2 3 0 1 1 8 8 1 1 1 1 0 2 6 2 4 3 4 2 2 7 and tsud saG y t 37 th B 7 [ e D 4 in ra um ,21 07 7 .91 2 4 5 3 .54 22 7 5 d s ic ,66 0 ,20 4 ,74 03 .11 .31 .33 ,50 76 6 4 ,04 3 .52 ,45 .19 81 tre ion em 2V 76 32 34 .20 63 41 37 33 05 93 63 31 .20 24 21 12 04 .43 78 52 52 21 eop tta ra r o a , n P s n o d a t an eh tae a li 7 i T r rsxeeau ,,66214 0 ,2007 4 ,099 .2639 .628 .858 .53 ,099 .0808 6 ,6637 2 .404 3 ,278 .23 81 iIlltuhm.i[]8n iteeng . 1 7 3 3 .2 4 4 1 6 6 3 4 3 .2 2 0 8 .6 1 6 1 7 w ed ra P V 6 2 4 0 3 1 7 3 5 4 3 1 0 1 2 3 3 9 2 3 9 r e n 7 ft w lla e ,853 40 1 .22 3 6 8 9 .78 27 1 6 lya tum .nA 4 6 ,0 4 0 .8 .3 .0 7 9 ,4 .2 8 2 6 b a o 2 4 0 5 .2 0 4 3 7 1 3 0 3 .2 6 3 2 1 .3 4 5 5 3 emaud ita ,3 2 9 5 ,7 3 5 0 3 ,1 5 6 2 5 1 2 2 ,6 .8 6 t V 7 1 1 0 4 1 7 2 4 3 4 1 0 5 1 2 4 3 9 2 2 1 ss c o lia 60 thd tao feo reu ,76 15 0 .56 3 7 1 0 .51 73 9 63 7 seu ann inb iab ,77 62 ,76 5 ,11 06 .14 .72 .97 ,11 57 6 ,81 0 .19 3 ,72 .73 ,81 on nd irg .P 1V 57 41 51 .20 93 41 17 03 64 53 93 31 .20 41 02 73 .63 01 62 23 21 itta lya liao o b r n e e e u o 1 4 Th omiv ,9 1 1 5 7 .] n b 2 ,1202 79 ,2188 .82 ,5133 .0499 5 .4488 88 .6877 ,4060 .3303 .3 703 ,6352 22 .113 .92 ,7411 5 .531 20 i[6n ve1g fsoo d V 7 6 4 0 4 1 7 2 3 2 4 1 0 1 1 2 4 3 9 2 2 7 ed e l o h f h t f s li d ca b n s u a e a li 3 e in m reua ,544 48 2 .8 3 7 3 2 .23 61 1 2 wila ehd rteo rtte 1 ,902 79 ,821 .82 ,469 334 5 .258 .215 .023 ,469 363 .3 ,203 03 .091 .82 ,820 5 .415 8 reua ilsb red P V 7 6 4 0 3 1 7 2 4 3 3 1 0 1 2 4 3 9 2 2 3 rt u te e p li .t e f P r s  e a f o w w s c i t s i t a t s lea1bittanononA i)(tszeeenonm lfffrsscaeuoobdm 05 ittcceenoonnCGm reeeunnbm l()tteeennhng irtcceenondg iirttsccaeeonndgd iirttsccaeeonndgd iirttsccaeeonndgd iiteeeononndgCG l()ttSenhngD iirteeeononndgCG i-ceeunononndggm rxeuonnbm il(txaeeennonnhdg l()ttxaeeenonnhng rxeeeonngp rrteuonnbm iil(rttaeennnonnhdg il()rtttaeennonnhng ifrrtse40uononb&gt;m lssyaaeeeeongnhbmmifttaooonnn.iilrePbaua ,ftrsaeeeoh.ilirePbaua T G N N G G G P In In In rP C P N E M M E In M M N T a c ) ) ) A G G G T T T A A t n e r t ↔ ↔ ↔ e n  b o A G A c r TG TA TG um C eb )t ( ( ( n G t v1 nd ree ) t n d a f sometimes overlap, some transcription units may be fused owing to continuous RNA-Seq coverage, a problem partially overcome by use of the predicted TSS and TTS (cf. Fig. 1b). Split or fused predicted transcription units do not compromise gene annotation (cf. next section), which also takes into account protein-coding potential to define gene models.  A total of 85,236 introns were identified in the predicted transcription units, corresponding to a mean of 2.25 introns per transcription unit, very close to the 2.3 introns per gene previously reported for P. tetraurelia [ 6, 15 ].    Gene annotation\r\n  Accurate, user-friendly gene annotation tools for eukaryotes, such as AUGUSTUS [ 16 ], would require code modifications to correctly identify tiny introns [ 17 ]. Indeed, ~ 98% of P. tetraurelia introns are 20-30 nt in size, with a mean of 25 nt. A handful of significantly larger introns, ~ 80 nt in size, contain snoRNAs [ 18 ]. We therefore decided to train the highly configurable EuGene annotation software [ 12 ] for Paramecium, using gene models completely confirmed by RNA-Seq coverage (see Methods).  Table 1 compares gene annotations predicted by EuGene for P. tetraurelia, using the transcription units assembled with TrUC and other lines of evidence (labeled 'v2'), with the v1 gene annotation for this species [ 6 ], long considered a gold standard for ciliate gene annotation. The statistical differences are slight, aside from the fact that the v1 statistics do not include any non-coding gene predictions. The v2 annotation contains about 800 more protein-coding gene models, probably because of the greater quantity of transcript evidence allowing prediction of short genes (the average CDS length is slightly smaller in v2 annotation). To determine sensitivity, a set of 1690 manually curated genes was constituted (available from ParameciumDB [ 19 ]). We found 95% of the gene structures (excluding UTRs) and 99% of the introns to be correctly annotated.  In order to analyze the impact of the Cap-Seq data, we ran the same gene annotation pipeline for P. tetraurelia without any Cap-Seq data and compared the two sets of gene models. We found 91.8% of the protein-coding gene models to be identical. Among the 3293 gene models that were different, there were 649 cases where addition of the Cap-Seq data split one gene model into two gene models. In most of the remaining cases (2149), the coding sequence was changed. The Cap-Seq data we generated thus have a modest but significant impact on the gene annotation. Knowledge of TSS for the P tetraurelia model will be particularly valuable for functional studies. We looked for possible alternative TSS by evaluating whether adjacent TSSs fall within the same gene. In &gt;99% of the cases, adjacent TSS were in different genes. In only 29 cases could we find adjacent TSS in the same gene. We consider it likely that they represent technical noise given their occurrence mainly in highly expressed genes (Additional file 1: Table S2) but we cannot exclude biological significance. We conclude that alternative TSS are extremely rare in Paramecium.  The size distribution of 10,087 5\u2032 UTRs that could be confidently predicted from the Cap-Seq data, shown in Fig. 2, confirms the unusually small size of 5'UTRs in Paramecium (mean 21.7 nt, median 19 nt), much smaller than in animals and fungi (100-200 nt average, [ 20 ]) or the ciliate Tetrahymena thermophila (n = 4149, mean = 95.6 nt, median = 88 nt) [ 21 ]. The 4641 3\u2032 UTRs predicted from polyA tracts present in the RNASeq data (Fig. 2) display a nearly Gaussian distribution (mean = 44.6 nt, median = 44 nt) and are much smaller than in animals and fungi (200-1000 nt average, [ 20 ]) or Tetrahymena thermophila (n = 1290, mean = 231 nt, median = 163 nt, [ 21 ]).  The Paramecium-trained annotation pipeline was used to annotate P. biaurelia, P. sexaurelia and P. caudatum genomes, using the P. tetraurelia v2 predicted proteins as evidence as well as the unoriented RNA-Seq data previously used for the published annotation of these species [ 7, 8 ]. In all 4 species, we observe ~2.3 introns/gene and the same median intron size in v1 and v2 annotations (Table 1). However, the average intron size is larger in the published P. biaurelia, P. sexaurelia and - to a lesser extent - P. caudatum v1 annotations than in the corresponding v2 annotations (Table 1). This is owing to the prediction of significant numbers of introns larger than 40 nt in the published v1 annotations (Table 1, Fig. 3). Far fewer questionable \u201clarge\u201d introns are found in the v2 annotations (10%, 12.5% and 30% with respect to the v1 annotations for biaurelia, sexaurelia and caudatum, respectively). This is because the v2 annotations integrate TrUC intron predictions. Most of these \u201clarge\u201d introns are likely to be incorrect and as a consequence, so are the thousands of gene models that contain them. The difference between v1 and v2 annotations is less pronounced for P. caudatum, which has the smallest introns so far reported for the Paramecium genus (median size 22 nt, average size 23 nt). We conclude that TrUC predictions improve Paramecium gene models not only by predicting TSS and TTS if Cap-Seq and oriented mRNA-Seq data are available, but also thanks to the intron predictions, which can be made even from unoriented RNA-Seq data.  Paramecium genomes are intron-rich, an ancestral property of eukaryotes [ 22 ]. Not only is intron size very small, but no exon-skipping has been observed [ 15 ]. These properties helped discover translational control of eukaryotic intron splicing [ 15 ]. In Paramecium, splice site signals are weak, presumably because of the mutational burden, and splicing is not very accurate. The nonsense-mediated decay (NMD) pathway [ 23 ] cleans up the mess on the pioneer round of translation, provided that there is a Premature Termination Codon (PTC) in the poorly spliced transcript. That this is the case is easily visualized in Paramecium, thanks to the unique TGA stop codon, by comparing the frequency of introns that do or do not contain a STOP codon in phase with the upstream exon, as a function of their size modulus 3. As shown in (Additional file 2: Figure S2), stopless introns that are 3n in size are counter-selected in all 4 species: these introns cannot give rise to a PTC if retained in the transcript so are potentially deleterious. The observed deficit of 3n stopless introns in all 4 species (Additional file 2: Figure S2) validates the annotation and extends previous observation of translational control of intron splicing [ 15 ] to other P. aurelia species and to P. caudatum.    Differential gene expression\r\n  In Paramecium, the sexual cycle encompasses meiosis, fertilization and development of a new MAC. The latter process involves programmed elimination of at least 25% of the germline DNA. Custom microarrays were previously used to characterize differential gene expression during autogamy (auto-fertilization) in P. tetraurelia [ 9 ].  We now report use of the mRNA-Seq samples for differential gene expression. Since cells enter autogamy from a fixed point in the vegetative cell cycle [ 24 ], which is not synchronized in our cell cultures, there is an asynchrony of at least 5 h in the samples. Therefore, cytology data (Additional file 2: Figure S3) and gene expression levels were used to cluster samples into 6 stages: vegetative (Veg, n = 2), meiosis (Mei, n = 2), fragmentation (Frg, n = 3), early development (Dev1, n = 2), intermediate development (Dev2 and Dev3, n = 4) and late development (Dev4, n = 2) (see Methods, Additional file 1: Table S1 and Additional file 2: Figure S3). These stages are equivalent to those used previously with microarrays [ 9 ], with the addition of one later stage, Dev4.  For analysis of differential gene expression (DGE) we first counted mapped RNA-Seq fragments for each v2 gene model (see Methods). We found 99.8% of the mapped fragments in the sense orientation and 0.2% in the anti-sense (AS) orientation (see Additional file 2: Figure S4). This low level of AS transcription might reflect biological noise or pervasive transcription [ 25 ], especially as pervasive non-coding transcription is required for genome rearrangements in Paramecium [ 26, 27 ]. We cannot formally exclude errors in strandspecificity during construction of the sequencing libraries [ 28 ]. An intriguing possibility is regulation of gene expression by AS transcripts at specific loci. Higher coverage or long reads will be needed to interpret the AS transcription we have detected.  The DGE analysis was performed with the sense fragment counts and DESeq2 software. Requiring an adjusted p-value &lt;0.01 and a fold-change of 2 in expression, we identified 17,190 genes whose expression varied during autogamy. We separated these genes into induced (n = 8220) or repressed (n = 8970) (cf.  Methods), and then used hierarchical clustering to visualize the induced or repressed genes and to define 6 clusters: 'Early peak', 'Intermediate peak', 'Late peak', Late induction', 'Early repression', 'Late repression' (Additional file 2: Figures S5 and S6).  To validate the clusters, we used published genes involved in autogamy and some negative control genes not involved in autogamy (Additional file 1: Table S3). All of the genes known to be induced during autogamy were classified in an appropriate RNA-Seq induced cluster, although 4 of the 26 genes had not been found in the microarray clusters. Genes known to be expressed during meiosis in Paramecium (SPO11, SPT5m, DCL2, DCL3, NOWA1, NOWA2, LIG4a and XRCC4) [ 29-33 ] are found in the Early peak. Only one of the 12 negative control genes was induced during autogamy: PTIWI14, characterized as a component of the vegetative siRNA pathway [ 34 ], appears in the Late induction cluster. We also compared the distribution of the genes in the microarray clusters with respect to the RNA-Seq clusters (Table 2). The microarray resource was of good quality and essentially all of the genes in the microarray clusters are found in the RNA-Seq clusters. The RNA-Seq approach is more powerful and allows more genes to be identified as differentially expressed during autogamy. A modest qualitative difference between microarray and RNA-Seq classification concerns the microarray 'Intermediate induction' cluster. The genes in this cluster are found in either the RNA-Seq 'Intermediate peak' or the RNA-Seq 'Late induction' clusters. This may be a consequence of the additional late time points in the RNASeq experiment (Dev4) which change the gene expression profiles being clustered.  To estimate how well RNA-Seq data can discriminate the ~12,000 pairs of paralogs created by the recent WGD (~83% nucleotide identity on average), we computed the number of identical stretches of 100 nt shared by each pair. We estimate that &gt;97% of the paralog pairs are devoid of common 100 nt stretches over &gt;90% of their length and should therefore be well-discriminated in the present analysis. This is likely to be an underestimate, since the procedure uses mapping of pairs of 100 nt reads, not of single 100 nt reads. Therefore, in theory, RNA-Seq should better discriminate the paralogs than the previous microarray data [   [9]. In addition, RNASeq has greater dynamic range and sensitivity than none 1 0 1 0 4 3 20,043 Distribution of differentially expressed (DE) genes in RNA-Seq clusters (columns) and microarray clusters [   [9] (rows). Overall, the two analyses provide similar results, but the RNA-Seq approach detects many more DE genes. Essentially all microarray DE genes were found by RNA-Seq. The main qualitative difference is that the genes in the microarray Intermediate induction cluster are now found in one or the other of 2 RNA-Seq clusters: Intermediate peak or Late induction. See Methods for details of the analysis microarray technology ([ ([ 35] and our data), which can also contribute to good paralog discrimination. The P tetraurelia microarrays involved hybridization of 6 probes (50 nt) per gene designed to optimize chances of discriminating the paralogs. However, it is difficult to evaluate the extent of microarray cross-hybridization. In order to compare the discrimination of WGD paralogs by the microarray and RNA-Seq methods, we calculated paralog expression level divergence (Additional file 2: Figure S7). Paralogs are more sensitively discriminated by RNA-Seq because of its greater dynamic range (Additional file 2: Figure S7a, b). When the paralogs share high nucleotide identity (with the greatest risk of crosshybridization), there is little difference between their expression levels in the microarray data, but the same is true of the RNA-Seq data (Additional file 2: Figure S7ad). We propose two, non-exclusive, explanations to account for this observation, the first technical, the second biological. The first explanation is that RNA-Seq cannot discriminate highly similar paralogs because reads are mapped randomly between 2 equivalent loci, which reduces the difference in the measured expression level of such loci. The second explanation is that the paralogs sharing high nucleotide identity probably tend to have similar expression levels. This could result from strong selective pressure on highly expressed genes [10 10 ] or from gene conversion between WGD paralogs, frequently observed for P. aurelia species [7] 7 ]. Gene conversion leads to increased GC content and high nucleotide identity between paralogs, and can extend to promotor regions depending on the recombination breakpoint. Indeed, expression level and GC content are correlated with the nucleotide identity of P. tetraurelia paralogs (Additional file 2: Figure S7e, f ).  We also looked at whether genes duplicated at the recent WGD have kept the same expression profiles. First, we removed paralogs that differ in length by more than 10%, a filter that removes potential pseudogenes (n = 10,323; 85% of the paralog pairs are retained after this stringent filtering, P. tetraurelia v2 annotation). For 22% of the pairs, we found both paralogs in the same autogamy cluster, and for 38%, neither paralog was differentially expressed during autogamy. Interestingly, we found 31.5% of the pairs had one paralog in an autogamy cluster and one paralog not differentially expressed. This might reflect an early stage of pseudogenization, shown to begin with changes in expression level [ 11 ], probably via mutations in promoter sequences. We also found 8.5% of pairs with paralogs in different autogamy clusters, a possible indication of neo- or sub-functionalization. The important point is that finding paralogs with different expression profiles, irrespective of the origin of the difference, is an indication that the paralogs are well-discriminated by the RNA-Seq data.  A qualitative picture of the biological processes turned on during autogamy can be obtained using Gene Ontology (GO) terms [ 36 ], with the caveat that Paramecium functional annotation has not been curated. We first made a high-confidence gene set by requiring a foldchange of 4 during autogamy (7065 genes). We then used GO Biological Process terms associated with protein domains (cf. Methods) to make word clouds (Additional file 2: Figure S8). The Early peak (Additional file 2: Figure S8a) covers meiosis and fertilization and is enriched in appropriate terms: DNA, repair, chromosome, mismatch, condensation, homologous, recombination, Okazaki, replication, chromatid, cohesion. The Intermediate peak (Additional file 2: Figure S8b) corresponds to development of the new MAC involving chromatin remodeling and shows enrichment in repair, chromatin, DNA, methylation, and chromosome. Many biological processes are activated in the Late peak and Late induction clusters so that the only over-represented informative words relate to signal transduction (GTPase, signal, transduction, phosphorylation, inositol) and membrane trafficking (vesicle-mediated, autophagy) (Additional file 2: Figure S8c, d). The cluster of genes that are turned off when cells leave vegetative growth and enter the sexual cycle (Early repression, Additional file 2: Figure S8e) is enriched for words that refer to translation and cellular homeostasis (rRNA, redox, homeostasis, ribosome, oxido-reduction, translation, pseudouridine).  For the Late repression cluster (Additional file 2: Figure S8f ), the only informative words refer to microtubulebased, movement and phosphorylation, dephosphorylation.  The gene expression atlas is provided as a table (Additional file 3: Table S4).     Conclusions\r\n  Plummeting genome sequencing costs and rising interest in Paramecium species for studies of genome evolution in unicellular eukaryotes, prompted us to build a new pipeline for gene annotation that takes into account specificities of the genus, in particular high gene density and stereotyped small intron size. This has been achieved with new software to predict transcription units (TrUC) and specific training of the highly configurable EuGene gene annotation software. High quality gene annotations will be important for future comparative and functional genomics analyses of Paramecium species. The mRNA-Seq data used to predict P. tetraurelia transcription units for the gene annotation enabled us to generate an improved gene expression atlas and carry out differential gene expression analysis of the sexual cycle of autogamy that is more complete than previously possible with microarrays.    Methods\r\n   RNA samples and mRNA sequencing\r\n  Three time-course experiments for the sexual process of autogamy of P. tetraurelia wild-type strain 51 were used. Some of the samples had previously been used for microarray experiments [ 9 ] (see Additional file 1: Table S1). Total RNA was Trizol-extracted as previously described [ 9 ]. PolyA+ RNA was extracted from each sample using the FastTrack MAG mRNA isolation kit (Thermo Fisher Scientific) following the manufacturer's instructions. Strand-oriented Illumina libraries were made with reagents from the Illumina TruSeq Small RNA library preparation kit using an adapted protocol. First, RNA was fragmented by incubation for 4 min at 94 °C with New England Biolabs' Mg2+ solution, yielding fragments with an average size of ~260 nt. The RNA was purified using RNeasy columns (Qiagen) followed by treatment with antarctic phosphatase and polynucleotide kinase (New England Biolabs) and another purification on RNeasy columns. Illumina adapter ligation and RT-PCR was done essentially following the Illumina protocol, except that for the final library purification step AMPure beads were used (Beckman Coulter). HiSeq paired-end sequencing was performed on the samples, yielding at least 2 × 30 million reads, 100 nt in length, for each of the samples.  Cap-Seq Five samples, representing 3 time points (Veg, T0 and T11; [ 9 ]) were used for 5\u2032 Transcription Start Site (TSS) mapping. Purified polyA+ RNA was dephosphorylated using Calf Intestine Phosphatase (CIP) prior to 5\u2032 cap removal with Tobacco Acid Pyrophosphatase (TAP), using a FirstChoice RLM-RACE kit (Life Technologies). Illumina 5\u2032 adaptors were ligated to the 5\u2032 monophosphate ends generated specifically at TSSs, followed by RNA fragmentation by incubation for 4 min at 94 °C with New England Biolabs' Mg2+ solution. This yielded an average fragment size of 260 nt. Following CIP treatment to convert the 3\u2032 monophosphate ends generated by RNA fragmentation to 3'OH ends, Illumina 3\u2032 adapters were ligated to the fragments. The libraries were subjected to RT-PCR amplification (18-20 cycles) before Illumina HiSeq paired-end sequencing that yielded approximately 2 × 13 million reads of 100 nt per sample. Every step in TSS library preparation ended with purification using RNAeasy columns (Qiagen) or phenol/ chloroform extraction and isopropanol precipitation. The final PCR amplification products were purified using AMPure beads (Beckman Coulter) before sequencing.    Transcription unit determination\r\n  We developed the multi-threaded Perl software TrUC (Transcription Units by Coverage), dependent on the Bio::DB::Sam module. The software is organized in 3 independent modules (Fig 1). The TSS module uses Cap-Seq data, which need not be paired-end, to predict transcription start sites. The predicted TSS is the position with the highest Cap-Seq coverage in the interval defined by the size of the fragments. The TTS module uses oriented paired-end mRNA-seq reads. If one of the reads in the pair maps partially on the reference genome and ends in polyA, then the insert is used to specify a transcription termination site. The predicted TTS is the position with the highest polyA coverage in the interval defined by the size of the fragments. The transcript module takes paired-end TopHat2 mapping (BAM files; [ 14 ]) and optionally, the output of the TSS and TTS modules, to predict transcription units including intron positions, based on fragment coverage. TrUC was run with the following parameters for P. tetraurelia annotation: truc TSS -min_coverage 15 -nb_replicates 2 -min_score 500; truc TTS -min_coverage 5 -nb_replicates 2 -min_score 10 -nb_min_A 5; truc transcript -min_splicing_rate 0.7 -no_overlap -min_coverage 15 \\ -intron_consensus -min_intron_length 15 -max_intron_length 100 \\ -min_intron_coverage 15 -min_length 300 -min_score 45 -tss [truc TSS GFF3 output file] -tts [truc TTS GFF3 output file].  For P. biaurelia, P. sexaurelia and P. caudatum, TrUC was used with unoriented RNA-Seq data reported in [ 7, 8 ] (Accessions PRJNA268243, PRJNA268244 and PRJNA268245, respectively), to predict introns, with the following parameters: truc transcript -not_stranded -min_splicing_rate 0.7 -min_coverage 10 -intron_consensus -min_intron_length 10 \\ -max_intron_length 100 -min_intron_coverage 3 -min_length 300 -min_score 10. TrUC is distributed under a GNU GPL v3 license at https://github.com/oarnaiz/TrUC.    Gene annotation\r\n  The workflow for gene annotation is schematized in Fig. 1. EuGene software [ [1 2] was used to predict gene structure. EuGene was trained with 1597 curated Paramecium tetraurelia genes to generate a prediction matrix that takes into account the unusually small size of Paramecium introns [ [1 5]. The prediction matrix is available from http://paramecium.i2bc.paris-saclay.fr/download.The evidence sets used for annotation of the 4 Paramecium genomes are available on request.    Comparative genomics\r\n  UTR lengths for Tetrahymena thermophila were calculated using the June 2014 annotation available at http://www.ciliate.org/system/downloads/ T_thermophila_June2014.gff3.    Differential gene expression\r\n  Paired-end RNA-Seq reads were mapped to the reference P. tetraurelia strain 51 genome [ 37 ] using TopHat2 (v2.0.12, −−mate-inner-dist 50 -mate-std.-dev 100 -minintron-length 15 -max-intron-length 100 -coveragesearch -keep-fasta-order -library-type fr-secondstrand -min-coverage-intron 15 -max-coverage-intron 100 -min-segment-intron 15 -max-segment-intron 100 -max-multihits 1 -read-mismatches 1 -max-deletionlength 1 -max-insertion-length 1). Raw fragment counts for the genes in each sample, determined using htseqcount (v0.6.0 -stranded = yes -mode = intersection-nonempty) on filtered BAM files (samtools v0.1.18 samtools view -q 30), were used as input for DESeq2 (v1.4.1) [ 38 ], an R Bioconductor package, which normalizes the fragment counts, calculates the dispersion in the data using the biological replicates, and then determines differential gene expression using negative binomial linear models. The samples were grouped into biological replicates (Veg, Mei, Frg, Dev1, Dev2_3, Dev4) using the cytology data and clustering of the sample normalized counts with a distance matrix (see Additional file 2: Figures S3 and S4 sample dendrogram; see Additional file 1: Table S1). We considered genes to be differentially expressed during autogamy if at least one contrast between Veg and any point in the autogamy time course had an adjusted pvalue smaller than 0.01 and a fold-change (FC) of expression greater than 2. We filtered out genes if there was not at least one time point with more than 20 normalized counts. The genes were classed as induced (FC &gt; 2) or repressed (FC &lt; ½) before hierarchical clustering.    GO term enrichment and word cloud visualization\r\n  To gain qualitative appreciation of the processes associated with the groups of co-expressed genes, we focused on a subset of differentially expressed genes with a foldchange &gt;4 (and an adjusted p-value &lt;0.01). GO biological process terms were electronically inferred using InterProScan (v5.7.48) domain annotation of the corresponding proteins. If more than one protein domain was associated with a protein, the domain with the lowest InterProScan P-value was retained. All words in the terms were counted for all the protein-coding genes in the genome and for the protein-coding genes in each co-expression group. After removing non-discriminatory words (\u201cprotein\u201d, \u201cprocess\u201d, \u201cdomain\u201d and the \u201cstopwords\u201d defined by the wordcloud R package, v. 2.5), a Fisher exact test was used to determine the word enrichment ratio (p-value &lt;0.05) in each co-expression group with respect to the word frequency for the whole genome. A score determined for each word (score = log2(pvalue−1)) was used as weight to draw each word cloud (R wordcloud v2.5). The protein domains and GO terms used for this analysis can be found in the gene expression atlas (Additional file 3: Table S4).     Additional files\r\n  Additional file 1: Table S1. RNA sequencing. Table S2. Genes with potential alternative TSS. Table S3. Differential expression of genes with known autogamy expression profiles. (XLSX 38 kb) Additional file 2: Figure S1. Comparison of the sizes of P. tetraurelia transcription units and genes. Figure S2. Intron size distributions. Figure S3. Autogamy time-course experiments. Figure S4. Anti-sense transcription. Figure S5. Hierarchical clustering of differentially expressed genes. Figure S6. Autogamy co-expression clusters. Figure S7. Paralog discrimination by microarrays and RNA-Seq. Figure S8. Word cloud analysis of biological processes in clusters. (PDF 8271 kb) Additional file 3: Gene expression atlas. All P. tetraurelia v2 genes ('ID') with their normalized RNA-Seq counts (last 15 columns, sample labels as in Additional file 2: Table S1) are given. The mean value for biological replicates are given in the columns VEG, MEI, FRG, DEV1, DEV2/3, DEV4. The 'P-value' 'Significant' and 'Expression profile' refer to the differential gene expression analysis (cf. Methods). 'Note' is the description of the best SwissProt BLASTP match. The GO ID and GO description were inferred electronically using InterProScan. The Biological Process GO term associated with the highest scoring protein domain is given. (TSV 12379 kb)   Abbreviations\r\n  AS: Anti-sense; DE: Differentially Expressed; DGE: Differential Gene Expression; GFF3: Generic Feature Format, version 3; GO: Gene Ontology; MAC: Macronucleus; MIC: Micronucleus; PTC: Premature Termination Codon; TrUC: Transcription Units by Coverage; TSS: Transcription Start Site; TTS: Transcription Termination Site; WGD: Whole Genome Duplication    Acknowledgements\r\n  We thank Thomas Schiex for advice about training EuGene. This work has benefited from the facilities and expertise of the High-throughput Sequencing Platform of the I2BC. We are grateful to the INRA MIGALE bioinformatics platform (http://migale.jouy.inra.fr) for providing help and support. This work was carried out in the context of the CNRS-supported European Research Group \u201cParamecium Genome Dynamics and Evolution\u201d.    Funding\r\n  This work was supported by Centre National de la Recherche Scientifique intramural funding, the Agence Nationale de la Recherche Scientifique ANR14-CE10-0005-04 'PIGGYPACK' to M.B., S.D. and L.S. and ANR-12-BSV6-001703 'INFERNO' to M.B., S.D. and L.S., the 'Comité d'Ile de France de la Ligue Nationale Contre le Cancer' to S.D. and an 'Equipe FRM DEQ20160334868' grant to S. D. The funding bodies had no role in the design of the study, analysis, or interpretation of data or in writing the manuscript.    Availability of data and materials\r\n  The 15 RNA-Seq and 5 Cap-Seq datasets were deposited in the ENA under the Project Accession PRJEB19343 (see Additional file 1: Table S1). All gene annotation versions are available as GFF3 files from http://paramecium.i2bc.paris-saclay.fr/download/.    Authors\u2019 contributions\r\n  EVD, MB, MLK, AV and SD prepared samples and acquired data; OA, ES, and JG developed methods and analyzed data; OA, MB, SD and LS conceived the study; OA and LS prepared the manuscript. All authors approved the manuscript.    Ethics approval and consent to participate\r\n  Not applicable.    Consent for publication\r\n  Not applicable.    Competing interests\r\n  The authors declare that they have no competing interests.     Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/oarnaiz/TrUC",
    "publicationDate": "0",
    "authors": [
      "Olivier Arnaiz",
      "Erwin Van Dijk",
      "Mireille Bétermier",
      "Maoussi Lhuillier-Akakpo",
      "Augustin de Vanssay",
      "Sandra Duharcourt",
      "Erika Sallet",
      "Jérôme Gouzy",
      "Linda Sperling"
    ],
    "status": "Success",
    "toolName": "TrUC",
    "homepage": ""
  },
  "87.pdf": {
    "forks": 2,
    "URLs": ["github.com/dvav/eQTLseq"],
    "contactInfo": ["dimitris.vavoulis@well.ox.ac.uk"],
    "subscribers": 1,
    "programmingLanguage": "Jupyter Notebook",
    "shortDescription": "Hierarchical probabilistic models for multiple gene/variant associations based on NGS data",
    "publicationTitle": "Hierarchical probabilistic models for multiple gene/variant associations based on next-generation sequencing data",
    "title": "Hierarchical probabilistic models for multiple gene/variant associations based on next-generation sequencing data",
    "publicationDOI": "10.1093/bioinformatics/btx355",
    "codeSize": 69483,
    "publicationAbstract": "Motivation: The identification of genetic variants influencing gene expression (known as expression quantitative trait loci or eQTLs) is important in unravelling the genetic basis of complex traits. Detecting multiple eQTLs simultaneously in a population based on paired DNA-seq and RNA-seq assays employs two competing types of models: models which rely on appropriate transformations of RNA-seq data (and are powered by a mature mathematical theory), or count-based models, which represent digital gene expression explicitly, thus rendering such transformations unnecessary. The latter constitutes an immensely popular methodology, which is however plagued by mathematical intractability. Results: We develop tractable count-based models, which are amenable to efficient estimation through the introduction of latent variables and the appropriate application of recent statistical theory in a sparse Bayesian modelling framework. Furthermore, we examine several transformation methods for RNA-seq read counts and we introduce arcsin, logit and Laplace smoothing as preprocessing steps for transformation-based models. Using natural and carefully simulated data from the 1000 Genomes and gEUVADIS projects, we benchmark both approaches under a variety of scenarios, including the presence of noise and violation of basic model assumptions. We demonstrate that an arcsin transformation of Laplace-smoothed data is at least as good as state-of-the-art models, particularly at small samples. Furthermore, we show that an over-dispersed Poisson model is comparable to the celebrated Negative Binomial, but much easier to estimate. These results provide strong support for transformation-based versus count-based (particularly NegativeBinomial-based) models for eQTL mapping. Availability and implementation: All methods are implemented in the free software eQTLseq: https://github.com/dvav/eQTLseq Contact: dimitris.vavoulis@well.ox.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-10-09T10:21:21Z",
    "institutions": [
      "University of Oxford",
      "John Radcliffe Hospital",
      "National Institute for Health Research Oxford Biomedical Research Centre",
      "The Nuffield Division of Clinical Laboratory Sciences"
    ],
    "license": "MIT License",
    "dateCreated": "2017-02-08T14:07:34Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx355   Hierarchical probabilistic models for multiple gene/variant associations based on next-generation sequencing data     Dimitrios V. Vavoulis  1  2  3  4    Jenny C. Taylor  2  4    Anna Schuh  0  1  2    0  Department of Oncology, University of Oxford ,  Oxford, OX3 7DQ   UK    1  National Health Service Translational Molecular Diagnostics Centre, Oxford University Hospitals, John Radcliffe Hospital ,  Oxford, OX3 9DU   UK    2  National Institute for Health Research Oxford Biomedical Research Centre ,  Oxford, OX3 9DU   UK    3  The Nuffield Division of Clinical Laboratory Sciences    4  The Wellcome Trust Centre for Human Genetics, University of Oxford ,  Oxford, OX3 7BN   UK     2017   1  1  7   Motivation: The identification of genetic variants influencing gene expression (known as expression quantitative trait loci or eQTLs) is important in unravelling the genetic basis of complex traits. Detecting multiple eQTLs simultaneously in a population based on paired DNA-seq and RNA-seq assays employs two competing types of models: models which rely on appropriate transformations of RNA-seq data (and are powered by a mature mathematical theory), or count-based models, which represent digital gene expression explicitly, thus rendering such transformations unnecessary. The latter constitutes an immensely popular methodology, which is however plagued by mathematical intractability. Results: We develop tractable count-based models, which are amenable to efficient estimation through the introduction of latent variables and the appropriate application of recent statistical theory in a sparse Bayesian modelling framework. Furthermore, we examine several transformation methods for RNA-seq read counts and we introduce arcsin, logit and Laplace smoothing as preprocessing steps for transformation-based models. Using natural and carefully simulated data from the 1000 Genomes and gEUVADIS projects, we benchmark both approaches under a variety of scenarios, including the presence of noise and violation of basic model assumptions. We demonstrate that an arcsin transformation of Laplace-smoothed data is at least as good as state-of-the-art models, particularly at small samples. Furthermore, we show that an over-dispersed Poisson model is comparable to the celebrated Negative Binomial, but much easier to estimate. These results provide strong support for transformation-based versus count-based (particularly NegativeBinomial-based) models for eQTL mapping. Availability and implementation: All methods are implemented in the free software eQTLseq: https://github.com/dvav/eQTLseq Contact: dimitris.vavoulis@well.ox.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  The identification of genetic variants affecting gene expression (known as expression quantitative trait loci or eQTLs) is an important step in unravelling the genetic basis of complex traits, including diseases  (Albert and Kruglyak, 2015; Cookson et al., 2009; Joehanes et al., 2017) . Powered by next-generation sequencing (NGS) and the simultaneous genome-wide profiling of genetic variation and gene expression, this task poses novel statistical challenges due to the idiosyncratic nature of the data generated by these technologies. Count data produced by assays such as RNA-seq  (Wang et al., 2009) , constitute a digital measure of gene expression, thus making methodologies developed for continuous microarray data not directly applicable.  A straightforward approach to eQTL mapping using RNA-seq would be to transform digital expression data  (Zwiener et al., 2014)  and then proceed using methodologies developed for micro-arrays, which usually assume normally distributed data  (Bottolo et al., 2011; Cheng et al., 2014; Flutre et al., 2013; Shabalin, 2012; Yi and Xu, 2008) . The basic obstacle in directly applying such methods on normalized RNA-seq data are the high degree of skewness, extreme values and a non-trivial mean-variance relationship, which commonly characterize such data. A simple, albeit imperfect, approach to ameliorate these effects is the application of a logarithmic transformation, making sure that zero counts are appropriately handled in order to avoid infinities. Two further options are power transformations, such as the Box-Cox transformation  (Box and Cox, 1964) , and rank-based transformations, such as the Blom transformation  (Beasley et al., 2009) , both of which aim to make the data more normal-like. While the aforementioned transformations are not specific to RNA-seq, variance-stabilizing approaches that explicitly model the mean-variance relationship in such data are provided by specialized software, such as DESeq2 (functions rlog and vst)  (Love et al., 2014)  and limma (function voom)  (Law et al., 2014) . The practical advantage of using appropriate data transformations is the immediate availability of analytical methods, which (being built around the assumption of normally distributed data) are powered by a tractable mathematical theory.  An alternative approach is to develop statistical methods that specifically address the discrete nature of digital expression data  (Kumasaka et al., 2016; Sun and Hu, 2013) . Explicitly modeling counts has been quite popular in the study of differential gene expression, and several methods have been developed for this purpose based on Poisson, Binomial and, especially, Negative Binomial distributions  (Kvam et al., 2012; Seyednasrollah et al., 2015; Soneson and Delorenzi, 2013) . These models directly address non-normality (particularly at small count numbers) and non-linear mean-variance trends, and they often adopt some form of information sharing between genes for shrinking dispersion estimates. A natural strategy to achieve the latter is by imposing a prior distribution on the dispersion parameter within a Bayesian inference framework and 'let the data decide' on an appropriate amount of shrinkage  (Vavoulis et al., 2015; Wu et al., 2013) .  Regardless of which approach is used to model digital expression data, the problem of finding associations with particular genetic variants (i.e. eQTLs) can be expressed as a regression problem, where gene expression is treated as the response variable and the genotypes of the variants as the explanatory variables. The aim of subsequent analysis is to estimate a set of coefficients that capture the strength of all possible associations between variants and genes. As such, the problem of eQTL mapping is intimately related to genetic (possibly genome-wide) association studies and, indeed, searching for eQTLs can be thought of as a genetic association task, where gene expression plays the role of the phenotype. The simplest approach is to examine each gene-variant pair independently by expressing their relationship as a simple, univariate regression problem  (Kumasaka et al., 2016; Shabalin, 2012; Sun and Hu, 2013) . This is perhaps the most computationally feasible choice, when a large number of gene-variant pairs is examined, but it typically requires some form of multiplicity correction in order to retain an acceptable Type I error rate and it bears the risk of missing possible synergistic effects of multiple variants on gene expression. If such a risk is not acceptable and, especially, if a set of genes and variants has been pre-selected (for example, on the basis of clinical criteria), an alternative would be to model the effect of multiple variants on the expression of multiple genes simultaneously, in which case a multivariate/multiple regression framework becomes appropriate  (Bottolo et al., 2011; Cheng et al., 2014; Flutre et al., 2013; Yi and Xu, 2008;) .  In this paper, we develop two classes of statistical models for detecting simultaneously multiple associations between gene expression and genomic polymorphisms in a population, as measured by paired DNA-seq and RNA-seq assays. The first class involves Poisson, Binomial and Negative Binomial models, which explicitly model digital gene expression as a function of genetic variation. The second class involves a Normal/Gaussian model, which relies on appropriate transformations of gene expression data. All models are embedded in a Bayesian multiple/multivariate regression and variable selection framework, which permits for fair comparison between them. Traditionally, Bayesian inference in regression models involving the Negative Binomial distribution has been discouraged due to the intractable form of the likelihood function   (Polson et al., 2013 ). An important contribution of this paper is expressing the posterior probability of multiple gene-variant associations in the Negative Binomial and the other count-based models in a convenient mathematical form through the introduction of latent variables, which facilitates sparse Bayesian learning. A second important contribution, is the introduction of Laplace smoothing   (Chen and Goodman, 1999 ) and the arcsin transformation   (Warton and Hui, 2011 ) for pre-processing digital gene expression data. Using carefully simulated RNA-seq and genotype data and natural data from the 1000 Genomes   (1000 Genomes Project Consortium et al., 2015 ) and gEUVADIS projects   (Lappalainen et al., 2013 ), we show that this pre-processing step in combination with a Normal model demonstrates top performance in a variety of scenarios, particularly at small samples sizes. The over-dispersed Poisson and Negative Binomial models also show excellent performance, when the sample size is sufficiently large. We examine model behavior in a variety of scenarios, including the presence of two different types of noise, violation of model assumptions, various association strengths and patterns of eQTL distribution. All methods are implemented in the freely available Python software eQTLseq (https://github.com/dvav/eQTLseq).    2 Materials and methods\r\n   2.1 Model overview\r\n  Below, we briefly describe the main elements of the proposed models. A detailed mathematical treatise and associated inference methods is given in the Supplementary Material. eQTLseq (Fig. 1a) implements a number of hierarchical probabilistic models, which represent gene expression as a function of genetic variation [Fig. 1(b-d)]. Three of these models (Binomial or bin, Negative Binomial or nbin and Poisson or pois) take explicitly into account the discrete nature of expression data measured in, for example, RNA-seq assays, while a fourth model is based on the Normal distribution and it relies on appropriate transformations of such expression data (see Data transformations). We assume that genotype data and count data measuring transcript abundance for N samples is summarized in an N M matrix X and an N K matrix Z, respectively, where M is the number of genetic markers and K is the number of transcripts. Under an additive genetic model, each element of X takes values in the set {0, 1, 2} indicating the number of minor alleles at a specific locus. eQTLseq takes matrices X and Z (or Y, a transformed version of Z) as input and employs Gibbs sampling  (Andrieu et al., 2003)  to estimate an M K matrix of regression coefficients B. The elements bjk of this matrix can be positive, negative or zero indicating positive, negative or no effect of marker j on the expression of transcript k. We assume that B is sparse (i.e. most elements bjk are zero), which implies that most markers in X have no influence on transcript abundance. In all models, sparsity is induced by assuming the following prior for each bjk: pðbjkÞ  Nð0; sk 1fjk1gj 1Þ where Nð0; n 1Þ is the Normal distribution with mean 0 and variance n 1, and sk, fjk and gj are precision parameters each following Jeffrey's prior. Jeffrey's prior was chosen because it is noninformative, it is invariant under re-parametrization, it has strong sparsity-inducing properties and, importantly, it is parameter-free, thus yielding excellent performance without the need for parameter adjustments  (Figueiredo et al., 2002) . Small values of the markerspecific parameter gj imply that marker j is likely to influence a large number of genes (indicating the presence of a hotspot). Similarly, small values of the transcript-specific parameter sk imply a transcript that is likely influenced by many markers (indicating a polygenic effect on transcript k). The synergistic effect of sk and gj is further refined in a marker- and transcript-specific manner through parameter fjk.  We are particularly interested in the posterior distribution pðbkj Þ of the vector of regression coefficients bk, which captures the effects of M genetic markers on the abundance of transcript k (notice that the symbol - is a shorthand for all random variables the posterior density of bk is conditioned on). A major contribution of this work is that we apply recent statistical theory  (Polson et al., 2013)  to express this posterior in closed form, which greatly facilitates Bayesian learning. Although this is trivial for the Normal model, it is not obvious for the Binomial and Poisson models and particularly difficult for the Negative Binomial, thus discouraging the use of the latter in a Bayesian multivariate regression setting. Here, we show that for all models, the above posterior is a multivariate Normal distribution, with mean vector mk and precision matrix Ak. In the case of the Normal model, these are simply functions of the genotypes X, the transformed expression data yk for transcript k and the precision parameters sk, fk and g. In the case of the Binomial and Poisson models, yk is an N-vector of normally distributed pseudo-data (i.e. latent or unobserved variables). These pseudo-data decouple the coefficients bk from the actually observed non-Normal gene expression data, thus making the above inference possible. Importantly, in the case of the Negative Binomial model, we show that parameters mk and Ak are functions of the genotypes X, the expression data zk for transcript k, the precision parameters sk, fk and g and a vector xk of auxiliary parameters, which follow a Polya-Gamma distribution. 2.2 Data transformations eQTLseq requires appropriately transformed expression data as input, when a Normal model is selected. In this paper, we examine various data transformation methods, including a simple logarithmic transformation (log), a Box-Cox transformation  (Box and Cox, 1964)  (boxcox), a Blom transformation  (Beasley et al., 2009)  (blom), the voom transformation  (Law et al., 2014)  provided by the R package limma and a variance-stabilizing transformation (vst) provided by the R package DESeq2  (Love et al., 2014) . A regularized log transformation provided also by DESeq2 was excluded, because it becomes prohibitively expensive when datasets with hundreds of samples are used, as in the analyses presented in this paper. For the log and Box-Cox transformations, a single pseudocount was added to the data in order to avoid infinities.  Furthermore, we introduce Laplace (also known as Lidstone or additive) smoothing  (Chen and Goodman, 1999)  followed by arcsin or logit transformations  (Warton and Hui, 2011)  as a preprocessing step. Additive smoothing is commonly used by naive Bayes classifiers and in natural language processing for smoothing categorical data. The inspiration for applying this type of smoothing stems from viewing an RNA-seq sample as the outcome of a multinomial experiment, where a large number of reads (i.e. the raw size of the library) is 'assigned' (i.e. mapped) to a number of 'categories' (i.e. transcripts). The smoothed mapping probability of each transcript can be expressed as follows:  zik þ c pij ¼ Pk zik þ cK where zik is the number of reads for transcript k in sample i, Pk zik is the total number of reads in sample i and c is a positive number, which equals 1 in this paper. zik may be normalized prior to Laplace smoothing, if necessary. Notice that, similarly to voom, pik lies in the interval (0, 1), but unlike voom, Pk pik ¼ 1. Also, the above probability lies between the empirical probability zik= Pk zik and the uniform probability 1=K. The smaller the library size and the larger the number of genes, the closer the factor cK in the denominator pulls pij towards 1=K. The probabilities pij are further processed using the arcsin or logit transformations, which are natural choices for proportions  (Warton and Hui, 2011) .     3 Results\r\n   3.1 Benchmarks based on simulated data\r\n  Benchmarks based on simulated data are essential, because they allow control of the exact conditions under which data is generated. At the same time, it is important that simulated data imitate faithfully the statistical characteristics of natural data. In this paper, we simulate read counts and genotypes using data in the public domain as templates (details are provided in the Supplementary Material). Briefly, artificial pairs of read counts and genotype matrices were generated according to the following protocol: (a) decide the number of samples N in the artificial dataset (assuming K ¼ 1000 genes and M ¼ 100 genetic markers, throughout), (b) decide the strength, directionality and pattern of associations between genes and genetic variants and generate an M K matrix of coefficients B using an Exponential distribution, (c) generate an N M matrix of genotypes X from a Binomial distribution using data from the 1000 Genomes project  (1000 Genomes Project Consortium et al., 2015)  as template, (d) given X, B and RNA-seq data from 60 HapMap individuals of European descent  (Frazee et al., 2011; Montgomery et al., 2010)  as template, generate an N K matrix of read counts Z from a Negative Binomial distribution. Different triplets of X, Z and B matrices were generated reflecting differences in: (i) the strength of gene/variant associations (i.e. effect sizes), (ii) the pattern of associations (e.g. the number of hotspots and polygenic effects), (iii) the type and level of random noise in the expression and genotype data, (iv) the presence or absence of over-dispersion and (v) the sample size. For each combination of the above factors, we generate three random replicates resulting in 1512 artificial datasets (i.e. pairs of X and Z matrices), which each model (nbin, bin, pois, log, arcsin, logit, blom, boxcox, voom, and vst) is assessed on, in a total of 15 120 simulations ran on the Wellcome Trust Center for Human Genetics local cluster.  The overall performance of each model at different sample sizes (while the remaining simulation parameters - noise, association strengths, etc. - were left to vary freely, as opposed to being fixed at specific values) is illustrated in Figure 2. We use simulated data with sample sizes equal to 250, 500, 1000 and 2000 subjects and we examine the ability of each model (a) to distinguish correctly between true and false positives and negatives and (b) to estimate correctly the strength of gene/variant associations in the simulated data. For the former, we use Matthews correlation coefficient  (Matthews, 1975)  or MCC, which is generally regarded as a balanced measure of performance for binary classifiers, even in cases where the two different classes in the data have very different sizes. For assessing how well each model estimates effect sizes, we calculate the root mean square error (RMSE) between estimated and true effect sizes (after these have been appropriately normalized; see Supplementary Material) among the true positives for each model.  As expected, the performance of all models increases with increasing sample size (Fig. 2a). At small sample sizes (N ¼ 250), the arcsin model clearly has the highest MCC value, while all other models perform similarly. The exception is the Binomial model (bin), which demonstrates the smallest MCC value at all sample sizes. As the sample size increases, the performance of the Negative Binomial (nbin) and Poisson (pois) models increases, slightly overtaking arcsin at very large sample sizes (N ¼ 2000). All other Normal models perform similarly. At the same time, all count-based models (bin, nbin and pois) are clearly more effective at estimating correctly the strength of gene/variant associations, albeit their distance from Normal models decreases with increasing sample size (Fig. 2b).  The observation that nbin and pois outperform arcsin only at large samples (N ¼ 2000) is at least partially explained by the relatively low true positive rate (TPR) and positive predictive value (PPV) of these two models (see Supplementary Fig. S1) and it should be viewed in terms of model complexity: to fully define the countbased models, we must estimate N K more variables than the Normal, which presumably requires larger samples. Interestingly, pois performs similarly to or better than nbin, although the former is significantly easier and faster to estimate. This is not surprising given that nbin can be thought of as a Poisson-Gamma mixture: read counts follow a Poisson distribution with a gamma-distributed rate. pois in this study is actually a Poisson-LogNormal mixture: the Poisson rate is log-normally distributed, instead of Gamma. The similar performance of nbin and pois implies that the latter provides a good approximation to the former. Further results are provided in Supplementary Figures S1-S6, S10 and S11.    3.2 Benchmarks based on gEUVADIS data\r\n  Next, we tested all models on data from the gEUVADIS project  (Lappalainen et al., 2013) . This includes mRNA and small RNA sequencing data on 465 lymphoblastoid cell line samples from the 1000 Genomes project, along with paired data on genomic variation. We kept only variants on bi-allelic loci with MAF larger than 5%, which overlapped any of 1672 human transcription factors available from the FANTOM5 project website  (Lizio et al., 2015) . These were subsequently annotated with the Variant Effect Predictor  (McLaren et al., 2016)  (VEP; distributed with the Ensembl Tools v84) and the genotypes of all variants predicted to have HIGH impact were retained for further analysis. gEUVADIS expression data were filtered by removing all transcripts that had on average less than 10 reads across all samples, resulting in two count data matrices with 408 miRNAs and 19 004 mRNAs, respectively.  Ideally, all models should be benchmarked against an objective truth, i.e. the true gene/variant associations in the data. However, as it is usually the case with natural datasets, this truth is not known. Under these circumstances, it is an established practice to adopt a cross-validation approach, which aims to evaluate the predictive value of a particular statistical model or models on independent datasets, i.e. datasets not previously utilized in estimating model parameters or structure. Here, we employ a Monte Carlo cross-validation methodology to objectively compare different models. Briefly, this consists of splitting the paired genotype and gene expression data randomly in two sets, training and validation, with ratio 3:1. Each model is estimated on the training set and its predictive performance is calculated on the validation set. This process is repeated 10 times and an average predictive performance is calculated for each model. As a measure of predictive performance we use the concordance correlation coefficient (CCC)  (Lin, 1989) , which measures the agreement between two sets of values in a scale-independent manner and, thus, ensures fair comparison between different models.  All models demonstrate very high CCC values (&gt; 0.95) in both mRNAs and miRNAs (Fig. 3a). arcsin and vst show top performance, with arcsin performing slightly better than vst for mRNAs. In the same group, blom and boxcox have average or above average performance, while bin, pois, log and logit perform similarly, just below average. The worst performance is demonstrated by voom and, particularly, by the Negative Binomial model. For miRNAs, the situation is the opposite for blom and boxcox, which now perform worse than all other models. The Binomial, Negative Binomial, Poisson, log, logit, and voom models all perform above average. It is interesting to observe that, in the case of mRNAs, arcsin gives the most sparse solutions (i.e. the smallest number of gene/variant associations) among all models, while vst gives the least sparse ones followed by voom and the Negative Binomial model (Fig. 3b). The Binomial and Poisson models are also quite conservative regarding the number of gene/variant associations they identify. In the case of miRNAs, boxcox gives on average the most sparse solutions, followed by voom and logit, while the Negative Binomial model is clearly the least conservative. We conclude that all models demonstrate excellent predictive performance, with arcsin and vst being the best among them. At the same time, the arcsin, boxcox, Poisson and Binomial models are the most conservative, depending on whether they are applied on mRNA or miRNA datasets. Analysis of all gEUVADIS samples, revealed 28 variants, which have been identified as eQTLs by at least one model (Fig. 3c). Among them, 5 have been consistently selected by more than half of the models (ID: 3, 4, 5, 22, 23; also, see Table 1). 4 Discussion Identifying gene/variant associations in a population, based on paired RNA-seq and DNA-seq assays, can be formulated in terms of multiple/multivariate regression, where digital gene expression and genotype data play the role of response and explanatory variables, respectively. We compare two competing approaches for modeling digital gene expression in this context: the first approach employs count-based (Binomial, Negative Binomial and Poisson) models; the second applies a Normal model on appropriately transformed data. Both approaches are embedded in a sparse Bayesian learning framework for regression and variable selection through shrinkage, which simplifies their implementation in software and permits fair comparison between different models. The methodological novelty of our approach is expressing the posterior probability density of gene/ variant associations in the form of a multivariate Normal distribution in all models. This is achieved in the case of count-based models through the introduction of latent variables, thus lifting one of the basic reasons discouraging the use of such models (particularly the Negative Binomial one) in a Bayesian learning framework, i.e. mathematical intractability.  Using artificial data modeled closely after data from the 1000 Genomes project, we demonstrate that a Normal model applied on Laplace-smoothed and arcsin-transformed data shows excellent performance particularly at small samples. Negative Binomial and Poisson models are also top performers, with the latter being at least as good as the former, but with smaller computational cost and despite the fact that simulated data assume a Negative Binomial distribution. Further benchmarks on natural data from the gEUVADIS project confirm the top performance of arcsin, but also show that the more computationally expensive vst is equally good. More generally, the predictive performance of all models is quite high, although not all are equally parsimonious.  Based on these results, we conclude that when mapping eQTLs in a population using NGS data, a Negative Binomial model is not the only or even the best option. This is important because the assumption of a Negative Binomial distribution has been extremely popular in modeling RNA-seq data, particularly in the context of differential gene expression. Instead, researchers can use a Poisson-LogNormal mixture, or a Normal model after appropriately transforming the expression data. The type of transformation that should be used is case-dependent, but our simulations indicate that transforming the RNA-seq data to multinomial probabilities and applying an arcsin or even a logit (as in the gEUVADIS data) transformation is a good start. In this respect, our conclusions are in agreement with previous work, which highlights the value of Normal models in modeling RNA-seq data  (Law et al., 2014; Soneson and Delorenzi, 2013) .  The sparsity-inducing priors we use for the regression coefficients B are related to the automatic relevance determination concept  (Tipping, 2001) , but other strategies are also possible, including the Bayesian lasso  (Park and Casella, 2008) , the spike-and-slab prior  (Ishwaran and Rao, 2005)  and others  (O'Hara and Sillanpaa, 2009) . The advantage of our choice of sparse priors is achieving excellent sparsity performance without the need to adjust any parameters. Applying the above statistical framework to possibly thousands of genomic variants involves splitting these variants in groups of several hundreds each, which can be viewed as a compromise between testing each variant independently or all of them simultaneously as a single group (see Supplementary Material for computational considerations). Alternatively, we can employ a dimensionality reduction method, e.g. in the form of sparse latent factor models  (Knowles and Ghahramani, 2011; West et al., 2003) , thus increasing the scale of model applicability, but at the same time introducing the issue of biological interpretability of the latent factors. Further work would also involve modifying the above models to account for strand- and isoform-specific expression data made uniquely available through RNA-seq. Previous work  (Kumasaka et al., 2016; Sun and Hu, 2013) , which however does not take into account the joint distribution of multiple transcripts and variants, as we do here, should be the logical starting point.     Acknowledgements\r\n  DVV would like to thank Dr George Nicholson for advice on the use of performance metrics and Drs Melissa Pentony and Erika Kvikstad for useful comments on an early version of this work.    Funding\r\n  This work was supported by the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre Program. The views expressed in this manuscript are not necessarily those of the Department of Health. We also acknowledge the Wellcome Trust Centre for Human Genetics Wellcome Trust Core Award Grant Number 090532/Z/09/Z. The funders of the study had no role in the study design, data collection, data analysis, data interpretation, or writing of the paper.  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/dvav/eQTLseq",
    "publicationDate": "0",
    "authors": [
      "Dimitrios V. Vavoulis",
      "Jenny C. Taylor",
      "Anna Schuh"
    ],
    "status": "Success",
    "toolName": "eQTLseq",
    "homepage": ""
  },
  "50.pdf": {
    "forks": 41,
    "URLs": [
      "github.com/hms-dbmi/UpSetR/",
      "gehlenborglab.shinyapps.io/upsetr/"
    ],
    "contactInfo": ["nils@hms.harvard.edu"],
    "subscribers": 22,
    "programmingLanguage": "HTML",
    "shortDescription": "An R implementation of the UpSet set visualization technique published by Lex, Gehlenborg, et al..  ",
    "publicationTitle": "UpSetR: an R package for the visualization of intersecting sets and their properties",
    "title": "UpSetR: an R package for the visualization of intersecting sets and their properties",
    "publicationDOI": "10.1093/bioinformatics/btx364",
    "codeSize": 28761,
    "publicationAbstract": "Motivation: Venn and Euler diagrams are a popular yet inadequate solution for quantitative visualization of set intersections. A scalable alternative to Venn and Euler diagrams for visualizing intersecting sets and their properties is needed. Results: We developed UpSetR, an open source R package that employs a scalable matrix-based visualization to show intersections of sets, their size, and other properties. Availability and implementation: UpSetR is available at https://github.com/hms-dbmi/UpSetR/ and released under the MIT License. A Shiny app is available at https://gehlenborglab.shinyapps.io/upsetr/. Contact: nils@hms.harvard.edu Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-10-19T19:05:55Z",
    "institutions": [
      "Harvard Medical School",
      "University of Utah"
    ],
    "license": "https://github.com/hms-dbmi/UpSetR/blob/master/LICENSE",
    "dateCreated": "2015-05-26T18:29:34Z",
    "numIssues": 32,
    "downloads": 0,
    "fulltext": "     10.1093/bioinformatics/btx364   UpSetR: an R package for the visualization of intersecting sets and their properties     Jake R. Conway  0    Alexander Lex  1    Nils Gehlenborg  0    0  Department of Biomedical Informatics, Harvard Medical School ,  Boston, MA 02115 ,  USA    1  SCI Institute, School of Computing, University of Utah ,  Salt Lake City, UT 84112 ,  USA     Motivation: Venn and Euler diagrams are a popular yet inadequate solution for quantitative visualization of set intersections. A scalable alternative to Venn and Euler diagrams for visualizing intersecting sets and their properties is needed. Results: We developed UpSetR, an open source R package that employs a scalable matrix-based visualization to show intersections of sets, their size, and other properties. Availability and implementation: UpSetR is available at https://github.com/hms-dbmi/UpSetR/ and released under the MIT License. A Shiny app is available at https://gehlenborglab.shinyapps.io/upsetr/. Contact: nils@hms.harvard.edu Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: John Hancock    1 Introduction\r\n  The visualization of sets and their intersections is a common challenge for researchers who are dealing with biological and biomedical data. For example, a researcher might need to compare multiple algorithms that identify single nucleotide polymorphisms  (Xu et al., 2012, Supplementary Fig. S1)  or show orthologs of genes in newly sequenced species across genomes of related species  (D'Hont et al., 2012, Supplementary Fig. S2) . Although many alternative set visualization techniques exist  (Alsallakh et al., 2016) , such data are typically visualized using Venn and Euler diagrams. Such diagrams can be generated with R packages such as venneuler  (Wilkinson, 2012)  and VennDiagram  (Chen and Boutros, 2011) . These closely related techniques have well known shortcomings, as they are hard to generate for more than a small number of sets. The visual representation of intersection size by irregularly shaped and unaligned areas makes it hard to answer essential questions such as 'What is the biggest intersection?' or 'Is intersection X larger than intersection Y?'  (Cleveland and McGill, 1984) .    2 Materials and methods\r\n  Here we present an R package named 'UpSetR' based on the 'UpSet' technique  (Lex et al., 2014; Lex and Gehlenborg, 2014)  that employs a matrix-based layout to show intersections of sets and their sizes. It is implemented using ggplot2  (Wickham, 2009)  and allows data analysts to easily generate generate UpSet plots for their own data. UpSetR support three input formats: (i) a table in which the rows represent elements and columns include set assignments and additional attributes; (ii) sets of elements names; and (iii) an expression describing the size of the set intersections as introduced by the venneuler package  (Wilkinson, 2012) . UpSetR provides support for the visualization of attributes associated with the elements contained in the sets, enabling researchers to explore and characterize the intersections. UpSetR differs from the original UpSet technique as it is optimized for static plots and for integration into typical bioinformatics workflows. We also provide a Shiny app that allows researchers to create publication-quality UpSet plots directly in a web browser.  UpSetR visualizes intersections of sets as a matrix in which the rows represent the sets and the columns represent their intersections (Fig. 1 and Supplementary Figs. S1 and S2 for comparisons of Venn and Euler diagrams with UpSetR plots). For each set that is part of a given intersection, a black filled circle is placed in the corresponding matrix cell. If a set is not part of the intersection, a light gray circle is shown. A vertical black line connects the topmost black circle  Site  Lung Head &amp; Neck Head &amp; Neck  Kidney  Blood Head &amp; Neck  Lung Lung freq with the bottommost black circle in each column to emphasize the column-based relationships. The size of the intersections is shown as a bar chart placed on top of the matrix so that each column lines up with exactly one bar. A second bar chart showing the size of the each set is shown to the left of the matrix.    3 Usage scenario\r\n  To illustrate the utility and features of UpSetR, we retrieved variant calls for eight cancer studies from from the ICGC Data Portal (see Supplementary Material). Each cancer study represents a set and each variant represents an element that is contained in one or more sets (Supplementary Fig. S3). UpSetR supports queries on the data to highlight features. Intersection queries can be used to select subsets of elements in the dataset defined by an intersection. Queries are assigned a unique color and their results are plotted on top of the intersection size bar chart. For example, this can be used to select elements in particular intersections (Supplementary Fig. S4). Additionally, UpSetR supports queries for the selection of elements based on attributes associated with the elements in the sets. Attributes can be numerical, Boolean or categorical. In our example, element attributes are chromosome, genomic location, and variant type (deletion, insertion, substitution) associated with each variant. UpSetR element queries select elements across intersections and sets based on particular attribute values. Basic built-in queries can be extended to arbitrarily complex queries by providing a custom query function that operates on any combination of attributes. Element queries can be used to select variants of a particular type, such as deletions, and to view them across intersections (Supplementary Fig. S5).  UpSetR provides integration of additional attribute plots that visualize attributes of elements selected by an intersection or element query. Support for scatter plots and histograms is built into UpSetR. Additional plot types can be integrated by providing in a function that returns a ggplot object to visualize the data. When attribute or intersection queries are applied, query results can also be overlaid on attribute plots in addition to the intersection size bar plot. Figure 1 demonstrates how these features, including the visualization of metadata about the sets, can be combined into a plot that among other issues, reveals a notable over-representation of unique deletions among the variants in the THCA-SA study.    4 Conclusion\r\n  UpSetR is a highly customizable tool for data exploration and generation of set visualizations. By making UpSetR compatible with the input formats of the existing popular Venn and Euler diagram packages and by offering a Shiny web interface, we incentivize the use of UpSet diagrams and enable users without programming skills to generate effective set visualizations. Through its seamless integration with ggplot2 and its ability to apply virtually any query, it is possible to customize and explore data in ways not supported by any other set visualization package. In addition, the integration of UpSetR with ggplot2 allows developers to extend UpSetR for use in their own software packages.    Acknowledgements\r\n  We acknowledge Megan Paul for her contributions and the National Institutes of Health for funding (R00HG007583, U54HG007963, U01CA198935).    ",
    "sourceCodeLink": "https://github.com/hms-dbmi/UpSetR",
    "publicationDate": "0",
    "authors": [
      "Jake R. Conway",
      "Alexander Lex",
      "Nils Gehlenborg"
    ],
    "status": "Success",
    "toolName": "UpSetR",
    "homepage": "https://cran.rstudio.com/web/packages/UpSetR"
  },
  "28.pdf": {
    "forks": 58,
    "URLs": [
      "github.com/ncbi/sra-tools",
      "github.com/lh3/seqtk",
      "www.ncbi.nlm.nih.gov/sra",
      "github.com/dohalloran/fastQ_brew",
      "github.com/dohalloran/fastQ_brew.Operating",
      "perldoc.perl.org/index-modules-A.html"
    ],
    "contactInfo": [],
    "subscribers": 38,
    "programmingLanguage": "Shell",
    "shortDescription": "SRA Tools",
    "publicationTitle": "fastQ_brew: module for analysis, preprocessing, and reformatting of FASTQ sequence data",
    "title": "fastQ_brew: module for analysis, preprocessing, and reformatting of FASTQ sequence data",
    "publicationDOI": "10.1186/s13104-017-2616-7",
    "codeSize": 28116,
    "publicationAbstract": "Background: Next generation sequencing datasets are stored as FASTQ formatted files. In order to avoid downstream artefacts, it is critical to implement a robust preprocessing protocol of the FASTQ sequence in order to determine the integrity and quality of the data. Results: Here I describe fastQ_brew which is a package that provides a suite of methods to evaluate sequence data in FASTQ format and eficiently implements a variety of manipulations to filter sequence data by size, quality and/or sequence. fastQ_brew allows for mismatch searches to adapter sequences, left and right end trimming, removal of duplicate reads, as well as reads containing non-designated bases. fastQ_brew also returns summary statistics on the unfiltered and filtered FASTQ data, and oefrs FASTQ to FASTA conversion as well as FASTQ reverse complement and DNA to RNA manipulations. Conclusions: fastQ_brew is open source and freely available to all users at the following webpage: https://github.com/dohalloran/fastQ_brew.",
    "dateUpdated": "2017-10-19T09:26:39Z",
    "institutions": ["The George Washington University"],
    "license": "https://github.com/ncbi/sra-tools/blob/master/LICENSE",
    "dateCreated": "2014-08-21T21:25:38Z",
    "numIssues": 31,
    "downloads": 0,
    "fulltext": "     O'Halloran BMC Res Notes     10.1186/s13104-017-2616-7   fastQ_brew: module for analysis, preprocessing, and reformatting of FASTQ sequence data     Damien M. O'Halloran  damienoh@gwu.edu  0  1    0  Department of Biological Sciences, The George Washington University ,  636 Ross Hall, 2300 I St. N.W., Washington, DC 20052 ,  USA    1  Institute for Neuroscience, The George Washington University ,  636 Ross Hall, 2300 I St. N.W., Washington, DC 20052 ,  USA     2017   10  2  5    8  7  2017    19  1  2017     Background: Next generation sequencing datasets are stored as FASTQ formatted files. In order to avoid downstream artefacts, it is critical to implement a robust preprocessing protocol of the FASTQ sequence in order to determine the integrity and quality of the data. Results: Here I describe fastQ_brew which is a package that provides a suite of methods to evaluate sequence data in FASTQ format and eficiently implements a variety of manipulations to filter sequence data by size, quality and/or sequence. fastQ_brew allows for mismatch searches to adapter sequences, left and right end trimming, removal of duplicate reads, as well as reads containing non-designated bases. fastQ_brew also returns summary statistics on the unfiltered and filtered FASTQ data, and oefrs FASTQ to FASTA conversion as well as FASTQ reverse complement and DNA to RNA manipulations. Conclusions: fastQ_brew is open source and freely available to all users at the following webpage: https://github.com/dohalloran/fastQ_brew.    FASTQ  NGS  Sequencing       Background\r\n  FASTQ format has become the principal protocol for the exchange of DNA sequencing files [  1 ]. The format is com posed of both a nucleotide sequence as well as an ASCII character encoded quality score for each nucleotide. Each entry is four lines, with the first line starting with a '@' character followed by an identifier. The second line is the nucleotide sequence. The third line starts with a ' +' character and optionally followed by the same sequence identifier that was used on the first line. The fourth line lists the quality scores for each nucleotide in the second line. In order to evaluate the quality of the FASTQ dataset and to avoid downstream artefacts, it is imperative for the user to employ robust quality control and preprocessing steps prior to downstream FASTQ applications. Furthermore, FASTQ has now become widely used in additional downstream applications and pipelines, and so diverse preprocessing tools are necessary to handle various FASTQ file manipulations [  2, 3 ]. Here, I describe fastQ_brew, which is a robust package that performs quality control, reformatting, filtering, and trimming of FASTQ formatted sequence datasets.    Implementation\r\n  fastQ_brew was developed using Perl and successfully tested on Microsoft Windows 7 Enterprise ver.6.1, Linux Ubuntu 64-bit ver.16.04 LTS, and Linux Mint 18.1 Serena. fastQ_brew does not rely on any dependencies that are not currently part of the Perl Core Modules (http://perldoc.perl.org/index-modules-A.html), which makes fastQ_brew very straight forward to implement. fastQ_ brew is composed of two separate packages: fastQ_brew. pm and fastQ_brew_Utilities.pm. fastQ_brew_Utilities. pm provides fastQ_brew.pm with access to various subroutines that are called to handle FASTQ manipulations and quality control. The fastQ_brew object is instantiated by calling the constructor subroutine called \u201cnew\u201d which creates a 'blessed' object that begins gathering methods and properties by calling the load_fastQ_brew method. Once the object has been populated, the user can call run_fastQ_brew to begin processing the FASTQ data. Sample data are provided at the GitHub repo and directions for usage are described in the README.md file.  ehT command-line arguments supplied to the fastQ_ brew object are as follows: (1) -lib, which can be either sanger or illumina; (2) -path, speciefis the path to the input life (can use \u201c./\u201d for current directory with UNIX or \u201c.\\\u201d on Windows cmd); (3) -i, this is the name of the lfie contain ing the FASTQ reads; (4) -smry, return summary statistics table on the unlfitered data and lfitered data; (5) -qf , this option will lfiter reads by Phred (also called Q score) qual ity score-any reads having an average Phred score below the threshold will be removed: e.g. -qf  =  20 will remove reads with Phred scores below 20; (6) -lf, this will lfiter reads below a speciefid length; (7) -trim_l, will trim the speciefid number of bases from the left end of each read; (8) -trim_r, same as left-trim except that here the reads will be trimmed from the right side; (9) -adpt_l, will remove a speciefid adapter sequence from the left end of a read; (10) -adpt_r, same as -adpt_l except that here the reads will be trimmed from the right side; (11) -mis_l, allows for a speciefid number of mismatches between the user pro vided -adpt_l sequence and each read e.g. a mismatch = 1, would match a hypothetical 3 base adapter, TAG, to the left end of a sequence that started with TAG or AAG or TAA or any of the nine possibilities; (12) -mis_r, same as -mis_l except that this relates to the adpt_r sequence supplied by the user; (13) -dup, removes duplicate reads; (14) -no_n, removes reads that contain non-designated bases i.e. bases that are not A, G, C or T e.g. N; (15) -fasta, this option will convert the FASTQ lfie to FASTA format; (16) -rev_comp, will reverse complement reads in the supplied FASTQ lfie; (17) -rna, will convert each read to the corre sponding RNA sequence in the supplied FASTQ lfie; (18) -clean, option to delete temporary lfies created during the run. If the summary option is selected, fastQ_brew will return a results table to STDOUT with summary statistics of the FASTQ data lfie prior to lfitering and after lfitering. ehT summary report will provide a table detailing max, min, and average GC% values for all reads; max, min, and average read lengths, max, min, and average Phred scores, and max, min, and average error probabilities. eTh Phred score (denoted as Q) represents the probability of an error for each base, and is logarithmically related to the basecalling error probability, P such that:  Q = −10 log10 P or  In the case of arguments 15-17 above, a new file will be generated in each case, whereas for all other options the user-supplied arguments will be chained together to return a single filtered file.    Results\r\n  Testing of fastQ_brew was performed by plotting runtime against file size (Fig.  1a). FASTQ formatted sequence data from 110 MB (462,664 reads) to 4.5 GB (24,159,698 reads) in size were used to benchmark the runtime of fastQ_brew. In each case, fastQ_brew eficiently returned summary statistics from each file in 36  s for 110  MB FASTQ file to 25  min and 33  s for 4.5  GB. The runtime will scale with the number of methods called within fastQ_brew.  To evaluate more specicfi methods within fastQ_brew, the relationship between nucleotide position within a given read and the corresponding Phred quality score was determined (Fig.  1b). iThs method tested the trim ming and Phred calculation methods within fastQ_brew. ehT Phred quality score is used as a metric to determine the quality of a given nucleotide's identicfiation within a read [ [ 4]. Phred quality scores are related (logarithmically) to the base-calling error probabilities [ [ 5] (see equation above). eTh average Phred quality scores for a randomly chosen FASTQ data lfie after left-side trimming (-trim_l) method invocations within fastQ_brew from position 1-20 were plotted (Fig.  1b). eThre was a negative corre lation between increasing nucleotide position and Phred quality score (R2  =  −0.99969), that is, bases closer to the beginning of each read exhibit higher Phred quality scores, as compared with nucleotides closer to the middle of the read. iThs observation is in keeping with previous observations on Phred quality across reads [ [6- 8] (http://www.bioinformatics.babraham.ac.uk/projects/fastqc/). ehT data set used in this test was comprised of 462,664 reads with an average read length of 99 bases. eTh small est read length was 25 bases and the largest was 100 bases.  To further examine the quality filtering method of fastQ_brew, FASTQ data were downloaded from the NCBI sequence read archive (SRA-https://www.ncbi.nlm.nih.gov/sra) using the sra-toolkit (https://github.com/ncbi/sra-tools). Distribution of read quality was plotted prior to filtering (blue bars) and after filtering (red bars) using fastQ_brew revealing a shift in Phred scores towards increased quality after filtering (Fig. 1c).  Finally, to compare fastQ_brew to other FASTQ filter ing tools, I examined the execution time for some of the most commonly used filtering tools in trimming FASTQ data, and compared their execution speeds to that of 5,000,000 10,000,000 15,000,000 20,000,000 25,000,000 30,000,000  Number of Reads 5  10 Nucleotide Position 15 20 b 40 e r co30 S y it la20 u Q d re10 h P 0 d s d a e 6r0 1 r e p ) s c e s ( e m it g n i m m i r T 0 fastQ_brew. For all analyses, the same FASTQ file was used, and in each case methods were invoked to trim 8 bases from the left and right sides of every read in the ifle. The following software were used: fastq_brew ver 1.0.2; Trimmomatic ver 0.36 [ 6  [9]; NGSQCToolkit ver 2.3.3 [ 3  [6]; Prinseq ver 0.20.4 [ 4  [10]; seqtk (https://github.com/lh3/seqtk); Fastxtoolkit ver 0.0.13 (http://hannonlab.cshl.edu/fastx_toolkit/index.html); BBDuk ver 37.22 (http://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/ bbmap-guide/); ngsShoRT ver 2.2 [11] 11 ]; and Cutadapt ver 1.9.1 (http://journal.embnet.org/index.php/embnetjournal/article/view/200). For some other software tools, this exact invocation was not possible due to limitations on the trimming method. The data from this analysis is pre sented in Fig.  1d. fastQ_brew compares well with other commonly employed filtering tools. The fastest tool was BBDuk which finished trimming all reads in only 1.532 s, and this was followed very closely by seqtk which completed the task in 1.99  s. By examining across these tools we can obtain some insight into how the execution speeds for fastQ_brew compares with commonly used trimming software. However, it is important to point out that each tool oefrs many specific adaptations and fea tures that are not reflected in a basic trimming task, and while speed is important when dealing with very large data-sets, other features that include accessibility, documentation, ease of use, as well as applicability of options are equally important.  In summary, I here describe fastQ_brew, a very lightweight Perl package for robust analysis, preprocessing, and manipulation of FASTQ sequence data files. hTe main advantage of fastQ_brew is its ease of use, as the software does not rely on any modules that are not currently contained within the Perl Core. fastQ_brew is freely available on GitHub at: https://github.com/dohalloran/fastQ_brew.   Acknowledgements\r\n  I thank members of the O'Halloran lab for critical reading of the manuscript.    Competing interests\r\n  The author declares no competing interests.    Availability of data and materials\r\n  Project name: fastQ_brew.  Project home page: https://github.com/dohalloran/fastQ_brew.Operating system(s): Platform independent.  Programming language: Perl.  Other requirements: none.  License: GNU.  Any restrictions to use by non-academics: no restrictions or login requirements.    Funding\r\n  The George Washington University (GWU) Columbian College of Arts and Sciences, GWU Ofice of the Vice-President for Research, and the GWU Depart ment of Biological Sciences.     Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afiliations.    ",
    "sourceCodeLink": "https://github.com/ncbi/sra-tools",
    "publicationDate": "0",
    "authors": [],
    "status": "Success",
    "toolName": "sra-tools",
    "homepage": ""
  },
  "6.pdf": {
    "forks": 0,
    "URLs": [
      "github.com/raunaq-m/MultiRes.©",
      "github.com/raunaq-m/MultiRes",
      "github.com/nh13/DWGSIM"
    ],
    "contactInfo": ["raunaq.123@gmail.com"],
    "subscribers": 3,
    "programmingLanguage": "C++",
    "shortDescription": "Frame Based Error Correction Algorithm Viral Populations",
    "publicationTitle": "A random forest classifier for detecting rare variants in NGS data from viral populations",
    "title": "A random forest classifier for detecting rare variants in NGS data from viral populations",
    "publicationDOI": "10.1016/j.csbj.2017.07.001",
    "codeSize": 9713,
    "publicationAbstract": "A B S T R A C T We propose a random forest classifier for detecting rare variants from sequencing errors in Next Generation Sequencing (NGS) data from viral populations. The method utilizes counts of varying length of k-mers from the reads of a viral population to train a Random forest classifier, called MultiRes, that classifies k-mers as erroneous or rare variants. Our algorithm is rooted in concepts from signal processing and uses a framebased representation of k-mers. Frames are sets of non-orthogonal basis functions that were traditionally used in signal processing for noise removal. We define discrete spatial signals for genomes and sequenced reads, and show that k-mers of a given size constitute a frame. We evaluate MultiRes on simulated and real viral population datasets, which consist of many low frequency variants, and compare it to the error detection methods used in correction tools known in the literature. MultiRes has 4 to 500 times less false positives k-mer predictions compared to other methods, essential for accurate estimation of viral population diversity and their de-novo assembly. It has high recall of the true kmers, comparable to other error correction methods. MultiRes also has greater than 95% recall for detecting single nucleotide polymorphisms (SNPs) and fewer false positive SNPs, while detecting higher number of rare variants compared to other variant calling methods for viral populations. The software is available freely from the GitHub link https://github.com/raunaq-m/MultiRes.© 2017 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).",
    "dateUpdated": "2017-05-31T17:01:47Z",
    "institutions": [
      "The Pennsylvania State University",
      "Indiana University"
    ],
    "license": "GNU General Public License v2.0",
    "dateCreated": "2016-01-15T19:47:36Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Computational and Structural Biotechnology Journal     10.1016/j.csbj.2017.07.001   A random forest classifier for detecting rare variants in NGS data from viral populations     Raunaq Malhotra  raunaq.123@gmail.com  2    Manjari Jha  2    Mary Poss  0    Raj Acharya  1    0  Department of Biology, The Pennsylvania State University ,  University Park, PA 16802 ,  USA    1  School of Informatics and Computing, Indiana University ,  Bloomington, IN 47405 ,  USA    2  The School of Electrical Engineering and Computer Science, The Pennsylvania State University ,  University Park, PA, 16802 ,  USA     2017   15  2017  388  395    3  7  2017    14  3  2017    1  7  2017     A B S T R A C T We propose a random forest classifier for detecting rare variants from sequencing errors in Next Generation Sequencing (NGS) data from viral populations. The method utilizes counts of varying length of k-mers from the reads of a viral population to train a Random forest classifier, called MultiRes, that classifies k-mers as erroneous or rare variants. Our algorithm is rooted in concepts from signal processing and uses a framebased representation of k-mers. Frames are sets of non-orthogonal basis functions that were traditionally used in signal processing for noise removal. We define discrete spatial signals for genomes and sequenced reads, and show that k-mers of a given size constitute a frame. We evaluate MultiRes on simulated and real viral population datasets, which consist of many low frequency variants, and compare it to the error detection methods used in correction tools known in the literature. MultiRes has 4 to 500 times less false positives k-mer predictions compared to other methods, essential for accurate estimation of viral population diversity and their de-novo assembly. It has high recall of the true kmers, comparable to other error correction methods. MultiRes also has greater than 95% recall for detecting single nucleotide polymorphisms (SNPs) and fewer false positive SNPs, while detecting higher number of rare variants compared to other variant calling methods for viral populations. The software is available freely from the GitHub link https://github.com/raunaq-m/MultiRes.© 2017 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).    Sequencing error detection  Reference free methods  Next-generation sequencing  Viral populations  Multi-resolution frames  Random forest classifier       -\r\n  A R T I C L E I N F O    1. Introduction\r\n  The sequence diversity present in a population of closely related genomes is important for their survival under environmental pressures. Viral population within a host is an example of such population of closely related genomes, where some viral strains survive even when large segments of their genome are deleted. The sequence variants that occur at low frequency in the population, also known as rare variants, have been known to impact the population's survival and understanding their prevalence is important for drug design and in therapeutics [ 1 ].  However, detection of rare variants from Next Generation Sequencing (NGS) data is still a challenge as the rare variants are tangled with errors in sequencing technologies due to their similar prevalence [ 2,3 ]. The NGS data technologies are error prone and even though their error profiles are well studied [ 4,5 ], removing sequencing errors is essential before downstream processing of NGS data such as assembly of haplotypes in viral populations [ 2,6-9 ] and variant calling for viral populations [ 7,9,10 ].  In order to remove sequencing errors from NGS data, the first step is detecting the errors from true biological sequences and then correcting the errors to the true sequence. For NGS data obtained from a viral population, the reads are mapped to a reference genome to detect true variants from sequencing errors based on a probabilistic model [ 6,7,9-11 ], and then the sequencing errors are corrected to the sequence of the reference genome. However, as virus population contains a large diversity of true sequences, accurate mapping of reads to any one reference may not be possible.  Alternatively, sampled reads are broken into small fixed length sub-strings called k-mers and their counts are used for error detection (e.g. [ 12-16 ]). The erroneous k-mers are corrected by changing minimum number of bases in the reads using the detected true k-mers. These methods use a generative model for k-mer counts to determine if an observed k-mer is erroneous or a true k-mer [ 12 ] based on a counts threshold [ 12-14,17 ].  For k-mer based error detection, the length of the k-mer and the frequency threshold are important parameters. The size of a k-mer can effect the performance of error detection method, as it either decreases the evidence for a segment of the genome for a large k, or combines evidences from multiple segments for a small k. However, a single appropriate k-mer size for error detection in viral populations is restrictive in nature, as a combination of different sized overlapping k-mers, although redundant, can provide richer information.  The error detection part in most k-mer based error correction tools [ 12-15 ] has been designed assuming the reads are sampled from a single diploid genome and rely on a single counts threshold. However, for viral populations a single threshold is not suitable as viral strains occur at different relative frequencies. Currently, a number of time and memory efficient k-mer counting algorithms are available [ 18,19 ]. Thus, choosing an appropriate size of k-mer is possible by performing k-mer counts at multiple sizes [ 20 ].  With the availability of large amounts of data from NGS technologies, data driven classifiers have also been used for detection of sequencing errors [ 21 ] and for variant calling [ 5,12,22,23 ]. However, identifying the features for classification of rare variants and sequencing errors is still a challenge, due to their similar characteristics in the NGS data.  We propose, MultiRes, a reference-free k-mer based error detection algorithm for a viral population. The algorithm uses k-mer counts of different sizes to train a Random Forest Classifier that classifies k-mers as erroneous or rare variant k-mers. We also propose a mechanism for selecting the optimal combination of k-mer sizes. The rare variant k-mers along with high frequency k-mers can be used as is in downstream tools for variant calling and for de novo assembly of viral populations.  MultiRes uses a collection of sizes of k-mers as features for detecting sequencing errors and rare variants. Our rationale to choose a combination of sizes for k-mers is rooted in signals processing, where analysis of signals at different resolutions has been used for noise removal [ 24 ]. Signals are projected onto a series of non-orthonormal basis functions, known as a frame, [ 25-27 ]; these projections are used for error removal and signal recovery [ 28,29 ].  The classifier in MultiRes is trained on a simulated dataset that models NGS data generated from a replicating viral population. We evaluate the performance of MultiRes on simulated and real datasets, and compare it to the error detection algorithms of error correction tools BLESS [ 13 ], Quake [ 12 ], BFC [ 14 ], and Musket [ 15 ]. We also compare our results to BayesHammer [ 30 ] and Seecer [ 31 ], which can handle variable sequencing coverage across the genome and polymorphisms in the RNA sequencing data respectively.  MultiRes has a high recall of the true k-mers, comparable to other methods and has 5 to 500 times better removal of erroneous k-mers compared to other methods. Our results demonstrate that the classifier in MultiRes performs well for error detection on real sequencing data obtained from the same sequencing technology. Thus, the classifier in MultiRes is generalizable to viral population data from the same sequencing technology.  As MultiRes detects the rare variant k-mers in an NGS data, its output can be directly used for identifying rare variants in a viral population. Variant calling for viral populations typically relies on a single reference genome or on a consensus genome generated from the population being studied [ 7,9,10 ]. We compare the rare variants detected by MultiRes to variant calling methods VPhaser-2 [ 9 ], LoFreq [ 10 ] and the outputs from haplotype reconstruction method ShoRAH [ 7 ]. MultiRes has the higher recall of true SNPs compared to the SNPs called by VPhaser-2, LoFreq and ShoRAH on both simulated and real datasets, and misses the least number of true SNPs amongst all methods. This demonstrates its applicability for rare variant detection in viral populations.    2. Methods\r\n  MultiRes is a classifier for detecting sequencing errors from rare variants. The counts of the k-mers along with the counts of their sub-sequences (sub k-mers within a k-mer) are used as features for training a classifier. The true k-mers observed in the viral haplotypes with counts in the reads less than a threshold THigh are defined to be rare variant k-mers, while the rest of k-mers with counts less than THigh are erroneous k-mers. The k-mers that occur at counts greater than THigh are known as common k-mers, as they occur frequently in the viral haplotypes. The common k-mers are assumed to be error-free and the classifier is trained only for the erroneous and rare variant k-mers.  The premise of our method is that reads sequenced from a population of genomes can be modeled as discrete spatial signals. Discrete spatial signals can be projected on to a frame [ 25-27,32 ] for their representation (See Supplementary Material for details), where the coefficients of projections characterize the discrete spatial signals. Similarly, we show that k-mers (of a given size k) form a frame and the maximal projection of k-mers correspond to their counts in a sequencing run. Additionally, a k-mer can be projected on to a collection of frames, where each frame represents counts of k -mers (k &lt; k) that are sub-strings of the given k-mer.  The choice of k for a frame is important and should be large enough such that a k-mer only occurs once in the haplotypes. On the other hand, it should be smaller than the read lengths so that k-mer counting is still meaningful.  The minimum k can be approximated by ensuring that the probability of picking a string of length equal to the genome length (say |H|) where all k-mers in it occur only once is low [ 12 ]. Thus the probability of picking approximately |H| unique k-mers out of a set of 4k/2 (considering reverse complements) should be low. We set 2 \u2022 |H|/4k ≈ 4, where 4 is a small number, to determine the smallest possible choice of k (kmin) for the frame.  As an example, a k-mer u occurring c(u) times in the reads when projected on a frame of size k is in-fact represented by its maximal projection c(u). The same k-mer u, can also be represented on frames of sizes (k , k in the range [kmin, k]. Now the maximal projections for u in these frames are the counts of k -mers and k mers present within u. This representation of k-mer u can be used to train a classifier for identifying erroneous versus rare variant k-mers.   2.1. MultiRes: Classification algorithm for detecting sequencing errors and rare variants\r\n  We define a classifier, EC, for classifying a k-mer as erroneous, a rare variant, or a common k-mer in the dataset. Algorithm 1 describes MultiRes, the proposed algorithm for detecting rare variants and sequencing errors. The algorithm takes as input the sampled reads, the classifier EC, an ordered array (k, k , k ), and a threshold parameter THigh. It outputs for every k-mer observed in the sampled reads a status: whether the k-mer is erroneous or a rare variant.  It first computes the counts of k-mers, k -mers, and k -mers using the dsk k-mer counting software [ 18 ]. The k-mers u that have counts greater than THigh are marked as true k-mers while the rest of the kmers are classified using the classifier EC based on their counts on k -mers, and k -mers.  The classifier EC captures the profile of erroneous versus rare variant k-mers from Illumina sequencing of viral populations. We used the software dsk (version 1.6066) [ 18 ] for k-mer counting, which can perform the k-mer counts in a limited memory and disk space machine quickly. The run time of MultiRes is linearly dependent on the number of unique k-mers in a dataset, as once the classifier EC is trained, it can be used for all datasets, and it can be easily parallelized.   Algorithm 1. MultiRes: Error detection in the sampled reads by frame-based classification of k-mers\r\n     2.1.1. Simulated data for classifier training\r\n  MultiRes assumes the availability of a classifier EC which can distinguish between the erroneous and rare variant k-mers. We use simulated datasets to train a series of classifiers and set EC to the classifier which has the highest accuracy. The simulated viral population consists of 11 haplotypes and is generated by mutating 10% of positions on a known HIV-1 reference sequence of length 9.18 kb (NC_001802). These mutations also model the evolution of a viral population under a high mutation rate. The mutations introduced are randomly and uniformly distributed across the length of the genome so that the classifier is not biased towards the distribution of true variants. This introduces a total of 195,000 ground truth unique 35-mers in the simulated HIV-1 dataset.  We next simulate Illumina paired-end sequencing reads using the software dwgsim (https://github.com/nh13/DWGSIM) at 400x sequencing coverage from this viral population. The status of each k-mer in this dataset is known as being erroneous, rare variant or a common k-mer. We use close to 100,000 k-mers in the training dataset. Thus, there is a test dataset of k-mers left for evaluating the efficacy of the classifiers.  In order to train a classifier, we need to choose the size of the k-mer, the sizes of k -mers for computing the projections of k-mer signals, and the number of such projections needed. The choice of the smallest of {k, k , . . .} should be above the minimum length kmin to ensure that each k-mer still corresponds to a unique location on a viral genome.  For HIV populations, with genome length 9180 base pairs (9.1 kbp) and taking 4 = 0.001 (a small value, as mentioned before), the minimum length of k-mer is kmin = log42 \u2022 G/4 = 12.06 = 13. As in signal processing domain, we choose k ≈ 3 \u2022 kmin = 35 (an integral multiple of kmin) as the largest k-mer, and consider its projections on frames of sizes ranging from 13 to 35.  MultiRes assumes that k-mers above the threshold count Thigh are error-free, and only classifies the k-mers with counts less than Thigh. The choice of Thigh should ensure that the probability of erroneous k-mers with counts above Thigh is negligible. We use the gamma distribution model mentioned in the Quake error correction paper [ 12 ] for modeling erroneous k-mers, as it approximates the observed distribution of errors. Based on this gamma distribution, we set Thigh = 30 for the simulated HIV population data. The classifiers are therefore trained on 35-mers with counts less than 30.  Three training datasets consisting of both erroneous and rare variant 35-mers are generated. The features in the three datasets are the projection of the 35-mers onto (i) the frame of size 23, (ii) the frame of size 13, and (iii) a combination of both frames (Fig. 1 a). The features in the three settings translate to the counts of the 13-mers and 23-mers observed within the 35-mer along with the counts of the 35-mer. We observed 11.9 million unique 35-mers in the simulated HIV-1 population, from which features from 76,000 erroneous 35-mers and 32,000 true variant 35-mers distributed uniformly over counts 1-30 were used for training the classifiers.    2.1.2. Classifier selection\r\n  Classifiers Nearest Neighbor, Decision Tree, Random Forest, Adaboost, Naive Bayes, Linear Discriminant Analysis (LDA), and Quadratic Discriminant Analysis (QDA) are trained on the three training datasets and evaluated based on their test data accuracy over a 5-fold cross validation dataset. The classifiers are implemented in the scikit-learn library (version is 0.16.1) in python programming language (version 2.7.6). For all the classifiers, the accuracy improves as the 35-mers are projected onto 13-mers rather than 23-mers (higher resolution, lower size of k -mers), and improves even further when 35-mers are resolved onto both 13-mers and 23-mers (Fig. 1). No further feature selection was performed when 35-mers were resolved onto 13-mers and 23-mers. The Random Forest Classifier performs the best on all three datasets, where the accuracy for dataset (iii) is 98.12%. The accuracy for Naive Bayes and QDA classifiers are lower for all datasets, and also decreases when the projections in both 13-mers and 23-mers are considered, indicating that inadequacy of their models for the classification of 35-mers in these projections. The performance of other classifiers are comparable and follows similar trends.    2.1.3. Exploring additional feature spaces\r\n  Additionally, we generate a series of 4 projections of the 35-mers onto frames of sizes a) 15, b) {15 + 20}, c) {15 + 20 + 25}, and d) {15+20+25+30} to evaluate the effect of number of frames used for projection on the performance (Fig. 1 b). Increasing the number of projections has no visible effect on increasing the accuracy of performance, although it increases the memory requirements and time complexity for computing counts of all five different values of k. Based on this, we chose the Random Forest classifier with a resolution of 35-mers decomposed into a combination of 13-mers and 23-mers for other simulated and real datasets.     3. Results\r\n   3.1. Error detection for reconstruction of haplotypes\r\n  We evaluate MultiRes on simulated HIV and HCV datasets and a laboratory mixture of HIV-1 strains. MultiRes is compared to the detection algorithms in the error correction tools Quake (last checked version Feb 2012) [ 12 ], BLESS (version 0.15) [ 13 ], Musket (last downloaded October 2015) [ 15 ], BFC (last downloaded October 2015) [ 14 ], BayesHammer (version 3.6.2) [ 30 ] and Seecer (version 0.1.3) [ 31 ]. As these tools are traditionally designed for error correction, the error corrected reads or k-mers from these methods were used for comparison with the rare variant k-mers and common k-mers predicted by MultiRes. ShoRAH [ 7 ] reconstructs a set of haplotypes as its final output rather than error corrected reads 1 and thus was not evaluated for error correction. The error corrected reads, although available as an intermediate output, are not reported due to their low precision numbers, but ShoRAH is used for single nucleotide variant calling and comparison later in the text. Other recent error correction methods available for viral populations such as PredictHaplo http://bmda.cs.unibas.ch/software.html, HaploClique [ 11 ], and Viral Genome Assembler (VGA) [ 8 ] were not evaluated in this study.  Three measures, defined in terms of the true and erroneous k-mers, are used for comparing the detection algorithms in all methods. Precision is defined as the ratio of the known true k-mers identified to the total number of k-mers predicted as true variants by an algorithm. Recall is defined as the ratio of the true variant k-mers identified to the total number of true k-mers by an algorithm and measures the goodness of a method to retain true k-mers for a dataset. False Positives to True Positives Ratio (FP/TP ratio) is the ratio of the erroneous k-mers predicted as true variants to the true variant k-mers identified by the algorithm. FP/TP ratio measures the number of erroneous k-mers identified by an algorithm to detect a single true variant k-mer and is a measure of the overall volume of k-mers predicted by an algorithm.    3.1.1. HIV simulated datasets\r\n  We first assess the performance of MultiRes on the reads simulated from the HIV-1 population containing 11 haplotypes, generated from a single HIV-1 reference sequence (NC_001802) as mentioned before. Two datasets are generated from the simulated reads: one with average haplotype coverage of 100x (denoted as HIV 100x), and second where the average coverage is 400x (denoted as HIV 400x) as increasing sequencing depth increases the absolute number of erroneous k-mers introduced in the data.  The recall of MultiRes is 95% and 98% on HIV 100x and HIV 400x datasets, respectively, indicating that performance of MultiRes improves with increasing sequencing depth as expected. The recall numbers are comparable to around 98% recall of other methods on the HIV 100x dataset and 94% to 99% for HIV 400x dataset (Table 1).  The precision of MultiRes is 89% in the HIV 100x while all other methods have low precisions for HIV 100x. While precision in all other methods is less than 5% for HIV 400x dataset, the precision of MultiRes is 95%, suggesting that precision decreases for other methods with increasing sequencing depth. As higher depth samples also have higher sequencing errors, the detection algorithms in these methods are not able to differentiate between rare variants and sequencing errors. Seecer and BayesHammer, methods which can handle variability in sequencing coverage, also have very low precision values compared to the proposed method.  The FP/TP ratio obtained by MultiRes are 4 to 500 times better than other methods and the number of k-mers retained is close to the true set of k-mers in the two datasets (FP/TP ratio is close to zero &amp; recall close to 95-98 %).Thus, while all methods retain the true k-mers to the same extent, only MultiRes reduces the number of false positive k-mers. This is important as the memory requirements for de novo assembly tools linearly increases based on the number of k-mers. Thus the k-mers predicted by MultiRes would have a 500 times reduction in memory consumption for downstream de novo assembly tools as compared to current error correction methods. The False positive/True Positive ratios (FP/TP ratios), Recall, and Precision are compared on two HIV datasets for the methods: Quake, BLESS, Musket, BFC, BayesHammer, Seecer, and the proposed method MultiRes. The error corrected reads from each method are broken into k-mers and compared to the true k-mers in the HIV-1 viral populations. Uncorrected denotes the statistics when no error correction is performed. Bold in each column indicates the best method for the dataset and the metric evaluated.    3.1.2. Generalizability: Testing MultiRes on a Hepatitis C virus dataset\r\n  We also evaluate our method on reads simulated from viral populations consisting of the E1/E2 gene of Hepatitis C virus (HCV). The purpose of using HCV strains is to understand the generalization of the MultiRes classifier on other viral population datasets. Two HCV populations observed in patients in previous studies are used as simulated viral populations. The first, denoted as HCV 1, consists of 36 HCV strains from E1/E2 region and are of length 1672 bps [ 33 ]. The second, denoted as HCV 2, consists of 44 HCV strains from the E1/E2 regions of the HCV genome with lengths 1734 bps [ 8,17 ]. We simulate 500 K Illumina paired end reads from both datasets under a power law (with ratio 2) of reads distribution amongst the strains [ 34 ]. The two simulated datasets are denoted as HCV1P and HCV2P respectively. The power law distribution of reads also helps in evaluating the performance of MultiRes when more than 50% of the haplotypes are present at less than 5% relative abundances.  All methods have recall greater than 90% on both datasets (Table 2). Again, the difference between MultiRes and other methods is evident from the FP/TP ratios and precision. The false positive to true positive ratios for MultiRes are less than other methods at least by a factor of 5 (Table 2). MultiRes still outperforms all other methods on predicting the smallest set of predicted k-mers while maintaining high recall levels of true k-mers.  The recall for MultiRes is respectively 96% and 94% on HCV1P and HCV2P datasets, which is less than the method Seecer that has recall values around 99%. Seecer marks more than 90% of the observed k-mers as true, which explains the high recall values. However, this also leads to a large number of false positive k-mers being predicted as true k-mers in Seecer, leading to low precision values. All other methods also achieve high recall by retention of all large fraction of observed k-mers, as indicated by their precision values being less than 1% and false positive to true positive ratios being greater than 100.  The similar performance of MultiRes on a dataset, such as the HCV population, which is diverse in genome composition from the simulated HIV-1 sequences used in simulation indicates the generalizability of the Random Forest Classifier in MultiRes. The classifier is capturing properties of the Illumina sequencing platform and the fact that both datasets contain a large number of rare variants occurring at k-mer counts close to the sequencing errors. Thus, MultiRes can be used as it is for error and rare variant detection in diverse datasets.  As the performance of MultiRes on HCV population is not as impressive as on the HIV simulated populations, it is also important to understand the cause for this decrease in performance. It is possible that the decrease in performance is correlated to the large number of low-frequency variants that are being misclassified by MultiRes. In order to test this, we investigate MultiRes' classification as a function of the count of the 35-mer which is being classified. MultiRes predicts about one-fourth of the observed k-mers as rare The false positive to true positive ratios, Recall, and Precision of error correction methods on the two simulated HCV datasets are shown. Uncorrected refers to the statistics when no error correction is performed. Bold font in each column indicates the best method for each dataset on the evaluated measure. 15 20 25 30 Multiplicity of 35−mers (b) True rare Variants Multiplicity plots Fig. 2. Performance of MultiRes on HCV datasets under power law distributions of viral haplotypes with respect to count of k-mer. 35-mer multiplicity plots for HCV1P and HCV2P datasets are shown. x-axis indicates the number of times a 35-mer was observed while y-axis indicates the number of 35-mers at a count. (a) The predicted true 35-mers from MultiRes (HCV1P red, HCV2P pink) compared to the uncorrected data (HCV1P blue,HCV2P green), and (b) The true positive rare variants 35-mers from MultiRes (HCV1P red, HCV2P pink) versus the ground truth 35-mers (HCV1P red, HCV2P pink). MultiRes predicts rare variants k-mers at all counts greater than 3, with its accuracy improving as counts of k-mer increases. variants for k-mer counts less than 15, and predicts more than 99% all of the observed k-mers as true for counts greater 20 (Fig. 2 (a)). This suggests that MultiRes predicts rare variant k-mers for all observed counts and detects more rare variant k-mers than a method based on a single threshold.  Most of the k-mer based error correction methods use a single threshold over the k-mer counts, which will clearly lose true rare variant k-mers (Fig. 2 (b)). On the other hand, MultiRes has a recall of 50% for k-mers observed 3 times, while still correctly identifying more than 75% of the k-mers as erroneous. The recall of MultiRes increases to 100% as the counts of the observed k-mers increases to 35. This indicates the importance of not having a single threshold for distinguishing between sequencing errors and rare variants in viral HCV 1 power uncorrected HCV 1 power MultiRes HCV 2 power uncorrected  HCV 2 power MultiRes 5 10 15 20 25 30 Multiplicity of 35−mers 35 40 45 (a) Total k-mer Multiplicity Plots  HCV 1 rare variantes original HCV 1 rare variants MultiRes HCV 2 rare variants original HCV 2 rare variants MultiRes population datasets, and our MultiRes bypasses a single threshold by training a Random Forest classifier.    3.1.3. Evaluation on population of 5 HIV-1 sequences\r\n  We also evaluate MultiRes on a laboratory mixture of five known HIV-1 strains [ 35 ], which captures the variability occurring during sample preparation, errors introduced in a real sequencing project, and mutations occurring during reverse transcription of RNA samples. Five HIV-1 strains (named YU2, HXB2, NL43, 89.6, and JRCSF) of lengths 9.1 kb were pooled and sequenced using Illumina paired end sequencing technology (Refer to [ 35 ] for details). Each HIV strain was also sequenced separately in their study and aligned to their known reference sequence (from Genbank) to generate a consensus sequence for each HIV-1 strain [ 35 ]. This provides us with a dataset of actual sequence reads where the ground truth is known allowing us to assess the performance of MultiRes and other methods. We extracted 35-mers from the paired end sequencing data and classify them using the Random Forest classifier of MultiRes trained on the simulated HIV sequencing data.  All the error correction methods and MultiRes have recall values around 97%, indicating that the performance for recovery of true k-mers is comparable across all methods (Table 3). The false positive to true positive ratio for MultiRes is 13 while all other methods have ratios more than 120. MultiRes predicts 359 thousand unique k-mers in the set of true k-mers while all other methods predict more than 5 million unique k-mers. Even methods that take variance in sequencing depths while performing error correction, such as BayesHammer and Seecer, predict 11.3 million and 6.3 million unique k-mers which is two orders more than the ground truth number of k-mers in the consensus sequence of the 5 HIV-1 strains (53 thousand unique k-mers). Thus, even considering the artifacts introduced in sequencing, MultiRes has by far the most compact set of predicted error free k-mers amongst all methods while retaining high number of true k-mers. As mentioned earlier, as the number of k-mers linearly affects the memory requirements for downstream de novo assembly methods, the error detection from MultiRes would translate to a 10-fold reduction in memory.    3.1.4. Runtime and memory\r\n  MultiRes has comparable running times to BayesHammer on the five-viral mix dataset (Fig. 3) on a Dell system with 8 GB main memory, and 2X Dual Core AMD Opteron 2216 CPU type. The performance on all other datasets was similar indicating that the timings are comparable. Additionally, while other methods have parallel implementations, the error detection classifier step in MultiRes is a single thread serial implementation. As the random forest classifier used by MultiRes is already trained and independent of the input k-mers for classification, the runtime of MultiRes can be significantly improved via parallelization of the k-mer classification step.    3.2. Comparison of MultiRes to variant calling methods for viral populations\r\n  As one of the objectives in NGS studies of viruses is to identify the single nucleotide polymorphisms (SNPs) in a population [ 2,7,9 ] which is sensitive to erroneous reads, we evaluate the inference of SNPs from the k-mers predicted by MultiRes, and compare it to known SNP profiling methods for viral populations. We first align the predicted k-mers from MultiRes to a reference sequence of the viral population using the bwa mem aligner and a base is called as a SNP when its relative fraction amongst the k-mers aligned at that position is greater than 0.01. All the variants that occur at a frequency greater than the error threshold at that position are reported as SNPs.  The choice of the reference sequence is based on the viral population data being evaluated, and the same reference sequence is used for calling true SNPs and the SNPs predicted by a method.  Each SNP detected at a base position of the reference and detection of the reference base itself are treated as true positives for a method; thus the number of true positives can be greater than the length of the reference sequence. All the SNPs predicted by a method and the number of bases mapped to the reference sequence are known as the total SNP predictions of a method. We use three measures for evaluating the SNPs called by any method. Precision is defined as the ratio of the number of true positives to the total SNP predictions made by a method, while recall is defined as the ratio of the true positives to the total number of SNPs and reference bases in the viral population. Finally, false positive to true positive ratio is a ratio of the number of false SNP predictions to the number of true positives detected by a method.  We compare our results to state-of-the-art variant calling methods for viral populations VPhaser-2 [ 9 ], a rare variant calling method LoFreq [ 10 ], and viral haplotype reconstruction algorithm ShoRAH [ 7 ] using the above three measures. The reference sequence used by variant calling methods VPhaser-2 and LoFreq is the same as that used by samtools to determine the true SNPs, while the SNPs predicted by ShoRAH at default parameters are compared directly to the true SNPs. We only used the SNP calls from VPhaser-2 for evaluation, as length polymorphisms are not generated by the other methods, but the results from VPhaser-2 were not penalized when comparing the SNPs.  We report results for LoFreq [ 10 ], VPhaser [ 9 ], ShoRAH [ 7 ] and our method MultiRes on all datasets (Table 4). Overall, MultiRes has greater than 94% recall and precision values greater than 83% in The recall, precision, and FP/TP ratios of each method are evaluated on the 5-viral mix HIV-1 dataset. The number of unique 35-mers indicates the number of unique 35-mers predicted by a method. There are 53 thousand true unique 35-mers in the consensus sequences of the 5 viral strains. Bold indicates the best method for the measure in each column. The Recall, false positive to true positive ratios (FP/TP), Precision, number of false negatives, and % of mapped reads by methods LoFreq, VPhaser-2, ShoRAH, and MultiRes are computed for listed datasets. All reads from a sample were aligned using bwa-mem tool for LoFreq and VPhaser-2 under default settings. ShoRAH uses its own aligner for read alignment and variant calling, while k-mers detected by MultiRes were aligned using bwa-mem for MultiRes. Outputs from LoFreq (version 2.1.2), VPhaser-2 (last downloaded version October 2015), and ShoRAH (last downloaded version from November 2013) are compared against known variants for simulated datasets. For 5-viral mix, the consensus reference provided by [ 35 ] was used to determine ground truth variants. MultiRes variants are determined by aligning 35-mers to a reference sequence and bases occurring at more than 0.01 frequency as variants. Bold for each dataset indicates the best method for the performance measures. all datasets. LoFreq and VPhaser have comparable recall but lower precision values and an increase in the FP/TP ratios on the HCV population datasets, indicating a decrease in performance. ShoRAH overall has lower recall values, nevertheless a 100% precision in all but the 5-viral mix dataset, suggesting that it misses true SNPs but is very accurate when it calls a base as SNP. Overall all methods have low values for FP/TP ratio as compared to before, indicating that the number of false positive SNP predictions are low. The metric where MultiRes outperforms others is the lowest number of true SNPs missed. This shows that even with a simplistic SNP prediction method used in MultiRes, it is able to capture the true variation of the sampled viral population and has the lowest false negatives of well established methods. This demonstrates that using error-free set of k-mers can vastly increase the variant detection in viral populations.  The number of reads or k-mers aligned to the reference sequence are comparable across the methods, except for HCV2P dataset where MultiRes has 85% k-mers mapped compared to 100% read mapping (Table 4). It is possible that the unmapped k-mers correspond to the length variants and could be verified by haplotype reconstruction using the predicted k-mers, but that was not the focus in this paper.     4. Discussion and conclusions\r\n  We have proposed a classifier MultiRes for detecting rare variant and erroneous k-mers obtained from Illumina sequencing of viral populations. Our method does not rely on a reference sequence and uses concepts from signal processing to justify using the counts of sets of k-mers of different sizes. We utilize the projections of sampled reads signals onto multiple frames as features for our classifier MultiRes.  We demonstrated the performance of MultiRes on simulated HIV and HCV viral populations and real HIV viral populations containing viral haplotypes at varying relative frequencies, where it outperformed the error detection algorithms used in error correction methods in terms of recall and the total number of predicted k-mers. Though, the error detection algorithms in the error correction methods evaluated assumed that sequenced reads originated from a single genome sequenced at uniform coverage, our method also works better than the method BayesHammer, which can tackle non-uniform sequencing coverage, and the method Seecer, which additionally incorporates methods for detecting alternative splicing and polymorphisms.  The error-free k-mers predicted by MultiRes enable the usage of de novo assembly methods for viral genomes. A major challenge for using De Bruijn graph based methods for viral populations has been the increased complexity of the graph due to the presence of large number of sequencing errors [ 36 ]. Moreover, the memory footprint of a De Bruijn assembly graph increases linearly with the number of k-mers in the NGS data. Thus the low false positives along with high recall of k-mers predicted by MultiRes drastically reduce the memory requirements for De Bruijn graphs. An edge-centric De Bruijn graph of size k − 1 can be directly generated from error free k-mers, such as in de novo assembly tools SPADES, Cortex [ 37,38 ] for reconstruction of viral haplotypes in a viral population. The graph can be used for calling structural variants in the viral population data. MultiRes has high recall of true k-mers while outputting the least number of false positive k-mers, thereby making de novo assembly graphs manageable.  MultiRes also can be directly used for SNP calling as the predicted error-free k-mers can be aligned to an existing reference genome or a consensus sequence of the current viral population. The SNPs called by MultiRes' data has either the highest or the second highest recall of the SNPs compared to other methods for viral population variant calling.  MultiRes relies on the counts of multiple sizes of k-mers observed in the sequenced reads, and the choice of k-mer length is an important parameter. The minimum value of k chosen should be such that a k-mer can only be sampled from a single location in the genome. This is possible in viral populations where there are small repeats present. Choosing the number of k-mer sizes used is another parameter, and while accuracy can be improved by increasing it, additional k-mer counting increased the number of computations. As demonstrated by our experiments, choosing three different values of k, namely (k, 2 \u2022 k, 3 \u2022 k) was sufficient for accurate results.  MultiRes also has applications for studying the large scale variation in closely related genomes, including as viral populations. The complexity of De Bruijn graphs, useful for studying structural variants and rearrangements in the population, increases because of sequencing errors. Our method can provide a compact set of k-mers while still retaining high recall of the true k-mers, which can be utilized for constructing the graph. Additionally, the error-free k-mers predicted by MultiRes can be directly used for understanding the SNPs observed in the viral population to a high degree of accuracy.  MultiRes' classifier also has its limitations. The model, although trained to model the features of an Illumina sequencing machine, does have a decreased performance on different viral populations with a large number of rare variants, as is evident from its 50% accuracy for HCV2P population for k-mers observed only 3 times. Although it is able to eliminate a large number of false positive k-mers (more than 75% of k-mers at counts of 3), the classifier model can be improved with additional training data and an ensemble of classifier models.  MultiRes was primarily developed for detection of sequencing errors and rare variants in viral populations, which have small genomes. Extending our method for larger genomes may require additional tuning of the parameters via re-training of the classifier, but the concepts developed here are applicable to studying variation in closely related genomes such as cancer cell lines. It is also applicable for understanding somatic variation in sequences as their variation frequency is close to the sequencing error rates. The technique can also be explored for newer sequencing machines, such as PacBio sequences and Oxford Nanopore long read sequencing, where the type of sequencing errors are different, but the concepts of projections of signals are still applicable. The software is available for download from the github link (https://github.com/raunaq-m/MultiRes).    Conflict of interest\r\n    Acknowledgments\r\n   The authors declare no conflict of interest. This research was funded by the National Science Foundation (NSF) Award #1421908.\r\n     Appendix A. Supplementary data\r\n    ",
    "sourceCodeLink": "https://github.com/raunaq-m/MultiRes",
    "publicationDate": "0",
    "authors": [
      "Raunaq Malhotra",
      "Manjari Jha",
      "Mary Poss",
      "Raj Acharya"
    ],
    "status": "Success",
    "toolName": "MultiRes",
    "homepage": "http://arxiv.org/abs/1604.04803"
  },
  "93.pdf": {
    "forks": 2,
    "URLs": [
      "github.com/bioinfo-ut/GenomeTester4/",
      "www.ebi.ac.uk/ena/data/view/ERP001960"
    ],
    "contactInfo": ["maido.remm@ut.ee"],
    "subscribers": 5,
    "programmingLanguage": "C",
    "shortDescription": "A toolkit for performing set operations - union, intersection and complement - on k-mer lists.",
    "publicationTitle": "FastGT: an alignment-free method for calling common SNVs directly from raw sequencing reads",
    "title": "FastGT: an alignment-free method for calling common SNVs directly from raw sequencing reads",
    "publicationDOI": "10.1038/s41598-017-02487-5",
    "codeSize": 525,
    "publicationAbstract": "Published: xx xx xxxx We have developed a computational method that counts the frequencies of unique k-mers in FASTQformatted genome data and uses this information to infer the genotypes of known variants. FastGT can detect the variants in a 30x genome in less than 1 hour using ordinary low-cost server hardware. The overall concordance with the genotypes of two Illumina \u201cPlatinum\u201d genomes is 99.96%, and the concordance with the genotypes of the Illumina HumanOmniExpress is 99.82%. Our method provides k-mer database that can be used for the simultaneous genotyping of approximately 30 million single nucleotide variants (SNVs), including >23,000 SNVs fromY chromosome. The source code of FastGT software is available at GitHub (https://github.com/bioinfo-ut/GenomeTester4/).",
    "dateUpdated": "2017-10-15T22:05:56Z",
    "institutions": [],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2015-05-19T12:13:01Z",
    "numIssues": 6,
    "downloads": 0,
    "fulltext": "     RepoRts |     10.1038/s41598-017-02487-5   FastGT: an alignment-free method for calling common SNVs directly from raw sequencing reads     Fanny-Dhelia Pajuste    Lauris Kaplinski    Märt Möls    Tarmo Puurand    Maarja Lepamets    Maido Remm  maido.remm@ut.ee    7  2537    12  4  2017    21  7  2016     Published: xx xx xxxx We have developed a computational method that counts the frequencies of unique k-mers in FASTQformatted genome data and uses this information to infer the genotypes of known variants. FastGT can detect the variants in a 30x genome in less than 1 hour using ordinary low-cost server hardware. The overall concordance with the genotypes of two Illumina \u201cPlatinum\u201d genomes is 99.96%, and the concordance with the genotypes of the Illumina HumanOmniExpress is 99.82%. Our method provides k-mer database that can be used for the simultaneous genotyping of approximately 30 million single nucleotide variants (SNVs), including &gt;23,000 SNVs fromY chromosome. The source code of FastGT software is available at GitHub (https://github.com/bioinfo-ut/GenomeTester4/).       -\r\n  OPEN  Next-generation sequencing (NGS) technologies are widely used for studying genome variation. Variants in the human genome are typically detected by mapping sequenced reads and then performing genotype calling1-4. A standard pipeline requires 40-50 h to process a human genome with 30x coverage from raw sequence data to variant calls on a multi-thread server. Mapping and calling are state-of-the-art processes that require expert users familiar with numerous available software options. It is not surprising that diefrent pipelines generate slightly diefrent genotype calls 5-9. Fortunately, inconsistent genotype calls are associated with certain genomic regions only10-12, whereas genotyping in the remaining 80-90% of the genome is robust and reliable.  hTe use of k -mers (substrings of length k) in genome analyses has increased because computers can handle large volumes of sequencing data more eficiently. For example, phylogenetic trees of all known bacteria can be easily built using k-mers from their genomic DNA13-15. Bacterial strains can be quickly identified from metagenomic data by searching for strain-specific k-mers 16-18. K-mers have also been used to correct sequencing errors in raw reads19-22. One recent publication has described an alignment-free SNV calling method that is based on counting the frequency of k-mers23. This method converts sequences from raw reads into Burrows-Wheeler transform and then calls genotypes by counting using a variable-length unique substring surrounding the variant.  We developed a new method that oefrs the possibility of directly genotyping known variants from NGS data by counting unique k-mers. eTh method only uses reliable regions of the genome and is approximately 1-2 orders of magnitude faster than traditional mapping-based genotype detection. uThs, it is ideally suited for a fast, preliminary analysis of a subset of markers before the full-scale analysis is finished.  ehT method is implemented in the C programming language and is available as the FastGT software package. FastGT is currently limited to the calling of previously known genomic variants because specific k -mers must be pre-selected for all known alleles. Therefore, it is not a substitute for traditional mapping and variant calling but a complementary method that facilitates certain aspects of NGS-based genome analyses. In fact, FastGT is comparable to a large digital microarray that uses NGS data as an input. Our method is based on three original components: ( 1 ) the procedure for the selection of unique k-mers, ( 2 ) the customized data structure for storing and counting k-mers directly from a FASTQ file, and ( 3 ) a maximum likelihood method designed specifically for estimating genotypes from k-mer counts.    Results\r\n    Compilation of the database of unique k-mer pairs. eTh crucial component of FastGT is a pre-com\r\n  piled flat-file database of genomic variants and corresponding k -mer pairs that overlap with each variant. Every bi-allelic single nucleotide variant (SNV) position in the genome is covered by k k-mer pairs, where pair is formed by k-mers corresponding to two alternative alleles (Figure S4). FastGT relies on the assumption that at least a number of these k-mer pairs are unique and appear exclusively in this location of the genome; therefore, the occurrence counts of these unique k-mer pairs in sequencing data can be used to identify the genotype of this variant in a specific individual.  hTe database of variants and unique k -mers is assembled by identifying all possible k-mer pairs for each genomic variant and subjecting them to several steps of filtering. The filtering steps remove variants for which unique k-mers are not observed and variants that produce non-canonical genotypes (non-diploid in autosomes and non-haploid in male X and Y chromosomes) in a sequenced test-set of individuals. Filtering of k-mers was performed using high coverage NGS data of 50 individuals from Estonian Genome Project (published elsewhere). ehT filtering steps are described in Methods section and in Supplementary File (Figure  S5).  Although one k-mer pair is theoretically suficient for genotyping, mutations occasionally change the genome sequence in the neighborhood of an SNV, eefctively preventing the detection of the SNV by a chosen k -mer. If the mutation is allele-specific, then the wrong genotype could be easily inferred. eThrefore, we use up to three k-mer pairs per variant to prevent erroneous calls caused by the occasional loss of k-mers because of rare mutations.  In the current study, we compiled a database of all bi-allelic SNVs from dbSNP and tested the ability of FastGT to detect these SNVs with 25-mers. After the filtering steps, 30,238,283 (64%) validated and bi-allelic SNVs remained usable by FastGT. We also used a subset of autosomal SNV markers present on the Illumina HumanOmniExpress microarray for a concordance analysis. In this set, 78% of the autosomal markers from this microarray were usable by FastGT. The number of SNV markers that passed each filtering step is shown in Table S1.  Algorithm and software for k-mer-based genotyping. The genotyping of individuals is executed by reading the raw sequencing reads and counting the frequencies of k-mer pairs described in the pre-compiled database of variants using the custom-made software gmer_counter and gmer_caller (Fig.  1).  ehT database of genomic variants and corresponding k-mers is stored as a text file. eTh frequencies of k-mers listed in the database are counted by gmer_counter. It uses a binary data structure, which stores both k-mer sequences and their frequencies in computer memory during the counting process. A good compromise between memory consumption and lookup speed is achieved by using adaptive radix tree (see Supplementary File for detailed description of the data structure). eTh first 14 nucleotides of a k-mer form an index into a table of sparse bitwise radix trees that are used for storing the remaining sequence of the k-mers. Two bytes per k-mer are allocated for storing frequencies. eTh current implementation of gmer_counter accepts k-mers with lengths between 14 and 32 letters. The frequencies of up to three k -mer pairs from gmer_counter are saved in a text file that is passed to gmer_caller, which infers the genotypes based on k-mer frequencies and prints the results to a text file.    Empirical Bayes\u2019 method for inferring genotypes from k-mer counts. Gmer_caller uses the\r\n  Empirical Bayes classifier for calling genotypes from k -mer frequency data, which assigns the most likely genotype to each variant. Allele frequency distributions are modeled by negative binomial distribution, described by eight parameters (see description in the Supplementary File). eTh model parameters are estimated separately for each analyzed individual using k-mer counts of 100,000 autosomal markers. eTh model allows us to estimate the most likely copy number for both alleles. Given the observed allele counts, gmer_caller calculates the probability of genotypes by applying the Bayes rule. As we do not require allele copy numbers to sum to 2 we can also call mono-, tri-, or tetra-allelic genotypes (which might correspond to deletions and duplications) in addition to traditional bi-allelic (diploid) genotypes (Fig. 2). eTh model parameters can be saved and re-used in subsequent analyses of the same dataset.  hTe gender of the individual is determined automatically from the sequencing data using the median frequency of markers from the X chromosome (chrX). If the individual is female, only the autosomal model is used in the calling process and Y chromosome (chrY) markers are not called. For men, an additional haploid model of Bayes' classifier is trained for calling genotypes from sex chromosomes. Parameters for the haploid model are estimated using 100,000 markers from chrX.    Assessment of genotype calling accuracy through simulations. In order to test the performance of\r\n  FastGT, we generated simulated raw sequencing reads from the reference genome and analyzed the ability of the Bayesian classifier of FastGT to reproduce genotypes of the reference genome (see Methods for detailed description of data simulation methods). Throughout this paper we denote A as reference allele and B as alternative allele. In this simulation, the reference genome was assumed to be homozygous in all positions. uThs, the correct genotype for all 30,238,283 tested markers would be AA genotype. eTh fraction of AA genotypes recovered from simulated reads varied between 98.94% (at 5x coverage) and 99.95% (at 20x coverage). The fraction of uncalled markers was between 0.001% (at 20x coverage) and 1.036% (at 5x coverage). eTh fraction of AB calls was in range of 0.02% to 0.05% at all coverages. eTh results are shown in Table  1.  ehT performance of calling AB and BB genotypes cannot be estimated from the reference genome. We created simulated genomes using genotypes from 5 sequenced individuals, each from diferent population (Yoruban, Chinese Han, CEPH, Puerto Rican and Estonian). iThs analysis helped to test the performance of Bayesian classiifer (gmer_caller) on calling the AB and BB variants from real-life data. Secondly, this analysis indicates whether the selection of markers that was done using Estonian individuals introduces any population-specific bias in genotype calling. The sensitivity (fraction of correctly called AB and BB variants) was strongly aefcted by coverage (61% at 5x coverage, 99.8% at 20x coverage), but remained almost identical for individuals from diefrent populations: 99.7-99.8% at 30x coverage (Fig. 3). eTh specificity (fraction of correctly called AA calls) was more uniform over diefrent coverage levels and remained between 99.60% to 99.95%. eThse results show that our set of 30 million markers is usable for studying diefrent populations without strong bias in sensitivity or specificity.    Assessment of genotype calling accuracy through concordance analysis. eTh accuracy of FastGT\r\n  genotype calls was analyzed by comparing the results to genotypes reported in two Illumina Platinum individuals, NA12877 and NA12878, which were sequenced to 50x coverage. eThse are high-confidence variant calls derived by considering the inheritance constraints in the pedigree and the concordance of variant calls across diefrent methods24. We determined genotypes for 30,238,283 millions of markers from the FastGT database using raw sequencing data from the same individuals and compared them to genotypes shown in the Platinum dataset. FastGT genotype calls  AA AB BB NC  ehT overall concordance of bi-allelic FastGT genotypes with genotypes from two Platinum genomes is 99.96%. hTe concordance of the non-reference (AB or BB) calls was 99.93%. The distribution of diferences between the two sets for diefrent genotypes is shown in Table  2. All of the genotypes reported in the Platinum datasets were bi-allelic; thus, we included only bi-allelic FastGT genotypes in this comparison. eTh fraction of uncertain (no-call) genotypes in the FastGT output was 0.24%. The uncertain genotypes are primarily mono-allelic (A) and tri-allelic (AAA) genotypes that might correspond to deletions or insertions in a given region. However, non-canonical genotypes in the default output are not reported, and they are replaced by NC (\u201cno call\u201d). All of the genotypes and/or their likelihoods can be shown in gmer_caller optional output.  We also compared the genotypes obtained by the FastGT method with the data from the Illumina HumanOmniExpress microarray. We used 504,173 autosomal markers that overlap our whole-genome dataset (Table S2), and the comparison included ten individuals from the Estonian Genome Center for whom both microarray data and Illumina NGS data were available. In these 10 individuals, the concordance between the genotypes from the FastGT method and microarray genotypes was 99.82% (Table 2), and the concordance of non-reference alleles was 99.69%. eTh fraction of mono-allelic and tri-allelic genotypes (no-call genotypes) in 10 test individuals is rather low (&lt;0.01% of all markers), indicating that our conservative filtering procedure is able to remove most of the error-prone SNVs.  Markers fromY chromosome. FastGT is able to call genotypes from the Y chromosome (chrY) for 23,832 markers that remain in the whole-genome dataset after all filtering steps. The genotypes on chrY cannot be directly compared with the Platinum genotypes because chrY calls were not provided in the VCF file of the Platinum individuals. To assess the performance of chrY genotyping, we compared our results to the genotypes of 11 men from the HGDP panel25 (http://cdna.eva.mpg.de/denisova/). The overall concordance of the haploid genotype calls of FastGT and the genotype calls in these VCF files was 99.97%. The fraction of non-canonical genotypes (no-calls) in the FastGT output was 1.27% (Table S3).  We also tested the concordance of chrY genotypes in seven father-son pairs in CEPH pedigree 1463 (http://www.ebi.ac.uk/ena/data/view/ERP001960). We assume that changes in chrY genotypes should not occur within one generation. Only one marker (rs199503278) showed conflicting genotypes in any of these father-son pairs. A visual inspection revealed problems with the reference genome assembly in this region, which resulted in conflicting k-mer counts and conflicting genotypes from diefrent k-mer pairs of the same SNV. This marker was removed from the dataset because it had a high likelihood of causing similar problems in other individuals. Eefct of genome coverage on FastGT performance. We also studied how the genome sequencing depth aefcts the performance of FastGT. eTh Platinum genomes have a coverage depth of approximately 50x, but in most study scenarios, sequencing to a lower coverage is preferred because it optimizes costs. For this analysis, we compiled diefrent-sized subsets of FASTQ sequences from the Platinum individual NA12878 and measured the concordance between called genotypes and genotypes from the Platinum dataset. We observed that the concordance rate of non-reference genotypes (AB and BB) declines signicfiantly as the coverage drops below 20x (Fig.  4). Relationship between k-mer length and number of usable variants. An obvious question is how the k-mer length aefcts the performance of FastGT. We used 25 nucleotides long k-mers throughout this article, but FastGT is able to use other k-mer lengths between 16 and 32 as well. We tested how many markers from dbSNP would remain usable for FastGT at diefrent values of k . From 47 millions validated markers 7-17% markers are removed in filtering step 1 due to closely located SNVs (Fig.  5). In filtering step 2 markers are removed if they have no unique k-mer pairs in the expanded reference genome. As expected, a rather large number of markers are eliminated from the dataset if k-mers shorter than 20 nucleotides are used. However, the number of usable markers does not increase significantly for k larger than 24. uThs, k values between 24 and 32 should be equally suitable for analyzing the human genome with FastGT. We have not compared the accuracy of genotype calls of diefrent k-mer lengths. However, we expect it to be relatively independent of k-mer length. Two main factors that might inuflence the accuracy (concordance) of genotypes are non-specific counts from shorter k-mers and drop of eefctive coverage of k -mers. The eefctive coverage is negatively correlated with k -mer length. This negative correlation is caused by the higher chance of accumulating sequencing errors within longer k-mers and by the end eefcts of the reads (lower number of long k -mers per sequencing read). On the other hand, shorter k-mers are more likely to pick up non-specific sequences due to sequencing errors and unknown variations in human genomes. Overall, these eefcts inuflence the eefctive coverage of k-mers and are only critical if genome coverage is low or if k-mer is shorter than 20 nucleotides. At high coverage (&gt;20x) conditions the k-mer length should not have significant inuflence to genotype accuracy.  Time and memory usage. The entire process of detecting 30 million SNV genotypes from the sequencing data of a single individual (30x coverage, 2 FASTQ files, 115GB each) takes approximately 40 minutes on a server with 32 CPU cores. Most of this time is allocated to counting k-mer frequencies by gmer_counter. The running time of gmer_counter is proportional to the size of the FASTQ files because the speed-limiting step of gmer_counter is reading the sequence data from a FASTQ file. However, the running time is also dependent on the number of FASTQ files (Fig.  6) because simultaneously reading from multiple files is faster than processing a single file. Genotype calling with gmer_caller takes approximately 2-3 minutes with 16 CPU cores.  hTe minimum amount of required RAM is determined by the size of the data structure stored in memory by gmer_counter. We have tested gmer_counter on Linux computer with 8 GB of RAM. However, server-grade hardware (multiple CPU cores and multiple fast hard drives in RAID) is required to achieve the full speed of gmer_counter and gmer_caller.    Methods\r\n  Compilation of database of unique k-mers. A k-mer length of 25 was used throughout this study, and the k-mers for genotyping were selected by the following filtering process (see also Figure  S4). First, the validated single nucleotide variants (SNVs), as well as the validated and common indels, were extracted from the dbSNP database build 14626, 27. Indels were used for testing the uniqueness of k-mers only; they are not included in the database of variants. For every bi-allelic SNV from this set, two sequences surrounding this SNV location were created: the sequence of the human reference genome (GRCh37) and the sequence variant corresponding to the alternative allele. The sequences were shortened to eliminate any possible overlap with neighboring SNVs or common indels. Essentially, this filtering step removed all of the SNVs that were located between two other SNVs (or indels) with less than 25 bp between them. iThs step was chosen to avoid the additional complexity of counting and calling algorithms because of the multiple combinations of neighboring SNV alleles. For all these SNVs that had variant-free sequences of at least 25 bp, the sequences were divided into 25-mer pairs.  In the second filtering step, we tested the uniqueness of the 25-mers compiled in the previous step. The uniqueness parameter was tested against the \u201cexpanded reference genome\u201d, which is a set of 25-mers from the reference genome plus all possible alternative 25-mers containing the non-reference alleles of the SNVs and indels. A k-mer pair is considered unique if both k-mers occur no more than once in the \u201cexpanded reference genome\u201d. All non-unique k-mer pairs were removed from the list. eTh Glistcompare tool 28, which performs set operations with sorted k-mer lists, was used in this step. eTh k -mer pairs demonstrating uniqueness even with one mismatch were preferred. This constraint was added to reduce the risk of forming an identical k-mer by a rare point mutation or a sequencing error.  In the third step, the k-mers were further refined using the k -mer frequencies and genotypes in a set of sequenced individual genomes. For this purpose, the k-mer counts and genotypes were calculated for all SNVs of 50 random individuals whose DNA was collected and sequenced during the Center of Translational Genomics project at the University of Tartu. Twenty-vfie men and 25 women were used for filtering the autosomal SNVs; for chrX and chrY, 50 men were used. eTh sequencing depth in these individuals varied between 21 and 45. rThee diefrent criteria were used for removing k-mer pairs and SNVs in this step. First, we excluded all chrY markers that had k-mer frequency higher than 3 in more than one woman. Second, autosomal k-mers showing abnormally high frequencies (greater than 3 times the median count) in more than one individual were removed. iThrd criterion was based on unexpected genotypes: the SNVs that produced a non-canonical allele count in more than one individual out of 50 were removed from the dataset. The non-canonical allele count is any value other than two alleles in autosomes or a single allele in male chrX and chrY. The criteria used in filtering step 3 should remove SNVs located in the regions that are unique in the reference genome, but frequently duplicated or deleted in real individuals.  hTe final set contained 30,238,283 SNVs usable by FastGT, with 6.8% ( 2,063,839 ) located in protein-coding regions. A detailed description of the filtering steps used in this article is shown in in Figure  S5. The number of markers removed in each step is shown in Table S1.  Statistical framework. The statistical framework for Empirical Bayes Classifier implemented in gmer_ caller is described in Supplementary File.  Generating and analyzing simulated data. FastGT was tested on simulated reads. Simulated sequencing reads were generated using WgSim (version 0.3.1-r13) software from samtools package 2. The following parameters were used: base_error_rate = 0.005, outer_distance_between_the_two_ends = 500, standard_deviation = 50, length_of_the_first_read = 100, length_of_the_second_read = 100. We used the base error rate 0.005 (0.5%) because this is similar to error rate typically observed in Illumina HiSeq sequencing data. We estimated average error rate in the raw reads of high-coverage genomes from Estonian Genome Center by counting the fraction of erroneous k-mers. eTh error rates in 100 individuals varied between 0.0030 and 0.0082, with average 0.0048 (CI95% = 0.0002). Previous studies have reported similar overall error rate in raw reads generated by Illumina HiSeq, varying between 0.002 and 0.00429, 30. The sequencing reads were simulated with diefrent cov erages: about 5, 10, 20, 30 and 40. eTh number of read pairs generated were 80 million, 160 million, 320 million, 480 million and 640 million respectively.  Reads were generated from standard reference genome, version GRCh37. For Fig. 3 the reads were also simulated using real SNV information for 5 individuals from 5 diferent populations (CEU, CHS, YRI, PUR and EST). The following individuals were used in simulations: HG00512 (CHS), NA19238 (YRI), HG00731 (PUR), NA12877 (CEU) and V00055 (EST). eThir sequencing data was retrieved from 1000 G project repository at pft:// ptf.1000genomes.ebi.ac.uk/vol1/pft/data_collections/hgsv_sv_discovery/data/ (CHS, YRI and PUR), from the ptf://pft.sra.ebi.ac.uk/vol1/ERA172/ERA172924/bam/ (CEU) or from the Estonian Genome Center. For each of these individuals, the standard reference genome was used as base and the corresponding SNV genotypes from their VCF files were added to generate the reads with realistic variants. eTh SNV genotypes were calculated from BAM or CRAM files using Genome Analysis Toolkit version 3.6 3.  hTe sensitivity and specificity were calculated for each individual and for each coverage. True positive was defined as AB or BB genotype that was correctly called by FastGT in simulated data. True negative values were defined as correctly called AA genotypes. eTh genotypes called from sex chromosomes were not used for sensitivity and specificity calculations.  Testing genotype concordance. Version 20160503 of the FastGT package was used throughout this study. For the concordance analysis with the Platinum genotypes, gmer_counter and gmer_caller were run with the default options. eTh performance was tested on a Linux server with 32 CPU cores, 512GB RAM, and IBM 6 Gbps and SAS 7200 rpm disk drives in a RAID10 configuration.  High-quality genotypes were retrieved from the Illumina Platinum Genomes FTP site at pft://ussd-pft.illumina.com/hg38/2.0.1/. BAM-format files of NA12877 and 12878 were downloaded from pft://pft.sra.ebi.ac.uk/ vol1/ERA172/ERA172924/bam/NA12877_S1.bam and pft://pft.sra.ebi.ac.uk/vol1/ERA172/ERA172924/bam/ NA12878_S1.bam.  FASTQ files were downloaded from the European Nucleotide Archive at http://www.ebi.ac.uk/ena/data/view/ERP001960. FASTQ files for the chrY genotype comparison were created from the corresponding BAM files using SAMtools bam2fq version 0.1.18. eTh read length of the Platinum genomes was 101 nucleotides.  Illumina HumanOmniExpress microarray genotypes and Illumina NGS data (read length 151 nt) for individuals V00278, V00328, V00352, V00369, V00402, V08949, V09325, V09348, V09365, and V09381 were obtained from the Estonian Genome Center. For the concordance analysis with the microarray genotypes, gmer_caller was run with the microarray markers (504,173) only.  hTe 5x, 10 × 20x, 30x, and 40x data points for Fig. 4 were created using random subsets of reads from raw FASTQ files of 50x coverage from the Platinum individual NA12878.  Code availability. hTe binaries of FastGT package and k -mer databases described in the current paper are available on our website, http://bioinfo.ut.ee/FastGT/. eTh source code is available at GitHub (https://github.com/bioinfo-ut/GenomeTester4/). Gmer_counter and gmer_caller are distributed under the terms of GNU GPL v3, and the k-mer databases are distributed under the Creative Commons CC BY-NC-SA license.    Discussion\r\n  FastGT is a flexible software package that performs rapid genotyping of a subset of previously known variants without a loss of accuracy. Another similar approach of genotype calling has been published before23. Both methods need to pre-process the reference genome and personal short-read data. Our method pre-processes the genome by selecting the SNVs and compiling the database of k-mers that can be used for calling these SNVs. eTh short-read data is pre-processed by counting and storing the k-mer frequencies using gmer_counter. eTh method by Kimura and Koike uses dictionary-based approach for storing both reference sequence and short reads. The dictionary is implemented by means of the Burrows-Wheeler transform (BWT). eTh main advantage of BWT is the ability of storing and comparing long strings eficiently. eThrefore, this method can be used to call all SNVs, including those that are in repeated genomic regions. FastGT uses fixed length k -mer with maximum length of 32. This limits the number of variants that can be called from the human genome. On the other hand, using fixed length k-mers allows faster processing of data due to 64-bit architecture of computer hardware. uThs, FastGT essentially sacrifices calling some SNVs (up to 36%) from dificult genomic regions to minimize data processing time. Another diefrence between FastGT and the method used by Kimura and Koike is handling of the de novo mutations. Kimura and Koike implemented two methods (drop-scan and step-scan) to detect de novo variants based on k-mer coverage and/or by local alignment of surrounding region. FastGT has currently no ability to call de novo variants and is limited to calling sub-sets of pre-defined variants. uThs, FastGT functions in principle as a large digital microarray with millions of probes.  Numerous software packages can organize the raw sequencing data of each individual into comprehen sive k-mer lists28, 31-34, which can be later used for fast retrieval of k-mer counts. However, the compilation of full-genome lists is somewhat inecfiient if the lists are only used once and then immediately deleted. FastGT uses adaptive radix tree, which allows us to store frequencies for only the k-mers of interest, instead of for all k-mers from the genome. This approach is particularly useful for genotyping only a small number of variants from each individual. Storing only the frequencies of relevant k-mers avoids the so-called \u201ccurse of deep sequencing,\u201d in which a higher coverage genome can overwhelm the memory or disk requirements of the sowftare 35. eTh disk and memory requirements of FastGT are not directly aefcted by the coverage of sequencing data.  Our analysis focuses on genotyping SNVs. However, FastGT is not limited to identifying SNVs. Any known variant that can be associated with a unique and variant-specific k -mer can be detected with FastGT. For example, short indels could be easily detected by using pairs of indel-specific k -mers. In principle, large indels, pseudogene insertions, polymorphic Alu-elements, and other structural variants could also be detected by k-mer pairs designed over the breakpoints. However, the detection of structural variants relies on the assumption that these variants are stable in the genome and have the same breakpoint sequences in all individuals, which is not always true for large structural variants. The applicability of FastGT for detecting structural variants requires further investigation and testing.  hTis software has only been used with Illumina sequencing data, which raises the question of whether our direct genotyping algorithm is usable with other sequencing technologies. In principle, k-mer counting should work with most sequencing platforms that produce contiguous sequences of at least k nucleotides. eTh uniformity of coverage and the fraction of sequencing errors in raw data are the main factors that inuflence k -mer counting because a higher error rate reduces the number of usable k-mers and introduces unwanted noise. eTh type of error is less relevant because both indel-type and substitution-type errors are equally deleterious for k-mer counting.  NGS data are usually stored in BAM format, and the original FASTQ files are not retained. In this case, the FASTQ file can be created from available BAM files. This can be performed by a number of software packages (Picard, bam2fq from SAMtools package1, bam2fastx from TopHat package36). We have tested FastGT software with raw FASTQ files and FASTQ files generated from the BAM-formatted files and did not observe significant diefrences in the k -mer counts or genotype calls. In principle, care should be taken to avoid multiple occurrences of the same reads in the resulting FASTQ file. Regardless of the method of genome analysis, contamination-free starting material, diligent sample preparation, and suficient genome coverage are the ultimate pre-requisites for reliable results. eTh \u201cgarbage in, garbage out\u201d principle applies similarly to mapping-based genome analyses and k-mer based genome analyses.    Acknowledgements\r\n  hTis work was funded by institutional grant IUT34-11 from the Estonian Ministry of Education and Research, grant SP1GVARENG from the University of Tartu, and the EU ERDF grant No. 2014-2020.4.01.15-0012 (Estonian Center of Excellence in Genomics and Translational Medicine). eTh cost of the NGS sequencing of the individuals from the Estonian Genome Center was partly covered by the Broad Institute (MA, USA) and the PerMed I project from the TERVE program. eTh genome data was collected and used with ethical approval Nr. 206T4 (obtained for the project SP1GVARENG). eTh computational costs were partly covered by the High Performance Computing Centre at the University of Tartu. eTh authors thank Märt Roosaare, Ulvi Talas, and Priit Palta for performing a critical reading of the manuscript.    Author Contributions\r\n  F.D.P. compiled the databases of unique k-mers and conducted the genotype concordance analyses. L.K. invented the data structures and algorithms for gmer_counter and gmer_caller and implemented their code in C. M.M. wrote the initial code of the Bayesian classiefir for genotype calling and supervised the development of a statistical framework. T.P. validated the genotyping results by performing a manual analysis of BAM files and providing expertise for NGS data management. M.L. performed an initial survey of the optimal number of k-mer pairs per variant. MR supervised the work and wrote the final version of the manuscript. Competing Interests: eTh authors declare that they have no competing interests.  Publisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afiliations.  Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.    ",
    "sourceCodeLink": "https://github.com/bioinfo-ut/GenomeTester4",
    "publicationDate": "0",
    "authors": [
      "Fanny-Dhelia Pajuste",
      "Lauris Kaplinski",
      "Märt Möls",
      "Tarmo Puurand",
      "Maarja Lepamets",
      "Maido Remm"
    ],
    "status": "Success",
    "toolName": "GenomeTester4",
    "homepage": ""
  },
  "14.pdf": {
    "forks": 0,
    "URLs": [
      "www.ncbi.nlm.nih.gov/pubmed",
      "github.com/behroozt/PiiL.git",
      "gdc-portal.nci.nih.gov/legacy-archive/search/f",
      "github.com/behroozt/PiiL"
    ],
    "contactInfo": ["Manfred.grabherr@imbim.uu.se"],
    "subscribers": 1,
    "programmingLanguage": "Java",
    "shortDescription": "Pathway interactive visualization tool. For documentation see: http://behroozt.github.io/PiiL/",
    "publicationTitle": "PiiL: visualization of DNA methylation and gene expression data in gene pathways",
    "title": "PiiL: visualization of DNA methylation and gene expression data in gene pathways",
    "publicationDOI": "10.1186/s12864-017-3950-9",
    "codeSize": 100747,
    "publicationAbstract": "Background: DNA methylation is a major mechanism involved in the epigenetic state of a cell. It has been observed that the methylation status of certain CpG sites close to or within a gene can directly affect its expression, either by silencing or, in some cases, up-regulating transcription. However, a vertebrate genome contains millions of CpG sites, all of which are potential targets for methylation, and the specific effects of most sites have not been characterized to date. To study the complex interplay between methylation status, cellular programs, and the resulting phenotypes, we present PiiL, an interactive gene expression pathway browser, facilitating analyses through an integrated view of methylation and expression on multiple levels. Results: PiiL allows for specific hypothesis testing by quickly assessing pathways or gene networks, where the data is projected onto pathways that can be downloaded directly from the online KEGG database. PiiL provides a comprehensive set of analysis features that allow for quick and specific pattern searches. Individual CpG sites and their impact on host gene expression, as well as the impact on other genes present in the regulatory network, can be examined. To exemplify the power of this approach, we analyzed two types of brain tumors, Glioblastoma multiform and lower grade gliomas. Conclusion: At a glance, we could confirm earlier findings that the predominant methylation and expression patterns separate perfectly by mutations in the IDH genes, rather than by histology. We could also infer the IDH mutation status for samples for which the genotype was not known. By applying different filtering methods, we show that a subset of CpG sites exhibits consistent methylation patterns, and that the status of sites affect the expression of key regulator genes, as well as other genes located downstream in the same pathways. PiiL is implemented in Java with focus on a user-friendly graphical interface. The source code is available under the GPL license from https://github.com/behroozt/PiiL.git.",
    "dateUpdated": "2016-11-01T18:06:51Z",
    "institutions": [
      "Uppsala University",
      "Uppsala University",
      "Umeå University",
      "Polish Academy of Sciences"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-01-13T09:52:33Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Moghadam et al. BMC Genomics     10.1186/s12864-017-3950-9   PiiL: visualization of DNA methylation and gene expression data in gene pathways     Behrooz Torabi Moghadam  0    Neda Zamani  1  2    Jan Komorowski  0  3    Manfred Grabherr  Manfred.grabherr@imbim.uu.se  1    0  Department of Cell and Molecular Biology, Computational and Systems Biology, Uppsala University ,  Uppsala ,  Sweden    1  Department of Medical Biochemistry and Microbiology/BILS ,  Genomics ,  Uppsala University ,  Uppsala ,  Sweden    2  Department of Plant Physiology, Umeå University ,  Umeå ,  Sweden    3  Institute of Computer Science, Polish Academy of Sciences ,  01248 Warsaw ,  Poland     2017   18    18  7  2017    17  1  2017     Background: DNA methylation is a major mechanism involved in the epigenetic state of a cell. It has been observed that the methylation status of certain CpG sites close to or within a gene can directly affect its expression, either by silencing or, in some cases, up-regulating transcription. However, a vertebrate genome contains millions of CpG sites, all of which are potential targets for methylation, and the specific effects of most sites have not been characterized to date. To study the complex interplay between methylation status, cellular programs, and the resulting phenotypes, we present PiiL, an interactive gene expression pathway browser, facilitating analyses through an integrated view of methylation and expression on multiple levels. Results: PiiL allows for specific hypothesis testing by quickly assessing pathways or gene networks, where the data is projected onto pathways that can be downloaded directly from the online KEGG database. PiiL provides a comprehensive set of analysis features that allow for quick and specific pattern searches. Individual CpG sites and their impact on host gene expression, as well as the impact on other genes present in the regulatory network, can be examined. To exemplify the power of this approach, we analyzed two types of brain tumors, Glioblastoma multiform and lower grade gliomas. Conclusion: At a glance, we could confirm earlier findings that the predominant methylation and expression patterns separate perfectly by mutations in the IDH genes, rather than by histology. We could also infer the IDH mutation status for samples for which the genotype was not known. By applying different filtering methods, we show that a subset of CpG sites exhibits consistent methylation patterns, and that the status of sites affect the expression of key regulator genes, as well as other genes located downstream in the same pathways. PiiL is implemented in Java with focus on a user-friendly graphical interface. The source code is available under the GPL license from https://github.com/behroozt/PiiL.git.    DNA methylation  Gene expression  Visualization  KEGG pathways       Background\r\n  DNA methylation (DNAm) is a key element of the transcriptional regulation machinery. By adding a methyl group to CpG sites in the promoter of a gene, DNAm provides a means to temporarily or permanently silence transcription [ 1 ], which in turn can alter the state or phenotype of the cell. DNAm of sites outside promoters can also take effect, where for example methylation in the gene body might elongate transcription, and methylation of intergenic regions can help maintain chromosomal stability at repetitive elements [ 2 ]. Change in DNAm has been observed to occur with age in the human brain [ 3, 4 ], as well as in various developmental stages [ 5 ]. It is also a hallmark of a number of diseases [ 6, 7 ], including cancer [ 8, 9 ]. A prominent example is the methylation of the promoter of the tumor suppressor protein TP53 [ 10-12 ], which occurs in about 51% of ovarian cancers [ 13 ]. Since TP53 is a master regulator of cell fate, including apoptosis, disabling its expression has a direct impact on the function of downstream expression pathways.  Different cancers or cancer subtypes, however, might deploy different strategies to alter expression patterns to increase their viability, which might be visible in the methylation landscape. In gliomas, for instance, it has been reported that mutations in the IDH (isocitrate dehydrogenase genes 1 and 2, collectively referred to as IDH) genes result in the hyper-methylation of a number of sites [ 14 ].  However, with a few exceptions, the exact relation between DNA methylation and the expression of its host gene remains elusive and is still poorly understood. One confounding factor is the many-to-one relationship between CpG sites and genes or transcripts. A global association of lower expression with increased promoter methylation, and increased expression with methylation of sites in the gene body has been observed [ 2, 15-17 ]. By contrast, an accurate means to predict the effect of methylating or de-methylating any given site, or clusters thereof, is still lacking. In addition, altering the expression of certain genes might not be relevant for disease progression but rather becomes a side effect, whereas changes in key regulators of networks might result in large-scale effects. Characterizing the methylation patterns that differ between tumor types allows for a more accurate diagnosis and can thus inform the choice of treatment. Moreover, examining the effect on the regulatory machinery in a pathway or gene expression network level might give insight into how the disease develops, progresses, and spreads [ 18 ].  Here, we present PiiL (Pathway interactive visualization tool), an integrated DNAm and expression pathway browser, which is designed to explore and understand the effect of DNAm operating on individual CpG sites on overall expression patterns and transcriptional networks. PiiL implements a multi-level paradigm, which allows examining global changes in expression, comparisons between multiple sample grouping, play-back of time series, as well as analyzing and selecting different subsets of CpG sites to observe their effect. Moreover, PiiL accepts precomputed sub-sets that were generated offline by other methods, for example the bumphunter function in Minfi [ 19 ], Monte Carlo Feature Selection (MCFS) [ 20 ], or unsupervised methods, such as Saguaro [ 21 ]. PiiL accesses pathways or gene networks online from the KEGG databases [ 22, 23 ], and allows for visualizing pathways from different organisms with up-to-date KEGG pathways.  In keeping a sharp focus on methylation, expression, and ease-of-use, PiiL builds upon the user experience with other, typically more general visualization tools. For example, Cytoscape [ 24 ] is a widely-used, open source platform for producing, editing, and analyzing generic biological networks. The networks are dynamic and can be queried and integrated with expression, protein-protein interactions data, and other molecular states and phenotypes, or be linked to functional annotation databases. Due to the extensibility of the core, there are multiple plugins available, some specifically for handling KEGG databases, such as KEGGscape [ 25 ] and CyKEGGParser [ 26 ], features that are natively built into PiiL. Pathview [ 27 ], an R/Bioconductor package, also visualizes KEGG pathways with a wide range of data integration, such as gene expression, protein expression, and metabolite level on a selected pathway, but, unlike PiiL, lacks the ability to examine methylation at the resolution of individual sites. Pathvisio [ 28 ], another tool implemented in Java, provides features for drawing, editing, and analyzing biological pathways, and mapping gene expression data onto the targeted pathway. Extended functionality is added via different available plugins, but similar to Pathview, it does not provide functionality specific to analyze the effects of DNAm based on individual sites. KEGGanim [ 29 ] is a web-based tool that can visualize data over a pathway and produce animations using high-throughput data. KEGGanim thus highlights the genes that have a dynamic change over conditions and influence the pathway, a feature that is also available in PiiL.  In the following, we will first describe the method, and then exemplify how PiiL benefits the analysis of large and complex data sets without requiring the user to be an informatics expert.    Implementation\r\n  PiiL is platform independent, implemented in Java with an emphasis on user-friendliness for biologists. It first reads KGML format pathway files, either from a storage media, or from the online KEGG database (using RESTstyle KEGG API), where in case of the latter, a complete list of available organisms and available pathways for the selected organism is loaded and locally cached for the current session. Multiple pathways can be viewed in different tabs, with each tab handling either DNAm or gene expression data, referred to as metadata in this article.  According to the metadata, genes are color-coded based on individual samples, or a user-defined grouping. The user can also load a list of genes with no metadata, and find overlapping genes highlighted in the pathway of interest.   Obtaining information about the pathway elements\r\n  Gene interactions (activation, repression, inhibition, expression, methylation, or unknown) are shown in different colors and line styles. PiiL allows for checking functional annotations for any gene in the pathway by loading information from GeneCards (http://www.genecards.org), NCBI Pubmed (http://www.ncbi.nlm.nih.gov/pubmed), or Ensembl (http://www.ensembl.org) into a web browser through one click.    Highlighting DNAm level differences\r\n  DNAm data is read with CpG sites as the rows, and beta values (estimate of methylation level using ratio of intensities between methylated and unmethylated alleles) in the columns. PiiL accepts data from whole genome bisulfite sequencing (Bismark [ 30 ] coverage files), as well as any of Illumina's Infinium methylation arrays (HumanMethylation27 BeadChip, HumanMethylation450 BeadChip or MethylationEPIC BeadChip). In any of the input formats, the CpG/probe IDs or positions need to be replaced with their annotated gene name. A Java application named PiiLer, also distributed with the software, uses pre-annotated files (done by Annovar [ 31 ]), to perform the conversion.  Genes are colored on a gradient from blue for low methylation levels (beta-value or methylation percentage), through white (for methylation level close to 0.5) to red when methylation levels approach 1. Once loaded, the metadata can be reused in different pathways.  Since there are typically multiple CpG sites per gene, additional information, such as the CpG ID, genomic position, and genomic location relative to a gene (for example intronic, exonic, upstream, UTR5, etc.) can be added to the gene name (separated with an underscore), allowing to quickly group sites by location and putative function. In this case, the methylation levels of all sites are averaged to set the color, and the gene border is colored green as an indication. The methylation status of each of the multiple sites hitting a gene can be viewed in a pop up window allowing the user to select or deselect specific sites to be included/excluded in the analysis. Figure 1 shows a snapshot of the PiiL screen.    Selecting a subset of CpG sites\r\n  PiiL allows for selecting a subset of CpG sites to be included in the analysis (i.e. for assigning the color for a specific gene, producing plots and etc.). There are multiple options for including/excluding specific CpG sites: a) Filtering out the CpG sites that have very little variation by choosing a threshold for the standard deviation of the beta values for each site over all samples. b) Selecting CpG sites based on user defined ranges for beta values. c) Selecting CpG sites based on their annotated genomic position. For example, selecting the CpG sites that are exonic, UTR5, etc. d) Providing a list of pre-selected CpG sites with the  CpG ID or genomic position.  These functions facilitate the visibility of the difference between the methylation levels of different groups of samples. Since averaging the beta values of all sites including the ones that do not vary significantly between the samples for color-coding, the differentiating signal is weakened and often difficult to detect. The genes with no CpG site present on the list of selected sites or no site passing the standard deviation filtering criteria are colored in gray.    Highlighting gene expression level differences\r\n  FPKM (Fragments Per Kilobase of transcript per Million mapped reads) gene expression values are the second type of metadata that can be loaded into a pathway. Genes are colored for each sample according to the log2-fold difference between the expression value of the current sample and the median of expression values of all samples. The user can set the difference scale; by default, ranging from −4 to +4. To make colors comparable with DNAm beta values, the n-fold over-expressed genes are colored in blue, and the n-fold underexpressed ones are colored in red, with white indicating little or no differences. We note that this color convention is inverse to expression-centric color schemes, but greatly facilitates finding patterns that are shared between DNAm and expression in case higher methylation correlates with lower or silenced expression.    Different view modes\r\n  There are three different view modes for reviewing the data and highlighting potential patterns: 1) single-sample view, 2) multiple-sample view and 3) group-wise view, where the median methylation/expression level is shown for each group of samples. More details can be found in the Additional file 1: S1.    Finding similar-patterns\r\n  The \u201cfind similar-patterns\u201d function allows for mining for genes with similar or dissimilar patterns of methylation or expression to any given gene or set of CpG sites, based on the Euclidian distance (check Additional file 1: S2).    Browsing pathway independent genes\r\n  Genes that are not part of any known pathway can be displayed in a grid of genes, termed PiiLgrid. While not constituting a connected pathway, all functionalities of PiiL are also applicable to that set of genes. This option is useful after finding the genes with identical methylation pattern to a targeted gene. The set of genes can be browsed in a new tab for further analysis, for example, comparing their expression level with the targeted gene.    General functions\r\n  For both methylation and expression values, the metadata over all samples can be viewed as a bar plot or histogram for each gene. In group-wise view, the members of each group are shown in the plots. Pathways, color-coded metadata and all the plots generated by PiiL can be exported to vector quality images in all viewing modes, which can be used in posters or publications. The manual page is accessible directly from the tool and users can send their feedback via the options in the tool. An option is provided to check for the latest available version and provides a downloadable runnable file of the latest version.  After checking multiple files in different pathways, a summary can be generated reporting the file name and the pathway that it was checked against followed by the list of matched genes.     Results\r\n  \u201cGlioma\u201d refers to all tumors that originate from glial cells, non-neuronal cells that support neuronal cells in the brain and nervous system. Gliomas are classified by the World Health Organization (WHO) as grades I to IV [ 32, 33 ]. Lower Grade Gliomas (LGG) comprises diffuse low-grade and intermediate-grade gliomas (WHO grades II and III), with a survival ranging widely from 1 to 15 years [ 34 ]. Glioblastoma multiform (GBM), also known as astrocytoma WHO grade IV, is the most common type of glial tumors in humans, and also the most fatal brain tumor with a median survival time of 15 months [ 35 ]. A recent study, however, reported this classification as obsolete. They identified a different grouping that is based on mutations in the IDH1 and IDH2 genes, which allows for a more accurate classification [ 14 ]. To examine the possible downstream effects in more depth, we extracted 65 and 100 samples with GBM and LGG from the TCGA (The Cancer Genome Atlas) datasets accordingly [ 34, 36 ], for which both methylation (profiled using Illumina's HumanMethylation450 BeadChip) and expression data are available (https://gdc-portal.nci.nih.gov/legacy-archive/search/f ).   Pathways at a glance\r\n  For a first assessment of the data, we examined the \u201ccytokine-cytokine receptor interaction\u201d subsection of the \u201cpathways in cancer\u201d expression network from KEGG (Fig. 2a), showing methylation of CpG sites that exhibit a standard deviation of more than 0.2 across all 165 samples, and grouping the data by IDH mutation status, i.e. wild-type, mutant, or unknown. Several genes are associated with CpG sites that drastically differ in methylation, shown in dark blue (unmethylated) and dark red (methylated), among them, ERBB2, a member of the epidermal growth factor (EGF) family and known to be associated with glioma susceptibility [ 37-40 ]. Gene expression of ERBB2 is also altered and 2-fold lower in the IDH mutant samples, as shown in dark red (Fig. 2b). We next examined methylation values across samples using the bar plot view feature and using different groupings according to recorded phenotypes or molecular alterations in Glioma studied by [ 41 ] (Fig. 3). Here, we can visually confirm that the mutation status of IDH is the best predictor for methylation (Fig. 3a). In addition, all samples without known IDH status are lowly methylated and could thus be putatively classified as 'wild-type'. By contrast, codeletion of chromosome arms 1p and 19q (1p/ 19q codeletion), reported to be associated with improved prognosis and therapy in low-grade gliomas patients [ 42 ], appears to have no effect on the methylation of ERBB2. Likewise, neither mutations in the promoter of the TERT (Telomerase Reverse Transcriptase) gene [ 41 ], nor the promoter methylation status of the gene encoding for repair enzyme O6-methylguanine-DNA methyltransferase (MGMT), which has been reported to be correlated with long-term survival in glioblastoma [ 43, 44 ], plays an obvious role in the methylation of this and other genes in the pathway.  For an overall survey of how many genes exhibit methylation patterns similar to ERBB2, we applied PiiL's \u201cfind similar-patterns\u201d feature, listing genes with the least Euclidian distance of beta values. The top three genes (Fig. 4) with the most similar patterns are FAS, a gene with a central role in the physiological regulation of programmed cell death; DAPK1, Calcium/calmodulindependent serine/threonine kinase involved in multiple cellular signaling pathways that trigger cell survival, apoptosis, and autophagy; and SMO, G protein-coupled receptor that probably associates with the patched protein (PTCH) to transduce the hedgehog proteins signal (http://www.genecards.org). There, we found that in FAS, SMO and ERBB2, the average expression level of the samples in IDH mutants is lower than the average expression level of the wild-type samples, while for DAPK1 the mutants exhibit higher expression levels. On the other end of the scale, BMP2 and BIRC5 host sites with the most distant pattern to ERBB2 (Fig. 4). BIRC5 is a member of the inhibitor of apoptosis gene family, negatively regulating proteins involved in apoptotic cell death (genecards.org). BMP2 is a member of transforming growth factor superfamily with a regulatory role in adult tissue homeostasis, reported to be significantly down-regulated in recurrent metastases compared to non-metastatic colorectal cancer [ 45 ]. Interestingly, expression of BMP2 is suppressed in wild type and unknown IDH status cancers, but high in some mutant samples in this data set.    DNA methylation and gene expression\r\n  To demonstrate the effect of selecting different subsets of CpG sites, we examined both PiiL's filters, as well as other DNAm analysis methods (Fig. 5). We first applied the unsupervised classification software Saguaro [ 21 ] to all CpG sites, detecting one pattern that perfectly coincides with IDH mutation status. Overall, genes with at least 10 CpG sites include MYADM, CFLAR, PAX6, FRMD4A, MEIS1, TNXB, MACROD1, CHST8, SRRM3, CPQ, TBR1, SYT6, RNF39, ISLR2, EML2, BCAT1, ACTA1, and, confirming results from our earlier visual inspection, ERBB2, which we examined earlier. The top pathways these genes are a part of include \u201cpathways in cancer\u201d, \u201cmTOR signaling\u201d, and \u201cTNF signaling\u201d. For the latter, we show the average methylation over all sites of all genes (Fig. 5a) and sites located upstream (Fig. 5b). Figure 5c shows the sites with a standard deviation smaller than 0.2, coloring genes without sites in light gray. The sites and genes identified by Saguaro (Fig. 5d); the log-fold changes in expression (Fig. 5e), and genes with sites exhibiting Speaman's correlation &lt; −0.7 between methylation and expression (Fig. 5d) are also shown for comparison.  Throughout this progression, we note that methylation values already change dramatically, mostly increasing, but in some cases decreasing, e.g. TNFRSF. In terms of correlation, we found four genes in this pathway at Spearman's rank correlation coefficient (rho) &gt; 0.7, TNFRSF, TRADD, MAP2K3, and CASP8, (Fig. 5f ) for which hypermethylation of the promoter has previously been reported [ 46 ]. Two of these genes coincide with Saguaro, which reports two additional genes, CFLAR and MAP3K8, but not TNFRSF and MAP2K3 (Fig. 5d).    Methylation blocks expression in pathways\r\n  Figure 6 shows the downstream part of the TNF signaling pathway that regulates or initiates the apoptosis pathway, consisting of FADD, CASP8 and CASP10, which regulates CASP7 and CASP3. Sequential cascadelike activation of caspases plays a central role in activating apoptosis, and both CASP3 and CASP7 appear downregulated or almost silenced. While both CASP10 and CASP8 are affected by changes in methylation, the beta values increase from less than 0.2 to more than 0.7 in CASP8 in the CpG sites selected by Saguaro. In addition, expression is highly negatively correlated with methylation (Spearman's rho = −0.81, p-value &lt;2*10−16), suggesting that CASP8 acts as the blocking factor in the expression cascade. None of CASP3, CASP7 or FADD, which are situated upstream in the pathway, are differentially methylated, and the decreased expression of FADD can possibly be explained by differential methylation/expression of the upstream TRADD gene.  An alternative way to visualize changes in a large number of samples is implemented in PiiL's 'playback' feature. After sorting the samples by methylation of CASP8 in increasing order, Additional file 2: Video S1 shows methylation and expression in the TNF signaling pathway, rendering TRADD, CASP8, CFLAR and MAP3K8 dark blue in the beginning (low methylation), and then sharply turning red when switching from Fig. 6 A subsection of the \u201cTNF signaling pathway\u201d leading into the apoptosis pathway, showing genes FADD, CASP10, CASP8, CASP7 and CASP3. FADD, CASP7 and CASP3 are not subject to DNAm changes when comparing IDH wildtype to mutants. CASP10 exhibits somewhat higher methylation levels, but remains stable at expression. By contrast, sites selected by Saguaro in CASP8 exhibit higher methylation levels in mutants, as well a correlated downregulation of expression. CASP3 and CASP7, which are downstream of the CASCADE from CASP8, are almost entirely silenced in expression showing wild type samples to IDH mutant samples. Expression changes follow methylation but more loosely, with several genes appearing blue (high expression) in the beginning, and transitioning to red (low expression) later on, as shown side by side with methylation in Additional file 2: Video S1.    Genes inside and outside of known pathways\r\n  Changes in methylation and expression can affect many genes, a large fraction of which may not be members of known pathways. To provide all analysis and visualization features for these genes as well, PiiL implements the \u201cPiiLgrid\u201d feature, which allows to display a any set of genes regardless of the pathway, but giving access to all analysis features. An example, genes that harbor sites similar to ERBB2, is shown in Fig. 7.     Conclusions\r\n  Advances in RNA and DNA sequencing allow for generating large amounts of RNA expression and DNA methylation data. Following the relatively inexpensive DNAm Bead Chip for human studies, we anticipate that genome-wide bisulfite sequencing will add more data and for a number of different organisms. While tools and methods for analyzing differential methylation and expression exist, any functional interpretation is best understood when integrating and visualizing the data in context of expression networks or pathways. PiiL is a browser for DNAm and RNA-Seq data, allowing direct comparison and testing specific hypotheses, in particular in model organisms for which pathway and expression network data exists. Its integrated analysis features provide the ability to quickly assess large amounts of data points, genes, and CpG sites, and navigating within and between pathways. Using the publicly available glioma data set, we have shown that a rich set of interesting aspects about this data is accessible with a few mouse clicks and within a few minutes. We thus anticipate that PiiL, and perhaps other interactive visualization tools, will be as common and widely used for epigenomic analyses as genome browsers are today for genomic analyses.    Additional files\r\n  Additional file 1: S1. View Modes. S2. Calculating the Euclidian distance. (DOCX 90 kb) Additional file 2: Video S1. PiiL showing DNA methylation and gene expression along samples. Color-coded data of all samples is shown consecutively using PiiL's playback feature. For each gene, the box on top shows methylation, and the box behind shows gene expression. For the methylation data, genes with sites selected by Saguaro are highlighted. The samples are sorted according to the ascending order of beta values for the CASP8 gene. (MP4 837 kb) Abbreviations GBM: Glioblastoma Multiform; IDH: Isocitrate dehydrogenase; KEGG: Kyoto Encyclopedia of Genes and Genomes; LGG: Lower Grade Glioma; TCGA: The Cancer Genome Atlas Acknowledgements We would like to thank Thomas Källman for testing our tool and introducing it to the research community at the Science for Life Laboratory and through the Bioinformatics Infrastructure for Life Sciences (BILS) in Sweden. We would extend our warmest thanks to the many testers and users of the software, in particular for their invaluable feedback, both on functionality and usability, which greatly helped in the development process.  Funding BT and JK were supported by a contract from FORMAS; the work was supported by a FORMAS grant to MGG. JK was supported in part by an eSSENCE grant and in part by a grant from the Polish National Science Centre [DEC-2015/16/W/NZ2/00314].  Availability of data and materials PiiL is coded in Java and runs on Mac OS, Linux and Windows. PiiL is open source and the source code is distributed under the GNU GPL v.3 license, available at the following GitHub repository: https://github.com/behroozt/PiiL.Authors' contributions BTM designed and implemented the software and user interface, with feedback from NZ and MGG. BTM, NZ, JK, and MGG performed the analyses described in this manuscript. All authors wrote the manuscript. All authors read and approved the final manuscript.  Ethics approval and consent to participate Not applicable.  Consent for publication Not applicable.  Competing interests All the authors declare they have no competing interests.    Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/behroozt/PiiL",
    "publicationDate": "0",
    "authors": [
      "Behrooz Torabi Moghadam",
      "Neda Zamani",
      "Jan Komorowski",
      "Manfred Grabherr"
    ],
    "status": "Success",
    "toolName": "PiiL",
    "homepage": ""
  },
  "62.pdf": {
    "forks": 82,
    "URLs": [
      "github.com/phe",
      "phenopolis.github.io/",
      "github.com/hail-is/hail",
      "github.com/phenopolis/pheno4j/",
      "github.com/SciGraph/SciGraph",
      "bridgestudy.medschl.cam.ac.uk"
    ],
    "contactInfo": ["n.pontikos@ucl.ac.uk"],
    "subscribers": 57,
    "programmingLanguage": "Scala",
    "shortDescription": "Scalable genomic data analysis.",
    "publicationTitle": "Pheno4J: a gene to phenotype graph database",
    "title": "Pheno4J: a gene to phenotype graph database",
    "publicationDOI": "10.1093/bioinformatics/btx397",
    "codeSize": 27204,
    "publicationAbstract": "Summary: Efficient storage and querying of large amounts of genetic and phenotypic data is crucial to contemporary clinical genetic research. This introduces computational challenges for classical relational databases, due to the sparsity and sheer volume of the data. Our Java based solution loads annotated genetic variants and well phenotyped patients into a graph database to allow fast efficient storage and querying of large volumes of structured genetic and phenotypic data. This abstracts technical problems away and lets researchers focus on the science rather than the implementation. We have also developed an accompanying webserver with end-points to facilitate querying of the database. Availability and implementation: The Java and Python code are available at https://github.com/phe nopolis/pheno4j. Contact: n.pontikos@ucl.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-10-14T13:44:28Z",
    "institutions": [
      "Computer Science Department",
      "Globe View",
      "University College London",
      "Moorfields Eye Hospital",
      "John Radcliffe Hospital"
    ],
    "license": "MIT License",
    "dateCreated": "2015-10-27T20:55:42Z",
    "numIssues": 125,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx397   Pheno4J: a gene to phenotype graph database     Sajid Mughal  1    Ismail Moghul  5    Jing Yu  4    UKIRDC    Tristan Clark  0    David S. Gregory  0    Nikolas Pontikos  2  3  6    0  Computer Science Department    1  Globe View ,  London, EC4V 3PP ,  UK    2  Institute of Ophthalmology, University College London ,  London, EC1V 9EL ,  UK    3  Moorfields Eye Hospital ,  London EC1V 2PD ,  UK    4  Nuffield Department of Clinical Neurosciences, University of Oxford, John Radcliffe Hospital ,  Oxford OX3 9DU ,  UK    5  UCL Cancer Institute, University College London ,  London WC1E 6DD ,  UK    6  UCL Genetics Institute, University College London ,  London WC1E 6BT ,  UK     2017   1  1  3   Summary: Efficient storage and querying of large amounts of genetic and phenotypic data is crucial to contemporary clinical genetic research. This introduces computational challenges for classical relational databases, due to the sparsity and sheer volume of the data. Our Java based solution loads annotated genetic variants and well phenotyped patients into a graph database to allow fast efficient storage and querying of large volumes of structured genetic and phenotypic data. This abstracts technical problems away and lets researchers focus on the science rather than the implementation. We have also developed an accompanying webserver with end-points to facilitate querying of the database. Availability and implementation: The Java and Python code are available at https://github.com/phe nopolis/pheno4j. Contact: n.pontikos@ucl.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  A recurring theme in clinical genetics is to annotate large numbers of genetic using the Variant Effect Predictor (VEP)  (McLaren et al., 2016)  variants from well phenotyped patients, and load them into a database for efficient querying and filtering. However, a sequenced human genome typically produces more than 4 million genetic variants per individual, at least 100 000 of which are novel. This introduces a number of challenges for the conventional relational table based database model, which are better handled by graph databases.  The first challenge for a relational database is the efficient storage and querying of many-to-many relationships such as genetic variant to individual relationships. In a relational database, genetic variants and individuals would typically be stored as rows in two distinct tables and, in order to link them, a third table, which would be a very large mapping table known as a 'join' table, would need to be queried. While workable with small relational databases, 'join' queries quickly become inefficient as the number of relationships increases. On the other hand, in a graph database, data is stored in a manner such that 'join' queries are not required. Instead of using tables, each data record is stored as a distinct node with added relationships linking nodes stored internally as pointers. As such, analysing the relationship between nodes representing individuals and nodes representing genetic variants, is as simple as finding the connected nodes. This implies that query time remains constant despite a growing number of relationships. Additionally, by supporting multiple types of relationships that can be labelled, graph databases have an intuitive schema (Fig. 1).  The second challenge for a relational database is the extensibility of the database schema. Since each genetic variant is associated with an increasing number of annotation sources, which tend to be sparse and not always consistently formatted, the schema of relational database would have to be redefined every time a new annotation source is added. In a graph database, the schema is dynamically extensible to accommodate new sources of information by adding new types of nodes, node attributes or relationships (see Supplementary Section S3). downloadedHPO HPOParser file  Finally, directed acyclic graph ontologies such as the Human Phenotype Ontology (HPO)  (Robinson et al., 2008)  and the Gene Ontology can be directly stored and queried in graph databases whereas complex operations would be required to achieve the same in a relational database.  In order to address these challenges, we have developed Pheno4J (https://github.com/phenopolis/pheno4j/), a tool implemented in Java that parses, integrates and imports genotype, annotated genetic variants and patient phenotype files into a Neo4J graph database. Using the Cypher querying language, it is then possible to perform sophisticated queries in real-time (Supplementary Section S2). In our live installation, we have loaded 5025 exomes and 4M variants. This amounts to 8M nodes, 487M relationships and 296M properties; which when stored in memory takes up approximately 20 Gb of memory (runtime 40 minutes). The scalability of our solution with respect to the number of exomes stored has been demonstrated in Supplementary Section S4.    2 Implementation\r\n  In order to build the database, a total of five data files are required as input. These include three user generated files and two publically available downloadable files (Fig. 1). Trimmed down versions of these files have been included in the GitHub repository for testing purposes. The three user generated input files are: \u2022 VCF file containing the person to genetic variant relationships. \u2022 JSON generated by the VEP containing the annotation for each variant. This produces the genetic variant-to-gene, transcript-togene and genetic variant-to-transcript relationships. \u2022 Phenotype CSV file containing the link from persons to HPO terms.  The other two input file are publicly downloadable: \u2022 The HPO ontology which is obtained automatically from the  HPO website. \u2022 The gene to HPO file that can be downloaded from the HPO website.  These files are parsed and then loaded into the database. Supplementary Section S1 shows the steps required for building and running the database.    3 Use cases\r\n  Once the database is loaded, the data can then be queried using the Cypher language. One basic application could be to identify rare damaging variants by filtering by frequency and Combined Annotation Dependent Depletion (CADD) score  (Kircher et al., 2014) . For example, returning all variants that have a frequency less than 0.001 and a CADD score greater than 20, yields 171,532 variants from our cohort of 6467 exomes (runtime 2.6 seconds). In Cypher this would be:   MATCH (gv:GeneticVariant)\r\n    WHERE gv.cadd_phred &gt; 20 AND gv.allele_freq &lt; 0.001 AND gv.kaviar_AF &lt; 0.0001\r\n    RETURN count(gv.variantId);\r\n  Another application could be to identify related individuals by counting the number of rare heterozygous variants shared with 'person1'. Here we return the list of ten individuals by decreasing shared rare variant count (runtime 1.2 seconds).    MATCH (k:Person)\r\n    WITH count(k) as numberOfPeople\r\n    MATCH (p:Person {personId:\u201dperson1\u201d})&lt;\r\n  [:HetVariantToPerson]-(gv:GeneticVariant)    WHERE gv.allele_freq &lt; 0.001 AND gv.kaviar_AF &lt; 0.001\r\n    WITH size(()&lt;-[:HetVariantToPerson]-(gv)) as het_count, gv, p, numberOfPeople\r\n    WHERE het_count &gt; 1 AND ((het_count/\r\n  toFloat(numberOfPeople)) &lt; ¼ 0.05) //Sharing of variants with Person q.    MATCH (gv)-[:HetVariantToPerson]- &gt; (q:Person)\r\n    WHERE p &lt; &gt; q\r\n    WITH p,q,count(gv) as c\r\n    ORDER BY c desc LIMIT 10\r\n    RETURN p.personId,q.personId, c;\r\n  In this query, we make use of Cypher's efficient edge counting size(()&lt;-[:HetVariantToPerson]-(gv)) as het_count to get the number of outgoing relationship of a node.  Another use case might be to find all individuals with a given HPO term such as for example 'Retinal dystrophy' (HP:0000556), which returns 521 individuals in our cohort (runtime 0.3 seconds). This query returns all individuals including those that might be annotated with a child term (direct or indirect) of 'Retinal dystrophy' such as 'macular dystrophy'.    MATCH (p:Term {name: 0Retinal dystrophy0})\r\n  [:TermToDescendantTerms]- &gt; (q:Term)&lt;[:PersonToObservedTerm]-(r:Person)    RETURN r.personId;\r\n  In order to obtain a list of candidate variants, we can query all rare damaging homozygote variants seen in people with 'Retinal dystrophy' that belong to a known 'Retinal dystrophy' gene. In our data, this returns 69 distinct variant ids (runtime &lt;1 second).    MATCH (p:Term {name:\u2019Retinal dystrophy\u2019})[:TermToDescendantTerms]- &gt; (q:Term)&lt;-[:GeneToTerm](gs:Gene)\r\n    WITH distinct gs as retinal_dystrophy_genes\r\n    MATCH (retinal_dystrophy_genes)\r\n  [:GeneToGeneticVariant]- &gt; (gv:GeneticVariant)    WHERE gv.allele_freq &lt; 0.001 AND gv.cadd_phred &gt; 20 AND gv.kaviar_AF &lt; 0.001\r\n    WITH distinct gv, retinal_dystrophy_genes\r\n    MATCH (r:Person)&lt;-[:HomVariantToPerson]-(gv)\r\n    WITH distinct gv, retinal_dystrophy_genes, r\r\n    MATCH (p:Term {name:\u2019Retinal dystrophy\u2019})[:TermToDescendantTerms]- &gt; (q:Term)&lt;[:PersonToObservedTerm]-(r)\r\n    RETURN distinct r.personId, gv.variantId, retinal_dystrophy_genes.gene_name;\r\n  Conversely, we can suggest 'Retinal dystrophy' as a phenotype for individuals that have rare damaging homozygote variants in recessive retinal dystrophy genes:    MATCH (p:Term)-[:TermToDescendantTerms]- &gt; (q:Term)&lt;[:PersonToObservedTerm]-(r:Person) WHERE p.name=\u2019Retinal dystrophy\u2019\r\n    WITH COLLECT(distinct r) as persons\r\n    MATCH (p:Person) WHERE NOT p IN persons WITH COLLECT(p) as non_retinal_dystrophy_persons_list\r\n    MATCH (t:Term)&lt;\u2013(g:Gene)\u2013 &gt; (t2:Term) WHERE t.name=\u2019Retinal dystrophy\u2019 AND t2.name=\u2019Autosomal recessive inheritance\u2019\r\n    WITH COLLECT(DISTINCT g) as recessive_retinal_dystrophy_genes_list, non_retinal_dystrophy_persons_list\r\n    UNWIND recessive_retinal_dystrophy_genes_list as\r\n  recessive_retinal_dystrophy_genes    MATCH (recessive_retinal_dystrophy_genes)\u2013 &gt; (gv:GeneticVariant)\r\n    WHERE gv.allele_freq &lt; 0.001 AND gv.cadd_phred &gt; 25 AND gv.kaviar_AF &lt; 0.0001 WITH gv, non_retinal_dystrophy_persons_list\r\n    UNWIND non_retinal_dystrophy_persons_list as non_retinal_dystrophy_persons\r\n    MATCH (recessive_retinal_dystrophy_genes)\u2013 &gt; (gv)[:HomVariantToPerson]&gt; (non_retinal_dystrophy_persons)\r\n    RETURN distinct gv.variantId, gv.most_severe_conse\r\n  quence, gv.cadd_phred, gv.kaviar_AF, recessive_retinal_dystrophy_genes.gene_name, non_retinal_dystrophy_persons.personId ORDER BY gv.cadd_phred DESC; Further queries and analysis are described in the Supplementary Section S2.     4 Discussion\r\n  Currently, there are bespoke tools such as GQT  (Layer et al., 2016)  and BGT  (Li, 2016) , which are very space efficient and excel at querying large annotated VCFs in real-time.  However, they do not build on existing database technologies making them harder to extend and do not support HPO querying.  At the other end of the spectrum, there are also solutions such as hail.is (https://github.com/hail-is/hail) for very large genomic datasets that are distributed over a cluster of computers. However, these require access to significant infrastructure and the overheads of installation are not trivial. Our solution requires minimal programming and is particularly suitable to a web front end for interrogating the data of an exome database of around 10 000 well phenotyped individuals as typical in rare disease groups such as BRIDGE (https://bridgestudy.medschl.cam.ac.uk) and the UK Inherited Retinal Disease Consortium. We are using Pheno4J as a backend for our Phenopolis (https://phenopolis.github.io/) platform to replace our current NoSQL MongoDB. While still relatively in their infancy, we predict graph databases will become pervasive in biology with a growing number of projects adopting them (Pep Tracker DB (https://www.peptracker.com), OwlSim (www.owlsim.org) and SciGraph (https://github.com/SciGraph/SciGraph)).    Acknowledgements\r\n    Funding\r\n  IM is supported by the Biotechnology and Biological Sciences Research Council [grant number BB/M009513/1]. NP and JY are supported by the UK Inherited Eye Disease Consortium, funded by Retinitis Pigmentosa Fighting Blindness and Fight for Sight. NP is also funded by the National Institute for Health Research Biomedical Research Centre at Moorfields Eye Hospital, the National Health Service Foundation Trust, and the UCL Institute of Ophthalmology.  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/hail-is/hail",
    "publicationDate": "0",
    "authors": [
      "Sajid Mughal",
      "Ismail Moghul",
      "Jing Yu",
      "UKIRDC",
      "Tristan Clark",
      "David S. Gregory",
      "Nikolas Pontikos"
    ],
    "status": "Success",
    "toolName": "hail",
    "homepage": "https://hail.is"
  },
  "32.pdf": {
    "forks": 0,
    "URLs": ["github.com/atiselsts/spacescanner.Contact:"],
    "contactInfo": ["egils.stalidzans@lu.lv"],
    "subscribers": 2,
    "programmingLanguage": "Component Pascal",
    "shortDescription": "SpaceScanner: biological model optimization space scanner",
    "publicationTitle": "SpaceScanner: COPASI wrapper for automated management of global stochastic optimization experiments",
    "title": "SpaceScanner: COPASI wrapper for automated management of global stochastic optimization experiments",
    "publicationDOI": "10.1093/bioinformatics/btx363",
    "codeSize": 38613,
    "publicationAbstract": "Motivation: Due to their universal applicability, global stochastic optimization methods are popular for designing improvements of biochemical networks. The drawbacks of global stochastic optimization methods are: (i) no guarantee of finding global optima, (ii) no clear optimization run termination criteria and (iii) no criteria to detect stagnation of an optimization run. The impact of these drawbacks can be partly compensated by manual work that becomes inefficient when the solution space is large due to combinatorial explosion of adjustable parameters or for other reasons. Results: SpaceScanner uses parallel optimization runs for automatic termination of optimization tasks in case of consensus and consecutively applies a pre-defined set of global stochastic optimization methods in case of stagnation in the currently used method. Automatic scan of adjustable parameter combination subsets for best objective function values is possible with a summary file of ranked solutions. Availability and implementation: https://github.com/atiselsts/spacescanner.Contact: egils.stalidzans@lu.lv Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-08-18T20:16:33Z",
    "institutions": [
      "University of Bristol",
      "University of Latvia"
    ],
    "license": "No License",
    "dateCreated": "2016-01-12T18:45:11Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx363   SpaceScanner: COPASI wrapper for automated management of global stochastic optimization experiments     Atis Elsts  0    Agris Pentjuss  1    Egils Stalidzans  1    0  Department of Electrical and Electronic Engineering, University of Bristol ,  Bristol BS8 1UB ,  UK    1  Institute of Microbiology and Biotechnology, University of Latvia ,  Riga LV-1004 ,  Latvia     2017   1  1  2   Motivation: Due to their universal applicability, global stochastic optimization methods are popular for designing improvements of biochemical networks. The drawbacks of global stochastic optimization methods are: (i) no guarantee of finding global optima, (ii) no clear optimization run termination criteria and (iii) no criteria to detect stagnation of an optimization run. The impact of these drawbacks can be partly compensated by manual work that becomes inefficient when the solution space is large due to combinatorial explosion of adjustable parameters or for other reasons. Results: SpaceScanner uses parallel optimization runs for automatic termination of optimization tasks in case of consensus and consecutively applies a pre-defined set of global stochastic optimization methods in case of stagnation in the currently used method. Automatic scan of adjustable parameter combination subsets for best objective function values is possible with a summary file of ranked solutions. Availability and implementation: https://github.com/atiselsts/spacescanner.Contact: egils.stalidzans@lu.lv Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Jonathan Wren    1 Introduction\r\n  Global stochastic optimization methods that are implemented in COPASI software tool  (Hoops et al., 2006)  are popular due to their applicability independent of the nonlinearity of the model. Another important feature is easily switching between global stochastic methods as no transformation of the original problem is needed  (Moles et al., 2003) . At the same time, their application may lead to incorrect conclusions in the case of inappropriate optimization duration, optimization method or its settings. The performance of a global stochastic optimization method also depends on the type of problem  (Balsa-Canto et al., 2012)  and testing several methods for the same task is important. Parallel optimization runs have been applied to address the drawbacks of global stochastic optimization methods by studying their convergence  (Kostromins et al., 2012) , convergence and stagnation criteria  (Mozga and Stalidzans, 2011) , automatic detection of stagnation and consensus  (Sulins and Mednis, 2012) . SpaceScanner (abbreviated from 'adjustable parameter solution space scanner') is a handy optimization automation and analysis oriented COPASI wrapper with graphical user interface for systems biology specialists without programing background. It supports: (i) parallel optimization runs with automated recognition of consensus and stagnation situations, (ii) automatic switching between different user-selected global stochastic optimization methods in case of stagnation in the current method, (iii) determination of the best sets of adjustable parameters  (Stalidzans et al., 2016)  for a pre-set range of a number of adjustable parameters in combination and (iv) search for the minimal number of adjustable parameters that can reach the requested fraction of the total optimization potential  (TOP approach; Stalidzans et al., 2016) . Parallel optimization runs can be initiated using the COPASI wrapper Condor-COPASI \u2022 \u2022  (Kent et al., 2012)  while the rest of SpaceScanner functionality is not supported. Furthermore SpaceScanner differs as it (i) can be run locally on the user's PC since it does not require a complex installation, (ii) guarantees real-time simultaneity of parallel optimizations via CPU core load-balancing and (iii) graphs the results in real-time, allowing for better interactivity.    2 Implementation and features\r\n  The kinetic model file of interest has to be in COPASI format (.cps)  (Hoops et al., 2006)  and set up for execution of optimization task: must include an objective function (optimization criterion) and a list of adjustable parameters with a permitted range of values (Supplementary Material S1). Internally, SpaceScanner uses COPASI to execute the optimizations.  SpaceScanner helps to automate execution of multiple parallel optimization runs by starting multiple identical COPASI optimizations (they differ during execution because of the stochastic component). The tool is capable of automatic termination when the runs have reached nearly identical (consensus) values  (Sulins and Mednis, 2012) , and automated change of the optimization method in case of stagnation  (Sulins and Mednis, 2012) . The user is able to observe the history of optimization runs in dynamically updated graphs (Fig. 1).  SpaceScanner can be configured to automatically analyse the space of all or user defined subsets from the set of adjustable parameter combinations. This is useful for more advanced automation tasks, e.g. to rank the subsets according to their objective function value. This is handy when a limited number of parameters from the adjustable parameter set can be improved during strain engineering (e.g. because of biological constraints or economical limitations). Another application of this functionality is to determine the minimal subset of parameters that gives 'good enough' results according user-defined rules, e.g. that gives 90% of TOP  (Stalidzans et al., 2016) .  SpaceScanner outputs the following per optimization task: a .csv file with optimization results, with a row for each parameter subset evaluated; a .log file with SpaceScanner execution history.  SpaceScanner outputs the following for each optimized set of parameters: a .txt file for each optimization run with COPASI optimization history; \u2022 a .cps COPASI file for each optimization run with the parameters set to their best (final) values.  SpaceScanner at the moment supports a greedy and exhaustive (bruteforce) search of all adjustable parameter subsets. More advanced search strategies (e.g. global stochastic search, parameter sensitivity-informed search, MFA-informed search) are planned as future additions.  SpaceScanner is implemented in Python and works on all major operating systems, is easy to use and configure, and features two interfaces: \u2022 \u2022 a command-line interface that expects a configuration file in JSON format as the only argument; a platform-independent web interface that allows the user to interactively configure, start, and stop optimization runs, as well as see their results graphically (Fig. 1).    3 Discussion\r\n  The automation of optimization by global stochastic methods and automated representation of results are the main SpaceScanner features that are very important when many combinations of parameters are being analysed (solution space scanning) and/or in case of a model with unknown optimization convergence speed.  Automatic termination of optimization runs in combination with change of optimization methods in case of stagnation enables automatic analysis of a minimal set of adjustable parameters to reach a pre-defined fraction of TOP  (Stalidzans et al., 2016)  (case study in Supplementary Material S2) generating a ranked summary file with thousands of analysed combinations.  SpaceScanner can have more conservative settings: (i) increasing number of parallel runs, (ii) reducing consensus corridor and (iii) increasing delay time settings. Thus, if high confidence about reaching global optima is needed (still with no guarantee), conservative consensus criteria may be used requiring increased time and computational costs of optimization. When looking for a fast scan of solution space to find, for instance, the minimal set of parameters  (Stalidzans et al., 2016) , the settings may be more relaxed (case study in Supplementary Material S3).  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/atiselsts/spacescanner",
    "publicationDate": "0",
    "authors": [
      "Atis Elsts",
      "Agris Pentjuss",
      "Egils Stalidzans"
    ],
    "status": "Success",
    "toolName": "spacescanner",
    "homepage": ""
  },
  "75.pdf": {
    "forks": 1,
    "URLs": [
      "github.com/ThakarLab/FIGS",
      "github.com/Thakar-Lab/FIGS",
      "github.com/Thakar-Lab/FIGS-Influenza.In"
    ],
    "contactInfo": ["juilee_thakar@urmc.rochester.edu"],
    "subscribers": 0,
    "programmingLanguage": "Matlab",
    "shortDescription": "Fuzzy Inference of Gene-Sets (FIGS) ",
    "publicationTitle": "Meta-analysis of cell- specific transcriptomic data using fuzzy c-means clustering discovers versatile viral responsive genes",
    "title": "Meta-analysis of cell- specific transcriptomic data using fuzzy c-means clustering discovers versatile viral responsive genes",
    "publicationDOI": "10.1186/s12859-017-1669-x",
    "codeSize": 7449,
    "publicationAbstract": "Background: Despite advances in the gene-set enrichment analysis methods; inadequate definitions of gene-sets cause a major limitation in the discovery of novel biological processes from the transcriptomic datasets. Typically, gene-sets are obtained from publicly available pathway databases, which contain generalized definitions frequently derived by manual curation. Recently unsupervised clustering algorithms have been proposed to identify gene-sets from transcriptomics datasets deposited in public domain. These data-driven definitions of the gene-sets can be context-specific revealing novel biological mechanisms. However, the previously proposed algorithms for identification of data-driven gene-sets are based on hard clustering which do not allow overlap across clusters, a characteristic that is predominantly observed across biological pathways. Results: We developed a pipeline using fuzzy-C-means (FCM) soft clustering approach to identify gene-sets which recapitulates topological characteristics of biological pathways. Specifically, we apply our pipeline to derive genesets from transcriptomic data measuring response of monocyte derived dendritic cells and A549 epithelial cells to influenza infections. Our approach apply Ward's method for the selection of initial conditions, optimize parameters of FCM algorithm for human cell-specific transcriptomic data and identify robust gene-sets along with versatile viral responsive genes. Conclusion: We validate our gene-sets and demonstrate that by identifying genes associated with multiple genesets, FCM clustering algorithm significantly improves interpretation of transcriptomic data facilitating investigation of novel biological processes by leveraging on transcriptomic data available in the public domain. We develop an interactive 'Fuzzy Inference of Gene-sets (FIGS)' package (GitHub: https://github.com/Thakar-Lab/FIGS) to facilitate use of of pipeline. Future extension of FIGS across different immune cell-types will improve mechanistic investigation followed by high-throughput omics studies.",
    "dateUpdated": "2017-03-21T16:14:22Z",
    "institutions": [
      "601 Elmwood Avenue",
      "University of Rochester"
    ],
    "license": "MIT License",
    "dateCreated": "2017-03-21T13:56:08Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Khan et al. BMC Bioinformatics     10.1186/s12859-017-1669-x   Meta-analysis of cell- specific transcriptomic data using fuzzy c-means clustering discovers versatile viral responsive genes     Atif Khan  2    Dejan Katanic  2    Juilee Thakar  juilee_thakar@urmc.rochester.edu  0  1  2    0  601 Elmwood Avenue ,  Rochester, NY 14618 ,  USA    1  Department of Biostatistics and Computational Biology, University of Rochester ,  Rochester, NY 14642 ,  USA    2  Department of Microbiology and Immunology, University of Rochester ,  Rochester, NY 14642 ,  USA     2017   18  2  14    3  5  2017    12  12  2016     Background: Despite advances in the gene-set enrichment analysis methods; inadequate definitions of gene-sets cause a major limitation in the discovery of novel biological processes from the transcriptomic datasets. Typically, gene-sets are obtained from publicly available pathway databases, which contain generalized definitions frequently derived by manual curation. Recently unsupervised clustering algorithms have been proposed to identify gene-sets from transcriptomics datasets deposited in public domain. These data-driven definitions of the gene-sets can be context-specific revealing novel biological mechanisms. However, the previously proposed algorithms for identification of data-driven gene-sets are based on hard clustering which do not allow overlap across clusters, a characteristic that is predominantly observed across biological pathways. Results: We developed a pipeline using fuzzy-C-means (FCM) soft clustering approach to identify gene-sets which recapitulates topological characteristics of biological pathways. Specifically, we apply our pipeline to derive genesets from transcriptomic data measuring response of monocyte derived dendritic cells and A549 epithelial cells to influenza infections. Our approach apply Ward's method for the selection of initial conditions, optimize parameters of FCM algorithm for human cell-specific transcriptomic data and identify robust gene-sets along with versatile viral responsive genes. Conclusion: We validate our gene-sets and demonstrate that by identifying genes associated with multiple genesets, FCM clustering algorithm significantly improves interpretation of transcriptomic data facilitating investigation of novel biological processes by leveraging on transcriptomic data available in the public domain. We develop an interactive 'Fuzzy Inference of Gene-sets (FIGS)' package (GitHub: https://github.com/Thakar-Lab/FIGS) to facilitate use of of pipeline. Future extension of FIGS across different immune cell-types will improve mechanistic investigation followed by high-throughput omics studies.    Epithelial cells  Dendritic cells  Gene-sets  Influenza infections  Gene-gene mutual information   Overlapping gene-sets       Background\r\n  Microarrays and RNA-seq have made simultaneous expression profiling of many thousands of genes across several experimental/clinical conditions widely accessible. However, interpreting the profiles from such large numbers of genes remains a key challenge. An important conceptual advance in this area was a shift from a focus on differential expression of single genes to testing sets of biologically related genes [ 1-5 ]. Gene-sets are defined a priori as sharing some biologically relevant properties (e.g. members of the same pathway, having a common biological function, presence of a binding motif, etc.). In addition to the obvious advantage in interpretability, a key benefit of analyzing gene-sets compared with individual genes is that small changes in gene expression are unlikely to be captured by conventional single-gene approaches, especially after correction for multiple testing [ 1 ].  Despite advances in the methods for gene-set enrichment analysis [ 2, 6-8 ]; inadequate definitions of gene-sets cause a major limitation in the discovery of novel biological processes. Typically, gene-sets are obtained from pathway databases available in the public domain such as Kyoto Encyclopedia of Genes and Genomes (KEGG). However, recent advances have led to development of data-driven approaches to identify gene-sets [ 9-13 ]. These are powerful approaches that expand search for biological mechanisms based on datasets in public domain leading path towards discovery.  The data-driven identification of gene-sets is performed by measuring pair-wise co-expressions or association between genes which is followed by different, typically unsupervised hard (such as K-means and hierarchical) clustering approaches [ 14-17 ]. However, there are two limitations- first, biological pathways show a large overlap pertaining to the modular structure of signal transduction processes which is not reproduced by hard clustering algorithms and second, functional interpretation of novel gene-sets is difficult if they are not enriched in any known pathways. Here we propose a computational pipeline (Fig. 1) based on Fuzzy C-Means (FCM) clustering method [ 18, 19 ] which allows overlap across gene-sets, thus reproducing the observed topology of biological pathways, and associate novel gene-sets to other gene-sets with enrichment of known pathways. Particularly, we apply the FCM pipeline to our previously curated context-specific data [ 20 ]. To facilitate use of our pipeline we developed a downloadable 'Fuzzy Inference of Gene-sets (FIGS)' package available at GitHub (https://github.com/ThakarLab/FIGS). Here, we demonstrate its application using transcriptomic data obtained from Gene Expression Omnibus (GEO) measuring response to infections of monocyte derived dendritic cells (DC) and A549 epithelial cells (EC) with influenza virus [ 20 ]. The genesets and overlapping genes identified in this study are validated by assessing enrichments of known pathway genes. Thus, robust data-driven gene-sets identified by FIGS retain the characteristics of known pathways and expand the search of new mechanisms.    Methods\r\n   Datasets\r\n  Transcriptomic data was obtained from GEO and was integrated in cell-specific manner. Integration procedure and calculations of associations between genes has been described in detail previously [ 20 ]. Briefly, transcriptomic data measuring changes in gene-expression in monocyte derived dendritic cells (DC) and A549 epithelial cells (EC) upon influenza infections were used. There were two datasets for DCs (GSE41067 and GSE55278) and 9 datasets for ECs (GSE19580, GSE31469, GSE31470, GSE31471, GSE31472, GSE31473, GSE31474, GSE31518 and GSE47937). All the datasets were log2 transformed and quantile normalized individually in a platform specific manner as described previously [ 20 ]. To facilitate comparison across independent datasets, 14,894 genes commonly present across all the studies were used in this analysis. Fold changes in influenza infected samples were calculated relative to the noninfected samples and genes with absolute fold change &gt; 1 in atleast one sample were kept. After this filtration, 3846 and 5789 genes were present in EC and DC dataset respectively. Mutual information (MI) was calculated to describe the associations between 3846 and 5789 genes within EC and DC respectively [ 20-22 ]. The computational pipeline proposed below was developed on DC data and was applied to EC data. Moreover, for comparison and validation of our method we used filtered set of immunologically relevant pathways from Kyoto Encyclopedia of Genes and Genomes (KEGG) [ 8, 23 ].  Where, µk is the centroid of the kth cluster and xi is the ith observation.  Unlike hard clustering techniques, FCM method [ 18, 19 ] allows a data point to belong to multiple clusters. FCM is a soft version of k-means, where each data point has a fuzzy degree of belonging to each cluster. The fuzzy degree of belongingness ranges from 0 to 1 where 0 shows no association and 1 shows complete association of a data point to the corresponding cluster. The FCM was performed with the following objective function:  n J ¼ X  c  Xwimj xi−cj i¼1 j¼1 Where, wimj ¼  1 Xk¼1 kxi−cjk c kxi−ckk 2 m2−1  Thus, FCM algorithm assigned genes to one or more clusters with different degrees of memberships.    Optimization of fuzzy c-means clustering parameters\r\n  Determination of the initial number of clusters is a question of ongoing debate, especially when overlap (fuzzy) across clusters is expected [ 26 ]. We determine the initial number of clusters (K) by taking into account average number of genes per cluster based on known pathways and the underlying structure of data from principal component analysis (PCA) [ 27 ]. Specifically, for DC and EC first 50 principal components explained &gt;90% of the total variance. Hence, we used equivalent (50) number of clusters for the following analysis. Note that, the algorithm could converge to a different number of clusters, than what had been defined initially. These clusters are referred to as gene-sets in the results section due to their usability in gene-set enrichment analysis.  FCM requires three additional pre-defined parameters: fuzziness (the amount of overlap between the clusters), initial cluster centroids and cluster association criteria which is specific to the data distribution [ 28 ]. The fuzziness and cluster association are inversely related since fuzziness defines the belongingness of the genes to specific clusters. Thus, the selection of fuzziness and the    Soft and hard unsupervised cluster analysis\r\n  To assess the usability of the FCM clustering to identify gene-sets, it was compared with previously used hard clustering approaches [ 20 ]. Particularly, k-means clustering [ 24, 25 ] was performed with the following objective function: ð1Þ ð2Þ ð3Þ clusters' association determines the size and amount of overlap between the clusters. Here, the objective was to identify the functionally related genes which typically range from 100 to 500 depending on the biological process [ 29 ]. The length of 45 selected immunologically relevant KEGG pathways ranged from 23 to 362 with an average of 100 genes. Fuzziness (m) ranging from 1.1 to 1.5 was evaluated. Fuzziness m = 1.1 preserved strong primary association of a gene to one cluster and intermediate association to another (Fig. 2a). With m &gt; 1.1, the average membership value per cluster decreased thus increasing the uncertainty in gene-sets (Fig. 2a). Also, the size of the clusters increased with m (Fig. 2b), making functional interpretations difficult. Thus, in the following analysis fuzziness (m) was set to 1.1.  The threshold for associating genes to the clusters was determined by evaluating distribution of membership values of genes across 50 clusters. Specifically, the ith gene gi belonged to the clusters for which it had membership values greater than (μi + σi), where μi and σi are mean and standard deviation of membership values of gi respectively.    Ward\u2019s minimum variance assigns robust initial cluster\r\n  centroids Typically, random initial assignment of the cluster centroids is used in FCM algorithms [ 28, 30 ]. However, previous studies and our analysis shows that random initialization leads to inconsistent and unreliable clustering results [ 31, 32 ]. In our analysis, only 16% of the clusters were consistent across all 50 iterations of the FCM upon random initialization of the centroids (Fig. 2c). The variation in clustering solutions across 50 iterations showed that FCM is sensitive to initial assignment of the cluster centers and that solution frequently converged at local minima instead of finding the global optimal solution. To overcome this problem, Ward's minimum variance method was used to estimate the initial centers for FCM which produced stable and consistent clusters [ 33 ]. Ward's method (based on analysis of variance) minimized the total within-cluster variance and maximized betweenclusters variance. Cluster membership was evaluated by calculating the total sum of squared deviations from the mean of a cluster. At the initial step, all clusters were singletons (each cluster containing a single gene), which were merged in each next step so that the merging contributed least to the variance criterion. This distance measure called the Ward distance was defined by:  na : nb dab ¼ na þ nb : kxa − xb k2 ð4Þ  Where a and b denote two specific clusters, na and nb denote the number of data points in the two clusters. xa and xb denote the cluster centroids and \u2016\u2016 is the Euclidean norm.  Ward's method produced hierarchical cluster tree that was cut to produce 50 hard clusters where each gene was fully associated to a unique cluster. The centroids of these 50 clusters were calculated and used for FCM initialization. It was found that the objective function of Ward-optimized FCM solution not only converged faster than that of randomly assigned initial centroids (Fig. 2d) but also provided a stable clustering solution.    Cluster validation and enrichment with KEGG pathways\r\n  The clusters of genes identified by FIGS were tested for their cohesiveness and biological significance. To test the cohesiveness of the clusters a weighted clustering coefficient (CC) was measured. CC provided a measure of the degree of relatedness between the genes in a cluster. The tendency of genes in the cluster to tightly knit groups was estimated by a ratio of means of CCs calculated using only genes in the cluster over all the genes [ 34, 35 ]. CC was calculated using functions from gaimc library in MATLAB. The ratios were compared for k-means, Ward's hierarchical method, and FCM solutions.  We expect that the clusters of genes identified in this study are to be functionally related. In other words, genes belonging to the same pathways were expected to group together. To test this hypothesis, we evaluated whether genes belonging to a same known immunologically relevant pathway cluster together [ 36 ]. A set of 44 immunologically relevant pathways obtained from KEGG database along with interferon stimulated genes set (ISGs) defined by Schoggins [ 37, 38 ] were compared with the clusters identified by FCM pipeline using hypergeometric test [ 39, 40 ].     Results\r\n   Identification of the gene-sets by FCM\r\n  Signalling pathways from public repositories are generalized static instances of cascades that are frequently derived by curation. Increasing use of high-throughput assays in the biomedical field allows identification of context-specific set of functionally related genes, which can be loosely defined to include genes regulated by a same set of transcription factors or sets of genes involved in same pathways. Recently, use of clustering algorithms has been proposed to identify the \u201cfunctionally related genes\u201d or gene-modules from publicly available transcriptomics datasets [ 11, 12, 41 ]. However, frequently used algorithms such as K-means and hierarchical clustering, for this purpose do not allow overlap between the clusters (referred as gene-sets in rest of the manuscript), although such overlap between biological pathways is inevitable given modular topology of biological response [ 42 ]. Specifically, 44 immunologically relevant pathways from KEGG databases suggest a minimum of 0% and maximum of 63% overlap between the two pathways (Fig. 3a). For example, Cytokine-Cytokine receptor interaction and JAK-STAT signaling pathways have 96 genes in common. Interestingly, some genes like AKT1, MAPK1, PIK3CA, and TNF were found involved in more than 10 different pathways (Fig. 3b). Other antiviral genes like IFNA1, IFNB1, NFKBIA, and IL6 were found involved in at least 5 different pathways.  Here we propose to use FCM not only to identify viral responsive gene-sets to the influenza infection but also to identify the genes overlapping across different genesets. FCM is a soft version of K-means clustering that a allows overlap between the gene-sets reproducing the topology of the known pathways. Here we optimized the parameters on DC dataset and validated those on EC dataset (refer to methods). FCM pipeline described in methods led to an average size of gene-sets 167 (standard deviation of 45), with smallest gene-set having 63 and largest gene-set having 230 genes. With this configuration one third of the genes exhibited overlapping behavior where 1943 out of 5789 genes belonged to more than one gene-sets (Fig. 3c and d).    Validation of FCM Gene-Sets\r\n  To assess if gene-sets identified by FCM pipeline indeed grouped the functionally related genes, we compared the FCM-gene-sets with the pathways defined in KEGG and by Schoggins [ 37, 38 ]. Schoggins-gene-set defines Interferon Stimulated Genes (ISGs) and has been reported to be significantly enriched upon influenza infections by previous studies [ 8, 23 ]. 43 out of 50 FCM-gene-sets were found enriched in at least one of the pathways (p value &lt;0.01) (Fig. 4a and b). FCMgene-sets DC21, DC26, DC36 and DC45 were found significantly enriched with ISGs (p values 3.3 e− 10, 1.19 e− 11, 5.36 e− 29 and 1.72 e− 60 respectively). Cluster 45 was also found enriched with RIG-I-Like and TollLike receptor signaling pathways (p values 3.04 e− 6 and 1.32 e− 5) which are critical pathogen recognition receptor mediated pathways known to be induced upon viral infections [ 23 ]. Similarly, gene-set DC42 was enriched with other well-known anti-viral pathways (JAK-STAT, Chemokine and Cytokine-Cytokine signaling pathways (p values 4.69 e− 6, 1.5 e− 6 and 3.22 e− 16 respectively)). The enrichment results indeed corroborates with the previously published results validating FCM-gene-sets [ 20, 23 ]. Interestingly, there were 7 (gene-sets DC1, DC3, DC4, DC9, DC19, DC34 and DC35) novel sets, which were not significantly enriched in any of the pathways. Most of these gene-sets had genes a c overlapping with other gene-sets enriched in known pathways, suggesting multi-functionality of the overlapping genes (Additional file 1: Figure S1). Thus, FCM pipeline not only validated previously known functionally related genes but also identified new sets of genes.    Genes associated with multiple gene-sets are identified by FCM-pipeline\r\n  FCM pipeline was developed to find genes that are associated with multiple gene-sets. There were 1943 overlapping genes associated with minimum 2 and maximum 5 gene-sets. Interestingly 113 genes involved in multiple KEGG pathways were also found by our pipeline (Table 1). While involvement of genes across multiple KEGG pathways is not evidence for the multifunctionality of the genes it is the only available data for systematic comparison. Indeed, gene like PIK3R1 involved in 14 pathways (Table 1) could be due to bias in the studies associated with that gene. Genes overlapping between the gene-sets DC45 (82 genes) and DC36 (107 genes) were particularly of interest since both the genesets were enriched in anti-viral pathways [ 23 ]. 9 genes (GBP1, SP140, PHF15, DHX58, NCF1, PLSCR1, CD80, PI4K2B and NR4A2) were in common between DC45 and DC36 gene-sets, and their membership values ranged from 0.2 to 0.8 (Fig. 4d). Genes closer to geneset DC45 or gene-set DC36, showed stronger association in the corresponding gene-sets, e.g. DHX58 belonged to gene-set DC36 with membership value of 0.675 and gene-set DC45 with membership value of 0.325 suggesting that DHX58 have a more dominant (67.5%) association with gene-set DC36 and less dominant but considerably significant (32.5%) association with gene-set DC45 (Fig. 4d).  One overlapping gene of a particular interest was CD80, a protein found on monocytes that provides a costimulatory signal necessary for T cell activation and survival. It is a ligand for two different proteins on the T cell surface: CD28 (for auto-regulation and intercellular association) and CTLA-4 [ 43, 44 ]. CD80 was associated with gene-sets DC45, DC36 and DC46 suggesting that CD80 has a multifunctional role in induction of several gene-sets. Genes like CD80 are involved in stimulating multiple down-stream events and therefore do not have a strong membership to one particular gene-set. These genes are critical in developing intervention strategies and understanding mechanisms of cross-talk, however, are typically ignored by hard clustering algorithms.    Gene-sets enriched in ISGs have distinct temporal patterns\r\n  The data-driven clustering in context-specific manner can reveal sets of genes which are functionally diverse even though they are typically grouped together [ 37, 38 ]. Specifically, previously known ISGs were grouped into 4 gene-sets (DC21, DC26, DC36 and DC45). Gene-sets DC21 and DC26 were down-regulated with time whereas gene-sets DC36 and DC45 were up-regulated with time (Fig. 5a). The mean temporal expression pattern of geneset DC26 was different than that of gene-set DC21 (Fig. 5a). Similarly, at any given time, the mean expression of gene-set DC45 was more than twice compared to that of gene-set DC36. Also, gene-sets DC45 and DC26 were more steeply up and down regulated as compared to the gene-set DC36 and DC21 respectively. Previously, time delays have been used to infer regulatory relationships [ 45 ] suggesting that gene-set DC45 might regulate geneset DC36 and gene-set DC26 might regulate gene-set DC21. Similarly, other clusters (Fig. 5b and c) that were enriched with same pathway showed differences in the magnitude of gene expression, rate of activation and sign of mean expression.    FCM clustering is flexible and comparable to other widely used clustering methods\r\n  The comparison of FCM with commonly used algorithms such as k-means and hierarchical clustering using Ward's method yielded comparable results. Both FCM and K-means clustering were performed by optimizing initial cluster centers by Ward's method. Genes from FCM solution were associated with a unique cluster (one with which a gene has a maximum membership value) thus producing hard clusters that can be compared to the solution of k-means and hierarchical clustering algorithms. Cluster sizes, mean node degrees, mean local CCs and mean global CCs were compared for the assessment of cluster quality. K-means, hierarchical clustering and FCM produced 45, 44 and 44 clusters respectively that had higher local CC than the global CC indicating the identification of a comparable number of cohesive clusters. K-means and hierarchical clusters had a minimum of 13% and 30%, and a maximum of 100 and 96% respective overlap with FCM clusters (Fig. 6a). This suggests that K-means, Ward's hierarchical method and FCM were able to pick fundamental characteristics of gene expression data. Additionally, enrichment of KEGG pathways and ISGs in the clusters from all three methods suggested that ISGs and genes involved in Cytokine-Cytokine receptor signaling pathways robustly cluster together (Fig. 6b). In conclusion, FCM is not only comparable with other clustering methods but also facilitate identification of genes with the possible multi-functional role.    Application of FCM to other cell-types\r\n  ECs and DCs are early responders to the viral infections, which signal through pathogen recognition receptor induced pathways. Comparison of genome-wide geneexpression profiles across two cell-types reveals a small overlapping sub-network and a large cell-specific response to influenza infections [ 20 ]. Application of FCM pipeline to EC dataset revealed 34% (1298) of overlapping genes and significant enrichment of several KEGG a b c pathways and ISGs in 39 out of 50 EC gene-sets (Fig. 7a and b). 167 overlapping genes were common in EC and DC (Fig. 7d), and 9 overlapping genes (PYCARD, ATP6V1H, ENO1, HSPA1A, PTPN11, CCNH, CSF1, CXCL2 and HK2) were common in DCs, ECs and also in KEGG pathways (Fig. 7d). In conclusion, FCM can be robustly applied to different cell-specific transcriptomic data to identify overlapping genes.    Development of FIGS: a Fuzzy Inference of Gene-sets package\r\n  The power of GSEA-like test will be improved by using robust context-specific gene-sets. To facilitate the use of computational model presented in this study we developed a Matlab-based installable package called 'Fuzzy Inference of the Gene-sets (FIGS)' (available at https://github.com/Thakar-Lab/FIGS). This package can be used to obtain gene-sets from matrix defining the pair-wise distance between the genes. FIGS also provide an option to upload pathways for enrichment analysis of gene-sets. FIGS package requires three parameters: number of clusters, fuzziness allowed between the clusters, and cluster association criteria to produce fuzzy gene-sets. Once the number of clusters and the amount of overlap between the clusters (fuzziness) is defined, the user has four different choices for associating genes to the clusters: 1) genes are assigned to a unique cluster based on their highest degree of membership, 2) distribution based association method described and used in this manuscript, 3) cluster with membership value higher than mean of the maximum membership values, and 4) user defined threshold (between 0-1). The results are stored in tabular form and are also displayed as interactive circular graphs. Other functionalities are described in the user's manual. For those interested in exploring or using the gene-sets produced from the meta-analysis of transcriptomics response of dendritic cells and epithelial cells to influenza infection can access FIGS-Influenza package at https://github.com/Thakar-Lab/FIGS-Influenza.In FIGS-Influenza users can upload their differentially expressed genes or genes of interest for enrichment across fuzzy clusters.     Discussion\r\n  Unsupervised clustering of genome-wide gene expression data is a frequently used tool to identify genes with similar patterns across treatments and/or time-points. We and others have frequently used hierarchical clustering algorithm to identify such groups of genes [ 20, 41 ]. Chaussabel et. al. introduced a concept of modules which are derived using K-means clustering and can be used as a set of a priori defined genes in pathway analysis [ 9, 10 ]. However, these hard clustering algorithms do not fully reproduce the observed topology of the biological pathways. Specifically, all public repositories of the biological pathways share genes across multiple pathways indicating diversity in the functional roles of these genes. Here we present a soft clustering technique to identify gene-sets with overlapping genes that reproduce the characteristics of the pathways in the public repositories and define robust gene-sets by meta-analysis.  We present a pipeline using FCM which has been optimized for cell-specific transcriptomic studies. The integration of multiple context-specific datasets provides more robust and universal gene-sets as compared to the FCM performed on individual data set. FCM parameters optimized in this study are based on the distribution of cluster association values. The upper bound of fuzziness values (m) and the distribution based cluster association have been suggested previously but never used for genegene association networks [ 28 ]. Additionally, our fuzziness selection criteria, selection of robust initial centroids by Ward's method and validation of the clustered gene-sets is extremely relevant to human immunology studies. Interestingly, FCM pipeline developed here produced gene-sets that were concise and robust compared to previously defined criteria for inference of gene-sets for pathway analysis [ 46 ].  FCM pipeline proposed here will improve the datadriven inference of gene-sets by two ways. First, by identifying overlapping genes that span across multiple gene-sets. These multi-functional genes have diverse roles in signal transduction (e.g. CCL23) and cross-talk between different pathways (e.g. MAP3K1 and GAB2) (Table 1). Thus, in addition to assessing activities of gene-sets by gene-set enrichment method, separate evaluation of multi-functional genes connected to the enriched gene-sets will improve follow-up experiments required to provide mechanistic insights. Second, connecting different gene-sets through the multi-functional genes will improve interpretation of gene-sets that are not enriched in known biological processes. Thus, FCM pipeline will significantly increase the number of novel pathways studied followed by high-throughput omics experiments. In conclusion, the results show that the FCM pipeline recapitulates topological characteristics of the biological pathways and will improve data-interpretation required for follow-up experiments.  We adapted Fuzzy-C-Means clustering algorithm, which is similar to previously used K-means clustering algorithm [ 9, 10 ], but in addition allows identification of the genes with functional roles across more than one cluster. One reason for the limited use of FCM in transcriptomic studies is the difficulty in optimizing the FCM parameters and initial centroids. Unlike previously suggested method of assigning centroids using prior biological knowledge [ 47 ] we use Ward's method. The Ward's method used in our study infers robust clusters. Moreover, our previous analysis shows that genes defined by the prior biological knowledge do not always form cohesive clusters leading to erroneous clustering solutions. Additionally, parameters optimized by the previous applications of FCM for yeast transcriptomic data cannot be applied to the transcriptomic data generated from humans [ 28, 48-51 ].  Use of gene-sets derived from context-specific transcriptomic data in the public domain will enhance the ability to develop hypotheses from high-throughput experiments. Cell-type is one of the critical contexts for all immunological studies and here we propose the FCM pipeline that can be applied to different cell-types. However, our previous study reveals that gene-gene associations inferred from cell-specific independent experiments are more robust than a mixture such as peripheral blood monocytes (PBMCs) [ 20 ]. Thus, even though FCM parameters optimized here could be applied to two different cell-types; it is likely that the parameters of FCM will need to be characterized separately for PBMC datasets.  In future the proposed pipeline will be applied to transcriptomic data measuring cell-type specific responses to the stimuli, purified proteins or viruses, and FIGS package will be expanded to include these results.    Conclusions\r\n  In this study we develop a pipeline using Fuzzy-C-Means clustering algorithm to identify multi-functional genes from meta-analysis of context-specific transcriptomic datasets. Additionally, the approach proposed here reveals novel gene-sets and facilitates their interpretation. Moreover, delivery of our pipeline by interactive FIGS package (https://github.com/Thakar-Lab/FIGS) will increase the accessibility and usability of the data-driven contextspecific gene-sets in future studies.    Additional file\r\n  Additional file 1: Figure S1. FCM pipeline facilitates functional interpretation of novel DC gene-sets. FCM DC gene-sets without enrichment of the immunological pathways (DC1, DC3, DC4, DC9, DC19, DC34 and DC35) were associated with gene-sets enriched in known-pathways facilitating interpretation of novel gene-sets. (PPTX 184 kb) Abbreviations CC: Clustering coefficient; DC: Dendritic cell; EC: Epithelial cell; FCM: Fuzzy cmeans clustering; FIGS: Fuzzy inference of gene-sets; ISG: Interferon stimulated genes; KEGG: Kyoto encyclopedia of genes and genomes; MI: Mutual information; PBMC: peripheral blood monocytes; PCA: Principal component analysis Funding This work is supported in part by Respiratory Pathogens Research Center (NIAID contract number HSN272201200005C), the University of Rochester Center for AIDS Research (NIH 5 P30 AI078498-08).  Availability of data and materials All the datasets used in this research were collected from public databases (cited in the manuscript). The FIGS package is publicly available at GitHub: https://github.com/Thakar-Lab/FIGS.  Authors' contributions JT conceived the study. AK, DK and JT performed data collection and developed the algorithms. AK and JT performed computational analysis and AK implemented the algorithm and developed the FIGS package. AK and JT wrote the manuscript. All authors read and approved the final manuscript. Competing interests The authors declare that they have no competing interests.  Consent for publication Not applicable.  Ethics approval and consent to participate Not applicable.    Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/Thakar-Lab/FIGS",
    "publicationDate": "0",
    "authors": [
      "Atif Khan",
      "Dejan Katanic",
      "Juilee Thakar"
    ],
    "status": "Success",
    "toolName": "FIGS",
    "homepage": ""
  },
  "11.pdf": {
    "forks": 4,
    "URLs": [
      "doi.org/10.1109/HiPC.2011.6152743",
      "doi.org/10.1038/nature15393",
      "github.com/StuntsPT/Structure_threader"
    ],
    "contactInfo": [],
    "subscribers": 3,
    "programmingLanguage": "Python",
    "shortDescription": "A wrapper program to parallelize and automate runs of \"Structure\", \"fastStructure\" and \"MavericK\".",
    "publicationTitle": "",
    "title": "",
    "publicationDOI": "10.1111/1755-0998.12702",
    "codeSize": 52108,
    "publicationAbstract": "Structure_threader is a program to parallelize multiple runs of genetic clustering software that does e not make use of multi-threading technology (STRUCTURE, FASTSTRUCTURE and MavericK) on multi-core comcputers. Our approach was benchmarked across multiple systems and displayed great speed improvements relative to the single threaded implementation, scaling very close to linearly with the numcber of physical cores used.",
    "dateUpdated": "2017-07-25T17:01:18Z",
    "institutions": [
      "National Institute of Health Dr Ricardo Jorge",
      "PT. d Keywords: Bioinfomatics/Phyloinfomatics"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2015-03-03T12:56:42Z",
    "numIssues": 2,
    "downloads": 0,
    "fulltext": "     DR. FRANCISCO PINA-MARTINS (Orcid ID :     10.1111/1755-0998.12702    0  Department of Human Genetics, National Institute of Health Dr Ricardo Jorge ,  Lisbon ,  Portugal    1  Instituto Superior de Agronomia, Universidade de Lisboa, CIFC - Centro de Investigação das Ferrugens do Cafeeiro, Oeiras, PT. d Keywords: Bioinfomatics/Phyloinfomatics ,  Genomics/Proteomics, Molecular Evolution, Population Genetics - Empirical, Clustering, Parallel computing     1836   000  0  0003   Structure_threader is a program to parallelize multiple runs of genetic clustering software that does e not make use of multi-threading technology (STRUCTURE, FASTSTRUCTURE and MavericK) on multi-core comcputers. Our approach was benchmarked across multiple systems and displayed great speed improvements relative to the single threaded implementation, scaling very close to linearly with the numcber of physical cores used.    Programs       -\r\n  l    1. pAbstract\r\n  Structure_threader was compared to previous software written for the same task - ParallelStructure and StrAuto, and was proven to be the faster (up to 25% faster) wrapper under all tested scenarios. Furthermore, Structure_threader can perform several automatic and convenient operations, e assisting the user in assessing the most biologically likely value of 'K' via implementations such as the l \u201cEvanno\u201d, or \u201cThermodynamic Integration\u201d tests and automatically draw the \u201cmeanQ\u201d plots (static or cinteractive) for each value of K (or even combined plots).  i Structure_threader is written in python 3 and licensed under the GPLv3. It can be downloaded free t of charge at https://github.com/StuntsPT/Structure_threader.  r 2. Introduction Clustering analyses are widely used in population genetics and, nowadays, population genomics. This  A technique of using multilocus genotype data to infer population clusters, is frequently performed based on multiple MCMC re-sampling. One of the most popular tools for performing this type of analyses is STRUCTURE  (Pritchard, Stephens, &amp; Donnelly, 2000) . Despite producing robust results, this d approach demands long run times, even in modern machines. This problem is aggravated as the type of eanalysed datasets, which gradually grow from relatively small, such as microsatellite loci  (De Bartro, 2005; Muchadeyi et al., 2007) , to high throughput sequencing  (Lamaze, Sauvage, Marie, Garant, &amp; Bernatchez, 2012; Renaut, Grassa, Moyers, Kane, &amp; Rieseberg, 2012) , consequently incpreasing run times by orders of magnitude.  The process can be sped up by either running multiple instances of the used software, which is an e inefficient and error prone method requiring constant attention and intervention from the user. Thecreare faster software alternatives to STRUCTURE, which can also be used to speed up the analysis process.  c One such option is analysing the data in the program FASTSTRUCTURE  (Raj, Stephens, &amp; Pritchard, 2014) , which decreases run times by up to two orders of magnitude. However FASTSTRUCTURE does not Asupport the popular \u201cno admixture\u201d model present in STRUCTURE, and is not capable of handling haploid data (and several other less widely used features), which limits its application to a wide range of data.  Another option in to analyse the data in the software MavericK (Verity &amp; Nichols, 2016), which is e also considerably faster than STRUCTURE, but not as fast as FASTSTRUCTURE by an order of magnitude. It l does, however support most of the same features as STRUCTURE and uses a built-in, improved mecthod for helping determine the most biologically relevant value of \u201cK\u201d called \u201cThermodynamic i Integration\u201d (Verity &amp; Nichols, 2016). Regardless of the speed gains these programs offer, they are t only able to use a single CPU core for their computations, which means that these methods too, do not rscale well with current multi-core IT infrastructure.  Alternatively, a method to bootstrap multiple simultaneous runs of the software can be used, such as A (R the R Core Team, 2013)  package ParallelStructure  (Besnier &amp; Glover, 2013) , or StrAuto  (Chhatre &amp; Emerson, 2017) , which does exactly that for the software STRUCTURE  (Pritchard et al., 2000) . ParallelStructure, however, has scaling problems, as described in the manuscript, considerably loodsingefficiency as more CPU cores are used. StrAuto is another option that does indeed scale well with the number of CPU cores used, but like ParallelStructure, it only works as a wrapper for the e software STRUCTURE, and cannot be used to speed up other popular genetic clustering programs.  t Furthermore, after the clustering step is finished, it is necessary to infer the number of clusters that make most biological sense for the data  (Earl &amp; vonHoldt, 2012) , using methods such as the \u201cEvanno p test\u201d  (Evanno, Regnaut, &amp; Goudet, 2005) , or the \u201cThermodynamic Integration\u201d (TI) method (Verity &amp; Nicehols, 2016). After this, it is also often necessary to plot the \u201cmeanQ\u201d values of each cluster per individual, to be able to interpret the biological significance of the data. This is usually done with c software such as DISTRUCT  (ROSENBERG, 2004) .  All cofthese steps typically require parsing the results files of each clustering run and manually running all the required steps until the final outcome is produced  (Earl &amp; vonHoldt, 2012) . This is not only time consuming as it is also error prone due to the large number of separate steps that must be  A taken during the process. Neither ParallelStructure nor StrAuto provide an automated and reproducible way to perform this task.  e Parltof this process is largely facilitated by the program STRUCTURE HARVESTER  (Earl &amp; vonHoldt, 2012) , which automates the parsing of STRUCTURE runs and uses that information to perform an \u201cEvanno c test\u201d on the data, which uses some heuristics to predict which value of 'K' makes the most biological i sense regarding the analysed data. Although this is a very convenient automation, it still relies on t manual user intervention to input the data from STRUCTURE, does not provide assistance with the plortting of the \u201cmeanQ\u201d values and only works for the software STRUCTURE. Other programs, such as FASTSTRUCTURE include the necessary software to perform these tests, and even to plot the \u201cmeanQ\u201d values, Abut still require manual intervention between these steps. MavericK goes further and presents the full posterior distribution for 'K' using the \u201cThermodynamic Integration\u201d test as an automatic last step of the analysis and even recommends some scripts for drawing \u201cmeanQ\u201d plots, but dthislast step also requires human intervention.  e To address these two problems (reducing run times and automating the analyses tasks), we herein t present Structure_threader: a program to parallelize STRUCTURE, FASTSTRUCTURE and MavericK runs that considerably reduces the scaling problems of previous approaches and automates the entire p process, - wrapping the runs, assisting in the choice of the most biologically relevant value of K, and draewing the \u201cmeanQ\u201d plots.  c Structure_threader is available on https://github.com/StuntsPT/Structure_threader. For the stable c (non development) versions, check the releases page, or get it from Pypi.  A    3.2. Threading strategy\r\n  The threading strategy used in Structure_threader is represented in Figure 1. Structure_threader takes the provided input file, the values of \u201cK\u201d to test and the required number of replicates, and creeates a job queue, which is sorted by decreasing complexity order. After this, P child processes are spalwned, (where P is the number of threads made available to the software) each containing one indcependent instance of the wrapped program. Each of these child processes takes the first available job ifrom the queue and once it is finished, its output is processed by the main process for error hantdling and logging. The child processes are spawned using python's \u201cmultiprocessing\u201d and \u201csubprocess\u201d modules from the standard library.  r    3.3. Benchmarking process\r\n  In oArderto assess the gains provided by Structure_threader, the program was benchmarked in four different systems, described in Table 1, with various specifications. Runs were performed twice to serve as replicates (Supplementary Table 1). Run times for STRUCTURE were assessed using both Strducture_threader v0.4.1, ParallelStructure v1.0 and StrAuto v1.0, which were then compared. FASTSTRUCTURE and MavericK runs were only wrapped in Structure_threader, since none of the other e programs supports this, and compared with the default, single threaded implementation.  t Usage of RAM was monitored during the benchmarking process, and it was never detected as a bottleneck on any of the systems. None of the wrapped programs is very I/O intensive (at least as far p as the tested systems were concerned), meaning that the present tests were always CPU bound. Run etimes were measured using zsh's time builtin method (wall time was used), and then normalized to a \u201cspeed up\u201d factor  (Besnier &amp; Glover, 2013)  by dividing the time of the multi-core c runs by the time of the single core runs, which were performed in the measured programs' default impclementations.  A    Benchmarking STRUCTURE 3.3.1. Test dataset description\r\n  The test file used for the benchmarks consists of 100 individuals, represented by 80 SNP loci without e missing data. This dataset was crafted based on data from the 1000 genomes project (The 1000 l Genomes Project Consortium, 2015) to perform the benchmarks and was constructed aiming for a c size that would be neither too small, which could bias the benchmarking towards very quick runs, i nor too large, to avoid the process taking too long to be practical.  t r This dataset was created from public data, and instructions on how to recreate it are available in the documentation and in the program's repository.  A 3.3.2.Benchmark details The benchmarking process consisted of running the test dataset on STRUCTURE v2.3.4 for 1x10\u2076 MCdMCiterations, and a \u201cburnin\u201d length of 5x10\u2074, under \u201cadmixture\u201d model (for other parameters check the program's repository). These settings were performed for values of \u201cK\u201d varying from 1 to e 4. Each value of \u201cK\u201d was run with 4 replicates, which means a total of 16 STRUCTURE runs were pertformed in each benchmark. All these runs were performed on the default, single threaded implementation and under the Structure_threader, ParallelStructure (using the \u201cparallel_structure()\u201d impplementation, which initial testing found to be faster in all used machines) and StrAuto wrappers.  e    3.c4.Benchmarking FASTSTRUCTURE 3.c4.1.Test dataset description\r\n  The test dataset used for benchmarking FASTSTRUCTURE runs, is different from the one used for benchmarking STRUCTURE, since this program was designed to analyse larger datasets. The tested file conAsistsof 1000 individuals, represented by 1000 SNP loci. Like the previous dataset, this one was also crafted from the same public data from the 1000 genomes project, and instructions for recreating it are available in the documentation. The used dataset itself is available in the program's repository.  e 3.l4.2.Benchmark details The benchmarking process consisted of running the above described dataset for values of \u201cK\u201d from 1 c to i16for each benchmark run.  t The raverage run time of both replicates was used to plot and analyse the data. Since a FASTSTRUCTURE runs do not require replicates for downstream analyses, each value of \u201cK\u201d was run only once per benchmark, which means that a total of 16 FASTSTRUCTURE runs were performed both in the default  A implementation and under the Structure_threader wrapper.  d 3.5. Benchmarking MavericK  e 3.5.1.Test dataset description The ttest file used for the MavericK benchmarks is the same that was used to benchmark STRUCTURE, which is described above.  p e 3.5.2.Benchmark details The benchmarking process consisted of running the test dataset on MavericK v1.0.4 for 1x10\u2074 MCMC c iterations, plus a \u201cburnin\u201d length of 2500 iterations, with 5 replicates each (for other parameters checckthe program's repository). These settings were performed for values of \u201cK\u201d varying from 1 to 16.  A    4. Results &amp; Discussion\r\n  Using Structure_threader as a wrapper for all wrapped programs has yielded increases in speed that scales almost linearly with the number of processes used, at least as long as physical cores are conecerned, as can be seen in Figure 2 and Figure 3.  l Considering the benchmark results present in Figure 2, it is clear that both Structure_threader and StrcAuto are more efficient methods to perform multiple STRUCTURE runs on multiple core systems thainParallelStructure (on average 7% faster in the tested systems, varying from 1% to 25% faster).  t Structure_threader is also always faster than StrAuto, but by much smaller margins than when comrpared with ParallelStructure (on average 3% faster, varying from 0.3% to 7% faster). Regardless of the tested system and number of cores used, the differences in \u201cspeed up\u201d are always in favour of Structure_threader. When compared to ParallelStructure, the difference increases with the  A requested scaling - the more physical cores are used, the better the relative performance of Structure_threader. Also worth noting is that the \u201cspeed up\u201d values obtained here with PardallelStructure when using physical cores, are somewhat better than what is described in  (Besnier &amp; Glover, 2013) , but this could be due to differences in benchmark workloads.  e Speed up differences between StrAuto and Structure_threader are small, but can be compared in Figture2. A more detailed comparison, can be made using the data tables available in Supplementary Material 1.  p Unlike ParallelStructure and StrAuto, Structure_threader can also speed up the runs of other proegrams by running them in parallel. Similar to what is done for running STRUCTURE, wrapping FASTSTRUCTURE and MavericK in Structure_threader, provides considerable speed improvements, c once again scaling almost linearly as long as hyper-threading is not in effect (Figure 3). Altchough ideally the \u201cspeed up\u201d factor should scale linearly with the number of used physical cores, this does not always happen in practice (Figure 2 and Figure 3). Of the three tested wrappers, Structure_threader scales the closest to linearly, even when using 8 physical cores, where the \u201cspeed  A up\u201d factor varies between 6.24 and 7.91, depending on the system and the wrapped program. ParallelStructure shows the worst scaling of the tested wrappers, especially on 8 physical threads, where the \u201cspeed up\u201d factor varies between 5.95 and 6.85.  e The large drop in performance increase, regardless of the used wrapper program, happens when l using hyper-threading (using more than eight cores in the Nehalem Rack system and more than four in tcheHaswell Desktop system - the CPUs of the other two systems do not have this feature), as is i sometimes described under certain workloads  (Leng, Ali, Hsieh, Mashayekhi, &amp; Rooholamini, 2002; t Marr et al., 2002; Saini et al., 2011) . We are not sure why this happens on this particular workload, but rthe issue is not as evident when analysing smaller datasets as the one from  (Besnier &amp; Glover, 2013) . It is therefore hypothesised that it may be related to \u201ccache thrashing\u201d, a phenomenon that occurs Awhen the CPU constantly refreshes the contents of L2 and L3 caches for quickly accessing different information. \u201cCache thrashing\u201d is more likely to happen when working with larger datasets and when hyper-threading is active since both logical cores share L2 and L3 cache  (Marr et al., 2002) . The dautomated plot drawing feature of Structure_threader is responsible for both a simplification of the analysis process (less steps per analysis), and also for the reduction in random error e (consequence of less human intervention).  t The mentioned plots produced by Structure_threader are provided in two formats. A static, vectorial image in \u201csvg\u201d format, especially suited for publication, and an interactive HTML version of the plot, p suited for results exploration.  e 5. Conclusions The cobserved difference in efficiency between Structure_threader and ParallelStructure can probably be explained by the programming languages utilized in the wrappers (Python vs. R) and the fact that c ParallelStructure solves tasks in increasing order of complexity, whereas Structure_threader sorts them in decreasing order. This strategy provides an increase in efficiency, since the sorting minAimizes the time each CPU core is left idle.  Another important difference between ParallelStructure and Structure_threader is that the former is a framework to build scripts that perform the requested analyses, and the latter can either be used as a framework, or directly from the command line. This makes Structure_threader much easier to use, ewhile simultaneously keeping the same type of flexibility ParallelStructure offers. Although both l programs can be used to draw the clustering plots from the STRUCTURE results, the features offered by cStructure_threader go far beyond the basic plotting that ParallelStructure is capable of.  i The speed gains obtained with Structure_threader and StrAuto are very similar, with only a marginal diffterence favouring Structure_threader. This difference is likely due to the efficiency of pytrhon's higher speed when compared to bash's, and eventually due to a smaller overhead of python's multiprocess module when compared to that of GNU parallel (Tange, 2011). Although both proAgramsare run from the command line interface, Structure_threader is more user friendly than StrAuto, since it includes built-in help, handles user errors, and allows for a lot of parameters to be defined directly in the command line.  Strducture_threader was designed to exploit the power of multi-core machines for speeding up multiple genetic clustering software runs, with emphasis on scalability. Our results demonstrate that e in every tested scenario this goal is fulfilled in a more efficient way than previous solutions.  t Furthermore, Structure_threader goes much farther than the two previous solutions in it's capabilities to perform tests for estimating the most biologically relevant \u201cK\u201d value, as well as p plotting flexibility.  Altehough the automation process that Structure_threader provides does not decrease computation time, it should significantly speed up the analyses process, due to the human time that is saved.  c Furthermore, this automation is also one important step for reproducibility of the studies that use this csoftware. That being said, it is also important that users interact with and explore the options and parametrization the wrapped programs offer. It is critical that these are well understood in order to obtain meaningful and statistically relevant results.  A  We find that the obtained decrease in run times, allied with the ease of use and automation, including that of follow up analysis, make Structure_threader a useful tool to any investigator working with population genetics/genomics data and the best current choice for performing genetic e clustering analyses.  l    6. Acknowledgements\r\n  c Thiisstudy was financed by Portuguese National Funds, through FCT - Fundação para a Ciência e a   Tectnologia, within the projects UID/BIA/00329/2013, SFRH/BD/51411/2011 and\r\n  SFRH/BD/86736/2012.  r We would further like to thank Bob Verity and an anonymous reviewer for their suggestions that thoroughly improved both the software and the manuscript.  A threading on processor resource utilization in production applications. In 2011 18th International Conference on High Performance Computing (pp. 1-10). e  https://doi.org/10.1109/HiPC.2011.6152743 l Tange, O. (2011). GNU Parallel-The Command-Line Power Tool. Login: The USENIX Magazine, 36(1), The i1000 Genomes Project Consortium. (2015). A global reference for human genetic variation. tNature, 526(7571), 68-74. https://doi.org/10.1038/nature15393 c42-47.  A     Data Accessibility\r\n  Structure_threader, the manual and example datasets are available on lAuthor Contributions F. Pina-Martins has conceived the concept of the study, written most of the program code and written the manuscript. D. Silva has contributed to the ideas of the software, written code and i revised the manuscript. J. Fino has contributed to the ideas of the software, written code and t revised the manuscript. O. S. Paulo has mentored the work and revised the manuscript.    Supporting Information\r\n  Spreadsheet with all run times (Supplementary Table 1).  A Tables e single threaded run.  p   System Name\r\n  e    Haswell Laptop\r\n    Ivy Bridge Desktop\r\n    Nehalem Rack\r\n  c    Sandy Bridge Rack\r\n     ",
    "sourceCodeLink": "https://github.com/StuntsPT/Structure_threader",
    "publicationDate": "0",
    "authors": [],
    "status": "Success",
    "toolName": "Structure_threader",
    "homepage": ""
  },
  "97.pdf": {
    "forks": 1,
    "URLs": [
      "zhiji.shinyapps.io/scrat",
      "github.com/zji90/SCRAT"
    ],
    "contactInfo": ["hji@jhu.edu"],
    "subscribers": 2,
    "programmingLanguage": "R",
    "shortDescription": "",
    "publicationTitle": "Single-cell regulome data analysis by SCRAT",
    "title": "Single-cell regulome data analysis by SCRAT",
    "publicationDOI": "10.1093/bioinformatics/btx315",
    "codeSize": 19503,
    "publicationAbstract": "Summary: Emerging single-cell technologies (e.g. single-cell ATAC-seq, DNase-seq or ChIP-seq) have made it possible to assay regulome of individual cells. Single-cell regulome data are highly sparse and discrete. Analyzing such data is challenging. User-friendly software tools are still lacking. We present SCRAT, a Single-Cell Regulome Analysis Toolbox with a graphical user interface, for studying cell heterogeneity using single-cell regulome data. SCRAT can be used to conveniently summarize regulatory activities according to different features (e.g. gene sets, transcription factor binding motif sites, etc.). Using these features, users can identify cell subpopulations in a heterogeneous biological sample, infer cell identities of each subpopulation, and discover distinguishing features such as gene sets and transcription factors that show different activities among subpopulations. Availability and implementation: SCRAT is freely available at https://zhiji.shinyapps.io/scrat as an online web service and at https://github.com/zji90/SCRAT as an R package.Contact: hji@jhu.edu Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-08-02T01:13:34Z",
    "institutions": ["Johns Hopkins University Bloomberg School of Public Health"],
    "license": "No License",
    "dateCreated": "2016-01-23T22:36:08Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx315   Single-cell regulome data analysis by SCRAT     Zhicheng Ji  0    Weiqiang Zhou  0    Hongkai Ji  0    0  Department of Biostatistics, Johns Hopkins University Bloomberg School of Public Health ,  Baltimore, MD 21205 ,  USA     2017   1  1  3   Summary: Emerging single-cell technologies (e.g. single-cell ATAC-seq, DNase-seq or ChIP-seq) have made it possible to assay regulome of individual cells. Single-cell regulome data are highly sparse and discrete. Analyzing such data is challenging. User-friendly software tools are still lacking. We present SCRAT, a Single-Cell Regulome Analysis Toolbox with a graphical user interface, for studying cell heterogeneity using single-cell regulome data. SCRAT can be used to conveniently summarize regulatory activities according to different features (e.g. gene sets, transcription factor binding motif sites, etc.). Using these features, users can identify cell subpopulations in a heterogeneous biological sample, infer cell identities of each subpopulation, and discover distinguishing features such as gene sets and transcription factors that show different activities among subpopulations. Availability and implementation: SCRAT is freely available at https://zhiji.shinyapps.io/scrat as an online web service and at https://github.com/zji90/SCRAT as an R package.Contact: hji@jhu.edu Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  Single-cell regulome (scRegulome) mapping technologies such as single-cell sequencing assay of transposase-accessible chromatin (scATAC-seq)  (Buenrostro et al., 2015; Cusanovich et al., 2015) , single-cell chromatin immunoprecipitation followed by sequencing (scChIP-seq)  (Rotem et al., 2015)  and single-cell DNase I hypersensitive site sequencing (scDNase-seq)  (Jin et al., 2015)  have been emerging as a powerful new approach to studying gene regulation. Unlike the conventional ChIP-seq  (Johnson et al., 2007) , DNase-seq  (Crawford et al., 2006)  and ATAC-seq (Buenrostro et al., 2013) technologies which measure average behavior of a cell population, single-cell technologies can measure regulatory element activities within each individual cell, thereby allowing one to examine the heterogeneity of a cell population. This is important for studying molecular mechanisms of tumors, immune responses, stem cell differentiation, and many other biological systems.  Typically, a scRegulome dataset contains cells sampled from a heterogeneous cell population. Two common data analysis problems are to identify subpopulations of cells and distinguishing features that show differential regulatory signals among different subpopulations. Currently, easy-to-use software tools for these tasks are still lacking. Unlike data from the traditional bulk technologies which are relatively continuous, scRegulome data are highly sparse and discrete. For instance, chromatin accessibility measured by scATAC-seq is nearly a binary signal at each genomic locus (Fig. 1a, Supplementary Fig. S1). Using these highly sparse and discrete data to discriminate signal from noise at each individual genomic locus is extremely difficult. For this reason, conventional tools developed for analyzing bulk data are not suitable for single-cell data. Aggregating signals across multiple genomic loci with shared biological functions can mitigate sparsity and discreteness and has been shown to be a useful way to analyze scRegulome data  (Buenrostro et al., 2015; Cusanovich et al., 2015; Rotem et al., 2015)  (Supplementary Material and Supplementary Fig. S2). However, systematically aggregating signals according to different genomic features (e.g. transcription factor binding motifs, gene sets) and using the aggregated signals to analyze sample heterogeneity is a non-trivial task for many investigators due to lack of software support, as demonstrated in Supplementary Tables S1 and S2. Here, we present SCRAT, a toolbox with a graphical user interface (GUI) for analyzing cell heterogeneity in single-cell regulome (i.e. scATACseq, scDNase-seq and scChIP-seq) data. It can be used to summarize data from each cell according to different genomic features, identify cell subpopulations based on these features, infer identities of cells in each subpopulation, and discover features that show differential regulatory signals among subpopulations (Fig. 1).    2 SCRAT functions and examples\r\n  The main functions of SCRAT are summarized below.   2.1 Data pre-processing\r\n  SCRAT takes aligned sequence reads (i.e. bam files) as input. Users have options to exclude artifact signals from the ENCODE blacklist regions  (ENCODE Project Consortium, 2012)  and filter out cells with low total read count.    2.2 Feature summarization\r\n  Next, users can aggregate reads from each cell according to different features, such as across all motif sites of each transcription factor binding motif (Motif), across co-regulated DNase I hypersensitive sites (DHSs) defined by ENCODE DNase-seq data (ENCODE Cluster), within a region of interest of each gene (Gene), and across all genes of each gene set in the MSigDB  (Liberzon et al., 2011)  database (Gene Set). Here, motifs, DHS clusters, genes, and gene sets are called 'features' (Fig. 1c). For human and mouse genomes, these features are pre-defined and stored in SCRAT. Users can also define their own features for aggregation by uploading one or more lists of genomic regions in BED file format (Custom Feature). After aggregation, the signals for each feature are normalized to adjust for library size.    2.3 Cell heterogeneity analysis\r\n  SCRAT uses the aggregated signals to cluster cells into subpopulations (Fig. 1d). Multiple clustering methods are provided. Clustering can be based on one or multiple sets of features chosen by users. The cluster number may be determined automatically. One can use the original features or the transformed features after dimension reduction. Multiple dimension reduction methods are provided.    2.4 Inferring cell identity\r\n  Users can compare each cell's regulome to a pre-compiled regulome database consisting of ENCODE DNase-seq profiles from a wide variety of cell types to infer the likely cell type of each cell. The similarity between each single cell and existing cell types in the database based on the aggregated signals can be visualized using a heatmap (Fig. 1e). Users can also select existing cell types in the database and project them to the principal component space of single cells to help illuminate the nature of the heterogeneity (Fig. 1d, green dots).    2.5 Differential feature analysis\r\n  Given cell subpopulations, users can identify features that are differential among subpopulations (i.e. heterogeneity-driving features). One can choose to run parametric (t-, ANOVA F-) or nonparametric (Wilcoxon rank-sum, Kruskal-Wallis or permutation) test on each feature to evaluate whether its aggregated signals are differential among the user-selected subpopulations. Differential features which pass certain false discovery rate cutoff will be reported (Fig. 1f). 2.6 GUI SCRAT has a GUI which makes the analysis user-friendly.  Details of these functions are provided in Supplementary Material. Supplementary Table S1 compares SCRAT with existing popular tools for regulome or differential feature analyses. To demonstrate SCRAT, we analyzed a scATAC-seq dataset consisting of GM12878 and HEK293T cells (Supplementary Material). Conventional bulk peak calling followed by clustering cells using peak-level signals failed to separate the two cell types (Fig. 1b). In contrast, SCRAT successfully identified the two cell subpopulations (Fig. 1d) and differential features that matched the cell identities (Supplementary Figs. S3-S11; Supplementary Table S3). We also applied SCRAT to scATAC-seq data from human and mouse embryonic stem cells (ESC) and found that a consistent feature driving cell heterogeneity in these ESCs was cell cycle genes (Supplementary Figs. S12 and S13, Supplementary Tables S4 and S5).  In summary, SCRAT provides a set of easy-to-use tools for cell heterogeneity analysis, and it addresses the pressing needs for software support for analyzing scRegulome data.     Funding\r\n  This work was supported by the National Institutes of Health [Grant No. R01HG006841, R01HG006282].  Conflict of Interest: none declared. Buenrostro,J.D. et al. (2013) Transposition of native chromatin for fast and sensitive epigenomic profiling of open chromatin, DNA-binding proteins and nucleosome position. Nat. Methods, 10, 1213-1218.    ",
    "sourceCodeLink": "https://github.com/zji90/SCRAT",
    "publicationDate": "0",
    "authors": [
      "Zhicheng Ji",
      "Weiqiang Zhou",
      "Hongkai Ji"
    ],
    "status": "Success",
    "toolName": "SCRAT",
    "homepage": ""
  },
  "54.pdf": {
    "forks": 0,
    "URLs": ["www.github.com/hzi-bifo/eden.Contact:"],
    "contactInfo": ["alice.mchardy@helmholtz-hzi.de"],
    "subscribers": 4,
    "programmingLanguage": "Python",
    "shortDescription": "EDEN: evolutionary dynamics within environments",
    "publicationTitle": "EDEN: evolutionary dynamics within environments",
    "title": "EDEN: evolutionary dynamics within environments",
    "publicationDOI": "10.1093/bioinformatics/btx394",
    "codeSize": 128594,
    "publicationAbstract": "Summary: Metagenomics revolutionized the field of microbial ecology, giving access to Gb-sized datasets of microbial communities under natural conditions. This enables fine-grained analyses of the functions of community members, studies of their association with phenotypes and environments, as well as of their microevolution and adaptation to changing environmental conditions. However, phylogenetic methods for studying adaptation and evolutionary dynamics are not able to cope with big data. EDEN is the first software for the rapid detection of protein families and regions under positive selection, as well as their associated biological processes, from meta- and pangenome data. It provides an interactive result visualization for detailed comparative analyses. Availability and implementation: EDEN is available as a Docker installation under the GPL 3.0 license, allowing its use on common operating systems, at http://www.github.com/hzi-bifo/eden.Contact: alice.mchardy@helmholtz-hzi.de Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-06-08T16:34:27Z",
    "institutions": [
      "Heinrich Heine University Dusseldorf",
      "Helmholtz Centre for Infection Research",
      "Partner Site LMU Munich",
      "Department of Algorithmic Bioinformatics",
      "German Centre for Infection Research (DZIF)",
      "Ludwig-Maximilian University of Munich"
    ],
    "license": "No License",
    "dateCreated": "2017-06-08T16:34:25Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx394   EDEN: evolutionary dynamics within environments     Philipp C. Mu¨ nch  1  4  5    Ba\u20acrbel Stecher  2  5    Alice C. McHardy  0  1  3  4    0  Cluster of Excellence on Plant Sciences (CEPLAS), Heinrich Heine University Dusseldorf ,  40225 Dusseldorf ,  Germany    1  Computational Biology of Infection Research, Helmholtz Centre for Infection Research ,  38124 Brunswick ,  Germany    2  DZIF, Partner Site LMU Munich ,  80336 Munich ,  Germany    3  Department of Algorithmic Bioinformatics    4  German Centre for Infection Research (DZIF) ,  Partner Site Hanover-Brunswick, 38124 Brunswick ,  Germany    5  Max von Pettenkofer-Institute for Hygiene and Clinical Microbiology, Ludwig-Maximilian University of Munich ,  80336 Munich  ,  Germany     2017   1  1  4   Summary: Metagenomics revolutionized the field of microbial ecology, giving access to Gb-sized datasets of microbial communities under natural conditions. This enables fine-grained analyses of the functions of community members, studies of their association with phenotypes and environments, as well as of their microevolution and adaptation to changing environmental conditions. However, phylogenetic methods for studying adaptation and evolutionary dynamics are not able to cope with big data. EDEN is the first software for the rapid detection of protein families and regions under positive selection, as well as their associated biological processes, from meta- and pangenome data. It provides an interactive result visualization for detailed comparative analyses. Availability and implementation: EDEN is available as a Docker installation under the GPL 3.0 license, allowing its use on common operating systems, at http://www.github.com/hzi-bifo/eden.Contact: alice.mchardy@helmholtz-hzi.de Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Inanc Birol    1 Introduction\r\n  Microorganisms can adapt to changing environmental conditions by evolutionary processes such as mutation, lateral gene transfer and recombination  (Bendall et al., 2016; Denef and Banfield, 2012; Koonin et al., 2002) . Only a small fraction of mutations are thought to be beneficial, and few will be fixed and contribute to the substitution rate measurable in phylogenomic studies  (Bendall et al., 2016; Nishant et al., 2009) . One of the most widely used measures for quantifying the type and extent of selection acting on a protein family site or sequence at the molecular scale is the dN=dS ratio  (Ford, 2002; Nielsen, 2005) . If the rate of change at nonsynonymous sites (dN) within a gene family exceeds the rate of change at synonymous sites (dS), i.e. dN=dS &gt; 1, positive selection is assumed to operate on the encoded protein. This indicates that adaptation to altered environmental conditions is taking place and that the observed changes increase the fitness of the respective organism. A dN=dS &lt; 1, on the other hand, is taken as an indicator for negative selection, with changes in the protein sequence or at a site decreasing fitness  (Hurst, 2002; Koonin and Rogozin, 2003) , such as for instance, changes in catalytic sites that would lead to a loss of function.  Calculating the dN=dS ratio for the large-scale sequence datasets that are being generated in metagenomics and comparative microbial genomics is very challenging, due to the run times of commonly used tree inference software and software for quantifying positive selection, such as FastCodeML  (Valle et al., 2014) , which relies on maximum likelihood methods  (Pond and Frost, 2005) . With EDEN, we provide a fully automated software package and visualization framework for a rapid meta- or pangenome wide analysis of the evolutionary processes affecting protein families and associated biological processes. EDEN is based on a fast approximate tree inference and count-based dN=dS inference for individual protein families. The software can be applied to compare the selection profiles of bacterial species with different phenotypes or lifestyles, such pathogens versus mutualists, and to study selection from metagenome datasets of microbial communities (Fig. 1b).    2 Implementation\r\n  EDEN is provided as a Docker image, which is a virtualization of the application that includes everything needed to run the program  (Merkel, 2014)  (Fig. 1a). Other than Docker, no further software has been installed. EDEN requires as input (meta-)genome DNA sequences in FASTA format, from which open reading (ORF) frame DNA and protein sequences will be generated with Prodigal  (Hyatt et al., 2010)  (Fig. 1a). Alternatively, the user can provide two files in FASTA format, corresponding to the DNA and protein sequences for a set of ORFs. Optionally one or multiple Hidden Markov Models (HMMs) can also be provided, which will be used to infer groups of ORFs, as well as a sample grouping table, with sample properties of interest, such as their origin, to enable comparative analyses of multiple input samples in groups representing these properties.  The first step in the assessment of evolutionary patterns for the input sequences is their division into groups of ORFs that are subsequently processed together. Groups can either be obtained (i) based on the user-provided grouping table, (ii) by searching for protein family members with hmmsearch against user-provided hidden Markov Models  (Eddy, 1998)  or (iii) by searching the ORFs using hmmsearch versus the complete TIGRFAM HMM collection  (Haft et al., 2003) . For each group, then a multiple DNA sequence alignment (MSA) is calculated with MUSCLE  (Edgar, 2004) . Subsequently, a multiple codon alignment is constructed using PAL2NAL 14 under consideration of the MSA and the protein (a) (b) sequences  (Suyama et al., 2006) . Based on the codon MSA, a phylogenetic tree is reconstructed with an efficient implementation of the neighbor-joining algorithm using a modified version of Clearcut  (Sheneman et al., 2006) . Specifically, we control for gaps in the alignment that are mostly of technical origin (due to the alignment of smaller assembled contigs to longer reference sequence), by excluding these from mismatch counts in calculation of the additive pairwise distance matrix. Next, dN=dS is calculated using the counting method  (Pond and Frost, 2005) , which achieves a trade-off between the computational effort and the quality of the estimates. For dN=dS calculation, the ancestral amino acid and coding sequences are reconstructed for all internal nodes of each protein family tree, using maximum parsimony as the optimization criterion. The values of dN and dS are then inferred from these sequences, considering the least costly of several different mutation paths between codons, as in  Tusche et al. (2012) . The dN=dS ratio is then calculated using a lookup table with the probabilities that a change will cause a nonsynonymous change for all possible codon comparisons possible  (Nei and Gojobori, 1986) .  For calculation of the average dN=dS for a considered group, low-confidence positions are excluded by filtering positions from the alignment with a user-defined proportion of gaps. P values are calculated using a one-sided Fisher's exact test, based on the dN and dS rates for every sequence group in comparison to the entire sample. The false discovery rate is used to control for multiple testing errors, using the Benjamini and Hochberg procedure  (Benjamini and Hochberg, 1995) , with a per default set to 0.05. To detect putative epitopes within a given set of homologs that are under positive selection, the P value for the sum of the dN and dS rates is calculated using a sliding window approach (with the size of 20 codons as a default) and a one-sided Fisher's exact test over the MSA  (Bulgarelli et al., 2015; McCann et al., 2012)  (see Supplementary Material for details).  As a 'sanity check', we compared the dN =dS of EDEN with HyPhy SLAC, which uses a derivative of the Suzuki-Gojobori counting approach, for 50 randomly selected protein families from the HMP dataset and found a high correlation (Pearson's R ¼ 0.873, P value ¼ 2.499e-16, Supplementary Fig. S2).  In comparison to FastCodeML  (Valle et al., 2014) , a run-time optimized version of the codeml program from the PAML package  (Yang, 2007) , EDEN has a drastically reduced run-time (Supplementary Fig. S2, see Supplementary Material for details). For further detailed analyses with FastCodeML, such as assessing selection for specific clades, the codon alignment and tree calculated by EDEN can be downloaded for individual protein families.    3 Application\r\n  We previously used EDEN to study protein families under selection from six assembled metagenome samples (150.000 ORF sequences) of the root microbiota for wild and domesticated barley (Hordeum vulgare). This delivered evidence for positive selection acting on protein families linked to pathogenesis, bacteria-phage interactions, secretion and nutrient mobilization in the barley root-associated microbiota  (Bulgarelli et al., 2015)  and for a higher degree of selection acting on protein families from the root-associated microbiota than on those found in bulk soil. EDEN was also used to compare the selection patterns for protein families from multiple strains of Colletotrichum  (Hacquard et al., 2016) .  We applied EDEN to 66 samples of the HMP project   (Consortium et al., 2012 ) from six body sites (body sites dataset on http://eden.bifo.helmholtz-hzi.de, Fig. 1b). These were sampled from healthy individuals and had similar alpha- and beta diversities, except for the stool samples, which were more diverse ( (Consortium et al., 2012) . The results indicate a positive relationship between dN =dS values and the exposure of body sites to the surrounding environment. The highest dN =dS values were found for samples from the external portion of the nose (anterior nares), followed by the oral microbiome (subgingival plague, palatine tonsils, throat) and the lowest values for stool. Also in comparison of samples of one body site to all others, a significantly increased dN =dS (FDR corrected P values &lt; 0.001) was observed (in that order) for the microbiome from the external portion of the nose, subgingival plaque and throat. Interestingly, across all six body sites, most protein families with significant signs of positive selection in comparison to all other protein families from the respective samples (FDR adjusted P value &lt;0.01) were annotated with transport and binding functions, suggesting the existence of a functional panselectome. Other than that, 'energy metabolism' was found as a prominent association for the oral and nose associated microbiomes.  We also used EDEN to characterize human gut metagenome samples from   (Qin et al., 2010 ) (BMI dataset on http://eden.bifo.helmholtz-hzi.de). EDEN determined a significantly higher dN =dS for the protein coding genes from lean individuals (BMI &lt; 25) compared to overweight (BMI 25-30) and obese individuals (BMI &gt; 30; P value &lt; 0.001), suggestive of a higher functional diversity in the guts of lean individuals. For lean individuals compared to obese individuals, this finding was consistent over all functional groups, except for regulatory functions. For these, dN =dS was slightly, though not significantly, higher for the obese than for the lean individuals.    4 Conclusion\r\n  EDEN can identify protein families under positive selection from metagenome and pangenome datasets. It reports gene families and regions thereof with a significantly elevated dN =dS in comparison to a specified background and allows comparative studies of multiple samples. The results obtained for metagenome samples from different demonstrate how these analyses provide insights into the relationship between the signs of molecular adaptation found in the microbiome to biological processes and their environments.    Author contributions\r\n  A.C.M. and P.C.M. conceived and designed the experiments. P.C.M. implemented the software. P.C.M. wrote the manuscript with comments from A.C.M. and B.S. All authors approved the final version of the manuscript.    Acknowledgement\r\n  We thank Lars Steinbr u¨ck for valuable comments and for providing and implementation of his original code.  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/hzi-bifo/eden",
    "publicationDate": "0",
    "authors": [
      "Philipp C. Mu¨ nch",
      "Ba\u20acrbel Stecher",
      "Alice C. McHardy"
    ],
    "status": "Success",
    "toolName": "eden",
    "homepage": "http://eden.bifo.helmholtz-hzi.de/"
  },
  "29.pdf": {
    "forks": 0,
    "URLs": [
      "github.com/raquele/GeNNet",
      "dx.doi.org/10.7717/peerj.3509#supplemental-information",
      "cran.r-project.org/web/packages/RNeo4j",
      "hub.docker.com/r/quelopes/gennet/",
      "hub.docker.com",
      "blog.docker.com/2016/08/docker-hub-hits-5-billion-pulls/"
    ],
    "contactInfo": [],
    "subscribers": 2,
    "programmingLanguage": "HTML",
    "shortDescription": "Integrated platform for unifying scientific workflow management and graph databases for transcriptome data analysis",
    "publicationTitle": "GeNNet: an integrated platform for unifying scientific workflows and graph databases for transcriptome data analysis",
    "title": "GeNNet: an integrated platform for unifying scientific workflows and graph databases for transcriptome data analysis",
    "publicationDOI": "10.7717/peerj.3509",
    "codeSize": 89183,
    "publicationAbstract": "There are many steps in analyzing transcriptome data, from the acquisition of raw data to the selection of a subset of representative genes that explain a scientific hypothesis. The data produced can be represented as networks of interactions among genes and these may additionally be integrated with other biological databases, such as Protein-Protein Interactions, transcription factors and gene annotation. However, the results of these analyses remain fragmented, imposing difficulties, either for posterior inspection of results, or for meta-analysis by the incorporation of new related data. Integrating databases and tools into scientific workflows, orchestrating their execution, and managing the resulting data and its respective metadata are challenging tasks. Additionally, a great amount of effort is equally required to run in-silico experiments to structure and compose the information as needed for analysis. Different programs may need to be applied and different files are produced during the experiment cycle. In this context, the availability of a platform supporting experiment execution is paramount. We present GeNNet, an integrated transcriptome analysis platform that unifies scientific workflows with graph databases for selecting relevant genes according to the evaluated biological systems. It includes GeNNet-Wf, a scientific workflow that pre-loads biological data, pre-processes raw microarray data and conducts a series of analyses including normalization, differential expression inference, clusterization and gene set enrichment analysis. A user-friendly web interface, GeNNet-Web, allows for setting parameters, executing, and visualizing the results of GeNNet-Wf executions. To demonstrate the features of GeNNet, we performed case studies with data retrieved from GEO, particularly using a single-factor experiment in different analysis scenarios. As a result, we obtained differentially expressed genes for which biological functions were analyzed. The results are integrated into GeNNet-DB, a database about genes, clusters, experiments and their properties and relationships. The resulting graph database is explored with queries that demonstrate the expressiveness of this data model for reasoning about gene interaction networks. GeNNet is the first platform to integrate the analytical process of transcriptome data with graph databases. It provides a comprehensive set of tools that would otherwise be challenging for non-expert users to install and use. Developers can add new functionality to components of GeNNet. The derived data allows for testing previous hypotheses about an experiment and exploring new ones through the interactive graph database environment. It enables the analysis of different data on humans, rhesus, mice and rat coming from Affymetrix",
    "dateUpdated": "2017-09-06T07:12:31Z",
    "institutions": [
      "Academic editor Camillo Rosano",
      "National Laboratory for Scientific Computing (LNCC)",
      "National Institute of Infectology Evandro Chagas",
      "National Institute of Cancer (INCA)",
      "Computational Biology Keywords GeNNet"
    ],
    "license": "No License",
    "dateCreated": "2016-12-15T13:26:55Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     10.7717/peerj.3509   GeNNet: an integrated platform for unifying scientific workflows and graph databases for transcriptome data analysis     Raquel L. Costa  0  1  3  4    Luiz Gadelha  0  1  4    Marcelo Ribeiro-Alves  0  2  4    Fábio Porto  0  1  4    0  Academic editor Camillo Rosano    1  DEXL Lab, National Laboratory for Scientific Computing (LNCC) ,  Petrópolis, Rio de Janeiro ,  Brazil    2  Laboratory of Clinical Research in DST- AIDS, National Institute of Infectology Evandro Chagas ,  Oswaldo Cruz Foundation, Rio de Janeiro ,  Brazil    3  National Institute of Cancer (INCA) ,  Rio de Janeiro, RJ ,  Brazil    4  Subjects Bioinformatics, Computational Biology Keywords GeNNet ,  Graph database, Software container, Scientific workflow, Provenance, Transcriptome, Microarray, Data-to-knowledge     5  7  2017     6  6  2017     There are many steps in analyzing transcriptome data, from the acquisition of raw data to the selection of a subset of representative genes that explain a scientific hypothesis. The data produced can be represented as networks of interactions among genes and these may additionally be integrated with other biological databases, such as Protein-Protein Interactions, transcription factors and gene annotation. However, the results of these analyses remain fragmented, imposing difficulties, either for posterior inspection of results, or for meta-analysis by the incorporation of new related data. Integrating databases and tools into scientific workflows, orchestrating their execution, and managing the resulting data and its respective metadata are challenging tasks. Additionally, a great amount of effort is equally required to run in-silico experiments to structure and compose the information as needed for analysis. Different programs may need to be applied and different files are produced during the experiment cycle. In this context, the availability of a platform supporting experiment execution is paramount. We present GeNNet, an integrated transcriptome analysis platform that unifies scientific workflows with graph databases for selecting relevant genes according to the evaluated biological systems. It includes GeNNet-Wf, a scientific workflow that pre-loads biological data, pre-processes raw microarray data and conducts a series of analyses including normalization, differential expression inference, clusterization and gene set enrichment analysis. A user-friendly web interface, GeNNet-Web, allows for setting parameters, executing, and visualizing the results of GeNNet-Wf executions. To demonstrate the features of GeNNet, we performed case studies with data retrieved from GEO, particularly using a single-factor experiment in different analysis scenarios. As a result, we obtained differentially expressed genes for which biological functions were analyzed. The results are integrated into GeNNet-DB, a database about genes, clusters, experiments and their properties and relationships. The resulting graph database is explored with queries that demonstrate the expressiveness of this data model for reasoning about gene interaction networks. GeNNet is the first platform to integrate the analytical process of transcriptome data with graph databases. It provides a comprehensive set of tools that would otherwise be challenging for non-expert users to install and use. Developers can add new functionality to components of GeNNet. The derived data allows for testing previous hypotheses about an experiment and exploring new ones through the interactive graph database environment. It enables the analysis of different data on humans, rhesus, mice and rat coming from Affymetrix       -\r\n  Additional Information and Declarations can be found on page 19 platforms. GeNNet is available as an open source platform at https://github.com/raquele/GeNNet and can be retrieved as a software container with the command docker pull quelopes/gennet.    INTRODUCTION\r\n  The passage of cellular information through the events of transcription and translation postulates the central dogma of molecular biology presented in 1958 by Francis Crick  (Crick, 1970) . Despite the knowledge of the structure of DNA and its main biological functions, it was only in the past few decades, with the advancement of high-throughput technologies, that it became possible to quantify the transcripts produced in large-scale. Since then, substantial progress has been noted, for instance, in the identification of prognostic genes and biomarkers, and in the classification and discrimination of subtypes of tumors  (Robles &amp; Harris, 2017; Guinney et al., 2015; Alizadeh et al., 2000; Golub et al., 1999; Zhang et al., 2012b) . Currently, microarray and RNA-Seq are the main technologies available and widely used  (Zhao et al., 2014)  in the quantification of gene expression, with advantages and disadvantages in the choice and use of each of them. For instance, on one hand, in RNA-Seq one may both identify new transcripts and observe isoforms  (Conesa et al., 2016) . On the other hand, the low cost of microarrays, in relation to RNA-Seq, still makes their use very appealing for well-known organisms.  Regardless of the technology employed, the results of transcriptome analysis can be represented as a complex interaction network. In such a network, nodes represent transcripts, genes or proteins and the connections between them can be modeled by edges having a weight assigned to them. For example, in gene co-expression networks the links may represent the correlation between the genes limited by a significant value through a cutoff value  (Zhang et al., 2012b; Choobdar, Ribeiro &amp; Silva, 2015; Zhang &amp; Horvath, 2005) . Strong (positives or negatives) significant correlations among a group of genes may indicate elements that participate in the activation or repression of pathways or biological functions relevant to the studied phenomenon (e.g., immunity, cell differentiation, angiogenesis, etc.). In addition, the same results can be enriched with information from external biological networks such as protein interaction networks (PPI) or even information on identification of key elements in the regulatory process such as the transcription factors  (Zhang et al., 2012a; Mathelier et al., 2014) . The analysis of the networks may explore topological metrics determining the connectivity between the nodes, of which the most connected can be indicated as targets in molecular modeling, development of biomarkers, etc. Through complex biological networks, we can extract topological properties such as `small-world', `hierarchically modular' and `scale-free network'  (Barabasi, 2009; Albert, 2005) .  Managing such complex network is, however, a challenge. Current approaches employ analysis and visualization software such as Cytoscape (Smoot et al., 2011) and Grephi  (Bastian, Heymann &amp; Jacomy, 2009) . While such programs make it possible to explore complex relationships between heterogeneous information in biological systems, the results of data analyses often remain fragmented. This imposes difficulties, either for posterior inspection of results or meta-analysis by the incorporation of new related data. Furthermore, the heterogeneity of biological data adds to the problem complexity  (Maule, Emmerich &amp; Rosenblum, 2008) , as it is difficult to find a conceptual data schema that follows a fixed and strict structure, such as in relational databases. Modifying the data schema in these cases can result in conflicts or inconsistencies in a database. In the era of expanding and interconnected information, new data models appeared, such as column-oriented, key-value, multidimensional, and graph databases. These are commonly called NoSQL (Not only SQL) databases and often have advantages regarding scalability (Stonebraker, 2010). Graph-based data models, in particular, are useful for data in which the relationship between attributes is one of the most important aspects to be taken into consideration during querying. The graph database is an intuitive way for connecting and visualizing relationships. In graph databases the nodes represent objects, and the edges represent the relationships among them. Both nodes and edges can hold properties, which add information about the objects or the relationships. In recent years, this database model has been used in many bioinformatics applications and are particularly promising for biological datasets  (Preusse, Theis &amp; Mueller, 2016; Johnson et al., 2014; Balaur et al., 2016; Henkel, Wolkenhauer &amp; Waltemath, 2015; Muth et al., 2015; Lysenko et al., 2016) . Have and Jensen  (Have, Jensen &amp; Wren, 2013)  observed that for path and neighborhood queries, Neo4j, a graph database, can be orders of magnitude faster than PostgreSQL, a widely used relational database, while allowing for queries to be expressed more intuitively.  Besides the growing need for an adequate representation of biological data, the accumulation of molecular biology data motivated the development of pipelines, scientific workflows, and platforms for analyzing data  (Shade &amp; Teal, 2015; Conesa et al., 2016) . Many researchers are using these integrative approaches for analyzing metagenomes, proteomes, transcriptomes and other `omics' data  (Joyce &amp; Palsson, 2006) . Regardless of the `omics' technology, there are many steps from the acquisition of raw data to the selection of a subset of representative genes that explain the hypothesis of the scientists. Combining databases and tools into computational analyses, orchestrating their execution, and managing the resulting data and its respective metadata are challenging tasks  (Ghosh et al., 2011) . Academic journals, for instance, are demanding better reproducibility of computational research, requiring an accurate record of parameters, data, and processes also called provenance  (Carata et al., 2014) , used in these activities to support validation by peers (Sandve et al., 2013).  Overcoming many of these challenges can be supported by designing and executing these computational analyses as scientific workflows  (Deelman et al., 2009) , which consist of compositions of different scientific applications. Their execution is usually chained through data exchange, i.e., data produced by an application is consumed by subsequent applications. Scientific Workflow Management Systems (SWMSs) enable for managing the life cycle of scientific workflows, which is usually given by composition, execution and analysis  (Liu et al., 2015) . Many SWMSs, such as Galaxy  (Giardine et al., 2005) , Taverna  (Oinn et al., 2004) , Tavaxy  (Abouelhoda, Issa &amp; Ghanem, 2012)  and Swift (Wilde et al., 2011), natively support reusing previously specified workflows  (Goble &amp; De Roure, 2007)  and gathering provenance  (Gadelha et al., 2012) . More recently, scripting languages such as R and Python incorporated features typically available in SWMS. RDataTracker  (Lerner &amp; Boose, 2015) , for instance, adds provenance tracking to R scripts and noWorkflow  (Murta et al., 2015)  adds the same functionality to Python. This facilitates the specification and execution of scientific workflows in scripting languages, which is the approach we use in this work. The scientific workflow we propose (GeNNet-Wf) is implemented in R and its activities are comprised of calls to functions of various R libraries, such as limma (Smyth, 2004), GOstats  (Falcon &amp; Gentleman, 2007) , affy  (Gautier et al., 2004)  and WGCNA  (Langfelder &amp; Horvath, 2008) . Integrating scientific workflows with database systems allows for managing and persisting the data manipulated in these workflows in a structured way. This allows for scientists to perform complex data pre-processing analysis and to make the resulting data available for further investigation using queries expressed in a high-level query language. This enables expressing declaratively what data is required without saying how data should be obtained. Moreover, it abstracts away from the user low-level data management details such as accessing files where contents of a database are stored  (Garcia-Molina, Ullman &amp; Widom, 2009) . We argue that integrated web applications, involving scientific workflows and databases, can hide the complexity of underlying scientific software by abstracting away cumbersome aspects, such as managing files and setting command-line parameters, leading to increased productivity for scientists. One critical aspect of enabling reproducible computational analyses is keeping track of the computational environment components, i.e., operating system, libraries, software packages and their respective versions  (De Paula et al., 2013) .  Currently, the vast quantity of functions performed by distinct software lead to a considerable amount of time being employed in installing and configuring them, requiring users to deal with sometimes complicated installation procedures and errors related to software dependencies and versions. Virtualization is a promising technique to tackle these problems  (Daniels, 2009) . In particular, operating system-level virtualization, as provided by `software containers', allows for running applications and services that are instantiated on isolated environments (containers) on a hosting computer system. Containers provide all the dependencies required for these applications and services to run and can be built in a programmable way to ensure that they will be composed of the same libraries and software packages every time they are instantiated. This considerably facilitates the deployment of software systems since developers can deliver software containers for their applications directly to users or data center administrators. Docker, for instance, is an open-source platform that allows for managing containers  (Merkel, 2014; Boettiger, 2015) . It has a container repository called Docker Hub (https://hub.docker.com) where developers can make software containers for their applications available for download. Many traditional software and tools are available on Docker Hub and it is widely used with around five billion software containers downloaded from the repository up to August, 2016 (https://blog.docker.com/2016/08/docker-hub-hits-5-billion-pulls/). In Bioinformatics, there are already tools that are available as Docker software containers and explore features such as reproducibility  (Hung et al., 2016; Belmann et al., 2015)  or applications areas such as transcriptomics (Zichen et al., 2016).  Di Tommaso et al. (2015 ) showed that containers have a negligible impact on the performance of bioinformatics applications. Other examples of software distributed as Docker software containers are available. AlgoRun  (Hosny et al., 2016)  provides a modular software container with frequently used bioinformatics tools and algorithms that can be accessed through a browser or a Web application programming interface. ReproPhylo (Szitenberg et al., 2015) implements a phylogenomics workflow with reproducibility features. GEN3VA  (Gundersen et al., 2016)  is a platform for gene-expression analysis available as a web-based system.  Considering other integrative tools for transcriptome data analysis, in the literature there are different integrative approaches for analyzing transcriptomes obtained with from high-throughput technologies, such as Babelomic  (Medina et al., 2010) , RobiNA  (Lohse et al., 2012) , Expander (Ulitsky et al., 2010) and RMaNI  (Madhamshettiwar et al., 2013) . Most of these tools support pre-processing, filtering, clustering, functional analysis, and visualization of results. Furthermore, the tools developed are available for download or as a web interface service. Specific portals for curated bioinformatics tools can be found, for instance, on OmicTools  (Henry et al., 2014) . However, most of these tools do not support reproducibility, database management with a flexible and adequate model of representation with persistence, freedom to query the database, and function customization.  In this paper, we present GeNNet, an integrated transcriptome analysis platform that unifies scientific workflows with graph databases for determining genes relevant to evaluated biological systems. It includes GeNNet-Wf, a scientific workflow that accesses pre-loaded back-end data, pre-processes raw microarray data and conducts a series of analyses including normalization, differential expression, gene annotation, clusterization and functional annotation. During these analyses, the results are stored in different formats, e.g., figures, tables, and R workspace images. Furthermore, experiment results are stored in GeNNet-DB, which is a graph database that can be persisted. The graph database represents networks that can be explored either graphically or using a flexible query language. Finally, GeNNet-Web offers an easy-to-use web interface tool developed in Shiny for automated analysis of gene expression. The implementation follows best practices for scientific software development (Wilson et al., 2014). For instance, our approach uses both software containers and provenance tracking to facilitate reproducibility. This allows for reproducing, without user intervention, the computational environment (e.g., operating system, applications, libraries) and recording the applications, data sets, and parameters used in the analyses, i.e., tracking data provenance. A graph data model is used to adequately represent gene expression networks and its persistence. Also, a high-level declarative language can be used to freely query the data, existing functions can be modified and new functions added to the analytical workflow. As far as we know, GeNNet is the first platform for transcriptome data analysis that tightly couples a scientific workflow with a persistent biological (graph) database while better supporting reproducibility.  To emphasize and demonstrate the usefulness of GeNNet, we will reanalyze data from hepatocellular carcinoma (HCC) in tumor versus adjacent non-tumorous liver cells under different scenarios of use and analysis from GEO repository (Gene Express Omnibus).  The first one (I) consists of executing the experiment using the user-friendly interface of GeNNet in which users choose the parameters and execute the experiment without needing to modify the lower-level scripts that compose GeNNet. The second one (II) is comprised of integrating data from different experiments that have the same hypothesis to be tested and evaluated using the RStudio IDE. The third and last one (III) uses the results of scenarios (I) and (II) to perform queries in the graph database. In (III), we highlight the use of the database persisted during the execution of GeNNet, as well as the integration of new information into it.    MATERIALS AND METHODS\r\n   Implementation\r\n  GeNNet innovates in its use of a graph-structured conceptual data model coupled with scientific workflow, software containers for portability and reproducibility, and a productive and user-friendly web-based front-end (Fig. 1). In the following subsections, we describe these components and functionalities in detail: scientific workflow (GeNNetWf), graph database (GeNNet-DB), web application (GeNNet-Web), software container, computational experiment reproducibility and experimental data.    GeNNet-Wf workflow\r\n  GeNNet-Wf is the composition of two sub-workflows: `Background workflow' and `Analysis workflow' (see in Fig. 2). The data obtained by the former persists into the graph-database.  The `Analysis workflow' processes the raw dataset enriching the background data.   Background workflow\r\n  The GeNNet `Background workflow' generates a database for a set of specified organisms pre-loaded into the system (Fig. 2, top). It includes genes annotated/described and their relationships, along with other associated elements, which contribute to posterior transcriptome analysis. In this version of the platform, the background data is comprised of two primary sources: (i) gene information about human, rhesus, mice and rat, obtained from NCBI annotations (Schuler et al., 1996); and, (ii) Protein-Protein Interaction (PPI) network, retrieved from STRING-DB  (Franceschini et al., 2013)  (version 10). All genes imported from NCBI become nodes in the graph database and some of the primary information associated with them (such as symbol, entrezId, description, etc.) are modeled as node properties. The information derived from STRING-DB PPI become edges (`neighborhood', `gene fusion', `co-occurrence', `co-expression', `experiments', `databases', `text-mining' and `combined score'). This layer of data is added to the graph database during the construction of the GeNNet container (Software container subsection). More detail about the representation and implementation can be found in section GeNNet-DB graph database. 7/26    Analysis workflow\r\n  The `Analysis workflow' stage is comprised of the execution of a series of tools and libraries to analyze the transcriptome data uploaded by the user in conjunction with the data generated by the `Background workflow' (Fig. 2, bottom). This module was written in R using several packages mainly from the Bioconductor  (Dudoit, Gentleman &amp; Quackenbush, 2003)  and CRAN repositories. The steps are detailed next.  Normalization. This step consists in normalizing the raw data from an informed Affymetrix platform using either RMA  (Irizarry et al., 2003)  or MAS5 methods, both available in the affy  (Gautier et al., 2004)  package. During this step, some quality indicator plots are generated (as boxplot of probe level, Spearman correlation, and density estimates) as well as a normalized matrix (log-normalized expression values). e-set. In this step, data about the experimental design should be added along with log-normalized expression values. This generates an ExpressionSet (eSet) object, a data structure object of the S4 class used as a base in many packages developed in Bioconductor transcriptome analysis  (Falcon, Morgan &amp; Gentleman, 2007)  . This format gives flexibility and access to existing functionality. The input file must be structured using mainly two columns: a column named SETS for the experimental design, and a column called SAMPLE_NAME for the names of the files containing raw sample expression matrix data. Filtering/Differential expression inference. Differential expression (DE) inference analysis allows for the recognition of groups of genes modulated (up- or down-regulated) in a biological system when compared against one or more experimental conditions. In many situations, this is a core step of the analysis, and there is a great diversity of experimental designs (such as control versus treatment, consecutive time points, etc.) allowing the inference. In our platform, we use the limma package to select the DE genes (Smyth, 2004) on single-factor experimental designs based on a gene-based hypothesis testing statistic followed by a correction of multiple testing given by the False Discovery Rate (FDR)  (Kendall &amp; Bradford Hill, 1953) . Furthermore, a subset of DE genes can be selected based on upand down-regulation expressed as an absolute logarithmic (base 2) fold-change (logFC) threshold. The latter can be set-up by the user, as described in Scenario I Experiment user-friendly interface. Results of this step are displayed as Volcano plots and matrices containing the DE genes.  Annotation. The annotation step consists of annotating the probes for the corresponding genes according to the Affymetrix platform used in the experiment.  Clusterization. This step consists in analyzing which aggregated genes have a similar pattern (or level) of expression. We incorporated clusterization analysis including hierarchical methods, k-medoids from the package PAM (Partitioning Around Medoids) (Reynolds et al., 2006) and WGCNA (Weighted Gene Coexpression Network Analysis)  (Langfelder &amp; Horvath, 2008) . 8/26 Functional analysis. Genes grouped by similar patterns enable the identification of over-represented (enriched) biological processes (BP). In our approach, we conducted enrichment analyses applying hypergeometric tests (with p-value &lt; 0.001) as implemented in the GOStats package  (Falcon &amp; Gentleman, 2007) . Ontology information for the gene is extracted from the Gene Ontology Consortium database  (Ashburner et al., 2000) . The universe is defined as the set of all genes represented on a particular Affymetrix platform, or, in the case of multiple platforms in a single experiment design, the universe is defined as the common and unique genes in among all Affymetrix platforms. The subset is defined either by the set of diferentially expressed (DE) genes between a test and a control condition (control versus treatment design) or by the union of the DE genes selected among the pairwise comparisons among groups in all other single-factor experimental designs. Although functional analyses can lead to biased results, as presented in Timmons, Szkop &amp; Gallagher (2015), we have added more restrictive cut-off with the purpose of reducing the detection bias of our platform.    Execution\r\n  GeNNet is designed to automatically execute the workflow through the web application interface (accessed via http://localhost:3838/gennet, when the software container is running). However, users that intend to implement new functions or even execute the workflow partially, can use the RStudio server interface in GeNNet (accessed via http://localhost:8787 after starting the software container). More details are available in Supplemental Information.     GeNNet-DB graph database\r\n  Although a NoSQL database has no fixed schema, we defined an initial graph model to help and guide the GeNNet-DB (Fig. 3). GeNNet database (GeNNet-DB) structure is defined on the Neo4j database management system, a free, friendly-to-use and with broad community support graph database, with its nodes, edges, and relationships. Vertices and edges were grouped into classes, according to the nature of the objects. We defined the labels as GENE, BP (Biological Process), CLUSTER, EXPERIMENT, ORGANISM, and a series of edges as illustrated in Fig. 3. In the GeNNet platform there is an initial database defined by interactions between genes as described in Background preparation section. During the execution of GeNNet-Wf, using Shiny or RStudio, new nodes and connections are formed and added to the database. The resulting information is stored in the graph database using the RNeo4j package available at: (https://cran.r-project.org/web/packages/RNeo4j). It can also be accessed directly through the Neo4j interface (accessed via http://localhost:7474).It is possible to query and access the database in this interface using the Cypher language, a declarative query language for Neo4j, or Gremlin, a general-purpose query language for graph databases. These query languages allow for manipulating data by updating or deleting nodes, edges, and properties in the graph. Querying also allows for exploring new hypotheses and integrating new information from different resources that are related to the targeted experiment. GeNNet-DB is persistent, and the resulting database is exported to a mounted directory. Its contents can be loaded to a similar Neo4j installation. For further details, one can read the Neo4j manual. 9/26    GeNNet-Web application\r\n  GeNNet-Web provides a user-friendly way to execute GeNNet-Wf. We developed an easy-to-use layout for providing the parameters and automatically executing all steps of the workflow experiment. The application was implemented using the Shiny library for R. This library allows for turning an R script that implements some analysis into a web application in a convenient manner. Shiny has a library of user interface elements can be used for entering input data and parameters, and for displaying the output of an R script. The parameters comprise the input of the web application, which includes: descriptors for experiment name and overall design; type of normalization; differential expression settings; experiment platform and organism; and clusterization method. After executing GeNNet-Wf, GeNNet-Web allows for easy retrieval and visualization of its outputs, which are given by a heatmap, graph database metrics (e.g., the number of nodes, the number of edges and relationships between nodes), and the list of differentially expressed genes selected. In addition to the outputs generated in the web application, the underlying workflow creates the output files described in subsection GeNNet-Wf workflow. 10/26    Software container\r\n  A Docker software container was built containing GeNNet and all its required libraries and dependencies. This enables users to download a single software container that includes all the components of GeNNet and instantiate this environment independently in any host that runs an operating system supported by Docker. The software container was successfully tested on CentOS Linux 7, Ubuntu Linux 14.04, MacOS X 10.11.6 and Windows 10 hosts. The software container for GeNNet, specified in a script named `Dockerfile', was built according to the following steps: (i) The operating system environment is based on Debian GNU/Linux 8 with software packages required by GeNNet, such as R (v. 3.3.1), installed from the official Debian repositories; (ii) The R software and the packages required by GeNNet, installed from the CRAN repository; (iii) RStudio (v. 1.0.44) server and the Neo4j (Community Edition v.3.0.6) graph database, installed from their respective official repositories; (iv) Supporting data sets, such as PPI, loaded to the graph database; (v) GeNNet-Wf, implemented in R, installed in RStudio; (vi) Shiny, a web application server for R, installed from its official repository. GeNNet-Web, which calls GeNNet-Wf, is loaded to Shiny.    Computational experiment reproducibility\r\n  Reproducibility is accounted in GeNNet in two aspects. Firstly, the platform provides a provenance trace record generated by the RDataTracker package  (Lerner &amp; Boose, 2015)  for R. The trace contains the activities executed by the workflow and the data sets consumed and produced by them. This trace is exported to a persistent directory. Secondly, the adoption of software containers allows for using the same environment (operating system environment, libraries, and packages) every time GeNNet is instantiated and used. Both the provenance trace and the preservation of the execution environment with software containers significantly help the computational experiment reproducibility since users can retrieve from the former the parameters and data sets used in analyses and, from the latter, re-execute them in the same environment, as provided by the GeNNet software container.    Experimental data\u2014use case scenarios\r\n  To illustrate the flexibility of GeNNet, we will conduct an experiment of re-analysis of HCC, considered the most common type of liver cancer. The HCC is highly complex, and the main risk factors are associated with prolonged abusive alcohol consumption and persistent infection of HBV (Hepatitis B Virus) and HCV (Hepatitis C Virus) (Siegel, Miller &amp; Jemal, 2017). We performed the re-analysis of microarray experiments deposited in the GEO repository and, to facilitate the understanding of GeNNet, we separated this case study in three different scenarios. The first one is to analyze the data using the friendly web interface for GeNNet developed in Shiny (described in `GeNNet-Web application'). The second scenario is to integrate an additional independent experiment to the data using the RStudio interface to create and modify their functions. The last scenario is to perform queries in the graph database generated during the execution and analysis of the experiment, highlighting the range of possibilities of the system we developed. 11/26     RESULTS AND DISCUSSION\r\n   Scenario I\u2014experiment user-friendly interface\r\n  As an example of a specific and more detailed case study, we re-analyzed a gene expression experiment from HCC obtained from the transcriptome repository GEO  (Barrett et al., 2013)  with accession number GSE62232 (Schulze et al., 2015). The study used the Affymetrix Human Genome U133 Plus (GPL570) and contained 91 samples, of which 81 samples are from HCC tumors and 10 from adjacent non-tumorous liver tissues.  Data was normalized using the MAS5 method and the differentially expressed gene selection criteria were FDR &lt;0.05 and absolute log2(Fold-Change) &gt;1. The initial threshold values chosen are the most used and recommended in the literature but the threshold values can be adjusted. The genes were clustered using the Pearson correlation method as a measure of dissimilarity. Next, the clusters were associated with biological functions through the hypergeometric test (with p-value &lt; 0.001 as threshold). All parameters were configured using the friendly interface built in Shiny as shown in Fig. 4 and accessed via http://localhost:3838/gennet. As a result, 3,356 differentially expressed genes were obtained, and 661 ontological terms were represented (p-value &lt; 0.001). A major part of the information arising from the analytical process was incorporated to GeNNet-DB. Besides the database, the results were exported to different formats such as figures (heatmaps, boxplots, etc.), tables and provenance (Fig. 5).    Scenario II\u2014RStudio environment in meta-analysis\r\n  In this scenario we explored the flexibility introduced by the integration of RStudio in our platform. Its availability enables more experienced users to extend existing functionality with new analyses over available data. In this scenario, we explore one example of such flexibility with a meta-analysis approach in which we combine results from different experiments. Meta-analysis experiments combine microarray data from independent yet similarly designed studies allowing one to overcome their variations, and ultimately increasing the power and reproducibility of the transcriptome  (Ewald et al., 2015)  analysis. We added a study with experimental design performed on the data described in the previous section. We used HCC data containing 18 tumor samples versus 18 adjacent non-tumorous liver tissues from Wang et al. (2014). The experiment was carried out with the Affymetrix Human Genome U133 platform and deposited in GEO under accession number GSE60502. The Fig. 6 shows the access via RStudio (accessed via: http:localhost:8787).  This scenario of use requires more advanced users in the R language. We exemplify the addition of an experiment to enhance the flexibility of our platform by making the analysis more robust and integrative between complex experiments as in cancer studies. However, the user can modify or even add a function by generating new analyses from GeNNet.    Scenario III\u2014querying and adding relationships\r\n  Biological information is typically highly connected, semi-structured and unpredictable.  The results obtained from the GeNNet analysis are stored in a graph database during the execution of the workflow. The database can be accessed via http://localhost:7474 using the Cypher declarative query language with direct access to the database, we formulated 12/26 some demonstration queries using as an example the dataset analyzed above. The database generated during GeNNet-Wf execution facilitates data representation as interaction networks, in an approach that allows for exploring a great variety of relationships among its composing entities, besides making new insights for subnetwork exploration possible.  Depending on the type of these interactions, different kinds of networks and topologies can be defined and analyzed. Through the data representation used in GeNNet-DB, traversal queries are possible. We illustrate typical examples in which the user just needs to query GeNNet-DB to solve them.  Query 1: What are the existing relationships among nodes in the database?  This is a simple query that returns all existing relationships among different node labels and types. The result of the query was represented as a graph in Fig. 7 retrieved the graph model as exemplified in Fig. 3. 14/26 Figure 6 Example of access mode to RStudio where it is possible to modify the predefined functions.  Figure 7 Database schema with all the existing nodes and relationships.  Costa et al. (2017), PeerJ, DOI 10.7717/peerj.3509  Query 2: Which nodes of type GENE were DE and present the highest number of connections associated to the protein interaction networks (PPI) according to a combined score threshold of &gt;0.80? Among these selected nodes, what are the clusters and associated biological processes?  Some common and important topological metrics in biological networks include: degree, distance, centrality, clustering coefficient. In this work, we use the degree metric ki of a node ni, defined as the number of edges that are adjacent (aij ) to this node, which is given by: ki D  Xaij : j2V  We use the Cypher query language to find the most connected DE genes in the network that establish known connections to the PPI network, having a high attribute value for the combined interaction score (provided by PPI association of protection interaction database STRING-DB). For these genes we computed the co-expression cluster and, subsequently, the biological processes attributed to these clusters. One can observe that the query is expressed concisely for answering a relatively complex topological question. The resulting  DE genes are displayed in Table 1. (1) 16/26  One of the main advantages of using the data model adopted in GeNNet is the availability of data and information that can be easily done without changing the data model. New nodes may add information such as metadata of samples (e.g., information on a patient's eating habits) or new edges may add new relationships (e.g., genes co-expressed in different methods used) or even both (e.g., addition of a database on microRNA interactions connected to existing genes in the database). In the example below, we add a HUB-like node from the result obtained in query 2. Through the CREATE clause, after obtaining 17/26 Figure 9 Result of query 4 in the Neo4j access interface displaying the genes co-expressed with CDK1 in the different experiments deposited in GeNNet-DB.  the selected genes, a new node and edges were created (Fig. 8). These queries demonstrate the flexibility of the database in adding new information that can be generated through existing data in GeNNet-DB.  Query 3: New node and edges inserted from the result of the previous query.  MATCH ( e:EXPERIMENT) [ s : W a s _ s e l e c t e d ] &gt; ( g:GENE) [ p : P P I _ i n t e r a c t i o n ] (h:GENE) [ : W a s _ c l u s t e r i z e d ] (c:CLUSTER) [ : W a s _ r e p r e s e n t e d ] ( b:BP ) WHERE p . c o m b i n e d _ s c o r e &gt; 0 . 8 0 WITH DISTINCT g , COUNT( d i s t i n c t h ) AS s c o r e WHERE s c o r e &gt; 50 WITH c o l l e c t ( g ) AS g s CREATE ( hub:Hub { name: 'HUB' } ) WITH g s , hub UNWIND g s AS g CREATE ( g ) [:AS_HUBS] &gt;( hub )  RETURN 18/26  Query 4: Given different experiments, which genes are co-expressed with a differentially expressed gene, for instance, gene `CDK1'?  Through this query, we can know which genes are co-expressed with CDK1 ranked in descending order on the number of experiments analyzed and deposited in the database.  As a result of this query, we obtain that 326 genes appear co-expressed with gene CDK1 in both experiments analyzed in scenarios I and II (Fig. 9).  MATCH ( g:GENE ) [ r : W a s _ c l u s t e r i z e d ] &gt; ( c:CLUSTER ) &lt; [ r 2 : W a s _ c l u s t e r i z e d ] ( h:GENE ) WHERE g . s y m b o l = 'CDK1 ' RETURN h . s y m b o l , c o u n t ( d i s t i n c t c ) AS s c o r e ,  c o l l e c t ( d i s t i n c t c . c l u s t I n f o ) a s c l u s t e r ORDER BY s c o r e DESC     CONCLUSION, UPDATES AND FUTURE WORK\r\n  The platform presented in this work is the first one to integrate the analytical process of transcriptome data (currently only available for microarray essays) with graph databases. The results allow for testing previous hypothesis about the experiment as well as exploring new ones through the interactive graph database environment. It enables the analysis of different data coming from Affymetrix platforms on humans, rhesus, mice and rat. GeNNet will be periodically updated, and we intend to extend the modules to include analyses of RNA-Seq and miRNA. We will incorporate additional experimental designs for DE and improve the execution time of the analyses. Moreover, we intend to add other model organisms to the background data, such as Arabidopsis thaliana and Drosophila melanogaster.  GeNNet-Web offers an interface that accommodates both experienced and inexperienced users. For the latter, the interface provides various filtering and parameter setup opportunities, in addition to some pre-defined queries. For more advanced users a plain query interface is provided so that more tailored analysis can be expressed. Due to the free access to GeNNet, we rely on the feedback of the community for improving the tool. The distribution of the platform in a software container allows not only for executing it on a local machine but also for easily deploying it on a server and making it available on the Web.    ADDITIONAL INFORMATION AND DECLARATIONS\r\n   Funding\r\n  This work has been supported by CAPES (Coordenacão de Aperfeicoamento de Pessoal de Nível Superior) and CNPq (Conselho Nacional de Desenvolvimento Cientíifico e Tecnológico) funding. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. 19/26    Grant Disclosures\r\n  The following grant information was disclosed by the authors: CAPES (Coordenacão de Aperfeicoamento de Pessoal de Nível Superior). CNPq (Conselho Nacional de Desenvolvimento Cientíifico e Tecnológico).    Competing Interests\r\n  The authors declare there are no competing interests.    Author Contributions\r\n  Raquel L. Costa conceived and designed the experiments, analyzed the data, contributed materials/analysis tools, wrote the paper, prepared figures and/or tables, reviewed drafts of the paper.  Luiz Gadelha conceived and designed the experiments, wrote the paper, reviewed drafts of the paper.  Marcelo Ribeiro-Alves contributed materials/analysis tools, wrote the paper, reviewed drafts of the paper.  Fábio Porto wrote the paper, reviewed drafts of the paper.    Data Availability\r\n  The following information was supplied regarding data availability: Github: https://github.com/raquele/GeNNet.  Docker hub: https://hub.docker.com/r/quelopes/gennet/.    Supplemental Information\r\n  Supplemental information for this article can be found online at http://dx.doi.org/10.7717/peerj.3509#supplemental-information. 20/26 21/26 22/26 23/26 24/26 enactment of bioinformatics workflows. Bioinformatics 20(17):3045 3054 DOI 10.1093/bioinformatics/bth361.  Preusse M, Theis FJ, Mueller NS. 2016. miTALOS v2: analyzing tissue specific microRNA function. PLOS ONE 11(3):1 15 DOI 10.1371/journal.pone.0151771. Reynolds AP, Richards G, De La Iglesia B, Rayward-Smith VJ. 2006. Clustering rules: a comparison of partitioning and hierarchical clustering algorithms. Journal of Mathematical Modelling and Algorithms 5(4):475 504 DOI 10.1007/s10852-005-9022-1. Robles AI, Harris CC. 2017. Integration of multiple ``OMIC'' biomarkers: a precision medicine strategy for lung cancer. Lung Cancer 107(2017):50 58  DOI 10.1016/j.lungcan.2016.06.003.  Sandve GK, Nekrutenko A, Taylor J, Hovig E. 2013. Ten simple rules for reproducible computational research. PLOS Computational Biology 9(10):1 4  DOI 10.1371/journal.pcbi.1003285.  Schuler GD, Epstein JA, Ohkawa H, Kans JA. 1996. [10] Entrez: molecular biology database and retrieval system. Methods in Enzymology 266:141 162 DOI 10.1016/S0076-6879(96)66012-1.  Schulze K, Imbeaud S, Letouzé E, Alexandrov LB, Calderaro J, Rebouissou S, Couchy G, Meiller C, Shinde J, Soysouvanh F, Calatayud A-L, Pinyol R, Pelletier L, Balabaud C, Laurent A, Blanc J-F, Mazzaferro V, Calvo F, Villanueva A, Nault J-C, Bioulac-Sage P, Stratton MR, Llovet JM, Zucman-Rossi J. 2015. Exome sequencing of hepatocellular carcinomas identifies new mutational signatures and potential therapeutic targets. Nature Genetics 47(5):505 511 DOI 10.1038/ng.3252. Shade A, Teal TK. 2015. Computing workflows for biologists: a roadmap. PLOS Biology 13(11):e1002303 DOI 10.1371/journal.pbio.1002303.  Siegel RL, Miller KD, Jemal A. 2017. Cancer statistics, 2017. CA: A Cancer Journal for  Clinicians 67(1):7 30 DOI 10.3322/caac.21387.  Smoot ME, Ono K, Ruscheinski J, Wang PL, Ideker T. 2011. Cytoscape 2.8: new features for data integration and network visualization. Bioinformatics 27(3):431 432 DOI 10.1093/bioinformatics/btq675.  Smyth GK. 2004. Linear models and empirical bayes methods for assessing differential expression in microarray experiments linear models and empirical bayes methods for assessing differential expression in microarray experiments. Statistical Applications in Genetics and Molecular Biology 3(1):1 26 DOI 10.2202/1544-6115.1027.  Stonebraker M. 2010. SQL databases v. NoSQL databases. Communications of the ACM 53(4):10 11 DOI 10.1145/1721654.1721659.  Szitenberg A, John M, Blaxter ML, Lunt DH. 2015. ReproPhylo: an environment for reproducible phylogenomics. PLOS Computational Biology 11(9):1 13 DOI 10.1371/journal.pcbi.1004447.  Timmons JA, Szkop KJ, Gallagher IJ. 2015. Multiple sources of bias confound functional enrichment analysis of global omics data. Genome Biology 16(1):Article 186 DOI 10.1186/s13059-015-0761-7. 25/26 Ulitsky I, Maron-Katz A, Shavit S, Sagir D, Linhart C, Elkon R, Tanay A, Sharan R, Shiloh Y, Shamir R. 2010. Expander: from expression microarrays to networks and functions. Nature Protocols 5(2):303 322 DOI 10.1038/nprot.2009.230.  Wang Y-H, Cheng T-Y, Chen T-Y, Chang K-M, Chuang VP, Kao K-J. 2014. Plasmalemmal vesicle associated protein (PLVAP) as a therapeutic target for treatment of hepatocellular carcinoma. BMC Cancer 14(1):815 DOI 10.1186/1471-2407-14-815. Wilde M, Hategan M, Wozniak JM, Clifford B, Katz DS, Foster I. 2011. Swift: a language for distributed parallel scripting. Parallel Computing 37(9):633 652 DOI 10.1016/j.parco.2011.05.005.  Wilson G, Aruliah DA, Brown CT, Chue Hong NP, Davis M, Guy RT, Haddock SHD, Huff KD, Mitchell IM, Plumbley MD, Waugh B, White EP, Wilson P. 2014. Best practices for scientific computing. PLOS Biology 12(1):e1001745 DOI 10.1371/journal.pbio.1001745.  Zhang B, Horvath S. 2005. A general framework for weighted gene co-expression network analysis. Statistical Applications in Genetics and Molecular Biology 4(1):Article17 DOI 10.2202/1544-6115.1128.  Zhang HM, Chen H, Liu W, Liu H, Gong J, Wang H, Guo AY. 2012a. AnimalTFDB: a comprehensive animal transcription factor database. Nucleic Acids Research 40(D1):144 149 DOI 10.1093/nar/gkr965.  Zhang J, Lu K, Xiang Y, Islam M, Kotian S, Kais Z, Lee C, Arora M, Liu Hw, Parvin JD, Huang K. 2012b. Weighted frequent gene co-expression network mining to identify genes involved in genome stability. PLOS Computational Biology 8(8):e1002656 DOI 10.1371/journal.pcbi.1002656.  Zhao S, Fung-Leung WP, Bittner A, Ngo K, Liu X. 2014. Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells. PLOS ONE 9(1):e78644 DOI 10.1371/journal.pone.0078644.  Zichen W, Ma'ayn A, Wang Z, Ma'ayan A. 2016. An open RNA-Seq data analysis pipeline tutorial with an example of reprocessing data from a recent Zika virus study.  F1000Research 5:Article 1574 DOI 10.12688/f1000research.9110.1.     ",
    "sourceCodeLink": "https://github.com/raquele/GeNNet",
    "publicationDate": "0",
    "authors": [
      "Raquel L. Costa",
      "Luiz Gadelha",
      "Marcelo Ribeiro-Alves",
      "Fábio Porto"
    ],
    "status": "Success",
    "toolName": "GeNNet",
    "homepage": ""
  },
  "63.pdf": {
    "forks": 2,
    "URLs": ["github.com/wyguo/RLowPC"],
    "contactInfo": ["Runxuan.zhang@hutton.ac.uk"],
    "subscribers": 1,
    "programmingLanguage": "R",
    "shortDescription": "R function to construct Relevance Low order Partial Correlation gene networks",
    "publicationTitle": "Evaluation and improvement of the regulatory inference for large co-expression networks with limited sample size",
    "title": "Evaluation and improvement of the regulatory inference for large co-expression networks with limited sample size",
    "publicationDOI": "10.1186/s12918-017-0440-2",
    "codeSize": 9087,
    "publicationAbstract": "Background: Co-expression has been widely used to identify novel regulatory relationships using high throughput measurements, such as microarray and RNA-seq data. Evaluation studies on co-expression network analysis methods mostly focus on networks of small or medium size of up to a few hundred nodes. For large networks, simulated expression data usually consist of hundreds or thousands of profiles with different perturbations or knock-outs, which is uncommon in real experiments due to their cost and the amount of work required. Thus, the performances of co-expression network analysis methods on large co-expression networks consisting of a few thousand nodes, with only a small number of profiles with a single perturbation, which more accurately reflect normal experimental conditions, are generally uncharacterized and unknown. Methods: We proposed a novel network inference methods based on Relevance Low order Partial Correlation (RLowPC). RLowPC method uses a two-step approach to select on the high-confidence edges first by reducing the search space by only picking the top ranked genes from an intial partial correlation analysis and, then computes the partial correlations in the confined search space by only removing the linear dependencies from the shared neighbours, largely ignoring the genes showing lower association. Results: We selected six co-expression-based methods with good performance in evaluation studies from the literature: Partial correlation, PCIT, ARACNE, MRNET, MRNETB and CLR. The evaluation of these methods was carried out on simulated time-series data with various network sizes ranging from 100 to 3000 nodes. Simulation results show low precision and recall for all of the above methods for large networks with a small number of expression profiles. We improved the inference significantly by refinement of the top weighted edges in the pre-inferred partial correlation networks using RLowPC. We found improved performance by partitioning large networks into smaller co-expressed modules when assessing the method performance within these modules. Conclusions: The evaluation results show that current methods suffer from low precision and recall for large co-expression networks where only a small number of profiles are available. The proposed RLowPC method effectively reduces the indirect edges predicted as regulatory relationships and increases the precision of top ranked predictions. Partitioning large networks into smaller highly co-expressed modules also helps to improve the performance of network inference methods. The RLowPC R package for network construction, refinement and evaluation is available at GitHub: https://github.com/wyguo/RLowPC.",
    "dateUpdated": "2017-10-16T10:38:37Z",
    "institutions": [
      "Invergowrie",
      "University of Dundee"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-06-08T20:44:55Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Guo et al. BMC Systems Biology     10.1186/s12918-017-0440-2   Evaluation and improvement of the regulatory inference for large co-expression networks with limited sample size     Wenbin Guo  2  3    Cristiane P. G. Calixto  3    Nikoleta Tzioutziou  3    Ping Lin  1    Robbie Waugh  0  3    John W. S. Brown  0  3    Runxuan Zhang  Runxuan.zhang@hutton.ac.uk  2    0  Cell and Molecular Sciences, The James Hutton Institute, Invergowrie ,  Dundee, Scotland DD2 5DA ,  UK    1  Division of Mathematics, University of Dundee ,  Nethergate, Dundee, Scotland DD1 4HN ,  UK    2  Information and Computational Sciences, The James Hutton Institute, Invergowrie ,  Dundee, Scotland DD2 5DA ,  UK    3  Plant Sciences Division, School of Life Sciences, University of Dundee, Invergowrie ,  Dundee, Scotland DD2 5DA ,  UK     2017   11    9  6  2017    4  11  2016     Background: Co-expression has been widely used to identify novel regulatory relationships using high throughput measurements, such as microarray and RNA-seq data. Evaluation studies on co-expression network analysis methods mostly focus on networks of small or medium size of up to a few hundred nodes. For large networks, simulated expression data usually consist of hundreds or thousands of profiles with different perturbations or knock-outs, which is uncommon in real experiments due to their cost and the amount of work required. Thus, the performances of co-expression network analysis methods on large co-expression networks consisting of a few thousand nodes, with only a small number of profiles with a single perturbation, which more accurately reflect normal experimental conditions, are generally uncharacterized and unknown. Methods: We proposed a novel network inference methods based on Relevance Low order Partial Correlation (RLowPC). RLowPC method uses a two-step approach to select on the high-confidence edges first by reducing the search space by only picking the top ranked genes from an intial partial correlation analysis and, then computes the partial correlations in the confined search space by only removing the linear dependencies from the shared neighbours, largely ignoring the genes showing lower association. Results: We selected six co-expression-based methods with good performance in evaluation studies from the literature: Partial correlation, PCIT, ARACNE, MRNET, MRNETB and CLR. The evaluation of these methods was carried out on simulated time-series data with various network sizes ranging from 100 to 3000 nodes. Simulation results show low precision and recall for all of the above methods for large networks with a small number of expression profiles. We improved the inference significantly by refinement of the top weighted edges in the pre-inferred partial correlation networks using RLowPC. We found improved performance by partitioning large networks into smaller co-expressed modules when assessing the method performance within these modules. Conclusions: The evaluation results show that current methods suffer from low precision and recall for large co-expression networks where only a small number of profiles are available. The proposed RLowPC method effectively reduces the indirect edges predicted as regulatory relationships and increases the precision of top ranked predictions. Partitioning large networks into smaller highly co-expressed modules also helps to improve the performance of network inference methods. The RLowPC R package for network construction, refinement and evaluation is available at GitHub: https://github.com/wyguo/RLowPC.    Gene co-expression networks  Gene regulatory networks  Network method evaluation  Partial correlation  Synthetic data       Background\r\n  Over the last fifteen years, there has been a growing interest in reverse engineering of Gene Regulatory Networks (GRNs) that aim to infer complex graphs representing transcriptional regulatory relationships, directly from gene expression profiles [  [1- 15]. Due to its low computational complexity as well as lower requirements for the number of samples, co-expression network analysis has been widely used to infer gene regulatory networks from high throughput expression data, such as microarray or RNA-seq data [  [10, 1619 ]. Typically thousands of genes/transcripts of special interest (e.g. differentially expressed) are utilized to construct the co-expression network in an experiment. Top candidates whose expression correlates with the gene of interest are usually further examined to identify novel regulators/targets. Despite this approach being widely used, there is a general lack of studies on the precision (the fraction of inferred regulatory relationships that are correct) and recall (the fraction of regulatory relationships that are inferred) expected.  Considerable effort has been made to evaluate the performance and robustness of GRN inference methods. The majority of evaluations were implemented on in silico datasets simulated from reference networks with sizes up to a few hundred or 1-2000 genes. Numerous studies using a range of network sizes, time-series data and perturbations have compared different analysis methods. Results are variable in terms of the top-performing method (Summaries in Additional file 1: Table S1). A series of studies have been carried out by the Dialogue for Reverse Engineering Assessments and Methods (DREAM) project, which generates challenges and organizes contests annually. The DREAM3 challenge presents gene network inference problems based on in silico networks of sizes ranging from 10, 50 and 100 genes [ 20-24 ]. Gene expression data was simulated using these networks for the following scenarios: 1) the steady state of the unperturbed networks, as well as steady state of the network where every gene is knocked out or down; and 2) 4, 23 and 46 different time series for the size 10, 50 and 100 networks respectively, with 21 time points for each time series. For example, for the network of size 100, there are a total of 1067 gene expression profiles with different perturbations and knockout/knockdown experiments available to make the inference. The inference methods: Scan Bayesian Model Averaging (ScanBMA), Gene Network Inference with Ensemble of trees (GENIE3) and Minimum Redundancy NETworks using Backward elimination (MRNETB) were the top performers in three different studies using the DREAM4 challenge time-series data, which is composed of five perturbation experiments for size 10 networks and ten perturbation experiments for size 100 networks, each with 21 time points [ 24-27 ] (Additional file 1: Table S1). Besides the DREAM benchmark datasets, the Bayesian Network (BN), Graphical Gaussian models (GGMs) and Relevance Network (RN) methods were compared using expression simulations of 100 sample points for a size 11 network with BN and GGM performing best [ 12 ]. The Algorithm for the Reconstruction of Accurate Cellular Networks (ARACNE) method had a much better performance than BN and RN on expression data with 1000 samples simulated from size 100 networks [ 28 ] while MRNET was the top ranked method when compared to the RN, ARACNE and Context likelihood or relatedness (CLR) methods on 30 datasets with different network sizes (from 100 to 1000) and sample sizes (from 100 to 1000) [ 29 ] (Additional file 1: Table S1).  A few studies aimed to evaluate network methods on larger networks of a few thousand genes. In the DREAM5 challenge, Least Absolute Shrinkage and Selection Operator (LASSO), CLR and GENIE3 are top performers among more than 30 network inference methods on a size 1643 network with 805 simulated gene expression profiles, where a list of regulators (potential transcriptional factors) are given [ 30 ]. Ten network inference methods on size 1000 network from S. Rogers [ 31 ], size 300 and 1000 networks from SynTReN [ 32 ] and size 1565 and 2000 networks from GeneNetWeaver (GNW) [ 24 ] were assessed using simulated datasets of 1000, 800, 1000, 1565 and 2000 experiments individually. CLR, GENIE3 and MRNET were the top performers in this study [ 33 ]. Similarly, ARACNE, GeneNet, Weighted Correlation Network Analysis (WGCNA) and Sparse PArtial Correlation Estimation (SPACE) were compared using size 17, 44, 83, 231, 612 and 1344 networks over datasets with 20, 50, 100, 200, 500 and 1000 sample points simulated from Gaussian distribution [ 34 ]. GeneNet ranked in the first place followed by ARACNE (Additional file 1: Table S1).  Despite the large number of evaluation studies, none have explored the normal experimental situation where a regulatory network is generated which involves hundreds and thousands of genes with only a small number of profiles being available. The assessments in the literature were based on either small and medium sized networks or datasets with a large number of samples. The evaluation conclusions were also based on a large amount of simulated expression profiles which would be difficult to validate experimentally due to the prohibitive cost or the amount of work in real experiments [ 35, 36 ].  Distinguishing direct regulatory interactions from indirect associations has been one of the major challenges in gene regulatory network constructions [ 2, 21 ] (see Fig. 1a). Partial Correlation (PC) is one of the methods used as a solution to distinguish direct from indirect edges of each pair of candidates by calculating the correlations after removing the linear dependencies from the remaining genes (see Fig. 1b). Other methods dealing with indirect connections include Partial Correlation coefficient with Information Theory (PCIT), ARACNE, MRNET and MRNETB. PCIT and ARACNE use the Information Theory of Data Processing Inequality method to remove the weakest gene association in each possible triplet structure in a network [ 37 ]. PCIT uses first order PC (removing the linear dependencies from the third gene in each possible triplet) to measure the significance of edge associations [ 38 ], whilst ARACNE uses Mutual Information (MI) to measure the associations between any two edges in each possible triplet [ 28 ]. MRNET uses a minimum redundancy feature selection method [ 39 ], where for each candidate gene in a MI network, it selects a subset of its highly relevant genes while minimising the MI between the selected genes [ 29 ]. MRNETB is an improved version of MRNET using a backward selection strategy starting from assuming that all genes are connected to the candidates. Less relevant genes are eliminated until the difference between the MI between a candidate and its neighbours and the MI within the neighbours are optimised [ 27 ].  Given that the search space for regulatory relationships expands factorially with the number of genes included in the network, the precision and recall of regulatory inference decrease with the increase of the network size. As gene clusters with highly cohesive patterns give rise to high correlations between all pairs of the genes in that cluster, the top ranked highly co-expressed genes may also be prone to errors of indirect associations. Here, we have developed a new method named Relevance Low order Partial Correlation (RLowPC), which is a refinement of top inferred edges by Partial Correlation methods. RLowPC selects top ranked edges from an inferred PC network as a reduced search space for indirect edges. We evaluated RLowPC alongside PC, PCIT, ARACNE, MRNET, MRNETB, and CLR on simulated time-series data and the summaries of the evaluated network inference methods is shown in Table 1. Precision and Area Under Precision-Recall curves (AUPR) were used as metrics to show that RLowPC outperforms the other methods.    Methods\r\n  Relevance low order partial correlation (RLowPC) The conventional pair-wise PC measures correlations after linear dependencies on all the remaining genes are removed, the majority of which may not connect to the candidates, especially in large networks where the majority of the genes only have few linked neighbours [ 40, 41 ]. Low order partial correlation methods have been proposed and  Cor-based Yes Yes Yes Yes  MI-based Yes Yes Yes Yes Yes  Ref. Nine correlation-based, MI-based and random network inference methods have been compared and evaluated in this study. The methods are classified into two main groups: Deal with indirect edges explicitly and Not deal with indirect edges utilized in the past to reduce computational complexity without much sacrifice in prediction accuracy. For example, de la Fuente et al. [ 42 ] proposed to calculate up to second order partial correlations regressing against all the remaining genes. This method was improved by confining the second order partial correlation calculation only in cases where both zero and one order PC are non-zero [ 43 ]. Our proposed RLowPC method, firstly, reduces the search space by only picking the top ranked genes from partial correlation analysis and, secondly, computes the PC by only removing the linear dependencies from the shared neighbours in the confined search space, largely ignoring the genes showing lower association and which are less relevant in the pair-wise PC calculation. The implementation details are shown below:  For PC and shrinkage PC calculation we have used ppcor R package [ 44 ] and corpcor R package [ 45 ], respectively.   Gene expression data simulation\r\n  The main purpose of this study is to evaluate the performance of different network inference methods on datasets that reflect real experimental setup: large number of genes in the network with limited sample sizes and perturbations. Here, to evaluate the proposed methods comprehensively, large scale gene expression datasets were generated based on a variety of network structures using GNW version 3.1 [ 22, 24 ]. We used in silico size 100 networks in DREAM4, extracted size 500 and 1000 networks from a source E.coli network with 1565 nodes and 3758 edges and size 2000 and 3000 networks from a Yeast source network with 4441 nodes and 12,873 edges as reference networks. The source networks were provided by GNW [ 22, 24 ]. The networks were denoted as GNW100, GNW500, GNW1000, GNW2000 and GNW3000. Summaries for data generation can be found in Table 2. For each size, network extraction was repeated five times yielding five networks with different structures and kinetics for statistical analysis of the results. To generate the time-series, transcription kinetic models of reference networks were firstly generated in GNW by removing self-regulatory interactions and randomly assigning transcription factor (TF) genes to groups to produce protein binding complexes. In the time-series simulation procedure, Stochastic Differential Equations (SDEs) were used to model the transcription kinetics, gene activation by protein complexes, gene perturbations, mRNA and protein production and degradation. One-third of the genes in each time-series were randomly selected and perturbed from steady state at the initial time-point. Perturbations were implemented by varying the activation strengths in the protein binding simulations to enhance or inhibit the downstream expression of target genes. The perturbations were sustained until the middle of the time-series at time point 11 when the activation strengths were changed back to initial levels. A random noise term proportional to production and degradation was introduced in the SDE model, inducing high noise for activated genes and low noise for inactivated genes. The coefficient to control the noise amplitude was set to 0.05. Another random noise, which was independent to the noise in SDEs, was added at the final step to the expression data to simulate technical variations [ 46 ]. The parameters for activation strengths, production, degradation and noises were set as defaults in GNW. The time-series generation were repeated five times yielding five different time-series with different initial conditions and perturbations. Average results obtained from these time series as well as five different network structures are reported in this study. Parameter setting details are shown in Additional file 1: Figure S3 and Additional file 2: Configuration file for GeneNetWeaver. Three biological replicates were generated for each timeseries. By using the replicates, analysis of variance was carried out to select genes with significant expression changes across all 21 time-points with p-value cut-off of 0.001. In each experiment, there are only 63 gene expression profiles generated from one perturbation used for the network construction. The repeated generation of time series data as well as the network extraction are only used for statistical purposes to take the average and calculate the variations.    Evaluation of the network inference methods\r\n  Besides the methods mentioned earlier, we also included Pearson correlation, which has been the most commonly used method to identify correlated gene pairs, as well as random guessed network, which serves a baseline for network inference performances. We also included the CLR method, which although not partial correlationbased, has been shown to perform well in several studies [ 30, 33, 47-49 ]. We divided the methods under investigation into two groups. Group one includes all the methods that deal with indirect edges explicitly, which are RLowPC, PC, PCIT, ARACNE, MRNET and MRNETB. Group two are the methods which do not deal with indirect edges explicitly and they are CLR, Pearson correlation and random guessed networks. For MI-based methods, such as ARACNE, MRNET, MRNETB and CLR networks, we have used the minet R package with default parameters [ 50 ]. The MI matrices of the methods were approximated using Pearson correlation directly from continuous timeseries data [ 27, 49 ]. The PC matrices were calculated by a shrinkage approach using corpcor R package [ 45 ]. The Boolean PCIT adjacency matrices were calculated using PCIT R package [ 38, 51 ], which was used as a weight to Pearson correlation networks [ 33 ]. For the RLowPC method, the top (1500, 2000, 3000, 5000, 8000) weighted edges of inferred PC networks in GNW100, GNW500, GNW1000, GNW2000 and GNW3000 datasets were selected as search space for indirect edges. Details for tools used in the network inference analyses can be found in Table S2 in Additional file 1. In each inferred network, the top 1000 edge predictions was used to calculate True Positive (TP), False Positive (FP), True Negative (TN) and False Negative (FN) by comparing to the reference networks. The precision (TP/(TP + FP)) and pAUPR (partial plot of Area Under Precision against Recall = TP/(TP + FN)) values were calculated by picking the top ranked edges. pAUROC (partial Area Under the Receiver-Operating curve) was also calculated and the results were shown in the Supplementary material. All the evaluation of network inference methods was based on undirected network structures and the self-regulation edges were removed.     Results\r\n  RLowPC significantly improves the precision and recall in top predictions Figure 2 illustrates the average pAUPR values, which are the partial Area Under Precision against Recall of the top 1000 predictions, for the different methods for different network sizes. Firstly, all methods except one case for ARACNE, outperformed the random guessed network, which proves the utility of such co-expression network analysis methods. Secondly, the performances of all methods are quite consistent across different network sizes. Within Group One, RLowPC consistently performs better than all of the other methods, with MRNET/MRNETB being the next best. Within Group two, CLR clearly outperforms the most commonly employed Pearson correlation method. The differences of pAUPR values between different methods were determined using a Student t-test in pairs between RLowPC and the other eight methods (Fig. 2). Results show that the RLowPC method is able to improve the pAUPR among the top edges significantly compared to other methods except for a few cases. The pAUROC show similar results (Additional file 1: Figure S1).  We further divided the top 1000 predictions into groups of top 1-100, 101-500 and 501-1000 (Fig. 3). The plots indicate that, once again, the precision of RLowPC method outperformed all others, regardless of which group within the top 1000 genes were selected for investigation. MRNET, MRNETB and CLR again showed slightly better performance than PC, PCIT and ARACNE and correlation methods. It is noteworthy that the precisions of all the methods are extremely low in large networks. For example, the precision median of RLowPC in the GNW3000 networks is around 0.006, which indicates that in the top 100 predictions, only 0.6 (0.6%) edges are true predictions.  Clustering before network inference could improve the precision and recall in top predictions Given that precision and recall is very low among the top predictions for all methods for large networks, we explored whether precision can be improved by dividing the large networks into smaller highly cohesive clusters. Using the time-series data generated for GNW3000 as described above, all genes were clustered into nonoverlapping co-expressed modules using the R package Weighted Correlation Network Analysis (WGCNA) with default settings [  [52,  53]. Then, network inference and evaluation were carried out separately and individually in each module. Essentially, WGCNA was used to break a big network into smaller non-overlapping subnetworks, at which point we carried out the network inference and evaluations within these smaller networks with the same time-series data. The pAUPR values were averaged across all the modules and it did not include genes that do not fit in any module (grey module). Similar to the simulation settings above, the clustering and evaluation procedures were repeated for five network structures, where five different time-series data were simulated for each structure. The average results were obtained. The average pAUPR values and precision distribution of the top 1000 predictions are presented in Fig. 4. Compared with the results of GNW3000 in Figs. 2 and 3, all methods evaluated have improved when the WGCNA method was used. This can be seen with the scale of average pAUPR values which increased from 1.0 × 10−5 to 1.0 × 10−3 (Fig. 4a), while the average precision of the top 1000 predictions has changed from 3.1 × 10−3 to 5.7 × 10−3 when the WGCNA method is used (Fig. 4b). The pAUPR value of RLowPC method is again significantly better than PC, PCIT, ARACNE, correlation and random networks. In the groups of top 1100 and 101-500, the precision of RLowPC is better than the other eight methods and in top 501-1000 it is only better than PC, PCIT, correlation and random networks. The superior performances of RLowPC when the WGCNA method is used are also observed on the pAUROC plots (Additional file 1: Figure S2).    Discussion\r\n  The performance of different network inference methods varies according to network structures, data quantity and quality, and methodologies. The insufficiency of sampling and the high complexity of regulation kinetics prevent precise predictions of large gene regulatory networks. As a large regulatory network is often underdetermined using a small number of samples, there exists multiple plausible solutions, which cannot be distinguished by the information presented in the sample.  This uncertainty in the inference of gene regulatory networks has been termed in some studies as \u201cinferability\u201d [ 54, 55 ]. Although our study mainly focuses on the network inference methods, special attention should be paid to generate the most informative data when trying to construct the accurate and comprehensive underlying GRNs.  The co-expression based methods capture the relationships between genes which are perturbed directly or indirectly. Therefore, the multifactorial intervention on the regulators, as discussed in [ 30 ], or hub genes rather than on target genes will generate expression data that is more informative for regulatory inference. Results presented here are based on the time-series data corresponding to one perturbation simulation to reflect more typical experimental conditions. When there are more experiments available with different sets of genes being perturbed, the inference accuracy tends to increase with the increased number of gene expression profiles available [ 35, 56 ]. Our data also show that the precision median increases as the experiment size increase (Fig. 5a). Using RLowPC, a precision of 0.014 is achieved in one experiment, while using PC on 10 experiments only leads to a precision of 0.012. Thus refining the top inferred edges using RLowPC is more effective in improving precision than generating data for nine more experiments.  With the number of possible edges growing factorially with increasing number of genes, the sparsity issue in large networks also becomes more prevalent. We observed that precision of the network inference methods increases with the increase of the network density (thus the decrease of network sparsity) as shown in Fig. 5b.  Several types of methods have been explored to alleviate this problem including using network inference methods that allow imposing sparsity constraints [ 31, 57, 58 ] or leveraging on multiple datasets on other species that are evolutionary connected [ 59 ], or incorporating prior information, such as genetic maps [ 60 ], pathways, transcription factor binding, protein-protein interactions, gene ontology, epigenetics, literature, as well as functional association databases to increase the efficiency and reduce the search space by focusing on the top weighted edges [ 61 ]. RLowPC method also uses a twostep approach to select on the high-confidence edges first. Thus there is enrichment of true regulatory relationships for the second step of the inference, which explains the improvement of gene regulatory inference performances. Similarly clustering using WGCNA also groups highly correlated and connected genes together, which we see an increase of proportion in the true regulatory relationships. This has a similar effect on the network inference performances.  AUROC and AUPR curves have been popular matrices in the evaluation of network performances [ 21, 30, 33, 34 ]. AUROC measures the area under the curve between true positive rate/recall, which is calculated as (TP/(TP + FN)) and false positive rate, which is calculated as (FP/(FP + TN) = FP/N). As in big sparse networks, the negatives (N) greatly exceed the positives (P), thus false positive rate is less discriminative when the network inference methods have very different abilities to largely reduce the false positive predictions.  In the meantime, AUPR measures the area under the curve between precision and recall. Precision, which is calculated as (TP/(TP + FP) = 1-FP/(TP + FP)), captures the impacts of TP or FP in the evaluation of big networks. Studies have shown that AUPR is more informative than AUROC in evaluation on datasets where the TP and TN is imbalanced. Large sparse networks are typical cases [ 62, 63 ]. As the purpose of this study is to focus on the utility of co-expression network inferences methods to prioritize the novel regulatory genes pairs for experimental validation from the top ranked edges, we mainly focused on partial AUPR curve to evaluate the accuracies and power of the network inference methods on the top weighted edges, which is more relevant than using the entire area under the curve [ 64, 65 ].  One parameter required by the RLowPC method is a number to define the search space for indirect edge reduction. For large networks, a reduction space larger than the size of the top weighted edges under investigation should be applied but has to take into account the computational search space and time required. Table 3 lists the average computational time for different sizes of search space. A useful prior may be to enrich the reduction space with true gene connections. For example, cluster analysis and functional annotation using other experimental data or regulatory databases could be carried out before network inference to investigate the functions and modules of interest. The computational time is calculated based on Dell, Windows 7, 64-bit Operating system with 16.0GB RAM and Intel(R) Core (TM) i7-4790 CPU @ 3.60GHz 3.60 GHz processor    Conclusions\r\n  In this paper, we present analysis of the evaluation of different regulatory network inference methods with special emphasis on large scale gene regulatory networks with limited sample size. We developed a new method, RLowPC, which improves the precision and recall in the top weighted PC network structures. We evaluated all methods on time-series datasets with only one perturbation for various sizes of networks using a small number of samples, which reflect better the high throughput gene expression data usually generated in laboratory experiments. We also demonstrated that clustering large co-expression networks into functional and informative co-expressed modules, improved the precision and recall of the regulatory inference.    Additional files\r\n  Additional file 1: File contains additional Figures and Tables. Figure S1. Bar plots of pAUROC values for top 1000 edge predictions. Figure S2. Bar plots of pAUROC values of top 1000 predictions for GNW3000 modulebased. Figure S3. GNW settings for data simulation. Figure S4. Examples of evaluation results. Table S1. Summaries of evaluation of gene network inference methods. Table S2. R packages used to construct and evaluate GRNs. (DOCX 1867 kb) Additional file 2: Configuration file for GeneNetWeaver (GNW). The settings in the file were load in GNW to generate synthetic data. (DOCX 28 kb) Abbreviations ARACNE: Algorithm for the reconstruction of accurate cellular networks; AUROC: Area under the receiver-operating characteristic curve; AUPR: Area under the precision recall curve; BN: Bayesian network; CLR: Context likelihood or relatedness; DREAM: Dialogue for reverse engineering assessments and methods; FN: False negative; FP: False positive; GCN: Gene co-expression network; GENIE3: Gene network inference with ensemble of trees; GNW: GeneNetWeaver; GRN: Gene regulatory network; MI: Mutual information; MRNET: Minimum redundancy networks; MRNETB: Minimum redundancy networks using backward elimination; pAUROC: Partial area under the receiver-operating characteristic curve; pAUPR: Partial area under the precision-recall curve; PC: Partial correlation; PCIT: Partial correlation coefficient with information theory; RN: Relevance network; RLowPC: Relevance low order partial correlation; SDEs: Stochastic differential eqs.; TN: True negative; TP: True positive; WGCNA: Weighted correlation network analysis Acknowledgements We would like to thank Dr. Katherine Denby (University of York) for suggestions on gene network construction and Iain Milne (James Hutton Institute) for technical assistance.  Funding This project was supported by joint PhD studentship Program from the James Hutton Institute and the University of Dundee [to W.G] and the Scottish Government Rural and Environment Science and Analytical Services division (RESAS) [to J.B., R.W. and R.Z.].  Availability of data and materials Project name: RLowPC Project home page: https://github.com/wyguo/RLowPC Version: 0.1 Operating system(s): Platform independent Programming language: R (version 3.2.3) Other requirements: corpcor, ggplot2, minet, PCIT, ppcor and WGCNA R packages.  Licence: GPL-3 Synthetic data are available upon request. Please contact corresponding author Runxuan Zhang: Runxuan.zhang@hutton.ac.uk Author's contributions RZ and WG defined the project and design the simulation experiments. WG carried out the simulations and analyses. WG, RZ, JB and CC wrote the manuscript. All the authors engaged in discussions to improve the project and made contributions to improve the final version of manuscript. All authors read and approved the final manuscript.  Competing interests The authors declare that they have no competing interests.    ",
    "sourceCodeLink": "https://github.com/wyguo/RLowPC",
    "publicationDate": "0",
    "authors": [
      "Wenbin Guo",
      "Cristiane P. G. Calixto",
      "Nikoleta Tzioutziou",
      "Ping Lin",
      "Robbie Waugh",
      "John W. S. Brown",
      "Runxuan Zhang"
    ],
    "status": "Success",
    "toolName": "RLowPC",
    "homepage": ""
  },
  "89.pdf": {
    "forks": 2,
    "URLs": [
      "bernatgel.github.io/karyoploter_",
      "bernatgel.github.io/karyoploter_tutorial"
    ],
    "contactInfo": ["bgel@igtp.cat"],
    "subscribers": 1,
    "programmingLanguage": "R",
    "shortDescription": "karyoploteR - An R/Bioconductor package to plot arbitrary data along the genome",
    "publicationTitle": "karyoploteR: an R/Bioconductor package to plot customizable genomes displaying arbitrary data",
    "title": "karyoploteR: an R/Bioconductor package to plot customizable genomes displaying arbitrary data",
    "publicationDOI": "10.1093/bioinformatics/btx346",
    "codeSize": 1439,
    "publicationAbstract": "Motivation: Data visualization is a crucial tool for data exploration, analysis and interpretation. For the visualization of genomic data there lacks a tool to create customizable non-circular plots of whole genomes from any species. Results: We have developed karyoploteR, an R/Bioconductor package to create linear chromosomal representations of any genome with genomic annotations and experimental data plotted along them. Plot creation process is inspired in R base graphics, with a main function creating karyoplots with no data and multiple additional functions, including custom functions written by the end-user, adding data and other graphical elements. This approach allows the creation of highly customizable plots from arbitrary data with complete freedom on data positioning and representation. Availability and implementation: karyoploteR is released under Artistic-2.0 License. Source code and documentation are freely available through Bioconductor (http://www.bioconductor.org/pack ages/karyoploteR) and at the examples and tutorial page at https://bernatgel.github.io/karyoploter_ tutorial. Contact: bgel@igtp.cat",
    "dateUpdated": "2017-10-20T13:47:31Z",
    "institutions": ["Program for Predictive and Personalized Medicine of Cancer - Germans Trias i Pujol Research Institute (PMPPC-IGTP)"],
    "license": "No License",
    "dateCreated": "2016-06-07T16:32:27Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx346   karyoploteR: an R/Bioconductor package to plot customizable genomes displaying arbitrary data     Bernat Gel  0    Eduard Serra  0    0  Hereditary Cancer Group, Program for Predictive and Personalized Medicine of Cancer - Germans Trias i Pujol Research Institute (PMPPC-IGTP) ,  Campus Can Ruti, Badalona ,  Spain     2017   1  1  3   Motivation: Data visualization is a crucial tool for data exploration, analysis and interpretation. For the visualization of genomic data there lacks a tool to create customizable non-circular plots of whole genomes from any species. Results: We have developed karyoploteR, an R/Bioconductor package to create linear chromosomal representations of any genome with genomic annotations and experimental data plotted along them. Plot creation process is inspired in R base graphics, with a main function creating karyoplots with no data and multiple additional functions, including custom functions written by the end-user, adding data and other graphical elements. This approach allows the creation of highly customizable plots from arbitrary data with complete freedom on data positioning and representation. Availability and implementation: karyoploteR is released under Artistic-2.0 License. Source code and documentation are freely available through Bioconductor (http://www.bioconductor.org/pack ages/karyoploteR) and at the examples and tutorial page at https://bernatgel.github.io/karyoploter_ tutorial. Contact: bgel@igtp.cat       -\r\n  *To whom correspondence should be addressed. Associate Editor: John Hancock    1 Introduction\r\n  Data visualization is an important part of data analysis. It efficiently summarizes complex data, facilitates exploration and can reveal nonobvious patterns in the data. A natural representation for genomic data is positioned along the genome next to the ideograms of the different chromosomes. This type of representation is specially useful to identify the relation between different types of experimental data and genomic annotations. Various genomic visualization tools are available. Circos  (Krzywinski et al., 2009)  produces highly customizable high quality circular plots, as does ites R counterpart RCircos  (Zhang et al., 2013) . There are other R packages capable of plotting whole genome diagrams such as: ggbio  (Yin et al., 2012) , based on the grammar of graphics that can produce different plot types including ideogram and karyogram plots; IdeoViz  (Pai and Ren, 2014) , to plot binned data along the genome either as lines or bars; or chromPlot  (Orostica and Verdugo, 2016) , to plot up to four datasets given in a predefined format. These packages are either limited in the amount or type of data they can plot (IdeoViz and chromPlot) or have limited customization options (ggbio). In addition, the Bioconductor package Gviz  (Hahne and Ivanek, 2016)  is a powerful tool to create trackbased plots of diverse biological data but it does not produce plots of the whole genome. There is a lack of a tool in R to create non-circular whole genome plots, able to plot arbitrary data in any organism and with ample customization capabilities.  Here we present karyoploteR, an extendable and customizable R/ Bioconductor package to plot genome ideograms and genomic data positioned along them. It's inspired on the R base graphics, building plots with multiple successive calls to simple plotting functions.    2 Features\r\n  The interface of karyoploteR and the process to create a complete plot is very similar to that of base R graphics. We first create a simple or even empty plot with an initializing function and then add additional graphic elements with successive calls to other plotting functions. The first call creates and initializes the graphical device and returns a karyoplot object with all the information needed to add data to it. The karyoplot object contains a coordinate change function mapping genomic coordinates into plotting coordinates, which is used by all plotting functions. Plotting functions are classified into three groups: the ones adding non-data elements to the plot and two data plotting groups, low-level functions and high-level functions. karyoploteR also takes some ideas from Circos, such as not defining fixed tracks but leaving complete freedom to the user with respect to data positioning using the r0 and r1 parameters. All non-data elements in the karyoplot (main title, chromosome names, . . .) are drawn by specific functions. These functions accept standard graphical parameters but it's also possible to swap them for custom functions if a higher level of customization is needed.   2.1 Ideogram plotting\r\n  Ideogram plotting is the basic functionality of karyoploteR. Default ideograms can be plotted with a single function call (Fig. 1A). However, itwe possible to customize them: positioning the chromosomes in different arrangements, representing just a subset of chromosomes or change whether the cytobands are included and how they are represented. It is also possible to create different data plotting regions either above or below the ideograms as well as customizing all sizings and margins by changing the values stored in plot.params.    2.2 Not only human\r\n  karyoploteR is not restricted to human data in any way. It is possible to specify other organisms when creating a karyoplot. Genome data for a small set of organisms is included with the package and it will use functionality from regioneR  (Gel et al., 2016)  to get it from UCSC or Bioconductor for other genomes. If an organism is not available anywhere, it is possible to plot it providing its genome information. Therefore, if required, it's possible to create custom genomes for specific purposes.    2.3 Data plotting\r\n  Data plotting functions are divided in two groups: low-level and high-level. Low-level data plotting functions plot graphical primitives such as points, lines and polygons. Except for the additional chr parameter, they mimic the behaviour of their base graphics counterparts including the usage of most of the standard graphical parameters. These plotting functions offer a flexible signature and are completely data agnostic: they know nothing about biological concepts, giving the user total freedom on how to use them. Highlevel functions, in contrast, are used to create more complex data representations. They understand some basic concepts such as 'genomic region' and they usually perform some kind of computation prior to data plotting (Fig. 1B).    2.4 Customization and extensibility\r\n  In addition to customizing sizings and margins and the using custom genomes, karyoploteR can be extended with custom plotting functions. All internal functions, including the main coordinate change function, are exported and documented in the package vignette. With this it is possible to create custom plotting functions adapted to specific data types and formats.     3 Conclusion\r\n  We have developed an R/Bioconductor package, karyoploteR, to plot arbitrary genomes with data positioned on them. It offers a flexible API inspired in R base graphics, with low-level functions to plot graphical primitives and high-level functions to plot complex data. The plots are highly customizable in data positioning and appearance and it is possible to extend the package functionality with custom plotting functions. karyoploteR requires R 3.4 and Bioconductor 3.5. More information and examples can be found at the package Bioconductor page and at https://bernatgel.github.io/karyoploter_tutorial    Acknowledgements\r\n  We thank Roberto Malinverni for his insightful comments and the IGTP HPC Core Facility and In~aki Martınez de Ilarduya for his help.    Funding\r\n  This work has been supported by: the Spanish Ministry of Science and Innovation, Carlos III Health Institute (ISCIII) (PI11/1609; PI14/ 00577)(RTICC RD12/0036/008) Plan Estatal de I þ D þ I 2013-16, and cofinanced by the FEDER program; the Government of Catalonia (2014 SGR 338); and the Spanish Association Against Cancer (AECC).  Conflict of Interest: none declared. karyoploteR: customizable genome plots displaying arbitrary data    ",
    "sourceCodeLink": "https://github.com/bernatgel/karyoploteR",
    "publicationDate": "0",
    "authors": [
      "Bernat Gel",
      "Eduard Serra"
    ],
    "status": "Success",
    "toolName": "karyoploteR",
    "homepage": ""
  },
  "37.pdf": {
    "forks": 0,
    "URLs": ["github.com/EBjerrum/pICalculax"],
    "contactInfo": ["esben@wildcardconsulting.dk."],
    "subscribers": 0,
    "programmingLanguage": "Python",
    "shortDescription": "Isoelectric point (pI) predictor for chemically modified peptides and proteins.",
    "publicationTitle": "pICalculax: Improved Prediction of Isoelectric Point for Modified Peptides",
    "title": "pICalculax: Improved Prediction of Isoelectric Point for Modified Peptides",
    "publicationDOI": "10.1021/acs.jcim.7b00030",
    "codeSize": 23,
    "publicationAbstract": "The isoelectric point of a peptide is a physicochemical property that can be accurately predicted from the sequence of the peptide when the peptide is built from natural amino acids. Peptides can however have chemical modifications, such as phosphorylations, amidations, and unnatural amino acids, which can result in erroneous predictions if not accounted for. Here we report on an open source program, pICalculax, which in an extensible way can handle pI calculations of modified peptides. Tests on a database of modified peptides and experimentally determined pI values show an improvement in pI predictions when taking the modifications into account. The correlation coefficient improves from 0.45 to 0.91, and the root-mean-square deviation likewise improves from 3.3 to 0.9. The program is available at https://github.com/EBjerrum/pICalculax",
    "dateUpdated": "2017-01-18T10:38:09Z",
    "institutions": [
      "Biochemfusion Aps",
      "Wildcard Pharmaceutical Consulting",
      "Zealand Pharma A/S"
    ],
    "license": "BSD 3-clause \"New\" or \"Revised\" License",
    "dateCreated": "2016-12-23T09:52:04Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     J. Chem. Inf. Model.     10.1021/acs.jcim.7b00030   pICalculax: Improved Prediction of Isoelectric Point for Modified Peptides     Esben J. Bjerrum  0  1    Jan H. Jensen  0    Jakob L. Tolborg    0  Biochemfusion Aps ,  Løvspringsvej 4C, 1.tv, 2920 Charlottenlund ,  Denmark    1  Wildcard Pharmaceutical Consulting ,  Frødings Alle 41, 2860 Søborg ,  Denmark    2  Zealand Pharma A/S ,  Smedeland 36, 2600 Glostrup ,  Denmark     2017   57  1723  1727    16  1  2017     The isoelectric point of a peptide is a physicochemical property that can be accurately predicted from the sequence of the peptide when the peptide is built from natural amino acids. Peptides can however have chemical modifications, such as phosphorylations, amidations, and unnatural amino acids, which can result in erroneous predictions if not accounted for. Here we report on an open source program, pICalculax, which in an extensible way can handle pI calculations of modified peptides. Tests on a database of modified peptides and experimentally determined pI values show an improvement in pI predictions when taking the modifications into account. The correlation coefficient improves from 0.45 to 0.91, and the root-mean-square deviation likewise improves from 3.3 to 0.9. The program is available at https://github.com/EBjerrum/pICalculax       INTRODUCTION\r\n  The isoelectric point (pI) of a protein or peptide is the pH at which the net charge on the molecular ensemble is zero. This physicochemical property can be determined experimentally with experiments such as fixed pH gradient gel electrophoresis1 or capillary electrophoresis.2 When the pH of the solution matches the pI, the solute will not migrate in an electric field. Knowledge about the pI can lead to a better understanding of function3 and be of practical aid in the laboratory to avoid precipitation and understand behavior during ion exchange chromatography.  The pI of a regular peptide under denaturing conditions can be predicted from the sequence.4 By counting the acidic and basic groups and using their known pKa values, the charge at a given pH can be estimated and the pH determined at zero charge. This is done by calculating and summing the partial charges for all acid−base groups by solving a modified Henderson−Hasselbalch eq (eq 1).  Determining a list of pKa values from protein and peptide sequences containing only natural amino acids is straightforward. However, proteins often undergo post-translational modifications, such as phosphorylations,3 which will lead to the introduction of novel acidic groups. Additionally, artificial modifications can arise during experimental handling of peptides, such as oxidation5 and deamidation.6 Moreover, artificial amino acids and modifications are used during peptide based drug discovery and development.7,8 If the modifications introduce or remove acidic or basic groups, the pI will be affected to a greater or lesser extent.6 To be accurate for modified peptides the pI prediction must take all these modifications into account.  Previous algorithms have covered a limited number of modifications such as terminal block,4,9,10 N-acetylneuraminic acid,4 phosphorylations,9,11 cyclizations,12 and methylations.11 ProMOST has the feature to add extra user defined pKa values for calculation of custom modifications.13 The program pIR combines the sequence information with calculation of chemical descriptors and machine learning predictions.14 However, the algorithms do not have a consistent way of handling the modifications informatically, making them difficult to extend, adapt, and combine.  Here we present an extensible pI prediction algorithm, pICalculaX, which can handle chemical modification effectively by combining bio- and chemoinformatics handling of the molecules. The software has been released as open source on Github (https://github.com/EBjerrum/pICalculax). The code can be used in conjunction with the Python interface to the proprietary Proteax Desktop software15 for handling sequence to structure conversions. No-cost academic licenses can be requested for Proteax Desktop. ■    METHODS\r\n  pI Prediction Algorithm. The algorithm consists of two major stages. First the molecule is analyzed and the pKa values16 of identified acidic or basic groups recorded. Then the pI is predicted by identifying the pH where the sum of partial charges is zero, by solving the Henderson−Hasselbalch equation for the ionization extent (eq 1). Acidic groups need a negative sign of the exponent and nominator.  Charge =  The algorithm works internally with the condensed molfile format.18−20 Molfile formats describe molecular structures in atomic detail with all atoms and bonds. The Proteax software converts between sequence based formats, full atomic description, and a condensed format in between (illustrated in Figure 1). In the condensed format the natural amino acids are substituted with pseudoatoms, which allows for efficient handling of large molecules (proteins), while still preserving a full description of chemical modifications.  The rule table is built from rules for matching acidic or basic groups, going from specific rules such as amino acid side chains, toward more generic groups such as carboxylic acids. Rules are specified as SMARTS together with their associated pKa and the charge for the group below the pKa (0 or 1). Rules allow for more pKa values and charges to be associated for complex groups. SMARTS are preferably designed to match the atom with the labile hydrogen or lone pair in a specific atomic environment.  RDKit17 is used for substructure matching between the molecule and the SMARTS. To allow for pseudoatom support, the RDKit source code was patched with an extension to the atomic table file. (patch included in the Supporting Information).  If a rule matches a pseudoatom, the matched pseudoatom is substituted with the glycine pseudoatom, whereas a matched atomic substructure is substituted with atomtype 0.  The Henderson−Hasselbalch equation is used together with the recorded pKa and charge values to calculate the partial charge of the molecule at a given pH (eq 1). The entire pH range from 0 to 14 is simulated, and the pI is estimated at zero charge. If none, only basic, or only acidic groups are present, the pI cannot be calculated.  Data Sets. The Reaxys database21,22 was searched for molecules with information about their isoelectric point which also contained a substructure matching the backbone of at least three amino acids. The SD file data set was manually curated and a few molecules removed based on manual judgment of their peptide content. The data set was divided into sets containing peptides with only natural amino acids or modified peptides. Finally, the SD file formatted molecules were converted to protein line notation (PLN) with Proteax and saved into Excel files together with the FASTA sequences. Molecules which failed the conversion to PLN were discarded. The final data sets and the associated PLN modification database are part of the Supporting Information.  Additionally, the data sets from Gauci et al.9 and the high quality PeptideProphet data set from Heller et al.23 were downloaded and prepared in FASTA and PLN format accounting for modifications. The average of the pI values assigned by the original authors for each gel fraction was assigned to all peptides from that fraction.  pKa Sets. Multiple different pKa value sets have been proposed for usage in peptide pI calculations. Five such sets from Solomons,24 Lehninger,25 Grimsley,16 IPC_peptide,26 and EMBOSS27 were tested.  pI Prediction and Comparison. The pI was estimated for all peptides using both the FASTA sequence and the PLN containing the modifications. The correlation coefficient (R2) and the root-mean-square deviation (RMSD) were calculated between the experimental and the predicted pI. Similar calculations were done with the program pIR,14,28 with the difference that the PLN was converted to the modification format used by pIR before pI estimation. Plots were made in Python with Matplotlib. ■    RESULTS\r\n  For this work, 511 molecules were found in the Reaxys database, which after data preparation resulted in final data sets with 335 unmodified and 99 modified peptides. The algorithm could not calculate the pI of 15 peptides from the data set since these structures contained only basic groups. They were filtered away before further data analysis. The Gauci, Gauci modified, and Heller data sets contained 5757, 250, and 2006 peptides, respectively.  The calculated R2 and the RMSD for the different data sets using different pKa sets are summarized in Table 1. The full benchmark matrix with all combinations of pKa sets, data sets, and methods as well as a comparison with the pIR program can be found in Supplementary Tables 1−3. The best performance is observed with the Grimsley derived pKa set on the unmodified peptides, with a correlation coefficient of 0.99 and an RMSD is 0.38. The excellent agreement is also evident from Figure 2. However, the test sets with the modified peptides show large differences, depending on whether they are data set−pKa set predicted from PLN or FASTA sequence. The results for using the FASTA sequences show no good correlation between data set pI and predicted pI for the Grimsley pKa set, and many outliers can be seen in Figure 3, Graph A (R2 of 0.45 and RMSD of 3.30).  By taking the modifications into account with the PLN notation, the excellent agreements between theoretical and literature values are restored to some degree (R2 of 0.91 and RMSD of 0.90 for the Grimsley pKa set and Figure 3, Graph B). The best performance on the modified peptides was seen with the EMBOSS pKa set29 using the PLN data set, with an R2 of 0.92 and RMSD of 0.76, which is on par with the performance of the IPC_peptide pKa set.26 ■ The results show the importance of taking chemical modifications into account when predicting pI values. However, the predicted pI was changed less than 1 pH unit for 49 of the 99 peptides in the Reaxys data set with modifications. Modifications that do not change or introduce an acidic or basic group will not affect the calculations, and even if a group is changed, it can have little effect if the pKa value is far away from the pI value and the charge after the modification remains the same.  During analysis, a group of peptides failed to produce pI values during prediction from the PLN sequence. They contained only basic groups, but the original reference showed that they had been assigned a pI value of 14 without any description of the experimental procedure.30 The values could be based on pI prediction or the value should have been assigned as &gt;14. For this analysis, the effect of predicted pI values is not detrimental as the focus is on the consequences of not including the modifications in the calculations. However, predicted pI values must not be used for tuning of pKa value sets and pI prediction algorithms.26    ASSOCIATED CONTENT\r\n  *S Supporting Information The Supporting Information is available free of charge on the ACS Publications website at DOI: 10.1021/acs.jcim.7b00030.  Supplementary tables (PDF) Information about the final Reaxys data sets and the predicted pI values and the modification database for Proteax Desktop and a patch file for RDKit to support condensed molfile formats of peptides (ZIP)    AUTHOR INFORMATION\r\n   Corresponding Author\r\n  *E-mail: esben@wildcardconsulting.dk.    ORCID\r\n  Esben J. Bjerrum: 0000-0003-1614-7376    Funding\r\n  The research and publication were completely funded by the authors' respective companies.    Notes\r\n  The authors declare the following competing financial interest(s): J.H.J. is the owner of Biochemfusion ApS which licenses Proteax Desktop used together with pICalculax in this study. The authors E.J.B. and J.H.J. are employed by Wildcard Pharmaceutical Consulting and Biochemfusion ApS, respectively. Both companies are usually contracted by biotechnology/pharma companies to provide third party services.     ABBREVIATIONS\r\n  pH, potential of hydrogen; pI, isoelectric point; pKa, acid dissociation constant; PLN, protein line notation; RMSD, root mean square deviations; SD file, structure-data file; SMARTS, Smiles arbitrary target specification  The Gauci9 and Heller23 data sets are derived from shotgun proteomics data. Isoelectric focusing were used to fractionate the digested protein samples before further separation and identification using MS-MS. During experimental procedures the free cysteine side chains are blocked with iodoacetamide9,23 which could lead to problems for calculations on peptides containing free cysteines. Moreover, during data filtering the predicted value of the peptide sequence is matched to the gel fraction, and hits outside two standard deviations are removed. This biases the data set with peptides that can be predicted with the pI prediction algorithm used in the proteomics study. These kinds of experimental, but filtered, data sets must not be used for further tuning of pKa value sets and pI prediction algorithms26 as the tuned pKa values will regress toward the pKa values used to filter the data sets.  pICalculax was tested with five different pKa sets. The pKa values from Grimsley16 are derived from protein NMR titration experiments, whereas the IPC_peptide pKa set has been tuned to give good pI predictions against experimental data sets.26 The best performance for the unmodified peptides was observed with the Grimsley pKa set, which had otherwise showed below average performance in a recent benchmark,26 whereas a previous benchmark28 had shown good performance. A possible reason for the bad performance in the most recent benchmark26 is that it was done on a data set derived from proteomics data; these are problematic to use for peptide pI algorithm tuning and benchmarking (see above). Moreover, the benchmark seems to have been done26 on the predicted values from the Heller SEQUEST and PHENYX data sets,23 rather than on the average pH value of the IEF gel fractions. The results of the benchmark on the Heller and Gauci data sets should thus be interpreted cautiously and can fully explain the differences in performance observed between the studies.  The pIR28 program performed equally well as pICalculax on the Gauci modified, but not the Reaxys modified data set (Supplementary Table 3). This may be because the Gauci modified data set only contains phosphorylations and nterminal acetylations which are modifications known by pIR. The Reaxys data set contains multiple other modifications which could not readily be converted to a format understood by pIR and thus not accounted for in the pI calculation.  The performance on the Reaxys modified data set predicted from PLN was better using the pKa sets IPC_peptide and EMBOSS. This suggests that a performance gain could be derived from a pKa set tuning, balancing the performance on modified and unmodified peptides if a suitable data set could collected. The default pKa set used in pICalculax is from Grimsley, but the other pKa sets are included and can be used instead through a setting in the program source code.  The current implementation of the algorithm uses the naı v̈e approach that the extent of protonation for each group is independent of each other, and this could lead to wrong predictions for peptides with many neighboring acidic and basic groups or with acidic or basic groups at the termini. We have presented pICalculax, a program which can be used to include modifications of peptides in pI calculations. The approach is extensible to new modifications or artificial amino acids, as the rule table can be updated with new SMARTS rules and pKa values as the need dictates. The use of PLN notation simplifies data management, as there is no need for storing modifications and sequence information separately. ■    CONCLUSION\r\n  ■ ■ ■ ■    ",
    "sourceCodeLink": "https://github.com/EBjerrum/pICalculax",
    "publicationDate": "0",
    "authors": [
      "Esben J. Bjerrum",
      "Jan H. Jensen",
      "Jakob L. Tolborg"
    ],
    "status": "Success",
    "toolName": "pICalculax",
    "homepage": ""
  },
  "46.pdf": {
    "forks": 1,
    "URLs": [
      "github.com/MolecularBioinformatics/sbml-mod-ws",
      "github.com/MolecularBioinformatics/PyCopasi",
      "www.ebi.ac.uk/biomodels-main/MODEL1310160000",
      "sbmlmod.uit.no",
      "www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE7905",
      "cancergenome.nih.gov"
    ],
    "contactInfo": ["ines.heiland@uit.no"],
    "subscribers": 6,
    "programmingLanguage": "Python",
    "shortDescription": "SBMLmod web service",
    "publicationTitle": "SBMLmod: a Python-based web application and web service for efficient data integration and model simulation",
    "title": "SBMLmod: a Python-based web application and web service for efficient data integration and model simulation",
    "publicationDOI": "10.1186/s12859-017-1722-9",
    "codeSize": 4854,
    "publicationAbstract": "Background: Systems Biology Markup Language (SBML) is the standard model representation and description language in systems biology. Enriching and analysing systems biology models by integrating the multitude of available data, increases the predictive power of these models. This may be a daunting task, which commonly requires bioinformatic competence and scripting. Results: We present SBMLmod, a Python-based web application and service, that automates integration of high throughput data into SBML models. Subsequent steady state analysis is readily accessible via the web service COPASIWS. We illustrate the utility of SBMLmod by integrating gene expression data from different healthy tissues as well as from a cancer dataset into a previously published model of mammalian tryptophan metabolism. Conclusion: SBMLmod is a user-friendly platform for model modification and simulation. The web application is available at http://sbmlmod.uit.no, whereas the WSDL definition file for the web service is accessible via http://sbmlmod.uit.no/SBMLmod.wsdl. Furthermore, the entire package can be downloaded from https://github.com/MolecularBioinformatics/sbml-mod-ws. We envision that SBMLmod will make automated model modification and simulation available to a broader research community.",
    "dateUpdated": "2017-02-08T12:53:18Z",
    "institutions": [
      "Uni Research Environment",
      "UiT The Arctic University of Norway",
      "University of Bergen",
      "Equal contributors",
      "Friedrich-Schiller-University Jena"
    ],
    "license": "GNU General Public License v2.0",
    "dateCreated": "2014-06-17T10:25:57Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Schäuble et al. BMC Bioinformatics     10.1186/s12859-017-1722-9   SBMLmod: a Python-based web application and web service for efficient data integration and model simulation     Sascha Schäuble  3  4    Anne-Kristin Stavrum  2  3    Mathias Bockwoldt  1  3    Pål Puntervoll  0    Ines Heiland  ines.heiland@uit.no  1    0  Centre for Applied Biotechnology, Uni Research Environment ,  Bergen ,  Norway    1  Department of Arctic and Marine Biology, UiT The Arctic University of Norway ,  Tromsø ,  Norway    2  Department of Informatics, University of Bergen ,  Bergen ,  Norway    3  Equal contributors    4  Jena University Language &amp; Information Engineering (JULIE) Lab, Friedrich-Schiller-University Jena ,  Jena ,  Germany     2017   18  2  9    9  6  2017    10  2  2017     Background: Systems Biology Markup Language (SBML) is the standard model representation and description language in systems biology. Enriching and analysing systems biology models by integrating the multitude of available data, increases the predictive power of these models. This may be a daunting task, which commonly requires bioinformatic competence and scripting. Results: We present SBMLmod, a Python-based web application and service, that automates integration of high throughput data into SBML models. Subsequent steady state analysis is readily accessible via the web service COPASIWS. We illustrate the utility of SBMLmod by integrating gene expression data from different healthy tissues as well as from a cancer dataset into a previously published model of mammalian tryptophan metabolism. Conclusion: SBMLmod is a user-friendly platform for model modification and simulation. The web application is available at http://sbmlmod.uit.no, whereas the WSDL definition file for the web service is accessible via http://sbmlmod.uit.no/SBMLmod.wsdl. Furthermore, the entire package can be downloaded from https://github.com/MolecularBioinformatics/sbml-mod-ws. We envision that SBMLmod will make automated model modification and simulation available to a broader research community.    Web application  Web service  Data integration  Model simulation       Background\r\n  Theoretical models of complex biological entities are fundamental to systems biology and systems medicine research [ 1, 2 ]. They provide summaries of metabolic, signalling or gene regulatory networks including information on e. g. stoichiometry or kinetic rate laws. To gain new biological insights into pathways of interest it is nevertheless crucial to integrate experimental data. The type of appropriate data is context dependent: While dynamic signalling or metabolic pathway studies may require metabolome or time course data, gene regulatory networks commonly ask for gene expression datasets. Such data are increasingly available from data repositories such as the Gene Expression Omnibus (GEO) [ 3 ], the NCI-60 tumour cell line screens [ 4, 5 ] and The Cancer Genome Atlas (TCGA, https://cancergenome.nih.gov).  Theoretical model generation and distribution itself is commonly achieved via multiple toolboxes and databases. Pathway Tools [ 6 ] and CellDesigner [ 7 ] are examples of software packages for biological model construction. Whereas COPASI [ 8 ] and Data2Dynamics [ 9 ] are toolboxes for investigating dynamic behaviour, the COBRA toolbox [ 10 ] is suited for constraint-based model analyses. Theoretical models are stored in public databases such as the BioModels database [ 11 ], which mainly covers small to medium scale models, or the BiGG model database (http://bigg.ucsd.edu/) for genome-scale models. Model accessibility is achieved by model definition standards, such as the Systems Biology Markup Language (SBML) [ 12 ].  Both vast amounts of data and standardised models are readily available, yet integrating and analysing data with a given model can still be a discouraging task. Nevertheless, programmatic access is commonly necessary to perform more complex operations than loading and simulating the initial model.  In recent years software packages have been made available to simplify model manipulation and simulation tasks [ 10, 13-15 ]. A Taverna workflow published by Li et al. [ 14 ] focuses on reconstruction, model manipulation and simulation. Data integration is realised via accessing the enzyme kinetics database SABIO-RK [ 16 ], or via an in-house database for specific metabolomics and proteomics datasets. It does not, however, include the possibility to integrate gene expression data. Setting up the workflow itself requires programmatic configuration including resolving software dependencies on e. g. the libSBML package [ 17 ]. Yizhak et al. [ 13 ] introduced a method termed IOMA, which quantitatively integrates proteomic and metabolomic data with genomescale metabolic models and calculates steady state solutions. IOMA assumes Michaelis-Menten-like kinetics and delivers steady state flux distributions, but no metabolite concentrations. GAM presented by Sergushichev et al. [ 15 ] provides a convenient network analysis platform to analyse metabolic networks. So far it covers four preassembled models and is specifically tailored towards identification of the most regulated subnetwork between two conditions.  These toolboxes are appropriate ways to create, modify or simulate theoretical models. Yet because they require a minimum level of programming proficiency, they are all effectively restrictive for scientists with little or no computational biology background.  We present and describe SBMLmod, a slim and easily accessible SBML model loading, data integrating and model simulation platform. SBMLmod can be accessed within any common web browser, circumventing the need to install or program software. Any valid SBML model and a dataset for parametrisation can be chosen to perform model modification and simulation operations. Advanced users can access SBMLmod programmatically via its Web Services Description Language (WSDL) interface. The WSDL interface circumvents the need to resolve software dependencies and allows for the integration of SBMLmod into analysis pipelines. Finally, the complete package can be downloaded, installed, set up locally and accessed from any Python shell prompt.    Implementation\r\n  Every SBMLmod task is based on a theoretical biological model encoded in SBML, which might be downloaded from e. g. the BioModels database [ 11 ]. Single or multiple data sets on either kinetic rate law or species concentration can be provided by the user. Steady state simulations can be calculated by making use of the web service COPASIWS from COPASI [ 8 ] to obtain system wide concentration and flux solutions feasible at steady state. SBMLmod can be accessed as a web application or as a web service for customised workflows. The respective WSDL file guarantees the same functionality as the web application.  SBMLmod is written in Python 2.7. Accessing and modifying SBML models is enabled via libSBML [ 17 ]. All model modification and simulation features are computed on the fly and scale efficiently with the number of data sets and data volume.  Web application guarantees OS independent access of SBMLmod The welcome screen of SBMLmod's web application is organised into two panels: A) choosing the input files; B) choosing the task to perform (Fig. 1a). The general workflow is shown in Fig. 1b.  Input files are comprised of a mandatory SBML model file and optional data files. The latter may concern either parameters of reaction rate laws or the initial concentrations of considered species in the model. An additional mapping file is mandatory whenever the identifiers given in the data file do not match the identifiers of the respective species or reaction in the model file. This may be the case, if, for instance, different identifier standards (e. g. ensembl, or entrez gene id) are used in the model and data file(s), or if different synonyms for the same species or reaction are used.  Users may furthermore choose to analyse multiple data sets by selecting the 'batch mode' option. If selected, each column of a given data file is processed individually and will yield a separate data specific model or simulation.  After selecting the necessary files, the user can either calibrate or simulate the given model by selecting the respective options (Fig. 1a, panel B). Calibrating the model parameters is accomplished by replacing or scaling reaction parameters such as the total amount of available enzyme concentrations. Replacing and scaling reaction parameters can be accomplished system-wide (globally) or on a per-reaction basis (locally). Should multiple rows of a given data file be associated with the same reaction (e. g. if isozymes are considered in the data file, but not in the model), the user may choose a specific merge mode. All merge options (e. g. maximum value selection) are described in detail in the online documentation and in the Additional file 1: S1. The initial concentrations of model species can also be modified. The most recently modified models are always available for download. They are identified by the respective column header in the data file (cf. Fig. 1c and Additional file 1: S1 for details on the data file format).  A warning feedback functionality is established and ensures that models are correctly encoded, all identifiers are assignable and mappings are unambiguous. The web application of SBMLmod is set up using Python Django [ 18 ] and is hosted at http://sbmlmod.uit.no. To demonstrate data format and warning feedback, example files are available at the website and in Additional file 2: S2.  Calculation of steady state concentrations and fluxes are enabled by linking the web application to the COPASI web service. Our web application returns the original output file(s) generated. In addition, results of generated and simulated models (in batch mode) are returned as accumulated, tab separated tables for the calculated concentrations and fluxes. To allow an initial inspection of the results, the web application generates a customisable graph showing all non-constant metabolite concentrations and fluxes (cf. Additional file 3: Figure S3 for an example output). Customisation includes selecting metabolite species and fluxes to be shown and also allows for grouping together different values (if batch mode was selected). See Additional file 1: S1 for details of customisation options. Web service accessibility enables automated high throughput data integration and analysis Next to the web application, a web service functionality of SBMLmod is available. It can be accessed via the WSDL interface, either from http://sbmlmod.uit.no/SBMLmod.wsdl or by downloading the whole package including the WSDL file at https://github.com/MolecularBioinformatics/sbml-mod-ws. The web service enables complete analysis workflows including a full sequence of model modification and simulation operations of the aforementioned features. By providing the WSDL file, we enable more advanced users to run data integration without the need to install software packages and resolve software dependencies. SBMLmod can thus be integrated into other existing or newly developed workflows for model manipulation or steady state simulation. Alternatively the web service can be installed and run locally (source files and technical documentation are available at https://github.com/MolecularBioinformatics/sbml-mod-ws). This enables faster processing especially for large datasets. Simulation results are summarised in textual output files. These can be further processed using our Python toolbox PyCopasi for parsing and manipulating COPASI files. PyCopasi is available at https://github.com/MolecularBioinformatics/PyCopasi.  Feasible model manipulations and basic scripts to run the data integration are exemplified by files provided in the 'testClient' folder of the package.    Results &amp; discussion\r\n  To demonstrate the usage of SBMLmod we analysed two publicly available datasets by integrating them into an existing model of tryptophan metabolism [  [ 19] (https://www.ebi.ac.uk/biomodels-main/MODEL1310160000). Tryptophan, an essential amino acid, has received increasing interest in recent years, since it is the precursor of several bioactive metabolites such as serotonin, kynurenine, melatonin and NAD. Consequently, imbalances in tryptophan metabolism have been related to several diseases, including neurodegeneration, gastrointestinal disorders and cancer. Tryptophan metabolism underlies tissue specific regulation [2 [2 0], resulting in a remarkable difference in metabolite concentrations and fluxes. In our earlier analyses we focused on differential tryptophan pathway activity in two human tissues (brain and liver), as well as the metabolite exchange between these tissues and its consequences for neurodegenerative diseases and potential treatments [1 [1 9]. We implemented a data driven modelling approach [2 [21, 2 2] by scaling maximal reaction velocities based on expression data [1 [1 9]. By integrating data from a tissue specific expression profiling study [2 [2 3], we showed that we were able to quantitatively reproduce metabolite concentrations measured in vivo as well as qualitative flux changes reported upon treatment with inhibitors specific for enzymes of sub-pathways in mice. Since the tryptophan catabolite kynurenine has been associated with increased malignancy in brain tumours [2 [2 4], we recently applied our model to calculate changes in tryptophan metabolism in different subtypes of breast cancer patients using RNA-sequencing datasets from The Cancer Genome Atlas (TCGA: https://cancergenome.nih.gov). We were able to show that our predictions are in agreement with kynurenine concentrations measured in patients [25 25 ]. Thus, incorporating theoretical model predictions allows us to predict patient specific diagnostic markers important for further treatment, emphasising the need for easily accessible data integration tools. Tissue specific differences in tryptophan metabolites Kynurenine and serotonin are products of competing branches of tryptophan metabolism (see simplified pathway scheme Fig. 2). Their ratio has been recognized to be important in depressive disorders, especially in the context of chronic inflammation [26 26 ].  Here we extend our earlier analysis [ [1 9] to better understand the tissue specific activity of tryptophan metabolism. For this purpose we integrated a published tissue specific gene expression dataset from 32 human tissues [ [2 3] (dataset: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE7905) and calculated steady state concentrations of kynurenine and serotonin with SBMLmod.  Our modelling approach predicts that liver as well as immuno-active tissues like lung and spleen have high kynurenine concentrations (Fig. 2a). In lung and spleen the activity of the kynurenine pathway depends on the induction of indoleamine 2,3-dioxygenase (IDO), especially during infection (for review cf. [ 27, 28 ]). The tryptophan pathway activity in the liver is regulated via the expression of tryptohpan 2,3-dioxygenase (TDO) catalysing the same reaction as IDO. TDO is furthermore known to be down-regulated when peripheral kynurenine levels are increased, for example during infection [ 29 ]. Changes in tryptophan metabolism during pregnancy have been described previously, for example high expression of IDO in the placenta might play a role in immune tolerance [ 30 ]. The calculated concentrations for the placental model resemble these observations. In contrast, brain tissues are predicted to have a low activity of the kynurenine branch in healthy individuals. This is reasonable as several intermediates of the kynurenine branch are known to be neurotoxic [ 31 ].  Serotonin production is predicted to be high in neuroendocrine tissues such as the prostate, but low in tissues with high kynurenine pathway activity (Fig. 2b) due to the competition for the substrate tryptophan. The comparatively high serotonin production in prostate epithelial cells has been described in the literature [ 32 ]. Our modelling approach furthermore predicts serotonin production to be high in the colon, but in this tissue the kynurenine route of the tryptohpan pathway is also partially active. This dual pathway activity in the colon has been reported earlier [ 33 ] and imbalances between the two branches might cause the development of irritable bowel syndrome [ 34, 35 ].  For a full overview of steady state concentrations of kynurenine and serotonin in all 32 available tissues see Additional file 4: Figure S4. Details on the statistical procedure are provided in the Additional file 5: S5. All pairwise statistical test results between all tissues are provided in Additional file 6: Table S6. The full dataset, mapping file and model are provided in Additional file 2: S2 and as example files in the web application (limited to the 10 tissues presented in Fig. 2a and b).  Different cancer types possess notable differences in kynurenine and serotonin concentrations In a second analysis, we integrated RNA-sequencing data from approx. 2000 patients available at TCGA (https://cancergenome.nih.gov; corresponding TCGA-IDs are provided in Additional file 7: S7). Using this approach, we predicted activation of the kynurenine pathway and thus increased kynurenine production for ovarian, prostate and colorectal cancer (Fig. 2c). Whereas the serotonin branch appears to be activated in acute myeloid leukemia, the kynurenine branch is largely inactive (Fig. 2d). This is supported by statistical analysis showing that the distributions of kynurenine and serotonin concentrations are significantly different between the different cancer types (Kruskal-Wallis test, p=1.5e-93 and p=7.2e-33, respectively). Subsequent pairwise comparison reveals that kynurenine concentrations are predicted to be significantly higher in breast, ovarian, prostate and colorectal cancer as compared to acute myeloid leukemia (Fig. 2, Bonferroni corrected p-values 2.6e-42, 2.3e-83, 8.2e-32, 3.5e-56, respectively). In contrast, pairwise comparison of serotonin concentrations among different cancer types shows significantly lower concentrations of serotonin in ovarian, prostate and colorectal cancer, but not in breast cancer, when compared to acute myeloid leukemia (Fig. 2, Bonferroni corrected p-values 1.1e-4, 2.2e-5, 1.7e-9, 1, respectively). This is in agreement with known changes in these tumour types [2 24, 25, 36, 37 ]. An extended statistical analysis is provided in Additional file 8: Table S8.    Conclusion\r\n  We presented SBMLmod, an SBML model modification and simulation tool. The platform-independent web application of SBMLmod allows for the automated integration of experimental data into theoretical models without requiring programming knowledge from the user. SBMLmod has two major advantages over existing methods: first, data integration and analysis are possible with a minimal number of user required operations; second, all operations can be performed without further software or programming dependencies. The easy accessibility of SBMLmod is accomplished by focusing on a limited number of essential model modification functions. These are complemented with steady state calculations of metabolite concentrations and fluxes. Additional flexibility is offered by accessing the application as a web service., which allows to further optimise and accelerate data integration and subsequent theoretical analyses.  Even though SBMLmod minimises the effort required by the user, we emphasise the need to ensure an accurate reaction or gene identifier mapping. Though models of sizes up to a genome-scale can be calibrated and simulated, ensuring correct mapping files is increasingly challenging if thousands of identifiers must be handled. Furthermore, increased simulation times due to the size of large models alone have to be considered; thus, SBMLmod is more suited for the manipulation and simulation of small and medium scale models. Of note, SBML is an XML format and is therefore not designed to be human readable. This can be compensated for by making use of the recently developed SBtab [ 38 ], which allows users to read and filter SBML files for relevant information such as metabolite names or reaction identifiers.  We demonstrated the usefulness of SBMLmod by calibrating a given tryptophan model to recapitulate an existing analysis of tryptophan metabolism and by evaluating the steady state concentrations of kynurenine and serotonin, two potential prognostic biomarkers in different diseases including cancer. We expect that SBMLmod will contribute to further improve data integration into modelling approaches especially with respect to accessibility.    Availability and requirements:\r\n   Project name: SBMLmod\r\n  Project home page: http://sbmlmod.uit.no and https://github.com/MolecularBioinformatics/sbml-mod-ws OS: any    Programming language: Python 2.7\r\n  Licence: GNU General Public License v2.0     Additional files\r\n  Additional file 1: S1 - documentation. Documentation of the usage and file formats of SBMLmod. Also available at http://sbmlmod.uit.no.(PDF 49 kb) Additional file 2: S2 - example files. Zipped example files usable to review specific data file format or to check SBMLmod web application and service functionality. These files resemble the first use case with 32 tissues in the manuscript. Note that mapping files, the SBML model and the data file limited to 10 tissues, can also be downloaded from the web application (http://sbmlmod.uit.no) using the download link at the lower part of the webpage under 'Example Files'. (ZIP 32 kb) Additional file 3: Figure S3 - visualisation of results by the web application. Example for the result visualisation of the 10 tissues (shown in Fig. 2a and b) that is provided as part of the web application. (PDF 87 kb) Additional file 4: Figure S4 - steady state concentrations of all 32 tissues. This figure provides a comprehensive overview over all 32 tissues that have been analysed. The figure complements Fig. 2a and b, where 10 selected tissues are shown. (PDF 103 kb) Additional file 5: S5 - details of statistical analysis. This file provides details of statistical analysis applied for the two use cases in this manuscript. (PDF 71 kb) Additional file 6: Table S6 - detailed statistical results for dataset of 32 tissues. This file provides ANOVA and post hoc pairwise test statistics for all 32 tissues that have been analysed and described in the subsection 'Tissue specific differences in tryptophan metabolites'. (XLS 71 kb) Additional file 7: S7 - TCGA sample IDs. List of TCGA sample IDs used to calculate the results presented in Fig. 2c and d. (TXT 76 kb) Additional file 8: Table S8 - statistics for TCGA dataset. This table provides ANOVA and post hoc pairwise test statistics for the TCGA data application as described in section 'Different cancer types possess notable differences in kynurenine and serotonin concentrations'. (XLS 11 kb) Abbreviations IDO: Indoleamine 2,3-dioxygenase; TDO: Tryptohpan 2,3-dioxygenase Acknowledgements We thank Christane A. Opitz for helpful comments and support with respect to tryptophan metabolism analysis, Siv Hollup and Espen Tangen for supporting us with the deployment of SBMLmod and Gabriela Wagner and Matthew Richards for proofreading the manuscript.  Funding The project has been funded by the DAAD-exchange program between Norway and Germany (57150435 and 244770/F11), by the Norwegian Research Council (178885/V30) and by the BMBF funded e:Med project GlioPATH (01ZX1402). The funding bodies played no role in the design or conclusion of our study.  Availability of data and materials The web application is accessible at http://sbmlmod.uit.no. The web service can be reached via its WSDL interface at http://sbmlmod.uit.no/SBMLmod.wsdl. The source for local use is available at https://github.com/MolecularBioinformatics/sbml-mod-ws.  Authors' contributions AS, MB, SS, PP have developed and revised the web application and web service SBMLmod. IH and SS integrated and analysed the expression datasets. SS and IH wrote the manuscript. All authors read and approved the final manuscript.  Competing interests The authors declare that they have no competing interests. Consent for publication Not applicable.  Ethics approval and consent to participate Not applicable.    Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/MolecularBioinformatics/sbml-mod-ws",
    "publicationDate": "0",
    "authors": [
      "Sascha Schäuble",
      "Anne-Kristin Stavrum",
      "Mathias Bockwoldt",
      "Pål Puntervoll",
      "Ines Heiland"
    ],
    "status": "Success",
    "toolName": "sbml-mod-ws",
    "homepage": ""
  },
  "20.pdf": {
    "forks": 0,
    "URLs": [
      "doi.org/10.1186/1471-2105-13-219",
      "github.com/lovemun/Genocore"
    ],
    "contactInfo": ["moonjk2@korea.kr"],
    "subscribers": 1,
    "programmingLanguage": "R",
    "shortDescription": "Simple and fast algorithm for selecting core subset applied to large genotype dataset",
    "publicationTitle": "GenoCore: A simple and fast algorithm for core subset selection from large genotype datasets",
    "title": "GenoCore: A simple and fast algorithm for core subset selection from large genotype datasets",
    "publicationDOI": "None",
    "codeSize": 9509,
    "publicationAbstract": "Selecting core subsets from plant genotype datasets is important for enhancing cost-effectiveness and to shorten the time required for analyses of genome-wide association studies (GWAS), and genomics-assisted breeding of crop species, etc. Recently, a large number of genetic markers (>100,000 single nucleotide polymorphisms) have been identified from high-density single nucleotide polymorphism (SNP) arrays and next-generation sequencing (NGS) data. However, there is no software available for picking out the efficient and consistent core subset from such a huge dataset. It is necessary to develop software that can extract genetically important samples in a population with coherence. We here present a new program, GenoCore, which can find quickly and efficiently the core subset representing the entire population. We introduce simple measures of coverage and diversity scores, which reflect genotype errors and genetic variations, and can help to select a sample rapidly and accurately for crop genotype dataset. Comparison of our method to other core collection software using example datasets are performed to validate the performance according to genetic distance, diversity, coverage, required system resources, and the number of selected samples. GenoCore selects the smallest, most consistent, and most representative core collection from all samples, using less memory with more efficient scores, and shows greater genetic coverage compared to the other software tested. GenoCore was written in R language, and can be accessed online with an example dataset and test results at https://github.com/lovemun/Genocore.",
    "dateUpdated": "2016-03-24T06:41:49Z",
    "institutions": ["UniversiteÂ Paris Diderot"],
    "license": "GNU General Public License v2.0",
    "dateCreated": "2015-04-21T01:21:08Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     July      GenoCore: A simple and fast algorithm for core subset selection from large genotype datasets     Seongmun Jeong  0    Jae-Yoon Kim  0    Soon-Chun Jeong  0    Sung-Taeg Kang  0    Jung- Kyung Moon  moonjk2@korea.kr  0    Namshin Kim  0    0  Editor: Alexandre G. de Brevern, UMR-S1134, INSERM, UniversiteÂ Paris Diderot ,  INTS ,  FRANCE     20  7  2017   20  2017    1  7  2017    16  11  2016     Selecting core subsets from plant genotype datasets is important for enhancing cost-effectiveness and to shorten the time required for analyses of genome-wide association studies (GWAS), and genomics-assisted breeding of crop species, etc. Recently, a large number of genetic markers (&gt;100,000 single nucleotide polymorphisms) have been identified from high-density single nucleotide polymorphism (SNP) arrays and next-generation sequencing (NGS) data. However, there is no software available for picking out the efficient and consistent core subset from such a huge dataset. It is necessary to develop software that can extract genetically important samples in a population with coherence. We here present a new program, GenoCore, which can find quickly and efficiently the core subset representing the entire population. We introduce simple measures of coverage and diversity scores, which reflect genotype errors and genetic variations, and can help to select a sample rapidly and accurately for crop genotype dataset. Comparison of our method to other core collection software using example datasets are performed to validate the performance according to genetic distance, diversity, coverage, required system resources, and the number of selected samples. GenoCore selects the smallest, most consistent, and most representative core collection from all samples, using less memory with more efficient scores, and shows greater genetic coverage compared to the other software tested. GenoCore was written in R language, and can be accessed online with an example dataset and test results at https://github.com/lovemun/Genocore.       -\r\n  Competing interests: The authors have declared that no competing interests exist.    Introduction\r\n   Selecting core subsets from large collections is an effective strategy to characterize and utilize\r\n  genetic resources of crop plants without the requirement of sampling the entire population. This concept was first proposed by Frankel et al. to select a subset of the data that is representative of the whole resource, by removing redundant samples and maximizing genetic diversity [ 1 ]. Several methods have been developed and recently implemented for core sample selection, such as    MSTRAT [2], PowerCore [3], Core Hunter [4], and Core Hunter II [5].\r\n  MSTRAT is a local search method that is based on a maximization strategy to maximize the richness of samples [ 2 ]. It randomly divides the entire sample into two groups, retains accessions to meet the greatest increase diversity criterion, and repeats the process until 30 iterations are completed or the richness no longer increases. Core Hunter I and II are based on an advanced stochastic local search algorithm implemented by Java [4±5]. Core Hunter II uses a mixed replica search method that is based on several search methods, including LR Semi Replica, Local Search Replica, Tabu Search Replica, and Simple Monte Carlo Replica. The first step in this process involves arbitrary selection of samples to shorten the execution time in common with MSTRAT. However, application of the default runtime option can vary greatly in each run in the case of a large dataset, which makes it difficult to decide on a key subset. Kim et al. developed PowerCore using a modified heuristic search algorithm, A algorithm, based on graph theory, which extracts a subgraph while minimizing elements and paths [ 3 ]. This program gives similar results to those obtained with MSTRAT and Core Hunter, but Power    Core cannot load a large dataset.\r\n    In recent years, the numbers of samples and markers available for investigating genetic\r\n  diversity has increased to thousands and millions, respectively, owing to the development of new high-throughout technologies such as 1,536 SNP chip of Illumina GoldenGate assay [ 6 ], Illumina MaizeSNP50 BeadChip [ 7 ], Axiom Soybean Genotyping Array [ 8 ], high-density rice array (HDRA, 700k SNPs) [ 9 ], Wheat 820k genotyping Array [10], and Axiom Wheat Breeder's Genotyping Array [10]. In order to effectively analyze these datasets with many samples, a high-performance system and efficient algorithm are required. Some methods are more focused on analysis of rare alleles than on polymorphisms in order to compensate for the lack of markers, but rare variants are often removed by filtering according to the minor allele frequency, which is common in genome-wide association studies. We are more focused on common alleles than rare alleles, because most of features of interests are complex traits. We concentrated on common alleles to represent the entire sample at best, which helps to increase the genetic coverage.    In this paper, we present GenoCore, which is implemented in R (version 3.2.5), as a new\r\n  method to select a core collection using modified statistical measures related to genetic allele coverage and diversity. By selecting samples with these measures, it is possible to quickly cover the entire samples, use less memory, and obtain a consistent final core subset. To compare    GenoCore with other programs (MSTRAT, Core Hunter II, and PowerCore), we applied example datasets to each program.\r\n     Results\r\n  Dataset   For comparison, we applied GenoCore, MSTRAT, Core Hunter, and random sampling to\r\n  wheat [10], 1.5k rice [ 6 ], and 700K rice [ 9 ] datasets, respectively. Table 1 provides the following key information of each dataset: single nucleotide polymorphism (SNP) chip name, marker, and the number of samples. Datasets of various sizes were tested to check the software for practicability, computation time, and accuracy, so that the optimal software can be chosen that offers the best solution for addressing the core collection problem.    Since there is a dataset loading problem, GenoCore is the only software that can incorporate all datasets, and PowerCore can only read the 1.5k rice dataset; the other software programs cannot load a high-density SNP-chip such as the 700k rice dataset. We set the default options\r\n  2 / 10  SNP Chip  Illumina GoldenGate Assay  Affymetrix Axiom 35K SNP array High Density Rice Assay 700K SNPs in GenoCore to a delta value of 0.01% and a coverage value of 99%. Since the first step of Core    Hunter and MSTRAT is random sampling, no difference exists from randomly selected sam\r\n  ples. Therefore, we increased the operation time using the runtime option for each method including random selection. The number of subsets from random sampling strategies is equal or similar to the number of subsets extracted from GenoCore. We analyzed the performance of the software using two computers, one running Windows 10 (Intel Core i5-3570K, 3.4 GHz    CPU, 4 GB memory) for PowerCore, and the other running CentOS 6 (64 core AMD Opteron\r\n    Processor 6380, 2.5 GHz, with 256 GB of main memory) for Core Hunter, MSTRAT, and\r\n    GenoCore.\r\n  Comparison    We compared the coverage (CV) [3], modified Rogers (MR) value [11], minimum MR value,\r\n    Shannon diversity index (SH) [12], and required memory of GenoCore to those of MSTRAT,\r\n    PowerCore, Core Hunter, and random sampling. Each of CV, MR and SH is defined by\r\n  CV \u0088  Here, m is the number of markers, pxij is the relative frequency of the j-th allele at the i-th locus for sample x, pi is the relative reference allele frequency, and GCi and GEi are the number of genotype classes for core collection Ci and entire collection Ei. Genotype class is the set of possible genotypes of a marker for all samples.    The software with superior core collection would show a higher genetic diversity (SH),\r\n  coverage (CV), and genetic distance (MR), and use less hardware and execution time for improved efficiency.  Fig 1 denotes that the cumulative genetic coverages versus the number of selected samples in the 1.5k rice (Fig 1A) and wheat (Fig 1B) datasets. This means that core subset by GenoCore definitely covers the entire samples in fastest time compared with the other methods. Geno    Core selected the core subset that achieved exactly or closest to 100% coverage, greater or simi\r\n  lar values of MR and SH, and the minimum MR compared to the other methods, as shown in    Tables 2 and 3. In particular, GenoCore showed a large difference in the minimum MR dis\r\n  tance for all datasets, indicating that our method does not select similar genotype samples.    Principal component analysis was conducted to show the distribution of the most informative\r\n  variables for the population and location of core samples. Selected core samples by GenoCore were evenly spread across the population for principal components, PC1 and PC2, for the 1.5k rice (Fig 2A) and wheat (Fig 2B) datasets. We constructed a Venn diagram to show the common part of the core collection lists obtained from all software for the rice 1.5k dataset (Fig 3). 3 / 10 Fig 1. Increase in coverage values versus number of selected samples for each software. (A) Rice 1.5K dataset, (B) wheat dataset.    Fourty-three samples were included for all methods, and 9 samples were uniquely selected in\r\n  GenoCore. Fig 3 indicates that subsets from GenoCore, MSTRAT, and PowerCore share 61 samples (76%), but not with Core Hunter (54%). It is believed that diversities from genotypes will also represent diversities from phenotypes. Thus if we select the most diverse samples from population, distribution of phenotypes will be very similar to a population. Only our method could select the core collection for the 700k rice dataset (Table 4). Fig 4 shows that the entire and core subsets had a similar phenotypic distribution. Hence, GenoCore will represent subsets from reasonable genotypes and phenotypes diversities.    If there are only thousands of markers, one rare allele could be located within a block of\r\n  phenotype-affecting rare alleles. However, high density SNPs and NGS could select multiple alleles as markers within those blocks. Most of previously developed software are focused on one of rare markers because of blindness in those blocks. Our approach assumes that we already know those blocks of rare alleles by high-throughput technologies.    In case that some samples have many minor allele markers, GenoCore properly selects core collection including those samples. Fig 5 shows that histogram of allele frequency between the entire and core collection in rice 1.5k and wheat dataset (A is the rice 1.5K dataset and B is the\r\n  MR 0.63968 0.64239 0.63339 0.63337 0.61619 0.61614  Min. MR 0.31785 0.10717 0.10610 0.30438 0.13179 0.04793  SH 7.7815 7.8155 7.7361 7.8010 7.7485 7.7572  CV 100 98.252 98.677  100 84.202 100 4 / 10  MR  CV wheat dataset). Our method is based on sampling with common alleles, but the result core subset of GenoCore still has a similar distribution for allele frequency of entire samples. Therefore,    GenoCore selects core collection controlling the minor allele frequency.\r\n    The input file size, computation time, and used memory used for each method are summa\r\n  rized in Tables 5 and 6. The file sizes are different among the methods due to the file formats used to represent multi-alleles. The required memory for each dataset was 4 GB for Core    Hunter, and was 80±700 MB for GenoCore, MSTRAT, PowerCore, and random sampling.\r\n    Although PowerCore used the least amount of memory (80 MB), the runtime was relatively long. By contrast, GenoCore not only used less random access memory but also took less time to execute the procedure.\r\n    Core Hunter and MSTRAT conduct random sampling at the first step; therefore, the execu\r\n  tion time will be reduced for a smaller dataset, and the results are more consistent with other software. However, these methods have low reproducibility using the default runtime option for large datasets; that is, the selected core samples are quite different for each trial. S1 Fig is a boxplot for the frequency of selected core samples by 100 replicates for each method using the default option in wheat dataset. MSTRAT and Random methods show a low frequency across Fig 2. Principal component analysis. (A) Rice 1.5K dataset, (B) wheat dataset. 5 / 10 Fig 3. Venn diagram (rice 1.5K dataset). the selected samples and Core Hunter has a wide spread distribution. The variation in Core Hunter is larger than other methods because it may have stopped the optimization process by the runtime option after random sampling. The higher values the runtime option, the lower the variation. To keep the dispersion close to zero, the runtime option must be more than twice the GenoCore calculation time. However, the frequency of selected core samples by GenoCore is 100 for all replicates. This means that GenoCore provides a consistent core collection for both a low-density SNP chip dataset and a high-density dataset, and even selects the key subset at a faster rate. We could select narrower differences of subsets from those runs if we increase random sampling times in Core Hunter.     Materials and methods\r\n  To develop a new core collection algorithm, we considered two measures that is simple, fast, and consistent to apply to a large dataset. Genotype data is a m × n matrix that rows are m samples, columns are n genetic markers, and the type of genotype consists of the four nucleotide bases (A, C, G, and T) or numeric values (representing zygosities for example, 0, 1, 2). Since some samples may have missing genotypes due to experimental errors, these genotypes can be treated as `ambiguous'. So, in the first step, candidate samples which have minimum count of missing genotypes are selected and then we calculate the statistical measure of the coverage score for j-th sample, denoted by Cj. This score means the representativeness of the sample  Used memory Fig 4. Phenotype density (rice 700K dataset). and is defined by  Cj \u0088  P  i2Nj figij n\u0085Nj\u0086 where figij is the genotype frequency gij for the i-th marker and j-th sample, Nj is the set of nonmissing genotype markers in sample j, and n(Nj) is the number of elements in Nj. A sample Fig 5. Histogram of allele frequency (rice 1.5k dataset). This is an allele frequency for reference allele of entire and core samples. A and B are the rice 1.5K and wheat dataset, respectively. They have similar distribution. 7 / 10  Runtime &lt;1 min &lt;1 min 5 min, 40 sec &lt;1min  Used memory 0.2 Gb 0.7 Gb 0.08 Gb 4.2 Gb with high coverage score have more highly genotype frequency markers than other samples.   This makes it possible to select samples that have the more common alleles. For this reason, we\r\n  select the sample having the highest score that makes the pre-defined coverage CV increasing. Because samples with identical coverage scores have the same or similar genotypes, choosing all of these samples reduces the time to total procedure, but it does not help to increase the genetic coverage. Therefore, if more than two samples have the same score, then diversity score is calculated for those samples and we select a sample with the minimum score to select one of those. Nevertheless, it remains samples after above steps, then the core sample is randomly selected. The diversity score means the variability for j-th sample, indicates a degree that contains common alleles, and is defined by  P Dj \u0088 i2Nj \u0085figij n\u0085Nj\u0086  Cj\u00862  The coverage and diversity scores measure the extent to which the sample covers the population and the genetic diversity for each sample. For each step, GenoCore selects a sample that is the most representative of the dataset and repeats the process until the coverage reaches 100% (or user can choose the percentage) or the coverage-increasing rate (difference between the coverage of i-th step and (i-1)-th step) reaches a user-defined threshold (default value is 0.01%). In the case that there are large number of markers or genetically similar samples, the more samples are selected, the lower coverage-increasing rate is.  Choosing a random sampling at the first stage and optimizing the core subset may be a good choice if there are only thousands of marker. However, when the number of markers increase to hundreds of thousands, it takes a lot of time and resources to obtain the core subset after random sampling and the results may vary whenever we calculate. It could give low reproducibility, and not easy to get consistent results from multiple runs. Since we use two simple statistical measures, the coverage and diversity scores, and each iteration reduces the size of data matrix by removing the selected sample and genetically identical samples, our method is capable of fast computation after a lot of iterations. If you have only small number of markers from tandem repeats or restriction fragments, GenoCore may not give better results. However, it will give optimized and consistent subsets for high-density markers.    The key subset from the population is important because this makes experimental cost and time to decrease, but there are a few software for this. In this study, we use an intuitive\r\n  Input file size 39 Mb 130 Mb 130 Mb  Runtime 10 min 10 min 26 min  Used maximum memory 1.6 Gb 9.7 Gb 14 Gb 8 / 10 approach to select a core collection from large, diverse genetic datasets. We define two statistics, the coverage score and diversity score (see Materials and Methods), that are simple and have computational advantage for large datasets. They represent that the average and variance genotype frequency of the sample for all markers. As a result, GenoCore chooses a sample with more common and less variated alleles rather than a sample including a very rare allele when there are millions of markers.    To assess the performance of our method, we use three real datasets (wheat 35k [10], rice\r\n    1.5k [6] and rice 700k [9]), three core collection programs (MSTRAT [2], Core Hunter [4],\r\n  and PowerCore [ 3 ], and three measurements. The newly defined coverage score makes the genetic coverage of core subsets to increase faster than other methods (Fig 1) and the diversity score is to correct the bias that can occur in the selection of focusing common alleles. Therefore, the minor allele frequency of our result is similar to that of the entire population (Fig 5). We conduct the principal component analysis to evaluate the position of core collection in the PCA plot of entire samples and confirm that the result of GenoCore is suitably spread (Fig 2). Our method shows a similar or better values for MR, minMR, SH, and CV when compared to the other programs (Tables 2 and 3). Especially, GenoCore has a biggest minMR, this means that the minimum genetic distance between samples in the core collection is the largest. Nevertheless, this does not make the results of GenoCore quite different from those of other methods.  One of the goals in developing new algorithm is to minimize system resource and to be able to calculate for a large dataset, for example, high-throughput array data and whole genome sequencing data. Our method requires less memory and execution time compared to the other core collection software (Tables 5 and 6). Other program cannot be executed for another large dataset, for example, 700k rice SNP chip, 180k soybean SNP chip [ 8 ] and whole genome sequencing data (data is not shown). Only GenoCore can be used for large datasets such as high-density SNP arrays and next-generation sequencing, because it is written with R statistical language, which is flexible and has efficient memory. GenoCore can be downloaded from https://github.com/lovemun/Genocore and includes an example.     Supporting information\r\n  S1 Fig. Boxplot for reproducibility using 100 replicates. (TIF)    Acknowledgments\r\n   This work was supported by grants from the National Research Foundation of Korea (NRF2011-0030049, NRF-2014M3C9A3064552), Next-Gen Bio-Green21 PJ011929, and the KRIBB initiative program.\r\n     Author Contributions\r\n   Conceptualization: Seongmun Jeong, Jung-Kyung Moon, Namshin Kim.\r\n    Data curation: Seongmun Jeong, Jung-Kyung Moon, Namshin Kim.\r\n  Formal analysis: Seongmun Jeong. Funding acquisition: Namshin Kim.    Investigation: Seongmun Jeong, Soon-Chun Jeong, Jung-Kyung Moon, Namshin Kim.\r\n    Methodology: Seongmun Jeong, Namshin Kim.\r\n  9 / 10 Project administration: Namshin Kim.    Resources: Soon-Chun Jeong, Sung-Taeg Kang, Jung-Kyung Moon.\r\n    Software: Seongmun Jeong, Namshin Kim.\r\n    Validation: Seongmun Jeong, Jae-Yoon Kim, Jung-Kyung Moon.\r\n    Visualization: Seongmun Jeong.\r\n  Writing ± original draft: Seongmun Jeong.    Writing ± review &amp; editing: Seongmun Jeong, Jae-Yoon Kim, Namshin Kim.\r\n  10. 11.  Wilkinson PA, Winfield MO, Barker GL, Allen AM, Burridge A, Coghill JA, et al. CerealsDB 2.0: an integrated resource for plant breeders and scientists. BMC Bioinformatics 2012; 13: 219. https://doi.org/10.1186/1471-2105-13-219 PMID: 22943283 Wright S. Evolution and the Genetics of Populations. A treatise in four volumes, Volume IV: Variability within and among natural populations. The University of Chicago Press; 1978     ",
    "sourceCodeLink": "https://github.com/lovemun/Genocore",
    "publicationDate": "0",
    "authors": [
      "Seongmun Jeong",
      "Jae-Yoon Kim",
      "Soon-Chun Jeong",
      "Sung-Taeg Kang",
      "Jung- Kyung Moon",
      "Namshin Kim"
    ],
    "status": "Success",
    "toolName": "Genocore",
    "homepage": ""
  },
  "3.pdf": {
    "forks": 0,
    "URLs": ["github.com/njau-sri/rtm-gwas"],
    "contactInfo": [],
    "subscribers": 1,
    "programmingLanguage": "C++",
    "shortDescription": "",
    "publicationTitle": "An innovative procedure of genome\u2011wide association analysis fits studies on germplasm population and plant breeding",
    "title": "An innovative procedure of genome\u2011wide association analysis fits studies on germplasm population and plant breeding",
    "publicationDOI": "10.1007/s00122-017-2962-9",
    "codeSize": 89,
    "publicationAbstract": "The previous genome-wide association studies (GWAS) have been concentrated on finding a handful of major quantitative trait loci (QTL), but plant breeders Vol.:(011233456789)",
    "dateUpdated": "2017-10-11T07:09:22Z",
    "institutions": [
      "Communicated by Dr. Mikko J. Sillanpaa",
      "Nanjing Agricultural University",
      "Ministry of Agriculture",
      "BGI-Shenzhen"
    ],
    "license": "MIT License",
    "dateCreated": "2016-11-24T08:11:27Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     10.1007/s00122-017-2962-9   An innovative procedure of genome\u2011wide association analysis fits studies on germplasm population and plant breeding     Jianbo He  0  1  2  3  4  5  6    Shan Meng  0  1  2  3  4  5  6    Tuanjie Zhao  0  1  2  3  4  5  6    Guangnan Xing  0  1  2  3  4  5  6    Shouping Yang  0  1  2  3  4  5  6    Yan Li  0  1  2  3  4  5  6    Rongzhan Guan  0  1  2  3  4  5  6    Jiangjie Lu  0  1  2  3  4  5  6    Yufeng Wang  0  1  2  3  4  5  6    Qiuju Xia  0  1  2  3  4  5  6    Bing Yang  0  1  2  3  4  5  6    Junyi Gai  0  1  2  3  4  5  6    0  Communicated by Dr. Mikko J. Sillanpaa    1  Jiangsu Collaborative Innovation Center for Modern Crop Production, Nanjing Agricultural University ,  Nanjing 210095 ,  China    2  Key Laboratory of Biology and Genetic Improvement of Soybean (General), Ministry of Agriculture ,  Nanjing 210095 ,  China    3  National Center for Soybean Improvement, Ministry of Agriculture ,  Nanjing 210095 ,  China    4  Soybean Research Institute, Nanjing Agricultural University ,  Nanjing 210095 ,  China    5  State Key Laboratory for Crop Genetics and Germplasm Enhancement, Nanjing Agricultural University ,  Nanjing 210095 ,  China    6  State Key Laboratory of Agricultural Genomics, BGI-Shenzhen ,  Shenzhen 518083 ,  China    2  18    1  8  2017    27  1  2017     The previous genome-wide association studies (GWAS) have been concentrated on finding a handful of major quantitative trait loci (QTL), but plant breeders Vol.:(011233456789)       -\r\n  Jianbo He and Shan Meng have contributed equally to this work. are interested in revealing the whole-genome QTL-allele constitution in breeding materials/germplasm (in which tremendous historical allelic variation has been accumulated) for genome-wide improvement. To match this requirement, two innovations were suggested for GWAS: first grouping tightly linked sequential SNPs into linkage disequilibrium blocks (SNPLDBs) to form markers with multi-allelic haplotypes, and second utilizing two-stage association analysis for QTL identification, where the markers were preselected by single-locus model followed by multi-locus multi-allele model stepwise regression. Our proposed GWAS procedure is characterized as a novel restricted two-stage multi-locus multi-allele GWAS (RTM-GWAS, https://github.com/njau-sri/rtm-gwas).The Chinese soybean germplasm population (CSGP) composed of 1024 accessions with 36,952 SNPLDBs (generated from 145,558 SNPs, with reduced linkage disequilibrium decay distance) was used to demonstrate the power and efficiency of RTM-GWAS. Using the CSGP marker information, simulation studies demonstrated that RTM-GWAS achieved the highest QTL detection power and efficiency compared with the previous procedures, especially under large sample size and high trait heritability conditions. A relatively thorough detection of QTL with their multiple alleles was achieved by RTMGWAS compared with the linear mixed model method on 100-seed weight in CSGP. A QTL-allele matrix (402 alleles of 139 QTL × 1024 accessions) was established as a compact form of the population genetic constitution. The 100-seed weight QTL-allele matrix was used for genetic characterization, candidate gene prediction, and genomic selection for optimal crosses in the germplasm population.    Introduction\r\n  Green revolution is a great success of conventional plant breeding in which the basic breeding procedure includes two major steps: the first is to choose parental materials from germplasm or breeding materials to design optimal crosses, and the second is to select the best progenies for further testing in the segregating generations. As the crosses determine the potential of progeny selection, optimal cross design is a key of the conventional breeding. The concept of \u201cBreeding by Design\u201d through designing parental crosses based on quantitative trait loci (QTL) was proposed for direct genotypic selection for potential recombination in plant breeding  (Peleman and van der Voort 2003) . Meanwhile, genomic selection (GS) focusing mainly on progeny selection was proposed as a markerassisted selection procedure for animal breeding with no need of QTL information but using a training population to establish the relationship between whole-genome markers and the phenotype values termed genomic estimated breeding value (GEBV) as selection criterion  (Meuwissen et al. 2001) . Although plant scientists also consider using GS for both cross selection and progeny selection in plant breeding  (Hefner et al. 2009; Mohammadi et al. 2015) , the GS approach for animal breeding cannot be readily applied to complex plant breeding  (Jonas and de Koning 2013) . The dificulties of using the classic GS approaches in plants lie in the uncertain accuracy of an indirect black box relationship, inadequate estimation of recombination potential, and high genotyping costs in large amount of progeny materials  (Desta and Ortiz 2014) . Although some eforts have been made to improve the accuracy of genomic prediction  (De Coninck et al. 2016; Jiang and Reif 2015; Vazquez et  al. 2016) , in plant conventional breeding, following the \u201cBreeding by Design\u201d concept, GS based on whole-genome QTL-allele constitution seems to be a potential approach to both optimal crosses and superior progenies. This GS approach can be termed as QTL-allele-based GS, which in fact is a direct genotype selection method.  To utilize QTL-allele-based GS eficiently for both the optimal design of crosses and the selection of superior progenies in plant breeding, a relatively thorough detection of the whole-genome QTL-allele constitution in a germplasm/ breeding population is an essential requirement. Plant germplasm is a genetic reservoir from which improved varieties were developed. The QTL-allele constitution of all the derived breeding populations can be inferred from that of the germplasm population. And if the genetic constitution of all breeding target traits in the germplasm has been explored, the GS can be utilized in both plant breeding stages, that means at the first stage with GS for optimal cross design based on the genetic structure information of the parental materials/germplasm and at the second stage with GS for elite progenies using a set of markers associated with target traits.  The genome-wide association study (GWAS) was found to be a potential approach for detecting whole-genome QTL with multiple alleles in a large natural population, especially in plant germplasm population, based on linkage disequilibrium (LD)  (Nordborg and Weigel 2008; Huang and Han 2014) . However, GWAS has been suefred from a high falsepositive rate caused by unknown population structure, which could interfere in the QTL detection. The population structure bias may be caused by the admixture of heterogeneous populations or the inbreeding-caused relatedness among the materials, or mainly a mixture of them  (Devlin and Roeder 1999; Campbell et al. 2005; Voight and Pritchard 2005) . The structured association analysis (SA) using model-based clustering and principal components analysis (PCA) of the marker covariance matrix are two widely used methods to correct admixture bias  (Pritchard et al. 2000; Price et al. 2006) . The approaches based on linear mixed models (LMM) that incorporate kinship matrix were proposed to correct population structure bias from both admixture and relatedness  (Yu et al. 2006; Kang et al. 2008, 2010; Zhang et al. 2010) , and the LMM method had been the preferred statistical method for GWAS in plants  (Atwell et al. 2010; Huang et al. 2010; Jia et al. 2013; Li et al. 2013; Morris et al. 2013; Dhanapal et al. 2015) . The widely used SA, PCA, and LMM methods in many studies performed association tests individually on a single marker basis (single-locus model), where the accumulated contribution of the detected QTL may be inflated obviously due to the correlations among neighboring loci, and, therefore, leading to the overflowing heritability problem under single-locus model. Therefore, it is more reasonable to explicitly include multiple loci in the statistical model  (Zeng 1994) . Recently, statistical methods based on multi-locus model under the framework of LMM have been also proposed for GWAS  (Segura et al. 2012; Rakitsch et al. 2013; Wang et al. 2016) , but they have not been widely used due to lack of user-friendly computer software and the time-consuming computations for large-scale GWAS.  However, there are still some dificulties in using the previously reported GWAS procedures for characterization of germplasm (or breeding populations) in plants. The ifrst is that the bi-allelic SNP marker could not match the multi-allele property of traits in the germplasm materials. The second is high false-negative rate or the missing heritability problem which causes the large gap between the detected QTL contribution and the total genetic contribution (the overall heritability) due to the stringent experimentwise significance level, such as the Bonferroni correction. For example, only an average of five loci was identified in GWAS of 41 traits in rice, accounting for about 22% of the total phenotypic variation  (Huang et al. 2010; Zhao et al. 2011) . Therefore, a relatively thorough detection of genomewide QTL is required to provide the full information of the genetic constitutions of the germplasm population. The third is high false-positive rate or the overflowing heritability problem which causes the inflation (even much more than 100%) of the total phenotypic variation explained by detected QTL under single-locus model. In addition, the dififculties also lie in how to choose the optimal crosses based on the genetic structure of the germplasm/breeding materials and how to reduce the genotyping cost for the large number of progenies at each selection generation in plant breeding.  In the present study, two innovations were suggested for a relatively thorough QTL-allele detection required in germplasm/breeding population study and genome-wide selection in breeding programs: rfist grouping tightly linked sequential SNPs into LD blocks (SNPLDB) to generate multi-allelic haplotypes, and second performing two-stage association analysis for QTL identification, with a single-locus model pre-selection of markers followed by multi-locus multi-allele model stepwise regression for QTL identification. Accordingly, a novel restricted two-stage multi-locus multi-allele GWAS procedure (RTM-GWAS) was assembled and then its efectiveness was demonstrated using simulation experi ments in comparison with a set of previous GWAS procedures. The usability of RTM-GWAS was further demonstrated with a case study on 100-seed weight of Chinese soybean germplasm population (CSGP), from which the established QTL-allele matrix was used in characterizing the germplasm population, annotating candidate genes, and applying genomic selection for optimal crosses.    Materials and methods\r\n   Plant materials and field experiments\r\n  The Chinese soybean germplasm population (CSGP) consists of 1024 soybean accessions, including wild soybean (Glycine soja Sieb. &amp; Zucc.) and cultivated soybean [Glycine max (L.) Merr.], which was sampled from the germplasm storage at the National Center for Soybean Improvement, Nanjing, China. The wild soybean part consists of 203 annual wild accessions (WA), and the cultivated soybean part consists of 375 farmers' landraces (LR) and 446 released cultivars (RC). The genetic relationship among the CSGP accessions was presented in Fig. S1. Theoretically, this population contains a wide range of genetic variation accumulated during the soybean domestication and breeding history in China, and therefore, the QTL-allele information obtained from the CSGP may be passed onto its derived breeding materials. In fact, the use of the germplasm from multiple sources covering WA, LR, and RC of the soybean is a challenge to the GWAS procedure to be established.  The CSGP accessions were planted in a randomized complete block design experiment with three replications, at Jiangpu Experimental Station of Nanjing Agricultural University, Nanjing, China, (32°07°N, 118°62°E), in 2010, 2011, and 2012. A modification was made for the experiment design that the wild accessions and cultivated accessions were planted in two separate sub-blocks in each replication/block because of the diefrent plot sizes between wild and cultivated soybeans, i.e., 1.0 × 1.0 m2 hill plots for WA and 0.8 × 0.8 m2 hill plot for LR and RC due to the large and vining plant of WA.  After the thinning at V3 (third node) stage, eight and five plants were kept in each hill plot for cultivated soybean and wild soybean, respectively. To hold the twining stem of the wild soybeans upward, a bamboo stick was used in each hill plot starting at V6 (sixth node stage). The field management including weed control and fertilizer application was conducted normally. The matured seeds were harvested and dried under 35-40 °C till a constant weight. The 100-seed weight (g) was measured five times per plot for the whole experiment.    Statistical analysis of the phenotypic data\r\n  The analysis of variance (ANOVA) of randomized block design under multiple years/environments was used for 100seed weight of the 1024 accessions, and here, we assume that the efects of the sub-blocks and plot sizes for WA and LR/ RC on 100-seed weight are small and may be compensated under multiple environments. The mixed-efects model was used for ANOVA, and the phenotypic value for ith environment (year), jth block nested in ith environment, and kth genotype is expressed as yijk = + ei + rj(i) + gk + (ge)ik + ijk, where μ is the overall mean, and the environment and genotype-by-environment interaction efects were considered as random efects. The heritability of 100-seed weight was estimated as ĥ2 =  ̂ 2∕[ ̂ g2 +  ̂ g2e∕s +  ̂ 2∕(s ⋅ r)], where  ̂ 2,  ̂ 2 , g g ge and  ̂ 2 are estimated variances of genotype, genotype-byenvironment, and random error, respectively, and s is the number of environments and r is the number of replications  (Hanson et al. 1956) . The variance components were estimated using the REML method of PROC VARCOMP in SAS/STAT software (SAS Institute Inc., Cary, NC, USA).    SNP genotyping\r\n  The RAD-Seq (restriction site-associated DNA sequencing) was used for SNP genotyping for all the 1,024 soybean accessions in the present study, which was done at BGI Tech, Shenzhen, China. The genomic DNA was extracted from 6-8 to 8-10 bulked leaves (V4-V5 vegetative growth stage) of G. max and G. soja per accession according to the hexadecyltrimethylammonium bromide (CTAB) method  (Murray and Thompson 1980) . Restriction site-associated DNA (RAD) libraries were created as previously described  (Baird et al. 2008) . The barcoded adapters were ligated to each sample. The barcodes were 6 nucleotides (nt) long and difered from each other by at least 2 nt to facilitate unambiguous identification of each specimen following sequencing. The specimens were pooled and sheared, and the 400-700 base pair (bp) size fractions were puriifed from an agarose gel after electrophoresis. Following ligation of a second adaptor, the fragments containing both adapters were PCR-amplified (12 cycles) and the 400-700 bp size fraction was isolated as described above. The RAD libraries were sequenced on an Illumina HiSeq 2000 instrument through multiplexed shotgun genotyping method  (Andolfatto et al. 2011) .  All sequence reads were aligned against the genome of Williams 82 using SOAP2  (Li et  al. 2009; Schmutz et  al. 2010) . The RealSFS was used for SNP calling at population level based on the Bayesian estimation of site frequency. For imputation quantity control, the resulted SNPs were pruned with a maximum missing and heterozygous allele call rate of 20% and a minimum minor allele frequency (MAF) of 1% (if a third allelic phenomenon appeared only in one individual, it was treated as missing allele) in the population. Then, missing genotype data were imputed using software fastPHASE  (Scheet and Stephens 2006)  and SNPs with MAF &lt; 1% were excluded.    Simulation studies for key issues in GWAS\r\n   Simulation study on LD decay for diferent inbreeding coeficients\r\n  A total of 13 populations with diferent inbreeding coeficient were simulated using forward-time population simulation package simuPOP  (Peng and Kimmel 2005) . The proportion of individuals under self-mating schemes (equivalent to inbreeding coeficient) was set as 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, and 1.0, respectively. The population size was set to 10,000. Each population was initialized from an F1 hybrid and then evolved for 100 generations with ten replications. The LD between loci with varying recombination fractions was recorded. For evaluating LD decay distance, the recombination fraction was approximately transformed to genetic distance and then to physical distance by assuming that 1 cM is approximately equivalent to 400 kb according to the soybean genetic and sequence map at SoyBase (http://soybase.org).    Simulation study on marker\u2013trait association power for diferent QTL heritabilities\r\n  The theoretical power of marker-trait association test for diferent LD and heritability levels was calculated using the PROC POWER in SAS/STAT software (SAS Institute Inc., Cary, NC, USA) based on tests for simple correlation coeficient between marker and trait with a sample size of 1000. The correlation between marker and trait under additive genetic model can be measured as √r2h2, where r2 is the LD between the typed marker and causal locus, and h2 is the heritability of the causal locus  (Weir 2008) .     Design and establishment of the RTM\u2011GWAS procedure\r\n   SNPLDB marker construction\r\n  Since the SNPs usually distribute unevenly on the genome, the tight and loose linkage between neighboring SNPs leads to a block-like structure of genomic sequences which may transmit together to the ofspring. The diferent combinations of linked SNPs in a block could be classified as multiple haplotypes/alleles. This should overcome the low polymorphism and redundant QTL detection of the bi-allelic SNP markers and theoretically more efec tive and powerful than SNP markers in GWAS. Accordingly, the multi-allelic marker-type termed SNP LD block (SNPLDB) was suggested as genomic markers for plant germplasm population.  Genomic blocks were defined first using the block partitioning approach with confidence intervals based on genome-wide LD (D\u2032) pattern  (Gabriel et al. 2002) . Then, the SNPs within an LD block were grouped into an SNPLDB marker with multiple haplotypes as its alleles. The genotype for each individual was determined by its corresponding SNP haplotype at each locus. To control the minor allele frequency for further statistical analysis, the haplotypes with extremely low frequency (such as &lt;0.01) were approximately replaced with the most similar haplotype in a same SNPLDB. The similarity between two haplotypes was defined as the proportion of SNP sites in identity-by-state. Any individual SNP that could not be grouped into blocks was treated as an individual SNPLDB marker. Therefore, there are two types of SNPLDB marker, the SNPLDB with multiple SNPs and the SNPLDB with a single SNP. The LD between the multi-allelic SNPLDBs was calculated as the weighted average of LD values for all allele combinations  (Farnir et al. 2000) .    Construction of the genetic similarity coeficient matrix using SNPLDB markers\r\n  Since the marker-based genetic relationship matrix used for correction of the population structure bias in the previous methods  (Patterson et  al. 2006; Price et  al. 2006; VanRaden 2008)  is suitable for the bi-allelic SNPs but not for the proposed multi-allelic SNPLDBs, here, we suggest a more flexible genetic similarity coeficient (GSC) matrix based on SNPLDBs to estimate the comprehensive population structure. The GSC between two individuals is defined as the proportion of loci that are in identity-bym state, sij = ∑k=1 cijk∕(2m), where cijk is the number of common alleles (taking a value of 0, 1, or 2) at kth SNPLDB between ith and jth individual, and m is the total number of SNPLDBs. In spite of the population structure varies uncertainly due to variable admixture components, inbreeding schemes, and even both, the GSC matrix may be used as a general approach to estimate the varied population structure biases without any requirement of pre-set assumptions. In practice, the top J eigenvectors with largest eigenvalues of the GSC matrix calculated from genome-wide SNPLDBs were suggested to correct the population structure bias in association analysis based on linear model. The population structure efect here is considered as fixed efect rather than random efect, since the population is pre-determined rather than randomly formed, and therefore, the GSC correction for population structure should be directly on the population.    Two\u2011stage multi\u2011locus multi\u2011allele association analysis\r\n  In general, hundreds of thousands or even millions of molecular markers are used in GWAS, but in fact, most of them may be irrelevant to the target trait. To eefctively reduce the huge model space for an eficient multiple QTL modeling, we suggest using a two-stage strategy to perform GWAS. Here, the individuals are all assumed homozygous at each locus for simplicity; otherwise, a genotypic genetic model can be implemented. At the first stage, a single-locus association test is performed to eliminate redundant SNPLDBs, and the linear model is expressed as: yi = +  J j=1 wij j +  L l=1 xil l + i, (1) where yi is the observed phenotypic value of the ith individual and μ is the overall mean; wij is the coeficient of the jth eigenvector of the SNPLDB GSC matrix of the ith individual, and aj is the efect of the jth eigenvector of the SNPLDB GSC matrix, while J is the number of eigenvectors chosen for population structure correction; xil is a value of 0 or 1, indicating the allele state of the lth allele (L is the J j=1  K Lk total number of alleles) at the SNPLDB under testing for the ith individual and βl is the efect of the l th allele; εi is the residual efect assumed to be normally distributed.  At the second stage, a multi-locus model extended from Eq. (1) is built based on the candidate SNPLDBs selected at the first stage, yi = + wij j + xikl kl + i, (2) where xikl is a value of 0 or 1, indicating the allele state of the lth allele at the kth locus for the ith individual, and βkl is the efect of the lth allele at the kth locus, while Lk is the total number of alleles at the kth locus. Other terms and parameters are the same as in Eq. (1).  Equation (1) can be solved using regression analysis, and Eq. (2) can be solved eficiently using stepwise regression featured with forward selection and backward elimination. The normal significance level 0.05 or 0.01 is recommended for the F test under single-locus model to pre-select candidate markers, and also for multi-locus model of the stepwise regression approach as the built-in control for the experiment-wise error rate. Since the QTL detection is carried out at the second stage under multi-locus model, the total genetic variation explained by detected QTL will be less than the total genetic variation among accessions or the heritability of the trait.    RTM\u2011GWAS\r\n  The two-stage multi-locus multi-allele association analysis combined with using SNPLDB markers is defined as restricted two-stage multi-locus multi-allele genome-wide association study procedure, coded as RTM-GWAS. The RTM-GWAS is characterized with the following key points: (1) SNPLDB genome markers with multiple alleles; (2) twostage association analyses, i.e., association analysis under single-locus model for pre-selection of markers and stepwise regression analysis under multi-locus model for genomewide QTL-allele detection, with total genetic contribution limited to the overall heritability value; (3) population structure correction using GSC matrix calculated from multiallelic markers; and (4) normal built-in experiment-wise significance criterion.     Simulation comparisons among diferent GWAS methods\r\n  The SNPLDB markers of the real CSGP and a simulated ideal population were used for simulation comparisons. The ideal population was simulated from the real SNPLDB markers of the CSGP, in which the genotype data were randomly shufled across whole genome. The population structure is assumed to be eliminated in the ideal population, and therefore, the ideal population provided a reference for other populations to be compared. To simulate the QTL detection, a total of 100 SNPLDBs were randomly sampled as the causal loci, and their associated efects were drawn from an exponential distribution with a rate of 1. The random error was drawn from the normal distribution with a scaled variance to fix the trait heritability to 0.9 (about the similar in the present study for 100-seed weight under replicated experiments). The phenotype value for each individual was obtained as the sum of overall genotypic value plus a random error.  In addition, to examine the performance of different GWAS methods under six diferent architectures, i.e., two QTL number levels (10 and 100) by three trait heritability levels (0.2, 0.5, and 0.9), were also simulated based on the real genotype data of CSGP using the same simulation method as above. To assess the influence of sample size on RTM-GWAS, four populations with diferent sample size, i.e., 200, 400, 600, and 800, were randomly sampled from the ideal population and used for simulations under the 100locus model with a trait heritability of 0.9.  The RTM-GWAS method was compared to the Naïve, PCA, LMM, and the haplotype-based association test (HAT) methods. The Naïve method performs the association test based on the simple linear model without control of population structure. The PCA method performs the association test based on a general linear model, in which the top 10 eigenvectors of SNPLDB GSC matrix are incorporated as covariates to correct for population structure. The LMM performs the association test based on a linear mixed model, in which the SNPLDB GSC matrix is incorporated as covariance structure of the random efect for population structure correction. In the present LMM analysis, the EMMAX algorithm  (Kang et al. 2010)  was used. For performing the HAT method, the --hap-assoc command in the PLINK software was used  (Purcell et al. 2007) . As the --hap-assoc command can include only one covariate, the phenotypic values were adjusted through regression analysis to leave out the efects of eigenvectors, and then, the adjusted phenotypic values were used in HAT. In this way, the HAT procedure can keep consistent with other GWAS procedures which use the top 10 eigenvectors of SNPLDB GSC matrix for population structure correction. A uniform Bonferroni-adjusted significance level of 0.05 was used for Naïve, PCA, LMM, HAT, and the second stage of RTM-GWAS, while a threshold of 0.05 was used for the first stage of RTM-GWAS. To evaluate the influence of significance level on GWAS, nine fixed significance levels without Bonferroni correction were also used in comparisons among the vfie GWAS methods, including the significance levels of 1e−10, 1e−9, 1e−8, 1e−7, 1e−6, 1e−5, 1e−4, 1e−3, and 1e−2, respectively.  In computing the detection eficiency or false discovery rate (FDR) and detection power, an associated marker was considered as a false positive if none of the causal loci were found within a 100-kb window centered at the causal locus; otherwise, it was considered as a true positive. A causal locus was counted as a detected locus if at least one marker was found significantly associated within a 100-kb window centered at the causal locus. A total of 100 independent replications were performed to get an average of the detection power and FDR. For comparisons among the diferent GWAS methods, the following indicators were calculated: Power, overall detection power of the 100 simulated causal loci; PVE, phenotypic variation explained by detected causal loci; FDR, false discovery rate; RPO, relative detection power, i.e., Power × (1 − FDR); RPV, relative PVE, i.e., PVE × (1 − FDR).    GWAS of 100\u2011seed weight in the CSGP\r\n  The genetic structure (neighbor-joining tree) of the population was estimated based on the GSC matrix calculated from genome-wide SNPLDBs. The pairwise distance matrix was calculated as one minus the SNPLDB GSC matrix built from all the SNPLDBs. Then, the PHYLIP 3.6 software  (Felsenstein 1989)  was used to construct the neighbor-joining tree. Based on it, the GWAS of 100-seed weight of the CSGP was performed using the RTM-GWAS procedure established above.    Gene annotation and candidate gene selection\r\n  The gene system conferring soybean 100-seed weight from the detected QTL system was established as follows: at first, the annotated genes were searched within the interval (with a 30-kb flanking expansion) of the associated SNPLDBs. Then, to identify the candidate genes for the trait, the Chisquare tests were performed to test the linkages between the detected SNPLDB and the SNPs within each annotated gene. The tests were conducted for all the SNPs in an annotated gene. The significance level was set to 0.05. The gene calls and annotations were retrieved from G. max (version Wm82. a1.v1.1) SoyBase (http://soybase.org).    Genetic diferentiation analysis of the population\r\n  The analysis of molecular variance (AMOVA) for WA-LR-RC subpopulation variation was conducted using the Arlequin software  (Excoffier and Lischer 2010)  for the complete SNPLDB data set and for the detected 100seed weight QTL/marker data set, respectively. Chi-square test was used to test the independence of allele frequency distribution among subpopulation for each QTL. FST was estimated for all SNPLDBs using a sliding window of 500kb interval centered on each SNPLDB.    Genomic selection for optimal crosses\r\n  For optimal cross selection in plant conventional breeding, all possible single crosses (523,776) were generated in silico, each with 2000 homozygous progenies derived randomly from an F1 hybrid through continuously selfing. For linkage model, the number of crossovers on a chromosome is simulated according to the Poisson process with a rate parameter of λ under no interference, where λ is the length of chromosome in Morgan. For independent assortment model, all loci were assumed independent from each other and the alleles of diefrent loci assorted independently. The genotypic value of a progeny was predicted as the sum of QTL genotypic values according to QTL-allele matrix. The predicted phenotypic value of a progeny derived from a cross between parental line i and j was calculated as yij = gij + (yi − gi + yj − gj)/2, where gij is the predicted genotypic value of the progeny, yi and yj are the observed phenotypic values of the two parental lines, respectively, gi and gj are the predicted genotypic values of the two parental lines, respectively, and the predicted genotypic value was calculated as the sum of QTL-allele efects. Accordingly, all the predicted values of the possible crosses were obtained for optimal cross selection based on genomic QTL-allele information.     Results\r\n   Key issues in GWAS of plant germplasm population\r\n  The theoretical base of using GWAS in detecting QTL in a natural population is the tight linkage disequilibrium (LD) between a marker and the involved QTL. In selfpollinated plants and the inbred lines of cross-pollinated plants, the inbreeding coeficient (IBC) is greater than 0.95, and it is about 0.50-0.95 in often cross-pollinated plants. The LD decay distance in populations with diferent IBCs was simulated (Fig. S2A). In general, the population with higher inbreeding exhibits slower LD decay, especially for IBC = 0.9-1.0. When IBC is less than 0.8, the LD starts to decay rapidly, but may still extend for a long distance. The estimated LD decay distances (Table S1) for IBC = 0.9-1.0 (self-pollinated plants) increase rapidly in comparison with those for IBC less than 0.8 (540-3822 kb vs. 306 kb for r2 = 0.5, and 1260-8919 kb vs. 714 kb for r2 = 0.3). Therefore, a key issue of GWAS in self-pollinated plants, such as soybean, is to cope with the inbreeding-caused population structure bias for a shortened LD decay distance and an efective QTL detection (here admixture being considered as the cause secondary to inbreeding for the increased LD decay distance).  The statistical power of GWAS in detecting QTL with varying heritability (h2) at an experiment-wise significance level of 5 × 10−7 (Bonferroni adjustment for 100,000 markers) was simulated (Fig. S2B). The QTL with higher heritability have higher statistical power to be detected in GWAS. For example, at r2 = 0.5, a QTL with h2 = 7% or larger can have a statistical power more than 0.8 to be detected, while for a small efect or a small h 2 QTL, the power drops down quickly (h2 = 1% with power less than 0.1). The GWAS can keep a relatively high power for QTL with high heritability even when the LD is not high; this means that in GWAS of populations with high inbreeding coeficient, the markers far away from the QTL can be significantly associated, but in fact as a noise rather than a useful locus. For QTL with low heritability, high power of GWAS requires very high LD and the detection power of a small genetic contribution QTL is much less than that of a large genetic contribution QTL. Therefore, the fluctuation of QTL detection will more likely happen for small contribution QTL than for large contribution QTL, especially in populations with increased inbreeding. Therefore, another key issue of GWAS in plants is to ensure that both large and small contribution QTL can be detected for understanding the entire genetic architecture of the trait.  The previous GWAS strategy in plants was mainly based on individual SNP association test. However, in fact, due to the historical mutation, recombination, and introgression, multiple alleles widely accumulated on each locus in natural populations. The bi-allelic SNP marker in the previous GWAS could not represent the multi-allelic nature of a QTL. The geneticists are interested in detecting QTL/genes, while the breeders are more likely interested in finding the best alleles on multi-allelic loci. Therefore, the third key issue of GWAS in plants is to detect the entire QTL-alleles with their efects estimated.    Properties of the SNPLDB marker\r\n  The SNPLDB markers in the Chinese soybean germplasm population (CSGP) were studied based on the genotyping by sequencing through RAD-seq (restriction site-associated DNA sequencing). A total of 145,558 high-quality SNPs with minor allele frequency (MAF) &gt;0.01 were identified (Table S2). Genome-wide LD patterns from SNPs indicated that the LD was high with the maximum of half decay distance about 500 kb for r2 (Fig. 1a, b). Accordingly, a total of 36,952 SNPLDBs were constructed with 2-14 haplotypes/ alleles per SNPLDB marker (Fig. 1c; Table S2). Among the SNPLDBs, 70.3% of them are composed of a single SNP termed as S.SNPLDB, while 29.7% are built from multiple SNPs termed as M.SNPLDB (Fig. 1c). A great part of the Fig. 1 Characterization of genome-wide SNP and SNPLDB. a Minor allele frequency of SNP in CSGP. b Allele frequency of SNPLDB in CSGP. c Decay of D\u2032 in CSGP. d Decay of r2 in CSGP. e Allele numSNPLDBs (74.2%) in CSGP have only two alleles. However, along with the increased coverage of the sequencing, the S.SNPLDBs would be very likely merged into LD blocks with multiple SNPs, as shown in Fig. 1d, where up to 78.2% of the SNPLDBs were M.SNPLDBs from the soybean resequencing genotype data by Zhou et al. (2015). The LD decay distance calculated from the SNPLDB marker data was shorter than those from the original SNP data. The distance at which D\u2032 decays to 0.6 was about 2000-3000 kb for SNPs but 200-500 kb for SNPLDBs (Table S3). Thus, the multiallelic SNPLDB marker with shortened LD decay distance could improve the power of GWAS.    Simulation demonstration of RTM\u2011GWAS in terms of detection eficiency and power\r\n  Simulations were performed to assess the detection eficiency and power of RTM-GWAS based on the SNPLDB data of CSGP. An ideal population without structure bias was simulated using the real CSGP SNPLDB data in which the alleles on each locus were randomly shufled among the 1024 accessions across the whole genome. This ideal population was used as a reference compared with the CSGP.  The detection eficiency (in terms of FDR) and power (in ber of SNPLDB in CSGP. f Allele number of SNPLDB in 302 resequenced soybean accessions population terms of detected QTL and PVE) were evaluated, and then, their combined indicators, relative detection power (RPO), and relative phenotypic variation explained by detected causal loci (RPV) were used as the comprehensive merit of the GWAS procedure. The RTM-GWAS method was compared to the PCA, LMM, and HAT methods, but SNPLDB GSC matrix rather than SNP covariance matrix was used for RTM-GWAS, PCA, LMM, and HAT. A uniform Bonferroni-adjusted significance level of 0.05 was used as the threshold to detect signicfiant QTL for all methods, even it is not necessary for RTM-GWAS (because Bonferroni correction was proposed for an experiment-wise test criterion in a single-locus model analysis, while the built-in experimentwise criterion is already in the multi-locus model analysis).  The simulation results of the 100-locus model with a trait heritability of 0.9 (Table 1) indicated that RTM-GWAS outperformed PCA, LMM, and HAT in both CSGP and the simulated ideal population. However, all methods encountered a high false discovery rate (FDR, generally &gt;0.3) in CSGP, but the RTM-GWAS method exhibited the lowest FDR, about 17% less than that of LMM with twofold detection power increase. In the simulated ideal population, all the singlelocus model methods, i.e., Naive, PCA, LMM, and HAT, performed similarly as expected, while RTM-GWAS showed Table 1 Performance comparison of the association analysis methods in 100-locus model simulations under a trait heritability of 0.9  Simulation CSGP Ideal Power, overall detection power of the 100 simulated causal loci; PVE, phenotypic variation explained by detected causal loci; FDR, false discovery rate; RPO, relative detection power [Power × (1 − FDR)]; RPV, relative PVE [PVE × (1 − FDR)]. Naïve, association test based on simple linear model without control for population structure; PCA, association test based on linear model, and the top 10 eigenvectors with largest eigenvalues of SNPLDB genetic similarity coeficient (GSC) matrix were incorporated as covariates to correct for population structure; LMM, association test based on linear mixed model, and SNPLDB GSC matrix was incorporated as covariance structure of random efect to correct for population structure; HAT, the haplotype-based association test implemented in PLINK software (--hap-assoc); RTM, association test using the proposed restricted two-stage multi-locus multi-allele GWAS method (RTM-GWAS). A uniform Bonferroni-adjusted significance level of 0.05 was used for Naïve, PCA, LMM, HAT, and the second stage of RTM-GWAS, while a threshold of 0.05 was used for the first stage of RTM-GWAS. The CSGP simulation was based on Chinese soybean germplasm population, comprising 1024 individuals, genotyped at 36,952 SNPLDBs; the ideal simulation was based on an ideal population simulated from real SNPLDB genotype data of CSGP, in which the genotype data were randomly shufled across whole genome a All computing was performed on a single core of an Intel Xeon E5-2670 2.60 GHz CPU, and the time is the total computing time elapsed for the 100 replications threefold increase in detection power. Furthermore, although the RTM-GWAS method showed a notable improvement in detection power, there was still a big gap between the variation explained by the detected QTL and total phenotypic variation. This indicated that the Bonferroni correction used in RTM-GWAS is too stringent, since the extra Bonferroni correction was added to the built-in experiment-wise error control, which caused a redundant error control. The power and FDR under diferent significance levels without Bonferroni correction were evaluated (Fig. S3), and the results indicated that the RTM-GWAS method performed the best at each signicfiance level for both CSGP and simulated ideal population. However, when using a very stringent significance level such as 1e−10, the RTM-GWAS method, and LMM had similar performance for CSGP.  The performance of association analysis methods was also evaluated under six different genetic architectures based on CSGP. The results of relative detection power (Table  2) showed that RTM-GWAS performed the best under 10-locus model at all heritability levels, and all methods failed (RPO &lt; 1%) to detect association under 100-locus model with an extremely low trait heritability (h2 = 0.2), but RTM-GWAS performed the best under 100-locus model with higher heritability (h2 = 0.5, 0.9). With the decrease of trait heritability, the detection power dropped rapidly as expected for all methods in the 100-locus model simulation (Table S4). For simple traits, i.e., the 10-locus model, the detection power decreased from 0.687 to 0.225 along with the decrease of trait heritability from 0.9 to 0.2 (Table S5).  These results indicated that the heritability of complex traits or the precision of the experiment is very important for the detection power of GWAS. The influence of sample size on the RTM-GWAS method was evaluated under the 100locus model with a trait heritability of 0.9 based on the ideal population. The simulation result (Table S6) showed that the detection power decreased and the FDR increased along with the decrease of sample size. For population with a small sample size, e.g., 200, the detection power of RTM-GWAS is as low as 0.022 with a high FDR of 0.326, but the detection power increases rapidly at a sample size of 400. The results indicated that both high heritability of the traits (or experiment precision) and large sample size are required for GWAS of complex traits.    The RTM\u2011GWAS of 100\u2011seed weight in Chinese soybean germplasm population\r\n  At first, the genetic structure of the population was estimated based on the GSC matrix calculated from genomewide SNPLDBs. The CSGP is highly structured and clearly divided into three clusters corresponding to WA, LR and RC (Fig. 2a). The frequency distribution, descriptive statistics, No. QTL, the number of causal loci simulated to generate a quantitative trait; h2, trait heritability; Naïve, association test based on simple linear model without control for population structure; PCA, association test based on linear model, and the top 10 eigenvectors with largest eigenvalues of SNPLDB genetic similarity coeficient (GSC) matrix were incorporated as covariates to correct for population structure; LMM, association test based on linear mixed model, and SNPLDB GSC matrix was incorporated as covariance structure of random efect to correct for population structure; HAT, the haplotype-based association test implemented in PLINK software (--hapassoc); RTM, association test using the proposed restricted two-stage multi-locus multi-allele GWAS method (RTM-GWAS). The simulation was based on the Chinese soybean germplasm population, comprising 1024 individuals, genotyped at 36,952 SNPLDBs; the relative detection power is calculated as [detection power × (1 − false discovery rate)] and analysis of variance are listed in Table S7 and Table S8. The distribution of 100-seed weight in CSGP exhibited a wide variation ranged 0.85-35.98 g, with a variation coefifcient of 54.4% and heritability of 98.9% (Fig.  2b; Table S7 and Table S8). Second, both RTM-GWAS and LMM methods were used to identify QTL for 100-seed weight. Only three loci were identified by LMM and 16 loci by RTMGWAS using the Bonferroni criterion (Fig. 2c; Table S9). The high false-negative rate indicated that the extra Bonferroni criterion is too stringent for detecting the genome-wide QTL. Therefore, the signicfiance level of 0.01 was then used for RTM-GWAS and 139 loci were identified covering 148 out of the 203 reported QTL regions for 100-seed weight, and 35 loci were new ones (Fig. 2d, Table S9 and Table S10) in comparison with the data in SoyBase (http://soybase.org).The Q-Q plot of association test p values (Fig. S4) showed that both LMM and RTM-GWAS exhibited an eficient cor rection for population structure.  The 139 loci for 100-seed weight distributed on all of the 20 soybean chromosomes, and there were 1-14 loci per chromosome with the chromosome 18 having the most. The phenotypic variation explained by each locus ranged from 0.07 to 9.84%. Among the 139 loci, there were 22 large contribution loci (R2 &gt; 1%) explaining a total of 61.8% phenotypic variation, and 117 small contribution loci (R2 &lt; 1%) explaining a total of 36.4% phenotypic variation; in a total, the 139 loci contributed 98.2% of the phenotypic variation. Since the undetected QTL (mainly small contribution QTL) explained only 0.7% of the phenotypic variation, a relatively thorough or full detection of the 100-seed weight was achieved using the RTM-GWAS under the experiment precision conditions. Furthermore, the most loci detected by Bonferroni-adjusted signicfiance level were also detected by non-adjusted significance level, but the detected loci by the latter usually cover those by the former. In the present study, the 139 loci include 15 out of the 16 loci identified using additional Bonferroni criterion and also include the two loci detected by LMM.    QTL\u2011allele matrix and candidate genes of 100\u2011seed weight in the CSGP\r\n  There were 47 loci with multiple SNPs (M.SNPLDBs) among the 139 detected loci. The allele number per locus ranged from 2 to 10, with an average of 2.9 and a total of 402 (detailed data in Spreadsheet S1). The efect of each allele was estimated by RTM-GWAS, including 201 positive and 201 negative eefct alleles. The allele eefcts ranged from 0.0189 to 9.5207  g for positive effect alleles and from −5.2758 to −0.0002 g for negative efect alleles, and approximately 89.3% of the allele eefcts were between −1.0 and 1.0 (Fig. 3a, Spreadsheet S1). The detected 100-seed weight loci with their 402 allele eefcts can be further organized into a 139 × 1024 (locus × accession) or 402 × 1024 (allele × accession) matrix (Fig. 3b), which in fact acts as the genetic structure of the whole population (CSGP). The matrix characterizes the population with the genetic richness and diversity on each locus as well as the population, from which all of the allele frequencies can be obtained for further study.  From the detected QTL system of soybean 100-seed weight, there are a total of 766 annotated genes within or neighboring to 126 out of the 139 loci according to SoyBase (http://soybase.org), among which 136 candidate genes (with 281 SNPs) were tightly associated with 74 loci (Table S11). The Gene Ontology analysis showed that these 136 genes are involved in various biological processes that could be grouped into nine categories, including primary metabolism, secondary metabolism, seed development, cell growth, photosynthesis, flower development, response to stress, signal transduction, and unknown process (Fig. S5A). In addition, the haplotypes of a candidate gene can be found from the genomic sequence data. For example, as shown in Fig. S5B, the candidate gene Glyma14g10915 has nine haplotypes based on the six SNPs, which correspond to vfie haplotypes/ alleles in the locus Gm14_BLOCK112_9120910_9131797 Fig. 2 Genetic dissection of phenotypic variation for 100-seed weight in CSGP. a Neighbor-joining tree of CSGP based on SNPLDB, where green for wild soybean, blue for soybean landrace, and red for released cultivar. b Histogram of 100-seed weight in CSGP. c Manhattan plot for LMM method. Association test was based on linear mixed model with EMMAX algorithm, and SNPLDB GSC matrix was incorporated as covariance structure of random efect. d Manhattan plot for RTMGWAS method with a threshold of 0.05 for the first stage and a significance level of 0.01 for the second stage, and the −log10 p values greater than 22.4 (the maximum is 129.8) were shown as 22.4 (color figure online) (The number of gene haplotype is greater than that of QTLallele, because the expanding region of ±30 kb was used to search candidate genes, and therefore, the first two SNPs of Glyma14g10915 were not in the SNPLDB interval). In this way, the RTM-GWAS can provide a way to predict the allelic information of gene-allele system in a germplasm population, rather than only a small part of them.    Genetic characterization of the CSGP\r\n  From the established QTL-allele matrix, the genetic differentiation can be detected and characterized at the entire population level or subpopulation level, involving whole-genome QTL or a group of QTL. The diferentiation in allele frequency between WA and LR subpopulations (WA-LR), between LR and RC subpopulations (LR-RC), and among the three subpopulations (WA-LR-RC) was examined based on both genome-wide SNPLDBs and QTL. The AMOVA based on genome-wide 36,952 SNPLDBs showed that 26.36, 4.09, and 19.31% of genomic variation were explained by the subpopulation for WA-LR, LR-RC, and WA-LR-RC, respectively (Table S12). The AMOVA based on the 139 loci for 100-seed weight exhibited a similar pattern with the results from the total markers, but with larger between(among)-subpopulation variation for LR-RC (4.82% vs. 4.09 in comparison with 20.7 1 vs. 26.36% for Fig. 3 QTL-allele matrix and optimal recombination design for soybean 100-seed weight. a Distribution of genetic efect of 402 alleles of the 139 QTL for soybean 100-seed weight. b Heat map representation of the QTL-allele matrix (locus × accession) of soybean 100seed weight, which the right color bar represents colors for the ten intervals of allele efects and the bottom color bar represents accessions with 100-seed weight arranged in ascending order where green for wild soybean, blue for landrace soybean, and red for released cultivar. c Distribution of predicted 100-seed weight of the simulated progenies of all possible single crosses, with the maximum and minimum (top and bottom horizontal line) values of parental lines. d Top 20 superior single crosses under linkage model. The optimal crosses were identified according to the 99th percentile of the sample as its breeding potential. Each cross was presented with QTL genotype of the two parental lines in one column, while diferent colors represent the size of QTL-allele efect as in b (color figure online) LR-RC and 15.90 vs. 19.31% for WA-LR-RC), which might coincide with the genetic improvement from LR to RC.  Each of the 100-seed weight loci was also statistically tested for diferentiation among subpopulations (Spreadsheet S1). Among the 139 loci, 83 (59.7%), 84 (60.4%), and 107 (77.0%) loci showed significant diferentiation in allele frequency for WA-LR, LR-RC, and WA-LR-RC, respectively, at a significance level of 0.01; and the proportion of loci (60.4%) with a significant diferentiation among LR-RC is greater than that on the genome-wide level (39.7%). The allele frequency of the 139 loci showed that 339 (84.3%) alleles existed in all three subpopulations, and 42 (10.4%) alleles existed in only two subpopulations, especially 21 (5.2%) alleles existed in only one subpopulation. The frequency of positive and negative efect alleles was investigated in each subpopulation and whole CSGP (Fig. S6). There are 46.1, 46.9, 48.3, and 47.4% positive efect alleles in WA, LR, RC, and CSGP, respectively. The subpopulation exhibited diferent pattern of distribution of positive and negative efect alleles. In general, the number of positive alleles was less than that of negative alleles for accessions with 100-seed weight &lt;10 g, but was greater for accessions with 100-seed weight &gt;10  g. Since the space in the present paper is limited, we will have another paper to present the diferentiation and evolutionary relationship among the subpopulations, including the newly emerged or extinct old alleles.    Genomic selection for optimal crosses of 100\u2011seed weight in the CSGP\r\n  Using the QTL-allele matrix in GS for optimal crosses, in the present study, all possible crosses (523,776) among the 1024 accessions each with 2000 homozygous progenies were simulated based on 100-seed weight QTL-allele matrix built from 139 QTL, with the 99th percentile representing the breeding potential for each single cross. The prediction was performed in two ways, with-linkage and without-linkage; for the with-linkage model, the natural linkage among the detected QTL in the population was maintained, while for the without-linkage model, independent assortment among the detected QTL was considered. Among the 523,776 single crosses, the top 20 crosses with largest predicted 100seed weight were identified with 12.4-19.9% improvement over the accession with largest 100-seed weight in the CSGP based on linkage model (Fig. 3c, d; Table S13). The linkage model is reasonable for optimal cross prediction in this case and the transgressive potential at both positive and negative directions is very large. However, the without-linkage or independent assortment model might be appropriate if breaking the negative linkages can reveal more potential (up to 30.7% improvement, Fig. S7 and Table S13).     Discussion\r\n   The eficiency and power of the RTM\u2011GWAS procedure\r\n  Our purpose was to explore the full QTL system in a plant germplasm/breeding population rather than only a handful of major QTL through the improvement of GWAS procedure, or in other words, to increase the QTL detection eficiency and power of the GWAS procedure. The above results indicated that the RTM-GWAS provides a much better QTL detection eficiency and power (about 5-8 or more times of) than the other four methods. The RTM-GWAS is characterized by two innovations: the multi-allelic SNPLDB genomic markers derived from bi-allelic SNPs and the twostage multi-locus multi-allele GWAS model.   First innovation: the SNPLDB marker with varied number of alleles\r\n  Compared with SNP-based GWAS methods, the multiallelic SNPLDB marker can match a QTL with multiple alleles and multiple SNPLDB markers can match a series of QTL with varied number of alleles. As the multi-allelic genetic variation is a natural property of the plant germplasm population, the SNPLDB marker theoretically matches the genetic loci with varied number of alleles and detecting QTL by SNPLDB marker should be more appropriate than the bi-allelic SNP marker. However, diferent haplo type block partitioning algorithms can be employed for SNPLDB marker construction, and may give very difer ent results  (Ding et al. 2005) . Therefore, the block definition may have strong efect on the construction of SNPLDB marker. Although several methods based on diferent concepts have been proposed for block partitioning, including the LD-based  (Gabriel et al. 2002) , the recombination-based  (Wang et al. 2002) , and the diversity-based  (Zhang et al. 2002)  methods, the approach based on LD confidence inter vals is suggested for SNPLDB construction in the present study. Because the genomic block is determined by the recombination history of the population, the LD pattern is the appropriate measurement of the historic recombination  (Pattaro et al. 2008) .    Second innovation: the two\u2011stage multi\u2011locus multi\u2011allele\r\n    GWAS model\r\n  The two-stage association analysis, where the markers were preselected by single-locus model followed by multi-locus multi-allele model stepwise regression, is benetfied from the distant noisy markers reduced at the rfist stage to keep a relatively less-noisy mapping environment, the total phenotypic contribution controlled within the trait heritability value, the built-in experiment-wise threshold without additional Bonferroni correction, and population structure adjustment with GSC. The simulation studies indicated that RTM-GWAS is much more eficient and powerful than the PCA, LMM, and HAT methods, especially under the conditions of high heritability, large sample size, and large number of involved QTL. Although the overall relative detection eficiency and power in Tables 1 and 2 are not high enough outwardly, there are two reasons involved. One is that the Bonferroni correction was used, which in fact is not appropriate for a procedure with a built-in experiment-wise significance criterion in RTM-GWAS. The results of the simulation studies were well supported by the case study on QTL detection of 100-seed weight in CSGP, where 16 QTL vs. 139 QTL were identiefid by the thresholds with vs. without Bonferroni cor rection, which showed very large diference in the detection power even for the same set of data. The large number of detected QTL (139) in the CSGP (with 1024 accessions) may be convinced by the fact that these QTL covered 148 of the 203 QTL regions reported at SoyBase (from about 59 parental materials). Another reason for a not high enough eficiency and power (but higher than the other procedures) in the simulation studies for RTM-GWAS is that the method to generate our simulation studies is diefrent from the previous studies. In the present study, 100 causal loci were simulated for a quantitative trait, and an overall detection power was calculated on all causal loci. However, in the previous studies in the literature, usually only a handful of causal loci were simulated or the detection power was only calculated on a part of the causal loci  (Segura et al. 2012; Wang et al. 2016) . As a quantitative trait is more likely controlled by a large number of genes, a 100-locus model for quantitative trait is more reasonable. Logically, the evaluation of detection power should be based on all causal loci rather than on a small part of them.    Benefit of GSC based on SNPLDB\r\n  In addition to the two innovations in the RTM-GWAS procedure, the use of GSC obtained from a large number of genome-wide SNPLDB for population structure correction also contributes to the improvement of detection eficiency and power. The germplasm/breeding population structure varies greatly, which can be caused from both admixture (such as geographic isolation, migration, and articfiial selection) and inbreeding (such as diefrential mating scheme), as well as their mixture with the proportions even not estimable  (Yu et al. 2006) . Usually, the accessions in germplasm were developed by farmers and collected from diferent geographic regions, which had inbred for many generations, even no pedigree/kinship can be traced for this kind of population. Therefore, separation of the whole population into subpopulations through model-based population structure correction does not necessarily match the real situation. Thus, GSC is the most possible estimation of the genetic relationship among accessions that can be estimated from the SNPLDB markers for population structure correction. The estimated relationship may contain all the genetic information of the population, including the comprehensive results from admixture and diferential inbreeding schemes, as well as both of them. The efectiveness of population structure correction with SNPLDB GSC matrix is about the same as EIGENSTRAT  (Price et al. 2006) , VanRaden1, and VanRaden2  (VanRaden 2008)  based on SNP markers (Table S14). However, the SNPLDB GSC matrix takes into account the varied number of alleles, and should tfi better the genetic relationship among the materials in the population. Therefore, using the SNPLDB GSC matrix for population structure correction (even based on sampling a representative working population if needed) is suggested as a universal approach in RTM-GWAS.    Further consideration on model selection\r\n  The stepwise regression algorithm used in the second stage of RTM-GWAS may not be the best solution for model selection. Actually, there are several sophisticated variable selection algorithms that have been widely used for QTL mapping, such as the Lasso methods  (Li and Sillanpaa 2012)  and the Bayesian methods  (Karkkainen and Sillanpaa 2012) . However, these algorithms are typically designed for continuous variable with one degree of freedom, and cannot be readily applied to the multi-allelic SNPLDB maker which can have two or more degrees of freedom. The group Lasso algorithm  (Yuan and Lin 2006)  may be a potential solution on this, but more investigations are still needed to adapt these algorithms to RTM-GWAS. Furthermore, as indicated by the simulation results, it is also suggested that the sample size in RTM-GWAS should be large enough (such as &gt;400), and the trait heritability should be controlled at a high level (such as &gt;0.8) which can be achieved through experimental design and careful operation of the experiments.     Potential utilization of the RTM\u2011GWAS procedure\r\n   Germplasm population characterization\r\n  As indicated above, the detected full-size QTL-allele matrix can be used directly to characterize the population, to study the diefrentiation of the population, and to reveal the evolutionary relationship among populations based on allele frequencies. Moreover, in plants, the collected germplasm is a genetic reservoir from which all varieties were developed. If the genetic constitution in terms of gene/QTL-allele composition of all breeding target traits in the germplasm reservoir has been explored, the genetic constitution of its derived breeding populations can be inferred from their genomewide markers using the established relationship between QTL-alleles and molecular markers in the reservoir. Since this kind of QTL/allele-marker relationship was obtained from the reservoir, the genetic information can be used for its derived population. For example, the present QTL-allele matrix obtained from the CSGP may be used to infer the QTL-allele structure of its derived populations, especially in the tested ecoregion (lower valleys of Changjiang and Huai-river and their neighboring regions in China, but not necessarily in regions far away from this region due to the environmental diferences of ecoregions which might cause diferent phenotypes and mapping results). Since the CSGP is composed of a large size (1024) of representative germplasm accessions, including the three sources of WA, LR, and RC from all regions in China, the QTL-allele structure of all materials derived from the population may be inferred accordingly, and the breeding population can be organized diferently with its corresponding QTL-allele matrix meeting the requirements of diferent breeding programs. The geneticists and breeders can use the QTL-allele structure of the secondary or re-organized breeding populations to do further genetic study and breeding operations. Thus, genotyping the germplasm resources thoroughly in a state or a country is an eficacious forever program and should be arranged as a state- or country-wide public program.    Genetic system and gene finding for quantitative traits\r\n  A series of parallel studies on the utilization of RTM-GWAS have been also carried out in our group  (Zhang et al. 2015a; Meng et al. 2016) , a recent work was on drought tolerance of soybean. Using the RTM-GWAS, a total of 268 QTL were detected for four drought-tolerance traits in a soybean germplasm population. From which, 684 genes were annotated, and the expression patterns of 320 genes in response to drought were verified with qRT-PCR. The annotated genes are involved in the biological processes of ABA responser, stress responser, transport, development factor, protein metabolism factor, transcription factor, protein kinase, and unknown others or involved in a gene network (Wang et al. 2017, personal communication). The verified expression pattern and annotation demonstrated the relevance of the detected QTL system of the traits, and, therefore, further demonstrated the usefulness of RTM-GWAS procedure in detection of the QTL system in plant germplasm population. Another example of potential utilization of the RTMGWS procedure was on mapping QTL conferring flowering date of soybean in a nested association mapping (NAM) population composed of four recombinant inbred line (RIL) populations with a joint parent, in which 139 QTL with 496 alleles were detected from RTM-GWAS, while only 7-16 QTL with 14-72 alleles were identified by other mapping procedures, including composite interval mapping (CIM), mixed model-based composite interval mapping (MCIM), joint inclusive composite interval mapping (JICIM), and mixed linear model GWAS (MLM-GWAS)  (Li et al. 2017) . The reason for a high power of RTM-GWAS in RIL and NAM populations is that these bi-parental populations fit well in the hypothetical conditions for using LD to detect the tightly associated QTL since there is no population structure problem in RIL populations.    Genomic selection based on QTL\u2011allele matrix in plant breeding\r\n  Following the \u201cBreeding by Design\u201d concept by Peleman and van der Voort (2003), we proposed the genomic selection based on QTL-allele matrix (QTL-allele-based GS). This strategy is diferent from the classic GS based on GEBV (GEBV-based GS) by Meuwissen et al. (2001). The latter was started mainly for animal breeding but has been considered to fit the requirement on GS for crosses and progenies in plant breeding, while the former was concentrated on cross and genotype design and selection. In fact, the QTL-allele-based GS is a direct genotype selection for trait alleles/genes, and the GEBV-based GS is a selection for a linear combination of SNPs for comprehensive traits or indirect selection for alleles/genes. Both strategies are characterized with their advantages and short comings, and are to be further improved and evaluated in breeding practices.  Theoretically, the QTL-allele-based GS is characterized with: direct alleles/genes selection but limited for additive efect selection; selection accuracy depends on QTL mapping accuracy; multiple usefulness of the obtained QTLallele matrix of the germplasm/breeding population; and easiness of optimal cross prediction with its recombination potential and reduced genotyping cost for progeny selection due to reduced marker number. While the GEBVbased GS is characterized with: indirect alleles/genes selection using GEBV with additive and epistasis efects mixed in GEBV; selection accuracy depends on the performance of prediction models; the usefulness of GEBV is limited to the breeding program; high genotyping cost for keeping all GEBV-related markers in progeny selection. According to the \u201cBreeding by Design\u201d concept, we tend to use a direct selection for alleles/genes if the QTL/gene mapping can be accurate and complete enough. It is especially due to the large number of materials (more than 100 crosses and 5000-10,000 progenies each year) to be tested and genotyped in plant breeding programs. We compared our detected QTL with those in the literature (SoyBase, http://soybase.org); our results can cover basically those in the literature, even with some more new ones, which convinces us of the accuracy of our QTL-allele matrix and GS prediction. In addition, in a QTL pyramiding study using marker-assisted selection for seed protein content, a transgressive progeny with 54.15% was obtained from a cross between two lines obtained from two respective RIL populations with protein content 35.35-44.83% in their four parents  (Zhang et al. 2015b) . This example also convinces us to use \u201cBreeding by Design\u201d strategy as our GS strategy.  However, the dificulty in genotyping a large amount of progenies is still a problem to be resolved, even the needed markers have been limited to the involved QTL in QTLallele-based GS. Furthermore, in a real breeding program, multiple traits might be considered simultaneously where multiple QTL-allele matrices are involved. In this case, the linear combination of the matrices should be considered in the two GS stages. In addition, GS for diferent crossing patterns (two-way, three-way, even multiple-way crosses) and GS for traits with pleiotropy and epistasis should also be considered in the future.  Author contribution statement JG conceived and designed the study. JH performed the simulations and developed the computer software. JH and SM analyzed and interpreted the results. SM, TZ, GX, SY, and YL performed the ifeld experiments. RG, JL, YW, QX, and BY performed the genome sequencing. JH, SM, and JG drafted the manuscript. Acknowledgements This work was supported by the China National Key R &amp; D Program for Crop Breeding (2016YFD0100304), the China National Key Basic Research Program (2011CB1093), the China National Hightech R&amp;D Program (2012AA101106), the Natural Science Foundation of China (31571695), the MOE 111 Project (B08025), Program for Changjiang Scholars and Innovative Research Team in University (PCSIRT13073), the MOA Public Profit Program (201203026-4), the MOA CARS-04 program, the Jiangsu Higher Education PAPD Program, and the Jiangsu JCIC-MCP Program. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.  Compliance with ethical standards Conflict of interest  The authors declare no conflict of interest.      ",
    "sourceCodeLink": "https://github.com/njau-sri/rtm-gwas",
    "publicationDate": "0",
    "authors": [
      "Jianbo He",
      "Shan Meng",
      "Tuanjie Zhao",
      "Guangnan Xing",
      "Shouping Yang",
      "Yan Li",
      "Rongzhan Guan",
      "Jiangjie Lu",
      "Yufeng Wang",
      "Qiuju Xia",
      "Bing Yang",
      "Junyi Gai"
    ],
    "status": "Success",
    "toolName": "rtm-gwas",
    "homepage": ""
  },
  "99.pdf": {
    "forks": 0,
    "URLs": [
      "www.mpi-magdeburg.mpg.de/1094756/rrqr",
      "github.com/gabora/visid",
      "sites.google.com/site/amigo2toolbox/download",
      "github.com/gabora/visid/tree/master/case_"
    ],
    "contactInfo": ["julio@iim.csic.es"],
    "subscribers": 1,
    "programmingLanguage": "Matlab",
    "shortDescription": "Visualization of Identifiability Problems in Dynamic Biochemical Models ",
    "publicationTitle": "Parameter identifiability analysis and visualization in large-scale kinetic models of biosystems",
    "title": "Parameter identifiability analysis and visualization in large-scale kinetic models of biosystems",
    "publicationDOI": "10.1186/s12918-017-0428-y",
    "codeSize": 12950,
    "publicationAbstract": "Background: Kinetic models of biochemical systems usually consist of ordinary differential equations that have many unknown parameters. Some of these parameters are often practically unidentifiable, that is, their values cannot be uniquely determined from the available data. Possible causes are lack of influence on the measured outputs, interdependence among parameters, and poor data quality. Uncorrelated parameters can be seen as the key tuning knobs of a predictive model. Therefore, before attempting to perform parameter estimation (model calibration) it is important to characterize the subset(s) of identifiable parameters and their interplay. Once this is achieved, it is still necessary to perform parameter estimation, which poses additional challenges. Methods: We present a methodology that (i) detects high-order relationships among parameters, and (ii) visualizes the results to facilitate further analysis. We use a collinearity index to quantify the correlation between parameters in a group in a computationally efficient way. Then we apply integer optimization to find the largest groups of uncorrelated parameters. We also use the collinearity index to identify small groups of highly correlated parameters. The results files can be visualized using Cytoscape, showing the identifiable and non-identifiable groups of parameters together with the model structure in the same graph. Results: Our contributions alleviate the difficulties that appear at different stages of the identifiability analysis and parameter estimation process. We show how to combine global optimization and regularization techniques for calibrating medium and large scale biological models with moderate computation times. Then we evaluate the practical identifiability of the estimated parameters using the proposed methodology. The identifiability analysis techniques are implemented as a MATLAB toolbox called VisId, which is freely available as open source from GitHub (https://github.com/gabora/visid).Conclusions: Our approach is geared towards scalability. It enables the practical identifiability analysis of dynamic models of large size, and accelerates their calibration. The visualization tool allows modellers to detect parts that are problematic and need refinement or reformulation, and provides experimentalists with information that can be helpful in the design of new experiments.",
    "dateUpdated": "2017-05-08T07:52:20Z",
    "institutions": [],
    "license": "https://github.com/gabora/visid/blob/master/LICENSE.txt",
    "dateCreated": "2016-11-14T15:37:03Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Gábor et al. BMC Systems Biology     10.1186/s12918-017-0428-y   Parameter identifiability analysis and visualization in large-scale kinetic models of biosystems     Attila Gábor    Alejandro F. Villaverde    Julio R. Banga  julio@iim.csic.es     2017   11  2  17    25  4  2017    4  1  2017     Background: Kinetic models of biochemical systems usually consist of ordinary differential equations that have many unknown parameters. Some of these parameters are often practically unidentifiable, that is, their values cannot be uniquely determined from the available data. Possible causes are lack of influence on the measured outputs, interdependence among parameters, and poor data quality. Uncorrelated parameters can be seen as the key tuning knobs of a predictive model. Therefore, before attempting to perform parameter estimation (model calibration) it is important to characterize the subset(s) of identifiable parameters and their interplay. Once this is achieved, it is still necessary to perform parameter estimation, which poses additional challenges. Methods: We present a methodology that (i) detects high-order relationships among parameters, and (ii) visualizes the results to facilitate further analysis. We use a collinearity index to quantify the correlation between parameters in a group in a computationally efficient way. Then we apply integer optimization to find the largest groups of uncorrelated parameters. We also use the collinearity index to identify small groups of highly correlated parameters. The results files can be visualized using Cytoscape, showing the identifiable and non-identifiable groups of parameters together with the model structure in the same graph. Results: Our contributions alleviate the difficulties that appear at different stages of the identifiability analysis and parameter estimation process. We show how to combine global optimization and regularization techniques for calibrating medium and large scale biological models with moderate computation times. Then we evaluate the practical identifiability of the estimated parameters using the proposed methodology. The identifiability analysis techniques are implemented as a MATLAB toolbox called VisId, which is freely available as open source from GitHub (https://github.com/gabora/visid).Conclusions: Our approach is geared towards scalability. It enables the practical identifiability analysis of dynamic models of large size, and accelerates their calibration. The visualization tool allows modellers to detect parts that are problematic and need refinement or reformulation, and provides experimentalists with information that can be helpful in the design of new experiments.    Parameter estimation  Dynamic models  Identifiability  Global optimization  Regularization  Overfitting       Background\r\n  The development of mechanistic (kinetic) models in order to quantitatively describe the dynamics of biological phenomena is one of the core research themes in systems biology. During the last decade, fostered by the greater availability of the necessary experimental data, the development of large (up to genome-scale) kinetic models has become one of the main objectives in the field, as well as in related areas such as synthetic biology, metabolic engineering, or industrial biotechnology [ 1-10 ]. More recently, the first steps towards comprehensive whole-cell models have been taken [ 11 ], which has great potential for applications e.g. in personalized medicine [ 12 ]. However, the development of these largescale integrated dynamic models poses severe challenges [ 13, 14 ]. Those associated with model building are common to the more general problem of reverse engineering of biological systems [ 15 ]. In this context, parameter estimation (i.e. model calibration) is arguably one of the most studied [ 16-19 ], yet more challenging step in model building.  Parameter estimation in nonlinear dynamic models can be an extremely hard problem mostly due to the following issues [ 15 ]: lack of identifiability, ill-conditioning, multimodality and over-fitting. The latter three can be handled via global optimization and regularization methods, as reviewed and illustrated recently [ 20 ]. The present paper begins by continuing the line of work in [ 20 ], addressing these three issues. To this end we introduce a combination of a global optimization metaheuristic, eSS [ 21 ], and an efficient local search method, the adaptive algorithm NL2SOL [ 22 ]. By using this optimization technique jointly with regularization it is possible to reduce the calibration times of large dynamic models and simultaneously avoid over-fitting. We show this for models from the recently presented BioPreDyn benchmark collection [ 23 ]. Then we focus on the remaining issue, that is, identifiability analysis of large dynamic models. Our aim is to develop a methodology which (i) is able to characterize high-order relationships among parameters, and (ii) scales up well with model size. Thus, our objective goes beyond finding the subset of identifiable parameters: we also aim to systematically characterize the space of non-identifiable parameters, and to facilitate the advanced analysis of the results with scalable visualization tools.  Identifiability analysis aims at establishing whether it is possible to determine the values of the unknown model parameters [ 24 ]. It is common to distinguish between structural and practical identifiability. Structural or a priori identifiability analysis decides whether the model parameters are uniquely determinable based on the model formulation, which includes the dynamic equations, observation functions and stimuli [ 25 ]. A parameter θ of the model is structurally identifiable if y(θ ) = y(θ ) ⇔ θ = θ , where y denotes the model predictions, which are observable in the experiments. A parameter θ is structurally locally identifiable if for almost any value θ ∗ there is a neighbourhood V (θ ∗) in which the above relationship holds. It is globally identifiable if the relationship holds in all the range of values of the parameter. If there is some region with non-zero measure where the relationship does not hold, θ is structurally unidentifiable. Structural identifiability analyses usually involve a high computational burden, which makes them difficult to apply to large models [ 26-28 ]. Furthermore, structural identifiability is only a necessary but not sufficient condition for identifiability. Very often a structurally identifiable parameter is practically unidentifiable, that is, its value cannot be determined with precision due to limitations in the available data. This can be quantified using practical or a posteriori identifiability analysis, which provides confidence intervals of the parameter values. The two main sources of practical non-identifiability are (1) lack of influence of a parameter on the observables, and (2) interdependence among the parameters. Obviously, if a parameter does not influence the observables (case 1) it is not possible to determine its value. The second situation, in which the effect on the observables of a change in one parameter can be compensated by changes in other parameters, can also prevent parameter identification. Both problems are related to the sensitivities of the observables to changes in model parameters. While (1) is related to the average sensitivity of the model outputs to a specific parameter, (2) can be investigated based on the collinearity of the parametric sensitivities [ 29 ].  In this paper we combine global optimization and regularization techniques to calibrate medium and large scale biological models (in this context, we will use the term \u201cmedium-scale\u201d for models with 10 to 50 parameters and \u201clarge-scale\u201d for models with more than 50 parameters). Then we evaluate the practical identifiability of estimated model parameters using sensitivity analysis and collinearity measures. We determine the largest identifiable subsets of parameters, characterize the interplay among non-identifiable groups of parameters, and visualize the results using Cytoscape. The visualization tool shows the identifiable and non-identifiable groups of parameters together with the model structure in the same graph. In this way, modellers can detect parts that are problematic and need refinement or reformulation, and experimentalists obtain information that can be helpful in the design of new experiments. The methods for identifiability analysis and visualization presented here have been implemented as a MATLAB toolbox called VisId, which is available from GitHub (https://github.com/gabora/visid) and as Additional file 1.    Methods\r\n   Parameter estimation with regularization and global optimization\r\n  Mathematical model We consider deterministic models of biological systems that can be described by nonlinear ordinary differential equations (ODEs) in the following form:  Here x ∈ RNx denotes the state vector (often concentrations), f describes the interactions among the state variables (often constructed from the reaction rate functions), and u(t) denotes the input variables (stimuli). The parameter vector θ ∈ RNθ contains the (positive) parameters, e.g. reaction rate coefficients or Hill exponents. Their values are often unknown and must be estimated from data.  The model variables x are mapped to the measurable output variables y ∈ RNy , also known as observables or model predictions, by the observation function g. These y signals are the quantities that can be experimentally measured. We will denote by yijk the model prediction for the j-th observed quantity in the k-th experiment at time ti ∈[ t0, tf ]. The corresponding measured data is denoted by y˜ijk.   Parameter estimation\r\n  The goal of parameter estimation is to determine the values of the unknown parameter vector θ . This is usually done by minimizing a distance between model prediction yijk and measured data y˜ijk. One of the simplest, but yet general, choices of this distance is the weighted sum-of-squares  Ne Ny,k Nt,k,j k=1 j=1 i=1 QLS(θ ) =  wijk yijk (x(ti, θ ), θ ) − y˜ijk 2 , where Ne is the number of experiments, Ny,k is the number of observed compounds in the k-th experiment, and Nt,k,j is the number of measurement time points of the j-th observed quantity in the k-th experiment, and the weights are denoted by wijk. The total number of data in all experiments is denoted by ND = kN=e 1 jN=y1,k iN=t,1k,j 1. In order to simplify the index triplet, from now on we will use only one index, i.e. the weights and observables are denoted by wi and yi for i = 1, 2 . . . ND. (1) (2) (3) (4)  Then the parameter estimation problem is formulated as an optimization problem in the following form: minimize QLS(θ ) + α (θ )  θ subject to θmin ≤ θ ≤ θmax, dx(t, θ )  dt y(x, θ ) = g(x(t, θ ), θ ),  = f (u(t), x(t, θ ), θ ), x(t0) = x0(θ ), t ∈ t0, tf . (5) (6) (7) (8) (9)  Here (θ ) is a a regularization term, which is described in the following subsection, and θmin and θmax are lower and upper bounds of the parameter values. The parameter vector θˆ that solves this minimization problem is called the optimal parameter vector or the parameter estimates.    Regularization\r\n  Large scale dynamic models are often over-parametrized, turning the estimation of their parameters into an illposed problem [ 30 ]. This means that the minimum of the least-squares cost function (4) is non-unique, or that even a very small perturbation of the data results in very different estimated parameters. Furthermore, due to the large number of degrees of freedom, these models tend to capture the artificial dynamics of measurement noise. This is known as overfitting [ 31, 32 ] and it usually results in poor predictive capability of the calibrated model.  Regularization techniques incorporate a priori knowledge about the parameter values to make the problem well-posed. The regularization parameter α in (5) balances the strength of this knowledge; its value can be found by regularization tuning methods [ 33 ]. Here we followed the guidelines presented in [ 20 ] and chose a small regularization parameter (α = 0.1), since we assume that we do not have good a priori estimates of the parameters.  Regarding the regularization function, (θ ), we chose the Tikhonov regularization framework to match the form of the penalty to the least squares formalism of the objective function. In this case the penalty is a quadratic penalty function, (θ ) = θ − θ ref T W T W θ − θ ref , (10) where W ∈ RNθ ×Nθ is a diagonal scaling matrix and θ ref ∈ RNθ is a reference parameter vector, which is problem dependent and determined by the available information about the model parameters.    Global optimization\r\n  We solve the minimization problem defined by (5)-(10) using optimization. Since the cost function (5) is usually multi-modal (i.e. it usually has several local minima) [ 34-37 ], it is necessary to use an efficient global optimization method. Deterministic global optimization methods [ 38-42 ] can guarantee global optimality of the solution. However, their computational cost increases exponentially with the number of parameters, which makes them unsatisfactory for large scale models. Stochastic and metaheuristic methods [ 17, 18, 35, 36, 43, 44 ], on the other hand, do not provide such guarantees, but are often capable of finding adequate solutions in reasonable computation times.  For this reason we use a method called enhanced scatter search (eSS) [ 21 ], which is an advanced implementation of a population-based algorithm called scatter search. The scatter search metaheuristic works by evolving a number of solutions (population members), which constitute the reference set (RefSet). Members of this set are selected due to their quality and diversity. They are updated at every iteration by combining them with other RefSet members and, occasionally, by applying an improvement method. This improvement consists of a local search to speed-up the convergence to optimal solutions. In the present work we have chosen NL2SOL [ 22 ] as a local method. NL2SOL is a quasi-Newton algorithm with trust region strategy that exploits the structure of the nonlinear least squares problem. Note that the combination of a global method (scatter search) with a local one makes eSS a hybrid algorithm.     Practical identifiability analysis\r\n  The shape of the cost function (5) in the surroundings of its optima determines the local identifiability of the parameters. We assess parametric identifiability in two consecutive steps: 1. First we calculate the sensitivity of the model outputs (observables) with respect to changes in the parameters. Those parameters which have no effect (or very little) on the observed signals are classified as non-identifiable. Note that this label is assigned on an individual basis, that is, taking only into account the effect of each parameter individually. 2. Even if a parameter influences the model output, it may still be unidentifiable if its effect can be compensated by changes in other(s) parameter(s). Hence in the second step we consider the interplay among parameters, aiming at finding groups of parameters which are non-identifiable due to their collinearity.  Note that, while it would be possible at least in principle to perform both steps simultaneously, in practice the curse of dimensionality hampers the application of such a global sensitivity approach to large models [ 45, 46 ].   Sensitivity analysis\r\n  The analysis of parametric sensitivity of kinetic models has a long tradition in model analysis [ 47, 48 ]. For the dynamical system (1)-(2), the parametric sensitivities of the observables can be accurately calculated by solving the forward sensitivity equations: dXi(t) dt = ∂f (x, u, θ ) ∂x  Xi(t) + ∂f (x, u, θ ) ∂θ for i = 1, . . . , Nθ si(t) = si(t0) = ∂g(x, θ ) ∂x  Xi(t) + ∂g(x, θ )  ∂θ 0 if θi is a model parameter 1 if θi is an initial condition for i = 1, . . . , Nθ (12) for i = 1, . . . , Nθ .  Here Xi = ∂∂θxi denotes the sensitivity of the state vector ∂y with respect to the i-th parameter and the vector si = ∂θi is the sensitivity of the observables with respect to this parameter. This calculation requires the solution of the Nx × Nθ ordinary differential Eq. (11) with initial conditions (13) for each experiment. The numerical solution is determined for the time points for which there are experimental data available, and then the algebraic Eq. (12) are evaluated. If the partial derivatives of the dynamic equations are not available, an alternative is to calculate the sensitivities using finite differences or automatic differentiation.  The sensitivities of the observables are scaled using the same weights as in Eq. (4), resulting in scaled sensitivities for an output j and a parameter i: s˜i j =  ∂yj wj ∂θi . s˜imsqr = 1  ND  ND j=1  For each parameter we calculate an overall scoring called root mean squared sensitivity, s˜imsqr, to take into account changes in time or across experiments [ 29, 49 ]: s˜i2j for i = 1, . . . , Nθ .  Below a certain threshold the parameters are considered non-influential to the outputs. We set the threshold to four orders of magnitude smaller than the maximum root mean square value (15). Parameters whose sensitivity falls below this cut-off value are considered practically non-identifiable and they are kept out of further analysis. The procedure is summarized in Algorithm 1.  We remark that the outcome of the sensitivity calculations depends not only on the parameters, but also on the choice of initial conditions and external stimuli, which can have a strong influence in the practical identifiability of a (11) (13) (14) (15) Algorithm 1 Finding sensitive model parameters Require: Obtain vector of calibrated parameters → θˆ = [ θˆ1, . . . , θˆNθ ] (solve Eqs. (5)-(10)) 1: Parameter index set I ← {1, 2, . . . , Nθ } 2: Compute the sensitivity matrix at the optimal parameter vector → s(θˆ) (solve Eqs. (11)-(13)) 3: Compute the weighted sensitivities → s˜ (solve  Eq. (14)) 4: Find the sensitive parameters by ranking the meansquare values of the sensitivity columns as in Eq. (15) and setting a cut-off value. The corresponding index set → Isensitive ⊂ I model. If insufficiently excitatory stimuli or initial conditions result in poor practical identifiability, a solution if it is possible to carry out additional measurements is to design and perform a new experiment to generate maximally informative data [17]. 17 ].    Collinearity of parameters\r\n  Interplay among influential parameters can result in an unidentifiable model, because a variation in the cost function value due to a change in a parameter can be compensated by changes in other parameters. Pairwise interplay can be detected by plotting contours of the cost function versus pairs of parameters. Largely eccentric contours or \u201cvalleys\u201d show that the cost function is almost unchanged in one direction, and the two parameters are highly correlated. This approach has two drawbacks: it involves a large computational effort and is limited to interplay between pairs of parameters. To compute higher dimensional interactions we use a different measure: the collinearity of parametric sensitivities.  To calculate collinearity we first normalize the scaled sensitivities (14) as follows:  s s¯i = sˆi for i = 1, . . . , Nθ . (16) ˆi  This normalization avoids biases caused by differences in the absolute values of the individual sensitivity vectors.  Let us consider a set K of k parameters and their corresponding sensitivity vectors. The parameters are linearly dependent if there exist k constants αi = 0 such that α1s¯K1 + α2s¯K2 + . . . αk s¯Kk = 0  If the above relation does not hold, the set is independent. When the equality (17) holds only approximately, the parameters are nearly dependent or nearly collinear. The degree of collinearity among a set of parameters can be measured by the collinearity index, CIK , which is defined as [ 29 ]: .  (17) (18) where S¯k is the sensitivity matrix built from the k sensitivity vectors, S¯K =[ s¯K1 , s¯K2 . . . s¯Kk ], and λK,min is the smallest eigenvalue of S¯ T S¯K . The larger the collinearity  K index is, the more dependent the corresponding parameters are. Brun and co-authors [ 29 ] proposed to classify a subset of parameters as identifiable if their collinearity index is smaller than a threshold which they chose as CIK &lt; 20. Roughly speaking, a value of 20 means that 95% of the variation in the model output caused by changing one of the parameters in the subset can be compensated by changing the other parameters in the set.  Other approaches for finding parameter correlations using sensitivity-based measures have been previously proposed in the literature. Li and Vu presented two methods [ 50, 51 ] that search for relationships among parameters in the context of a priori identifiability analysis (i.e. with noise-free, continuous data). The method in [ 50 ] provides a necessary but not sufficient condition for identifiability of nonlinear systems, which need to be fully observed (i.e. they must satisfy y = x). The method in [ 51 ] removes the requirement of measuring all the system states, replacing it with the restriction that the model must be linear. We remark that the method proposed in the present manuscript does not have these limitations: it can be applied to partially observed, nonlinear systems with noisy, discrete-time measurements.    Largest identifiable subset\r\n  As explained in the previous subsections, a subset of parameters is considered identifiable if its elements are influential and their sensitivity vectors are not collinear. We are interested in finding the largest set of parameters for which the collinearity of the corresponding sensitivity vectors is below the chosen threshold, CIK &lt; 20. Such a set of parameters represents all the degrees of freedom in the model. This means that perturbing a parameter not included in this set has an effect in the model predictions that can be compensated (at least by 95%) by changing other parameters in the set. However, a perturbation in a parameter belonging to the set cannot be compensated by changes in the remaining parameters.  Several methods have been developed for finding the group of identifiable parameters [ 30, 52, 53 ]. Iterative selection methods apply a step-wise procedure to select one parameter at a time, until no more parameters can be added to the identifiable set. In each step the parameter to be included is selected based on an optimality criteria. For example, the modified Gram-Schmidt orthogonalization method [ 54 ] projects all the remaining sensitivity vectors to the subspace spanned by the already selected sensitivity vectors, and includes the parameter corresponding to the one with the largest projection value. This step is repeated until the largest projection value falls below a threshold, which means that the next parameter would significantly interplay with the parameters already included. The computational cost of this method scales up well with the number of sensitivity vectors. However, the drawback of iterative procedures such as this one is that the solution might not be the global optimum, that is, it might fail to find the largest identifiable subset.  Alternatively, we propose to solve the problem of finding the largest identifiable subset of all the estimated parameters using combinatorial optimization. To this end we formulate it as a (nonlinear) integer optimization problem, where the goal is to maximize the number of sensitivity vectors included in the set, with the constraint that the corresponding collinearity index is below a threshold CI∗.  This algorithm can be stated as maximize i∈{0,1}Nθ  Nθ k=1  ik subject to Si = cat({sk |ik = 1, for k = 1, . . . , Nθ }) (20) CI(Si) &lt; CI∗ ik is a binary variable for k = 1, . . . , Nθ (19) (21) (22) where the binary variable ik indicates if the k-th parameter is included (ik = 1) or not included (ik = 0) in the identifiable group of parameters. The sensitivity matrix corresponding to the selected parameters is Si, and 'cat' stands for the concatenation of the column vectors in the constraint (20). The collinearity index of this matrix is CI(Si) and it is determined by computing the minimum eigenvalue as in (17).  This combinatorial optimization problem has an exponentially scaling computational cost, and thus its solution requires an efficient algorithm. We chose the Variable Neighbourhood Search (VNS) technique [ 55 ], which is a heuristic global optimization method for integer optimization problems. We used the version of VNS included in the MEIGO Toolbox [ 56 ], which is implemented in MATLAB.  We modified this initial formulation of the problem described in Eqs. (19)-(21) after finding that its solution is often not unique: even after maximizing the number of parameters in the subset, there may be multiple subsets that yield a collinearity index below the threshold CI∗. Indeed, we found large variability in the solutions if no initial guess was specified. Therefore, we reformulated the optimization problem in two ways, as described in the following paragraphs.  As a first modification, we transformed the collinearity requirement (21) from a 'hard' to a 'soft' constraint (or penalty). The modified optimization problem reads as  Nθ maximize i∈{0,1}Nθ k=1  ik − P1(i) − P2(i) subject to Si = cat ({sk | ik = 1, for k = 1, . . . , Nθ }) (24) 1 P1(i) = 2 CI(Si)/CI∗ P2(i) = 0 α CI(Si) − CI∗ β otherwise  if CI(Si) &lt; CI∗ ik is a binary variable for k = 1, . . . , Nθ  As above, the binary variable ik indicates if the kth parameter is included (1) or not included (0) in the selected group of parameters. The penalty P1 is a monotone increasing (linear) function of the collinearity index CI(Si), such that P1 is 0.5 when the collinearity equals to the threshold. Due to this small value, P1 does not influence the size of the largest subset below the threshold. In this way, when multiple sets of the same size co-exist, the set with smaller collinearity index is always favoured. This results in an unique solution of the optimization problem if there are no sets with identical collinearity index. The second penalty function P2 represents a soft constraint that is active when the collinearity exceeds the threshold. The steepness of this constraint is tuned by the values of α and β, which we set to α = 1 and β = 2.  Our second improvement of the formulation of the optimization problem consists in providing a good initial guess of the solution using QR decomposition. The rank revealing QR decomposition algorithm, or rrqr [ 57 ], rewrites a matrix S as (23) (25) (26) (27) (28)  S = QR, where Q is an orthogonal matrix, R is an upper triangular matrix, and is a permutation matrix. Due to the properties of this decomposition, the permutation matrix defines a reordering of the columns of S. In this re-ordered matrix Sro = S, the most orthogonal columns are located in the left. In other words, the first n columns of the reordered matrix define a linear subspace, and the (n + 1)-th column has the largest projection value on this subspace among the remaining Nθ − n columns located to the right of the n-th column. The outcome of the rrqr technique is similar to that of the aforementioned Gram-Schmidt orthogonalization method, but its implementation is more efficient.  We applied rank revealing QR decomposition to the sensitivity matrix, following the procedure described in Algorithm 2. Then, we used the resulted ordering of the sensitivity vectors to initialize the global optimizer. In this way we improved the performance of the global optimizer, which often found larger sets with collinearity index below the threshold value. The whole procedure for identifying the largest non-collinear subset of parameters is summarized in Algorithm 3.  Algorithm 2 Finding the largest identifiable subset of parameters by rank revealing QR decomposition (rrqr) Require: Find sensitive parameters by Algorithm 1 →  Isensitive Require: Define collinearity threshold: CI∗ 1: Number of sensitive parameters: Nsp = cardinality(Isensitive) 2: for all i ∈ Isensitive do 3: Normalize the sensitivity columns: s¯i ← ||ssˆˆii|| (Eq. (16)) 4: end for 5: Form S¯ ← cat({s¯i | i ∈ Isensitive}), where 'cat' stands for concatenation of a set of column vectors. 6: [ Q, R, p, r] ← rrqr(S¯ ), where vector p contains the permutation vector 7: for i = 2 to Nsp do 8: Sss ← S¯ (:, p(1 : i)) 9: CIss = collinearity(Sss) 10: if CIss &gt; CI∗ then 11: indexLargestIdSetQR = p(1 : i − 1) 12: break 13: end if 14: end for 15: return indexLargestIdSetQR Algorithm 3 Finding the largest identifiable subset of parameters by VNS Require: Find sensitive parameters by Algorithm 1 →  Isensitive Require: Find largest set by Algorithm 2 → indexLargestIdSetQR Require: Define collinearity threshold: CI∗ 1: number of sensitive parameters: Ns = cardinality(Isensitive) 2: xinit = zeros(1, Ns) 3: xinit(indexLargestIdSetQR) = 1 4: solve optimization (23)-(27) using xinit as initial guess The procedure presented in this subsection has similarities with the one proposed by Chu and Hahn [ 54 ]. One difference is that we maximize the subset size for a given collinearity threshold, whereas Chu and Hahn adopted the opposite approach, i.e., maximizing parametric identifiability for a pre-specified subset size. Additionally, both methods differ in the optimization technique: we use Variable Neighbourhood Search, which has better scalability than the genetic algorithm chosen in [ 54 ]. Recently, Nienałtowski et al. [ 58 ] have proposed a method for finding clusters of correlated parameters using so-called canonical correlation analysis (CCA). CCA is an extension of Pearson correlation for measuring multidimensional correlations between groups of parameters. Given two groups of parameters of sizes m and n, with m &lt; n, calculation of the canonical correlations provides m measures, which are summarized in a single measure, called MI-CCA. This similarity measure represents the mutual information between the two groups, although it should be noted that average mutual information is equivalent to canonical correlation only if the random variables follow an elliptically symmetric probability model. Nienałtowski et al. use MI-CCA to cluster parameters until an identifiable subset is reached. This approach is sequential and yields a single parameter subset, which is possibly not maximal. In contrast, the methodology described here combines an initial sequential phase with a subsequent combinatorial optimization procedure. The second phase yields several identifiable parameter subsets and usually improves the initial solution.    Finding all largest subsets\r\n  As mentioned above, the largest non-collinear subset of parameters is not unique. To realize this, imagine that we have a non-collinear set of the parameters, and consider an additional pair of highly collinear parameters. Since we may add either of these two parameters to the set, but not both of them, we have two potential solutions. The optimization algorithm described above would choose the option with a lower collinearity index.  However, we may also be interested in enumerating all the possible sets, instead of only one. Finding all the largest subsets is a combinatorial problem too, which is computationally expensive. A naive approach for solving it could be to generate all possible sets of parameters and compute the corresponding collinearity index. However, note that if two parameters θ1 and θ2 are collinear, then any sets including the pair {θ1, θ2} are highly collinear. Using this fact, we developed an incremental procedure for the systematic determination of the sets. We start by considering all possible pairs of parameters and determining their collinearity. Then we extend only those pairs which have a small collinearity index, by considering all possible combinations of a third parameter. This procedure is repeated until either all the sets are highly collinear, or there is only one set containing all the parameters. In this way, summarized in Algorithm 4, we can find all the largest subsets of non-collinear parameters. Algorithm 4 Finding all the largest identifiable subset of parameters Require: Sensitivity matrix S at the optimal parameters Require: Define collinearity threshold: CI∗ 1: Given the sensitivity matrix S =[ s1, . . . sNθ ] and a subset of column indexes K ⊆ I = {1, . . . Nθ } of S. Then let SK the sub-matrix of S containing the columns specified by indices in K, i.e. SK := cat({si | i ∈ K }) 2: Generate all combinations of pairs of parameter indexes: I2 = {(i, j) | i, j ∈ I, i &lt; j)} 3: Compute the collinearity index for each element of the set: CI2 = {CI(SK ) | K ∈ I2} 4: Find sets with small collinearity: I2∗ = {K | K ∈  I2, CI(SK ) &lt; CI∗} 5: for setSize = 3 to nθ do 6: generate all the extension sets IsetSize = {K ∪ i | K ∈ IsetSize−1, i ∈ I, i ∈/ K }  ∗ 7: Compute the collinearity index for each element:  CIsetSize = {CI(SK ) | K ∈ IsetSize} 8: Find sets with small collinearity: Is∗etSize = {K | K ∈  IsetSize, CI(SK ) &lt; CI∗} 9: if cardinality(Is∗etSize) = 0 then 10: report Is∗etSize−1 and CIsetSize−1 11: break; 12: end if 13: end for    Partitioning the non-identifiable parameters\r\n  The two procedures presented above can be used for finding (i) the largest, least collinear subset of parameters, and (ii) all the largest subsets; in both cases, restricted to those subsets whose collinearity falls below a threshold. However, it is often important to understand why certain parameters are not identifiable. For example, a parameter may be unidentifiable because the model output has very low sensitivity to changes in its value. But it could also be because it is highly correlated with another parameter, even when both parameters have high sensitivities. Finding small groups of highly collinear parameters can be helpful in determining the exact source of unidentifiability.  The collinearity of a subset always increases when a new parameter is added to the set. For example, considering three parameters, the collinearity of the triplet is always higher than the collinearity of any pairs. Therefore, if a larger set of parameters contains a collinear pair, then the collinearity index of the large set is also large.  If we are interested in finding the smallest groups of highly collinear parameters, we can proceed as follows. First we generate all possible pairs of parameters, and compute the collinearity of the corresponding sensitivity vectors. Then we evaluate all possible triplets. The procedure can be extended for the analysis of larger sets. However, due to the combinatorial explosion of the computational cost, this method can be applied only to models of moderate size (with a maximum of roughly 20 parameters).     Visualization of identifiable subsets\r\n  It can be useful to represent the identifiability results graphically, because such visualization can provide modellers with insight about how to reformulate their models and/or design new experiments in order to avoid nonidentifiable parameters.  With this aim, we display the model structure in the natural network visualization technique. An example is shown in Fig. 1c. The model structure is represented as a graph whose nodes are state variables, observables, stimuli, and model parameters. The edges - which can be directed (arrows) or undirected - have the following meaning: an arrow from node A to node B indicates that node B appears in the equation of A. For example, if the dynamic equation of a state x1 is x˙1 = p1 · x2, the corresponding graph would show two arrows x2 → x1 and p1 → x1.  More formally, we determine how the state, input variables, stimuli and parameters are connected and influence each other through symbolic manipulation of the model Eq. (1). For this purpose we compute: (i) the Jacobian ∂fi , (ii) the Jacobian matrix with respect to the states: Jis,sj = ∂xj of the observation functions with respect to the states: ∂gi , (iii) the Jacobian of the systems dynamics with Jis,oj = ∂xj respect to the stimuli Jis,ij = ∂∂ufij , and (iv) the Jacobian with respect to the parameters Ji,j = ∂∂θfij . All these matrices sp are evaluated symbolically, and then the expressions are converted to a logical 1 (if the symbolic expression is non zero) or 0 when the symbolic result is zero.  Additionally, we can connect parameters by undirected edges if their collinearity is larger than the collinearity threshold.    Implementation: the VisId software tool\r\n  We implemented the techniques proposed in subsections \u201cPractical identifiability analysis\u201d and \u201cVisualization of identifiable subsets\u201d as a MATLAB software package called VisId, which is provided as Additional file 1 and can also be downloaded from GitHub (https://github.com/gabora/visid). It is free software, made available under the terms of the GNU General Public License version 3. The VisId toolbox relies on three other MATLAB toolboxes, which are also freely available: AMIGO2 [5 O2  [59] (https://sites.google.com/site/amigo2toolbox/download), which is used to to store, simulate and calibrate the models; MEIGO [56 O  [56] (http://www.iim.csic.es/~gingproc/meigo.html), which implements the Variable Neighbouring Search (VNS) optimization method; and (optionally) RRQR (https://www.mpi-magdeburg.mpg.de/1094756/rrqr), which performs the rank revealing QR decomposition used to initialize the global optimizer. Network visualization is performed with Cytoscape [60]  [6 0] (http://www.cytoscape.org/). Further details can be found in Section 4 of Additional file 2.     Results\r\n  In this section we demonstrate the application of the methodology presented in the previous section using several dynamic systems biology models of different type and complexity. Their main characteristics are given in Table 1. First we present detailed results of identifiability analysis and visualization for a model of the TGF-β signalling pathway. We also provide similar results for the genetic network that controls the circadian clock in Arabidopsis thaliana. Due to their complexity and yet relatively moderate size, these models are well suited as case studies for illustrating the identifiability methodology in depth.  Then we study two large scale benchmark problems included in the BioPreDyn-bench collection [ 23 ]. Since the analysis of these latter models is more challenging due to their larger size, we start by demonstrating the performance improvements that can be achieved during parameter estimation using the model calibration procedure proposed in Section \u201cParameter estimation with regularization and global optimization\u201d. Then we perform identifiability analysis and report the corresponding B4 Metabolic model, Chinese Hamster Ovary results, including the graphical representation of the identifiable subset using the natural network visualization.   TGF-β signalling pathway\r\n  The dynamic model of the TGF-β signaling pathway was presented in [ 61 ] as a tutorial example for model calibration. It has 18 dynamic states and 21 kinetic parameters (k1-k21), of which 18 need to be estimated. Following [ 61 ], we assumed that all the concentrations, except the Smad RNAs (CI_Smad_mRNA1 and CI_Smad_mRNA2), can be measured in the experiments. The algebraic Equations of the reaction kinetics and the dynamic equations are provided in the Additional file 2.  For the purpose of testing the methodology we generated a training dataset by simulating the model equations using the nominal values of the parameters k1-k21 (numerical values are listed in Additional file 2: Table S1). Then we sampled the simulated trajectories at equidistant time points, and added normally distributed random numbers to the data to mimic measurement errors. Finally, we estimated the model parameters from the generated data set. This approach is widely used for testing calibration methods and assessing the extent to which they recover the nominal parameters. It should be noted that, as the amount of noise in the dataset increases, the information/signal ratio decreases, making the estimation problem more ill-conditioned. This makes it more difficult to recover the correct value of the parameters, but has a small effect in computation times. The numerical values of the estimated parameters are reported in Additional file 2: Table S2.  We started the identifiability analysis by computing the sensitivities of the observations with respect to the estimated model parameters, according to Algorithm 1. We found that all the parameters have a non-negligible influence on the model outputs, thus there are no individually non-identifiable parameters (see Fig. 1a).  Next, following Algorithm 2, we applied QR decomposition and ranked the parameters according to their orthogonality. We then solved the optimization problem (23)-(27) by initializing the variable neighboring search method with the results of the QR decomposition (Algorithm 3). Setting the threshold level for the collinearity index to CI = 20 yielded 14 identifiable parameters, which are shown as green nodes in the network in Fig. 1c. Parameters not present in the identifiable subset are shown as red nodes. Parameters are connected by arrows to state variables (represented by yellow nodes) if they appear in the equation of the corresponding dynamic equation. States which directly influence each other are also connected by directed edges in the same manner. Blue squares represent measurements; a state is connected to a blue square if it appears in the corresponding observation function.  To see how the size of the identifiable subset is influenced by the choice of the collinearity index threshold (CI), we solved the optimization problem for a range of threshold values. The results are depicted in Fig. 1b. As the collinearity index threshold decreases, less parameters are considered identifiable. We can see that the identifiability results are quite robust to the choice of threshold level: the number of identifiable parameters is always between 12 and 15, and it is constant (= 14) for a very wide range of CI, 15 ≤ CI ≤ 25.  The results presented so far tell us that the 14 parameters are not correlated. However, they do not inform of the relationships among identifiable and non-identifiable parameters. To investigate this point, we computed the smallest correlated subsets as described in Section \u201cPartitioning the non-identifiable parameters\u201d, up to groups of 6 parameters. Figure 1d shows such groups; parameters are depicted as blue circles connected with group identifying nodes (white squares). These nodes are labeled as GX(Y), where X indicates the number of parameters in the group and Y is the group index for a given number of parameters (e.g. G3(2) stands for the second group of three correlated parameters). We found that the large pairwise collinearity between k14 − k18 and k17 − k19 explains the non-identifiability of the model parameters only partially. There are 4 groups of triplets and a group of 5 parameters which are highly correlated. The members of the groups and the corresponding collinearity index are reported in Table 2.  It is important to note that collinearity might arise among multiple parameters, even if they are pairwise independent. For example, despite the fact that none of Set ID. the pairs in the group of k14, k16 and k17 has a high pairwise collinearity, the collinearity index of the triplet is extremely large.  Algorithm 4 found 40 different sets of identifiable parameters with collinearity index ranging between 12.4 and 16, less than the threshold (CI = 20). The sets are reported with the corresponding collinearity index in Additional file 2: Table S3. We can see that parameters {k1 − k7, k13, k15} are members of all the groups, and they do not participate in any of the small correlated groups in Fig. 1d. From each correlated group of size K, only K − 1 A parameters can participate in the largest set of identifiable parameters.  The aforementioned identifiability procedures can be carried out in a few seconds. Detailed computational costs are shown in Table S6 of the Additional file 2 for all the case studies considered in this paper.   Circadian clock in Arabidopsis thaliana\r\n  Locke and co-authors [ 62 ] described the genetic network controlling the circadian clock in Arabidopsis thaliana; the dynamic equations of this model are provided in the Additional file 2.  We generated training data by simulating the model equations with the nominal parameters (Additional file 2: Table S4) in two experimental conditions. In the first one, the model input was kept constant (θlight = 1), representing continuous light stimulation of the plant. In the second experiment the input was changed pulse-wise in 12-hour cycles, repeated 5 times. As in the previous example, the trajectories were sampled at equidistant time-points and disturbed by pseudo-random noise. Only two states, CTm and CLm, were observed. The estimated model parameters are collected in Additional file 2: Table S4.  Although the model outputs showed sensitivity to all the parameters (Fig. 2a), i.e. there were no zero sensitivity vectors, we found that most of the model parameters are non-identifiable due to heavy collinearities. The largest identifiable subset contains only 6 of the 27 parameters, depicted in Fig. 2d by green nodes. The enumeration of the largest sets of identifiable parameters by Algorithm 4 identified 1331 parameter sets.     Benchmarks B2 and B4 from the BioPreDyn-bench collection\r\n  In this subsection we analyze two large scale benchmark problems taken from the BioPreDyn-bench collection [ 23 ]: the metabolic models of Escherichia coli (B2) and Chinese Hamster Ovary cells (B4). They are highly non-linear, partially observed systems with more than 100 unknown parameters, which pose serious challenges for parameter identification. In B4 the calibration data was generated by model simulation and disturbed by random noise, while in B2 it was experimentally measured. Further details about the models and the parameter estimation challenge can be found in [ 23 ].  First we use these benchmarks to illustrate the benefits of the parameter estimation strategy proposed in subsection \u201cParameter estimation with regularization and global optimization\u201d, comparing it with the one used in [ 23 ].  Both approaches use a hybrid method, eSS [ 21 ], which combines a global optimization algorithm (scatter search) with a local search. In [ 23 ] the local method of choice was FMINCON; here we compare that configuration with NL2SOL (with and without regularization). Global optimization algorithms use pseudo-random numbers. Hence Fig. 4 Representation of the connections in the B4 model using the network diagram formalism. Nodes indicate states (yellow), identifiable (green) and not identifiable (red) parameters, observables (blue), and inputs (grey) their performance changes at every run, and the calibration problem should be solved several times to obtain more robust results. Since each optimization takes several hours we limited the number of runs to five for each problem. We used the approximate computation time (CPU time) reported in [ 23 ] as the stopping criterion for the model calibration. Convergence curves depict the best objective function value found versus CPU time, and can be used to compare the performance of different algorithms. An optimization method is preferred if it achieves a lower objective function value at earlier CPU time. The best convergence curves (out of 5) corresponding to B2 and B4 are shown in the Additional file 2 for 3 algorithms: (1) eSS-FMINCON, as reported in [ 23 ]; (2) eSSNL2SOL; and (3) eSS-NL2SOL using regularization as recommended in Section \u201cParameter estimation with regularization and global optimization\u201d. From those curves we see that the algorithm (3) proposed here converged earlier than the others to the optimal objective function value (note that log-log scale is used in these curves). We stress that the main purpose of regularization is to avoid overfitting: we do not wish to obtain an excessively good fit, which would indicate that we are reproducing noise instead of the true dynamics. Therefore, regularization should not achieve a smaller objective function value.  Next, we apply the identifiability analysis procedures presented in subsections \u201cPractical identifiability analysis\u201d and \u201cVisualization of identifiable subsets\u201d to these two models. For B2 they yield an identifiable subset of size 29, and for B4 of size 13 (recall that both models have a total of 116 parameters). The corresponding networks are shown in Figs. 3 and 4, respectively. It is also possible to find small groups of highly correlated parameters for models of this size; e.g. for B4 we obtained those depicted in Fig. 5.  The aforementioned results show that both models are poorly identifiable in practice for the considered datasets; more informative data would be needed in order to obtain accurate estimates of their parameters.     Discussion and conclusions\r\n  In this paper we have presented a workflow to efficiently estimate the parameters of dynamic models and analyze their practical identifiability. Our approach combines an advanced optimization technique, which reduces computation times in parameter estimation, and several identifiability analysis procedures, which can find subsets of identifiable and unidentifiable parameters. Results are visualized using network diagrams, which provide an intuitive representation of the findings and facilitate their analysis and understanding.  Many approaches have been applied to study identifiability of kinetic models, but they suffer from lack of scalability. An advantage of the integrated method presented here is its moderate computational cost, which enables its application to large-scale models; complete results can be obtained in a few hours for models of more than a hundred parameters. Another important aspect is the integration of identifiability analysis with visualization, which presents the results in a way that is easily interpretable for par58  par46 par8p9ar57 par56 par54par53par91  par51 par5p5ar90 par50 par48 par44par49par10p7ar43  par47 par108 par45 par7p5ar79  par82 par76par84par83 par77 par7p8ar81  par39 par42 par36  par41 par38 par40 par37 par27par31  par26 par29 par30par28 par3 par8 par5 par2 par99par10p3ar100 par17par96par18 par7 par4 par101  par19 par102 par94 par85 par87 par88 par86 par71 par73 par65 par67 par59 par63 par9 par11 par117 par116 par35 par34 par74 par72 par68 par66 par64 par62 par12 par10 par115 par33 par25 par24 par113 par111 par109 par104 par97 par20 par14 par13 par23 par114 par112 par110 par105 par98 par93 par16 par15 modelers and experimentalists. Currently, its main limitation arises when trying to find all the different existing groups of highly correlated parameters: the combinatorial explosion of this particular task makes it feasible only for models of moderate size, i.e. of a few dozens of parameters. However, all the remaining steps of the workflow presented in this manuscript scale up well up to several hundred parameters.  The usefulness of the methodology and workflow presented here goes beyond basic parameter identifiability analysis. The procedure not only (i) determines the largest subset of identifiable parameters, but also (ii) informs about the characteristics of the space of non-identifiable parameters, reporting small groups of highly correlated parameters, and (iii) presents all these results in a coherent and scalable way using visualization techniques, facilitating the understanding of the underlying complex interactions. Uncovering these higher order relationships helps in determining the causes of unidentifiability and provides guidelines for remedying them, e.g. by reformulating the model or by collecting new data through a new experimental design. All this information can be readily used to improve the iterative model-building cycle.  A MATLAB implementation of the identifiability and visualization methodology, which we have called the VisId software package (Additional file 1), is available from GitHub (https://github.com/gabora/visid) as free, open source software. This distribution includes the case studies discussed above.    Additional files\r\n  Additional file 1: VisId toolbox. This compressed folder contains the VisId MATLAB toolbox. (ZIP 1030 KB) Additional file 2: Supplementary material. This document contains detailed descriptions of the case studies and of the VisId toolbox, as well as additional details about the results. (PDF 306 KB) Abbreviations CCA: Canonical correlation analysis; CI: Collinearity index; CPU: Central processing unit; eSS: Enhanced scatter search; MI: Mutual information; ODE: Ordinary differential equation; QR: Decomposition of a matrix into an orthogonal matrix (Q) and an upper triangular matrix (R); RNA: Ribonucleic acid; RRQR: Rank revealing QR decomposition; TGF-β: Transforming growth factor beta; VNS: Variable neighbourhood search Acknowledgements Not applicable.  Funding This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 686282 (\u201cCANPATHPRO\u201d), from the EU FP7 project \"NICHE\" (ITN Grant number 289384), and from the Spanish MINECO project \"SYNBIOFACTORY\" (grant number DPI2014-55276-C5-2-R).  Availability of data and materials The datasets generated and/or analysed during the current study are available in the GitHub repository, https://github.com/gabora/visid/tree/master/case_ studies.  Authors' contributions JRB and AG conceived of the study. JRB coordinated the study. AG implemented the methods and carried out all the computations. AFV assisted in the development of the methodology. All authors analysed the results, drafted the manuscript, and read and approved the final manuscript. Competing interests The authors declare that they have no competing interests.  Consent for publication Not applicable.  Ethics approval and consent to participate Not applicable.    Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.  Author details 1BioProcess Engineering Group, IIM-CSIC, Eduardo Cabello 6, 36208 Vigo, Spain. 2JRC-COMBINE, RWTH Aachen University, Photonics Cluster, Level 4, Campus-Boulevard 79, 52074 Aachen, Germany.    ",
    "sourceCodeLink": "https://github.com/gabora/visid",
    "publicationDate": "0",
    "authors": [
      "Attila Gábor",
      "Alejandro F. Villaverde",
      "Julio R. Banga"
    ],
    "status": "Success",
    "toolName": "visid",
    "homepage": ""
  },
  "60.pdf": {
    "forks": 1,
    "URLs": [
      "github.com/izhbannikov/rqt",
      "github.com/izhbannikov/rqt.Contact:"
    ],
    "contactInfo": ["ilya.zhbannikov@duke.edu"],
    "subscribers": 1,
    "programmingLanguage": "R",
    "shortDescription": "This is a read-only mirror of the Bioconductor SVN repository. Package Homepage: http://bioconductor.org/packages/devel/bioc/html/rqt.html Contributions: https://github.com/izhbannikov/rqt. Bug Reports: https://support.bioconductor.org/p/new/post/?tag_val=rqt or https://github.com/izhbannikov/rqt/issues.",
    "publicationTitle": "rqt: an R package for gene-level meta-analysis",
    "title": "rqt: an R package for gene-level meta-analysis",
    "publicationDOI": "10.1093/bioinformatics/btx395",
    "codeSize": 371,
    "publicationAbstract": "Motivation: Despite recent advances of modern GWAS methods, it is still remains an important problem of addressing calculation an effect size and corresponding p-value for the whole gene rather than for single variant. Results: We developed an R package rqt, which offers gene-level GWAS meta-analysis. The package can be easily included into bioinformatics pipeline or used stand-alone. We applied this tool to the analysis of Alzheimer's disease data from three datasets CHS, FHS and LOADFS. Test results from meta-analysis of three Alzheimer studies show its applicability for association testing. Availability and implementation: The package rqt is freely available under the following link: https://github.com/izhbannikov/rqt.Contact: ilya.zhbannikov@duke.edu Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-09-04T10:31:08Z",
    "institutions": ["Duke University"],
    "license": "No License",
    "dateCreated": "2017-05-29T02:34:30Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx395   rqt: an R package for gene-level meta-analysis     Ilya Y. Zhbannikov  0    Konstantin G. Arbeev  0    Anatoliy I. Yashin  0    0  Biodemography of Aging Research Unit (BARU), Social Science Research Institute, Duke University ,  Durham, NC 27708 ,  USA     2017   1  1  2   Motivation: Despite recent advances of modern GWAS methods, it is still remains an important problem of addressing calculation an effect size and corresponding p-value for the whole gene rather than for single variant. Results: We developed an R package rqt, which offers gene-level GWAS meta-analysis. The package can be easily included into bioinformatics pipeline or used stand-alone. We applied this tool to the analysis of Alzheimer's disease data from three datasets CHS, FHS and LOADFS. Test results from meta-analysis of three Alzheimer studies show its applicability for association testing. Availability and implementation: The package rqt is freely available under the following link: https://github.com/izhbannikov/rqt.Contact: ilya.zhbannikov@duke.edu Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: John Hancock    1 Introduction\r\n  Advances in genome-wide analyses of complex traits allowed for detecting a number of strong associations between genetic factors and chronic diseases  (Kathiresan et al., 2009; Strawbridge et al., 2011) . However, many other associations detected in such studies are weak and did not reach genome wide levels of statistical significance. These results are in concert with the Fisher's conjecture that genetic variability of complex traits is the result of integration of the influence of many genetic factors each having small effect on the trait (Fisher, 1999). This situation generates an idea that evaluation of components of genetic influence on complex traits that are integrated by some parts of biological mechanisms may improve strength of the genetic estimates. Attempts to realize this idea resulted in a number of statistical methods focused on gene-level analyses of genetic data. In such analyses, information about detected genetic variants that belong to a particular gene is used to construct a score variable that integrates associations of these variants with a disease within a gene. Several methods were developed for such integration of common- and rare variants. This includes SKAT  (Wu et al., 2011) , KBAC  (Liu et al., 2010) , WSS  (Madsen et al., 2009)  and others. A set of methods and tools that perform gene-level metaanalysis was also proposed, e.g. MetaSKAT  (Lee et al., 2013) , seqMeta (https://CRAN.R-project.org/package¼seqMeta). The procedures of integrating genetic signals used in these methods are based on different ideas and result in different estimates of integrated associations of selected genes with the traits of interest. The methods that produce stronger genetic associations with phenotypic traits are usually considered as more preferable.  Recently, the QTests  (Lee et al., 2016)  for rare variants have been proposed. In these regression-based tests (QTest1-3), a score for the whole gene is built by using the pooled effect size obtained as a weighted sum of corresponding effect sizes from fitting a multivariate regression on all detected variants in a gene. However, it is important to take possible presence of linkage disequilibrium (LD) into account. LD is directly related to multicollinearity, which can significantly impact analysis by increasing the variance of the coefficient estimates (thereby making them very sensitive to small changes in the model); also the estimates become unstable and can switch signs. In addition, multicollinearity has negative effect on power  (Yoo et al., 2014) .  In this note we propose RQTests, which are modified QTests. RQTests directly address the problem of possible LD between variants . We also present a corresponding software tool-an R package rqt, which performs gene-level and meta-analyses taking into account rare genetic variants. rqt is available for download from the following link: https://github.com/izhbannikov/rqt.    2 Materials and methods\r\n  The workflow of gene-level meta-analysis consists of the following steps: (i) reducing the number of predictors (a.k.a. 'data preprocessing') to exclude multicollinearity. This is the primary contribution of our work; (ii) then the regression model is fitted on the reduced dataset to obtain corresponding regression coefficients; (iii) these coefficients are used to construct statistics representing a gene-level effect. P-values are then calculated using this statistics with asymptotic approximation or permutation procedure; (iv) combining gene-level p-values calculated from each study. Below we describe these steps in details: \u2022 In order to alleviate effects of correlation (LD-effects) between variants we employ a set of methods to preprocess the data first: PCA (principal component analysis), PLS (partial least square), LASSO  (Tibshirani, 1996)  and ridge regressions. By default, we use PCA and the number of principal components is used to capture 75% of explained variance. We should note that in PLS regression both the original predictors and response are decomposed into latent structures therefore the final p-values and pooled regression coefficient (see below) are estimated with respect to those new variables. The user can avoid this step (and, thereby, choose QTests) by supplying 'none' in the method parameter from funtction geneTest(. . .) of rqt. \u2022 Pooling regression coefficients and calculating statistics and gene-level p-value is performed according to the method proposed in  (Lee et al., 2016) . The pooled effect size bPooled is a weighted sum of coefficients: bPooled ¼ aTW b, where a ¼ ðakÞm 1, ak ¼ Pm1=varðb^kÞ - an inverse variance vector for k¼1ð1=varðb^kÞÞ the estimates of bk from the multivariate regression. W is a diagonal weight matrix that contains weighs for j-th variant: wj ¼ BetaðMAFj; 1; 25Þ where MAFj is a corresponding minor allele frequency; m is the number of variants in a gene. Then the corresponding statistics Q1 for the QTest1 is calculated as follows: Q1 ¼ ðaTW V W T aÞ bPooled 2 v2-1; where V ¼ varðbÞ. Other test statistics (Q2 for QTest2 and Q3 for QTest3) are described in  (Lee et al., 2016) . In our package rqt we implemented all of them. Since RQTests are implemented on top of QTests, RQTests1-3 take into account rare variants as well. \u2022  Calculating combined p-value for a gene from several studies is performed with one of the available combing probability methods (refer to the User Manual.)    3 Results\r\n  We developed an R package, rqt, where we implemented and improved QTests (rQTests1-3.) We also applied this methodology to a set of   PVRL2\r\n  TOMM40 APOC1 PPP1R3B APOE 3.000E 09 3.000E 09 9.030E 06 4.092E 04 3.512E 03 CHS FHS 1.030E 09 3,300E 08 3,010E 06 1,364E 04 2.048E 01 3.629E 01 2.600E 01 5.292E 02 9.995E 01 3.704E 01    LOADFS\r\n  Detailed analysis results by study are shown in Supplementary Materials, Tables 3C-6C. genes that are potentially involved in the development of Alzheimer's disease (AD), obtained from a literature search. We conducted a metaanalysis of the following studies: CHS (Cardiovascular Health Study, dbGaP accession: phs000287.v5.p1), FHS (Framingham Heart Study, dbGaP accession: phs000007.v22.p8) and LOADFS (Late Onset Alzheimer's Disease Family Study, dbGaP accession: phs000168.v2.p2), both females and males, in order to evaluate the possible associations between these genes and AD. Supplementary Table S1A from Supplementary Materials presents a description of the datasets used. Table 1 shows results for genes showed (P-value &lt; 10 2) associations to AD. These results are concordant to those previously found  (Corder et al., 1993; Linnertz et al., 2014) .  We also performed power simulation and type 1-error tests for rqt (RQTest1-3), QTest1-3, SKAT and SKAT-O (see Supplementary materials for simulation setup), for dichotomous and continuous phenotypes, assuming presence of LD between variants. Sample size was 3,000 and 50 SNPs. Percentage of causal SNPs was 10% and 25% in each test. Simulation methodologies and results are shown in Supplementary Figures S1C-S14C and Tables S7C-S13C from Supplementary. RQTests offer lowest variance inflation factor (VIF), type 1 error rate and highest power for the PCA preprocessing method, see Supplementary Figures S1C, S2C, and Tables S7C and S8C from Supplementary materials. Note: according to the QQ plots under the null hypothesis for the PLS method, the p-values have non-uniform distribution and therefore the PLS method should be used with care in rqt. Additional investigations are needed to address this issue.     Funding\r\n  This work was supported by the National Institute on Aging of the National Institutes of Health (NIA/NIH) under Award Numbers P01AG043352, R01AG046860 and P30AG034424. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIA/NIH. Acknowledgements on data used are given in Supplementary materials, Section D. Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/izhbannikov/rqt",
    "publicationDate": "0",
    "authors": [
      "Ilya Y. Zhbannikov",
      "Konstantin G. Arbeev",
      "Anatoliy I. Yashin"
    ],
    "status": "Success",
    "toolName": "rqt",
    "homepage": ""
  },
  "73.pdf": {
    "forks": 1,
    "URLs": ["github.com/hao-peng/DEIsoM"],
    "contactInfo": ["pengh@alumni.purdue.edu"],
    "subscribers": 2,
    "programmingLanguage": "C++",
    "shortDescription": "RNASeq Project",
    "publicationTitle": "DEIsoM: a hierarchical Bayesian model for identifying differentially expressed isoforms using biological replicates",
    "title": "DEIsoM: a hierarchical Bayesian model for identifying differentially expressed isoforms using biological replicates",
    "publicationDOI": "10.1093/bioinformatics/btx357",
    "codeSize": 8396,
    "publicationAbstract": "Motivation: High-throughput mRNA sequencing (RNA-Seq) is a powerful tool for quantifying gene expression. Identification of transcript isoforms that are differentially expressed in different conditions, such as in patients and healthy subjects, can provide insights into the molecular basis of diseases. Current transcript quantification approaches, however, do not take advantage of the shared information in the biological replicates, potentially decreasing sensitivity and accuracy. Results: We present a novel hierarchical Bayesian model called Differentially Expressed Isoform detection from Multiple biological replicates (DEIsoM) for identifying differentially expressed (DE) isoforms from multiple biological replicates representing two conditions, e.g. multiple samples from healthy and diseased subjects. DEIsoM first estimates isoform expression within each condition by (1) capturing common patterns from sample replicates while allowing individual differences, and (2) modeling the uncertainty introduced by ambiguous read mapping in each replicate. Specifically, we introduce a Dirichlet prior distribution to capture the common expression pattern of replicates from the same condition, and treat the isoform expression of individual replicates as samples from this distribution. Ambiguous read mapping is modeled as a multinomial distribution, and ambiguous reads are assigned to the most probable isoform in each replicate. Additionally, DEIsoM couples an efficient variational inference and a post-analysis method to improve the accuracy and speed of identification of DE isoforms over alternative methods. Application of DEIsoM to an hepatocellular carcinoma (HCC) dataset identifies biologically relevant DE isoforms. The relevance of these genes/isoforms to HCC are supported by principal component analysis (PCA), read coverage visualization, and the biological literature. Availability and implementation: The software is available at https://github.com/hao-peng/DEIsoM Contact: pengh@alumni.purdue.edu Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2016-11-15T15:25:03Z",
    "institutions": [
      "Purdue University",
      "Department of Computer Science",
      "Eli Lilly and Company"
    ],
    "license": "No License",
    "dateCreated": "2013-05-06T05:17:08Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx357   DEIsoM: a hierarchical Bayesian model for identifying differentially expressed isoforms using biological replicates     Hao Peng  1    Yifan Yang  0  1    Shandian Zhe  1    Jian Wang  3    Michael Gribskov  0  1    Yuan Qi  1  2    0  Department of Biological Sciences, Purdue University ,  West Lafayette, IN 47907 ,  USA    1  Department of Computer Science    2  Department of Statistics, Purdue University ,  West Lafayette, IN 47907 ,  USA    3  Eli Lilly and Company ,  Indianapolis, IN 46285 ,  USA     2017   1  1  10   Motivation: High-throughput mRNA sequencing (RNA-Seq) is a powerful tool for quantifying gene expression. Identification of transcript isoforms that are differentially expressed in different conditions, such as in patients and healthy subjects, can provide insights into the molecular basis of diseases. Current transcript quantification approaches, however, do not take advantage of the shared information in the biological replicates, potentially decreasing sensitivity and accuracy. Results: We present a novel hierarchical Bayesian model called Differentially Expressed Isoform detection from Multiple biological replicates (DEIsoM) for identifying differentially expressed (DE) isoforms from multiple biological replicates representing two conditions, e.g. multiple samples from healthy and diseased subjects. DEIsoM first estimates isoform expression within each condition by (1) capturing common patterns from sample replicates while allowing individual differences, and (2) modeling the uncertainty introduced by ambiguous read mapping in each replicate. Specifically, we introduce a Dirichlet prior distribution to capture the common expression pattern of replicates from the same condition, and treat the isoform expression of individual replicates as samples from this distribution. Ambiguous read mapping is modeled as a multinomial distribution, and ambiguous reads are assigned to the most probable isoform in each replicate. Additionally, DEIsoM couples an efficient variational inference and a post-analysis method to improve the accuracy and speed of identification of DE isoforms over alternative methods. Application of DEIsoM to an hepatocellular carcinoma (HCC) dataset identifies biologically relevant DE isoforms. The relevance of these genes/isoforms to HCC are supported by principal component analysis (PCA), read coverage visualization, and the biological literature. Availability and implementation: The software is available at https://github.com/hao-peng/DEIsoM Contact: pengh@alumni.purdue.edu Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  RNA-seq is a powerful tool for investigating the transcriptomes of various organisms. There are many complex issues in RNA-seq and transcriptome analysis ranging from RNA-seq read correction  (Le et al., 2013) , transcriptome assembly  (Martin and Wang, 2011)  to alternative splicing and gene fusion detection  (Ozsolak and Milos, 2011) . However, one of the most fundamental issues is to quantify and identify isoforms differentially expressed in two conditions, while each containing multiple replicates. Most DE isoform quantification methods treat each replicate independently, ignoring the fact that, because the underlying biological mechanism is the same in a given condition, the replicates tend to share similar expression patterns. DEIsoM improves DE isoform identification and quantification by catching the information shared between replicate samples; rather than separately estimating the isoform expression for each replicate, it captures the common expression pattern of the whole condition in one single model.  Although many computational tools have been developed for quantifying and identifying DE isoforms using RNA-seq data, nearly all approaches estimate the isoform abundance in each replicate separately, and do not attempt to actively capture the aforementioned shared information. For instance, Mixture of ISOforms (MISO)  (Katz et al., 2010)  infers the isoform fractions for each replicate and evaluates the DE of every pair of replicates using the Bayes Factor, not considering replicates as a group. Additionally, MISO is slow due to its use of MCMC sampling, which is computationally challenging to adapt to the rapid growth in the amount of RNA-seq data  (Kakaradov et al., 2012) . Dirichlet-Multinomial framework (DRIMSeq)  (Nowicka and Robinson, 2016)  infers the isoform fractions for each replicate in a Dirichlet-Multinomial model with a fixed hyperparameter and evaluates DE between two conditions by likelihood ratio test. Cufflinks  (Trapnell et al., 2012)  quantifies the isoform abundance in individual replicates by maximum a posteriori (MAP) and detects DE isoforms by the hypothesis test based on Jensen-Shannon divergence. RNA-Seq by Expectation Maximization (RSEM)  (Li and Dewey, 2011)  estimates isoform abundance for each replicate using an Expectation Maximization (EM) algorithm. Empirical Bayesian Seq (EBSeq)  (Leng et al., 2013)  then takes the expected counts from all replicates to fit a joint model and estimates the probability of DE for each isoform between multiple conditions. However, the variance of the expected counts stemming from ambiguous read mapping is simply lost in this process, compromising the DE isoform detection. Bayesian inference of transcripts from Sequencing data (BitSeq)  (Glaus et al., 2012; Hensman et al., 2015)  estimates the per condition mean isoform abundance from multiple replicates. However, BitSeq accomplishes this estimation in two stages rather than in an integrated model, which could potentially lose information when the \u201cpseudo-data\u201d from each fitted model in stage 1 is fed to the conjugate normal-gamma model in stage 2. Some other models do take the strategy of utilizing the shared information from multiple biological replicates, such as rMATS  (Shen et al., 2014) , and MAJIQ  (Vaquero-Garcia et al., 2016) . However, they are both exon-centric, quantifying and identifying alternative splicing at the exon level not the isoform level.  Here, we present DEIsoM, a hierarchical Bayesian model for quantifying and identifying DE isoforms between two conditions. Other than estimating the isoform abundance in each replicate separately, DEIsoM actively captures the shared information of perconditioned replicates in one principle framework. Specifically, DEIsoM uses a Dirichlet prior distribution to capture the shared information among replicates in each condition, and implements a  A  B fast Variational Bayesian (VB) method to gain computational efficiency instead of MCMC sampling when computing the posterior distributions of isoform fractions. Figure 1A shows a typical design for an RNA-Seq experiment with three replicates in each condition. Because we assume that the replicates in one condition share the same underlying biological mechanism, their expression patterns tend to be the same within a certain sample variance. We capture this common pattern through a Dirichlet prior with a tracable and effeciently updated hyperparameter. Additionally, we evaluate the DE isoforms by computing the Kullback-Leibler (KL) divergence between the posterior distributions of the two conditions, which is intrinsically fast in our model. Figure 1B gives a qualitative idea of how KL divergence is used to evaluate DE; the DE level is represented as the non-overlapping areas between the two posterior distributions.  Simulations in Section 3 demonstrate the superior performance of DEIsoM over alternative methods for quantifying and predicting DE isoforms, as well as the improved computational speed of VB method compared to MCMC sampling. Furthermore, on a real HCC dataset (Section 4), DEIsoM identifies HCC relevant DE isoforms which are supported by PCA, read coverage visualization, and the biological literature.    2 Materials and methods\r\n  DEIsoM consists of three parts: the hierarchical graphical model for isoform quantification (Section 2.1), the VB algorithm for model estimation (Section 2.2) and the identification of DE isoforms between two conditions (Section 2.3).   2.1 Model\r\n  Suppose we have collected RNA-seq data from M replicates in each condition. For the mth replicate, there are in total NðmÞ paired-end reads that can be aligned to a given gene with K isoforms. Here, we utilize the previous annotated or assembled isoforms, so K is known for each gene. We use a K-dimensional binary vector, RðnmÞ, to represent the read alignment to isoforms. If the nth read from the mth replicate maps to the kth isoform, the kth element of RðnmÞ; Rðnm;kÞ, is set to be 1, and 0 otherwise. The unsequenced fragment length between the nth paired-end reads is denoted as kðnmÞ ¼ ½kðnm;1Þ; . . . ; kðnm;KÞ .  First, we model how a read is generated from an isoform. We use a binary random variable Zðnm;kÞ to represent whether the nth read of the mth replicate is actually generated from the kth isoform. We call Zðnm;kÞ the latent read origin. Although a read can map to multiple isoforms, it can only be sequenced from one isoform. Therefore, ZðmÞ is a n K-dimensional vector with exactly one element equal to 1 and all the k¼1 Zðnm;kÞ ¼ 1. We assume that for the mth others equal to 0, where PK replicate, ZðmÞ follows a multinomial distribution pðZðnmÞjwðmÞÞ, where n wðmÞ is a K-dimensional vector representing the fractions of isoforms iPn kKt¼h1e wmðkmthÞ r¼ep1li.caTteheforfraacgtiivoenns goefnei.soTfhoursm, swðkfmwÞð2mÞ½g0m; ¼11::fMor caallnkvaanrdy among replicates, but we assume that the replicates all follow the same Dirichlet prior distribution pðwjaÞ in each condition. Different from MISO, which uses one fixed prior pðwÞ for each replicate, DEIsoM shares the same prior among replicates. The underlying reason is that the distributions of isoforms from different replicates of the same condition are not independent, but share some common patterns. DEIsoM summarizes the shared information in the hyperparameter a. In Section 2.2, we will further explain how the hyperparameter a is updated using the information from all replicates.  We assume that the observed read alignments Rðnm;kÞ and the unsequenced fragment length kðnm;kÞ are conditionally independent given the corresponding latent read origin ZðmÞ and some fixed parameters H:  pðRðnm;kÞ; kðnm;kÞjZðnm;kÞ; HÞ ¼ pðRðnm;kÞjZðnm;kÞ; HÞpðkðnm;kÞjHÞ where H includes lk, L, l and r2. lk is the length of the kth isoform; L is the sequenced read length; l and r2 are the mean and variance of kðnmÞ respectively. The first part, pðRðnm;kÞjZðnm;kÞ; HÞ, represents the probability that a read can be aligned to a specific region of the kth isoform conditioned on whether it is generated from this isoform. If the nth read is generated from the kth isoform, this read is assumed to be uniformly generated from one of all the possible positions in this isoform. Otherwise, pðRðnm;kÞjZðnm;kÞ; HÞ is 0. The number of all possible positions is l~ðnm;kÞ ¼ lk ð2L þ kðnm;kÞÞ þ 1, Then the conditional distribution is: pðRðnm;kÞ ¼ 1jZðnm;kÞ; HÞ ¼ 8 &lt; 1=l~ðnm;kÞ : 0 if Zðnm;kÞ ¼ 1 otherwise:  The second part, pðkðnm;kÞjHÞ, is the probability of observing a paired-end read with unsequenced length kðnmÞ, which follows a normal distribution with mean l and variance r2. Both l and r2 can be given or estimated from the aligned RNA-seq data. As a result, we have the following generative process for each of M replicates (Fig. 2):   1. wðmÞ DirichletðaÞ\r\n  2. For each of NðmÞ reads: a. ZðnmÞ Multinomialð1; wðmÞÞ b. Rðnm;kÞ pðRðnm;kÞjZðnm;kÞ; HÞ c. kðnm;kÞ Normalðl; r2Þ (1) (2)     2.2 Estimation\r\n  To compute the posterior distribution of isoform fractions and read assignments, pðw; ZjR; a; HÞ ¼ pðw; Z; Rja; HÞ pðRja; HÞ we need to compute the denominator: pðRja; HÞ ¼  Y ð pðwðmÞjaÞ Y X hpðZðnm;kÞ ¼ 1jwðkmÞÞ m n k pðRðnm;kÞ; kðnm;kÞjZðnm;kÞ ¼ 1; HÞidwðmÞ which is computationally intractable, so we have to use approximate inference techniques, such as Markov Chain Monte Carlo (MCMC) sampling method or Variational Bayesian method. Classical MCMC methods may take a long time to converge due to the high correlation between the latent variables (Section 3.2). The Variational Bayesian method  (Jordan et al., 1999)  tends to be faster and better scalable to large data for many graphical models. The VB algorithm approximates the intractable posterior p by a proposed distribution q, where q belongs to a family of distributions controlled by the variational parameters. We can optimize the variational parameters to minimize the Kullback-Leibler divergence between q and the posterior p, KLðqjjpÞ. This is equivalent to maximizing a variational evidence lower bound. In such a way, the inference problem is cast to an optimization problem, which can be efficiently solved by gradient-based optimization algorithms.  For our model, we propose a family of variational distributions, which has the form: qðw; ZÞ ¼  Y q wðmÞ; bðmÞ m  Y q ZðnmÞ; rðnmÞ ; n where qðwðmÞ; bðmÞÞ is a Dirichlet distribution parameterized by bðmÞ and q ZðmÞ; rðnmÞÞ is a multinomial distribution parameterized ð n by rðnmÞ.  We use the following iterative variational EM algorithm updates to find the optimal parameters for our model: 1. (E-step) For each replicate, estimate the variational parameters rðnmÞ; bðmÞ; 2. (M-step) Maximize the variational evidence lower bound with respect to the hyperparameter a.  In E-step, we estimate the posterior distribution using a very commonly used algorithm, coordinate ascent variational inference (CAVI)  (Bishop, 2006) . We iteratively update: where ðmÞ qn;k ðmÞ rn;k ¼ K lP¼1 qðnm;lÞ and bðkmÞ ¼ ak þ  NðmÞ X rn;k  ðmÞ n¼1 qðnm;kÞ ¼ p Rðnm;kÞ; kðnm;kÞjZðnm;kÞ ¼ 1; H  \" exp z bðmÞ k z and z denotes the digamma function which is the derivative of the log-gamma function.  In M-step, we can use the Newton-Raphson method to update the hyperparameter a. This method is widely used for parameter estimation of models with Dirichlet priors  (Blei et al., 2003; Minka, 2000; Ronning, 1989) . Here, we initialize the hyperparameter a ¼ 1. The Newton-Raphson method finds the stationary point of an objective function using the iterative updates: anew ¼ aold   HðaoldÞ 1gðaoldÞ\r\n  where g and H denote the gradient and the Hessian matrix of the objective function respectively. However, some new ak may become non-positive during the iterative updates, which is invalid for Dirichlet distributions. Therefore, instead of working on a directly, we update log ðaÞ first and then take the exponential of it. Let c ¼ log ðaÞ. The gradient and the Hessian of the variational lower bound with respect to c can be computed as: gkðcÞ ¼ M  z M X ak z bðmÞ  k m¼1  K X al l¼1 ! z  ! zðakÞ akþ where we define rði; jÞ ¼ 1 if i ¼ j, otherwise rði; jÞ ¼ 0; z0 is the trigamma function, and (4) (5) (6)  A drawback of taking the logarithm is that we can no longer use the special structure of Hessian to compute H 1g efficiently as in  Blei et al. (2003) . Since Hessian computation can be expensive for large K, we update c with L-BFGS method using the gradient only. Updates for a will terminate when the maximum number of iterations is reached or the change in evidence lower bound is smaller than our threshold.     2.3 Identification\r\n  The DE level of an isoform can be represented as the difference between the posterior distributions of isoform fractions under two conditions. As used in the Variational Bayesian method, KL divergence measures the difference between any two distributions. Therefore, we compute the KL divergence between the posterior distributions of isoform fractions under the two conditions to evaluate the DE level of the isoforms. A higher KL divergence implies that the isoforms of this gene are more differentially expressed under the two conditions. Specifically, we train the model and estimate the posterior distribution pðwjR; a; HÞ with data from healthy and diseased conditions respectively. As described in Section 2.2, although the exact posterior distribution cannot be computed, we use the approximate posterior distributions from two conditions, qðw; bÞ and q0ðw0; b0Þ, to compute the KL divergence. Because qðwmÞ or q0ðwmÞ are independent Dirichlet distributions, the KL divergence, DKL can be computed analytically as:  M ( DKLðqjjq0Þ ¼ X m¼1  K  P bðmÞ log kK¼1 k  K X hbðmÞ  k k¼1  \" b0ðkmÞi z bðmÞ k z    3.1 Comparison of five methods on synthetic data\r\n  To test whether the shared information contributes to DE isoform detection, we generate synthetic data and compare DEIsoM with four commonly used programs: Cufflinks (v2.2.1), MISO (v0.5.3), RSEM (v1.2.30), and BitSeqVB (v0.7.5). The synthetic data are generated as follows. We first randomly select 200 genes (1395 isoforms) from the annotation of chromosome 1 in the hg19 human reference genome, in which 100 genes are labeled as containing DE isoforms and the rest are non-DE. We sample the expression levels of genes from a log-normal distribution  (Gierlinski et al., 2015) . Isoform fractions are generated from a symmetric Dirichlet distribution with a ¼ 1, which means the chance of sampling any fraction of isoforms is equally probable. For instance, if there are three isoforms, the probability of sampling the isoform fraction as (0.1, 0.2, 0.7) is the same as (0.2, 0.3, 0.5). For DE isoforms, we draw two different samples for two conditions respectively; for non-DE, we draw only one sample shared by both conditions. To model the variation among replicates, we add Gaussian noise with a standard deviation equal to 10% of the expression level of each replicate. According to Standards, Guidelines and Best Practices for RNA-Seq V1.01, the number of paired-end RNA-Seq reads used in current studies is around 30 million per replicate. And for each tissue, it is generally expected more than 10, 000 genes are expressed  (Consortium, 2015) . Following the above empirical read numbers, we generate 600, 000 RNA-Seq reads for 200 genes using RNASeqReadSimulator 2 for each of five replicates in both conditions, using default settings.3 To test the robustness of DEIsoM, we repeat the above simulation process 10 times. For RSEM, BitSeq, MISO, and DEIsoM, the simulated reads are mapped back to the reference transcriptom using Bowtie2  (Langmead and Salzberg, 2012) . For Cuffdiff, the reads are mapped back to the hg19 reference genome using Tophat  (Trapnell et al., 2009) . The machine used to run all experiments has two 8-Core Intel Xeon-E5 processors and 64 GB memory.  First, we compare the quantification performance of DEIsoM with MISO, Cuffdiff, RSEM and BitSeqVB in terms of the correlations between the predicted isoform fractions and the ground truth on the synthetic data. Figure 3A summarizes the means and the standard errors of the correlation coefficients in 10 replicates. They show that the correlation coefficients in DEIsoM is higher than the alternative methods.  Second, we compare the DE isoform identification performance of DEIsoM with MISO, Cuffdiff, RSEM-EBSeq, and BitSeqVB in terms of the area under curve (AUC) of receiver operating characteristic (ROC) curves on the synthetic data. The ROC curves are computed based on different ranking criteria for the five methods. DEIsoM uses the KL divergence; MISO uses both the average of Bayes factors of all pairs of subjects (MISO-BF) and the average of KL divergences of posteriors of isoform factions (MISO-KL); Cuffdiff uses a log-fold-change based P-value; RSEM-EBSeq uses the Posterior Probability of Differential Expression (PPDE); BitSeqVB uses the Probability of Positive Log Ratio (PPLR). To make different criteria comparable, we take the minimum of all isoform P-values of the gene, and the maximum of all isoform PPDEs and PPLRs of the gene as the gene DE level. And we choose the \u201cisoform-centric\u201d mode for MISO. Also, PPLR is more sensitive to the upregulated DE isoforms than the downregulated ones by definition. Figure 4A shows the ROC curves for one of the 10 repeated experiments. Figure 3B summarizes the means and standard errors of the AUCs over 10 runs. They show that DEIsoM consistently outperforms MISO-BF, MISO-KL, Cuffdiff, and RSEM-EBSeq on the synthetic data under our settings.  Third, we compare the CPU time of DEIsoM, Cuffdiff, RSEMEBSeq and BitSeqVB. The time we count is from the point we give the alignment files as input to the point that the programs generate the quantification results. We summarize it in Supplementary Table S1 for one run of the simulated data and the real data which will be discussed in Section 4. The numbers of hours used by the three algorithms are comparable, where Cuffdiff is always the fastest in all methods. However, DEIsoM has better DE isoform identification and quantification performance than the alternative methods, which is shown in both Section 3 and Section 4.    3.2 Comparison of VB and MCMC on synthetic data\r\n  To test whether the VB inference algorithm speeds up the computation over MCMC sampling without loss of accuracy, we compare the ROC curves and running time of the two implementations. We set the maximum iteration number as 1500 for both VB and MCMC. The burn-in time of MCMC is 150 iterations. Note that the MCMC sampling here is not completely the same as MISO. MISO combines the Metropolis-Hasting algorithm with a Gibbs sampler. We follow the same approach to estimate w, but we iteratively sample a from its posterior distribution given a noninformative prior which depends on all five replicates. Details of our MCMC sampling method are described in the Supplementary. The VB inference shows an advantage over MCMC in both the ROC curve and computing time within the limited number of iterations. Figure 4B shows the ROC curves for both implementations; VB inference achieves an AUC ¼ 0.9445 in 1.4 CPU h, whereas the MCMC method has AUC ¼ 0.8844 in 56 CPU h. Although MCMC theoretically can give samples from the exact target posterior distribution, it converges slowly on this dataset, which may cause inaccurate predictions and long running time. However, VB usually converges before the limit is reached under the same number of maximum iterations. Therefore, the VB method achieves a faster speed and a higher accuracy than the MCMC sampling.    3.3 Comparison of sensitivity of five methods\r\n  To demonstrate the robustness of DEIsoM, we vary the parameter of Dirichlet distribution a used for generating isoform fractions. When we increase a, the variance of generated isoform fractions under two conditions becomes smaller, but the mean remains the same. As a result, the difficulty of distinguishing DE genes from non-DE genes increases. In this experiment, we set a ¼ 1, 3, and 5 and keep the other settings unchanged to simulate the data. We test all above five methods on the simulated reads to see whether they are sensitive to the change of a. Table 1 shows that as a increases, the AUCs of all methods decrease, since the task becomes harder. However, DEIsoM consistently outperforms the alternative methods throughout all a settings.    3.4 Comparison of abundance estimation\r\n  To test the quantification performance of DEIsoM under a more realistic setting, we simulate RNA-Seq reads using real data. Two RNA-Seq datasets of human stomach tissue were chosen from the ENCODE project4. Following the same percedure in  Hensman et al. (2015) , we estimate the abundance of 196, 317 transcripts using four models, RSEM, Cuffdiff, BitSeqVB and DEIsoM, as the ground truth for each scenario. By feeding the ground truth to Spanki  (Sturgill et al., 2013) , we generate about 10 millions paired-end reads for each of the five replicates under each scenario. Four different evaluation criteria are used, see Supplementary S.2.1: Theta, Theta-Group, WGE-True and WGE-Inter. Theta measures the accuracy of transcript fraction estimation for all the replicates; ThetaGroup measures the accuracy of transcript fraction estimation for the whole group; WGE-True measures the accuracy of within-gene relative fractional estimation; WGE-Inter measures the predictive consistency among all replicates. Figure 5 summarizes the relative root mean square errors (RMSE) of DEIsoM, RSEM, BitSeqVB, and Cuffdiff on four simulated datasets. They show that the DEIsoM RMSEs in both Theta-Group and WGE-Inter are lower than the other three methods, indicating that DEIsoM tends to give more consistent and accurate estimates for the whole condition. This MISO-BF MISO-KL Cuffdiff RSEM-EBSeq BitSeqVB DEIsoM result is consistent with the one in  (Hensman et al., 2015) . A similar result evaluated by the relative mean absolute errors (MAE) is shown in Supplementary Figure S1.     4 Real data experiments and results\r\n  In this section, we test whether DEIsoM successfully identifies DE isoforms in real data. We apply DEIsoM and alternative programs to a hepatocellular carcinoma (HCC) RNA-seq dataset, and evaluate the predicted DE isoforms by PCA, read coverage visualization, and comparison to the biological literature. Aberrant alternative splicing is known to be involved in HCC  (Berasain et al., 2010) , so DE isoforms should be present.   4.1 Data pre-processing\r\n  RNA-seq data was collected from nine pairs of HCC tumors and their matched adjacent normal tissues  (Kan et al., 2013; Sung et al., 2012) . The mRNA of each sample was extracted, amplified and sequenced. 150 base paired-end reads were generated and aligned to the hg19 human reference genome using RUM (RNA-Seq Unified Mapper)  (Grant et al., 2011) . The aligned reads are used as input to three methods, Cuffdiff, RSEM-EBSeq, BitSeqVB, and DEIsoM, for DE isoform detection. MISO is not included because it cannot perform a group-wise analysis. 4.2 PCA Because there is no exact ground truth for the HCC real data, we evaluate the quantification ability of each method by PCA plots. We first choose 38 significantly DE genes that are verified by polymerase chain reaction (PCR) from the previous publications  (Dong et al., 2009; Huang et al., 2017; Wang et al., 2015, 2017) . For each gene, we sum up the Fragments Per Kilobase of transcript per Million mapped reads (FPKM) of all the child isoforms as the gene expression. If the gene/isoform expressions associated with the HCC are correctly estimated, these gene/isoforms can be used as features to distinguish between the normal and tumor samples in PCA plots. Figure 6 shows that DEIsoM and RSEM can linearly separate tumor samples from their matched normal samples; BitSeqVB has one tumor sample (9) very closed to the normal cluster; Cuffdiff misses three tumor samples (4, 5, 6) in the normal cluster.    4.3 Read coverage visualization\r\n  To understand the expression patterns of the DE isoforms selected by DEIsoM, we visualize the read coverage on the hg19 reference genome. Because it may be possible to align a read to multiple isoforms, it is hard to determine the exact expression level of each isoform from the read coverage visualization. But it is possible to tell the change in isoform expression in some cases. A previous study successfully identified the genes with DE isoforms by testing the difference in read coverage between two conditions  (Stegle et al., 2010) . Following the same logic, we assume that if the read coverage of a gene is similar in the two conditions, the isoforms of that gene will be predicted as non-DE. Otherwise, they are more likely to be DE.  First, we examine the read coverage of IGF2, a gene identified by DEIsoM as having DE isoforms. IGF2 is the 2nd most DE gene identified by DEIsoM. Eight isoforms of IGF2 have been observed according to the human transcriptome annotation. Figure 7A and B show the read coverage of IGF2 in nine pairs of normal and tumor samples. Note that the reads aligned to the last two exons (in the box) can only contribute to isoform 4 (ENST00000300632). Figure 7B shows that the absolute numbers of reads aligned to the last two exons in all tumor samples are much lower than that in normal samples. Figure 7C is the same as Figure 7B but with an automatically scaled y-axis. (C) shows that in eight of nine tumor samples (1T, 2T, 4T - 9T), the fractions of reads aligned to the last two exons are much lower in the HCC samples than that in the normal samples. This indicates that IGF2 isoform 4 is down-regulated in HCC tumors. However, in the Cuffdiff results, this isoform has a P-value of 0.039 with rank 95; in RSEM-EBSeq, the PPDE equal to 1 out of 1147 DE isoforms all with PPDE ¼ 1. But if we further rank by transcript real fold change (condition 1 over condition 2) as recommended, it ranks 671 out of 1147 DE isoforms.  Second, we show the read coverage of IGF2BP1, a gene identified by Cuffdiff as having DE isoforms. Isoform 1 (ENST00000290341) of IGF2BP1 is the 6th most DE gene. Supplementary Figure S1 shows the read coverage of IGF2BP1 in normal and tumor samples. Note that the reads aligned to the last exon only contribute to isoform 1 (the box indicates the last exon). However, only four of nine tumor samples show moderate differential expression of isoform 1 (lower than 500), and the expression level is near zero in all normal samples and five of the tumor samples (1T - 4T, 8T). Cuffdiff evaluates DE level using the log-fold-change between the conditions. This \u201cfold\u201d will be extremely large when the expression of one condition is near zero and the other is slightly higher. However, due to the low count numbers in both conditions, the confidence of calling this gene as having DE isoforms is low. Often, an empirical value is set to avoid low signals (NOTEST or LOWDATA). On the contrary, DEIsoM ranks IGF2BP1 as 244. Because both large sample variance and low read coverage lead to relatively \u201cflat\u201d posterior distributions in both normal and tumor conditions, which are close to the prior distribution. Thus, the KL divergence between two posterior distributions is small and the isoforms are not identified as DE.  Lastly, we visualize the five least differentially expressed isoforms identified by DEIsoM, showing that the low ranked isoforms have very similar read coverage patterns in both normal and tumor samples. Supplementary Figure S2 shows COX16 has a similar read coverage pattern among all samples in both normal and tumor conditions. This is because a low KL divergence requires a high similarity between two posterior distributions of isoform fraction.    4.4 Biological relevance of predicted DE isoforms\r\n  To further understand the functions of DE isoforms selected by DEIsoM, we examine whether they are supported by HCC relevant literature. PubMed searches were performed using the keywords B D 'gene name þ hepatocellular carcinoma'. Since most current experimental work focuses on the expression levels of genes rather than isoforms, we associate the DE isoforms identified by DEIsoM, Cuffdiff, RSEM-EBSeq and BitSeqVB with their gene names. Also, we assume that if the expression of a gene changes, it is very likely caused by a change of its isoforms. DE isoforms/genes are then categorized into four groups (3, 2, 1, 0) according to their relevance to HCC. 'Category 3' refers to a gene whose function in HCC has been well studied and can be used as a potential biomarker for prognosis or diagnosis. 'Category 2' indicates that differential expression of a gene has been detected in vivo, but not used as a biomarker. 'Category 1' indicates a gene whose function has only been studied in vitro but not in patient biopsies. 'Category 0' indicates a gene for which we found no HCC relevant literature.  First, we compare the number of genes that are HCC biomarkers (Category 3) in the predictions by DEIsoM, BitSeqVB, RSEM-EBSeq and Cuffdiff (the first four columns in Fig. 8). In the top 10 lists, more genes are identified as HCC biomarkers by DEIsoM than BitSeqVB, RSEM-EBSeq or Cuffdiff. Specifically, 6/10 genes identified by DEIsoM (ASS1, TTR, IGF2, AHSG, GPC3, CRP) vs. 4/10 genes identified by BitSeqVB (GPC3, AFP, IGF2BP3, UBE2C), 3/10 genes identified by Cuffdiff (SKP2, C-FOS, SOCS2) and 3/10 genes identified by RSEM-EBSeq (PEG10, TERT, ACAN) belong to Category 3.  Second, we have examined the six specific HCC biomarkers status (ASS1, TTR, IGF2, AHSG, GPC3, CRP) in the top 10 list of DEIsoM. Specifically, ASS1 is detected to be down-regulated in HCC liver samples, which can be used to predict metastatic relapse with a high sensitivity and specificity  y (Tan et al., 201 4); TTR is down-regulated in HCC patient serum  m (Qiu et al., 200 8);  ; (Yim and Chung, 201 0) state that both IGF2 and GPC3 are effective biomarkers for HCC-particularly, circulating IGF2 mRNA is positive in 34% of HCC patients and 100% correlated with the extrahepatic metastasis; GPC3 has been reported to interact with the Wnt signaling pathway to stimulate cell growth in HCC; GPC3 has also been used combined with PEG10, MDK, SERPINI1, and QP-C as a classifier that successfully distinguishes noncancerous hepatic tissues from HCCs  s (Yim and Chung, 201 0); AHSG combined with two other HCC-associated antigensKRT23 and FTL-can be used to diagnose HCC with sensitivity up to 98.2% in joint tests and specificity up to 90.0% in serial tests. (W (Wang et al., 2009) ; CRP, an inflammatory cytokine, is highly expressed in HCC and its expression is correlated with tumor size, Child-Pugh function and survival time (J (Jang et al., 2012) .  Generally, DEIsoM ranks genes/isoforms highly associated with HCC on the top. In the top 10 list (the first four columns in Fig. 8), 60% of genes identified by DEIsoM as having DE isoforms are experimentally proven HCC biomarkers (Category 3), and 90% are HCC biomarkers plus DE genes verified in vivo (Category 3 þ 2). On the contrary, BitSeqVB, RSEM-EBSeq, and Cuffdiff show a lower performance than DEIsoM 30 to 40% of genes having DE isoforms that are experimentally proved HCC biomarkers (Category 3), and 40% to 50% are HCC biomarkers plus DE genes verified in vivo (Category 3 þ 2).  Even if we expand this search to top 50 lists (the fifth column in Fig. 8 and Supplementary Table S4), DEIsoM still identifies 18 genes (36%) as HCC biomarkers, and 10 genes (20%) as DE genes verified in vivo. However, BitSeqVB, RSEM-EBSeq, and Cuffdiff identify fewer literature proven DE genes than DEIsoM in the top 50 list (the last three columns in Fig. 8 and Supplementary Tables S5-7). BitSeqVB identifies 16 genes (32%) as HCC biomarkers, 12 genes (24%) as DE genes in vivo; RSEM-EBSeq identifies 12 genes (24%) as HCC biomarkers and 3 genes (6%) as DE genes verified in vivo; Cuffdiff identifies 11 genes (22%) as HCC biomarkers, 12 genes (24%) as DE genes in vivo. Therefore, DEIsoM has a clear superior ability to select DE genes that are supported by the published literature.  Moreover, the isoforms of four genes (FGFR2, survivin, ADAMTS13 and CD44) identified as DE by DEIsoM have been found to be up or down-regulated in HCC. This provides additional support for DE isoforms identified by DEIsoM. In the case of FGFR2 (ranked 62 of 11 950 genes), the FGFR2-IIIb isoform is downregulated and has been related to HCC aggressive growth, while the FGFR2-IIIc isoform is expressed at the same level in normal and HCC tissues  (Amann et al., 2010) . All three isoforms of survivin (ranked 120 of 11950 genes), survivin normal, survivin 2B and survivin Delta Ex3 have been detected in well, moderately and poorly differentiated HCC but none of these are found in normal tissues  (Takashima et al., 2005) . RT-PCR results are available for ADAMTS13 (ranked 201 of 11950 genes) showing differences in the expression of three known isoforms (WT and 1, 2) between normal liver tissue and hepatoma cell lines  (Shomron et al., 2010) . For CD44 (ranked 607 of 11950 genes), CD44-v6 is up-regulated in HCC, while CD44 standard form remains stable  (Zhang et al., 2010) .  To more clearly understand the performance of different methods, we also examine the overlapping DE genes in the top 200 lists from the compared methods. Supplementary Table S2 shows the overlapping DE genes by feeding the FPKM of all isoforms from each method to EBSeq. This tests the quantification similarity between any two methods. According to the number of overlapping DE genes, the quantification performance of RSEM and BitSeqVB are the most similar, followed by RSEM and DEIsoM. Supplementary Table S3 shows the overlapping DE genes using the DE evaluation methods of their own. This tests the performance of both the quantification and DE identification. After changing the DE evaluation method, the number of overlapping DE genes between RSEM and BitSeqVB decreases from 96 to 62, while this number between RSEM and DEIsoM decreases from 74 to 14, which suggests that KL divergence performs differently from PPDE or PPLR. PPDE and PPLR are only sensitive to the absolute abundance change of an isoform, while KL divergence is sensitive to the overall isoform fractional pattern change within a gene, not limited to the absolute abundance change. This is useful in searching isoform switching events in many cases.     5 Discussion\r\n  In contrast to the models that treat each biological replicate separately, DEIsoM incorporates all biollogical replicates in one seamless framework. By capturing the shared information across multiple biological replicates, DEIsoM achieves a higher prediction accuracy and inter-replicate consistency than the alternative methods in the simulation studies (Section 3.1, 3.3, 3.4). This shared information comes from the intrinsic fact that all the replicates in one condition share the same underlying biological mechanism. As described in model construction (Section 2.1), we use a Dirichlet prior to represent a base fraction, which is characterized by the hyperparameter a and learned from data, and then sample the instance-specific fraction for each replicate. The fractions for different replicates are not necessarily the same, because we allow some within-condition variance, however, those fractions retain underlying coherence since they are sampled from the same Dirichlet prior (or the base fraction). In addition, as the conjugate prior for the multinomial distribution, the Dirichlet prior enables close form, efficient updates in our VB inference, which greatly benefits the computation. Furthermore, faster computing speed is gained using the VB algorithm, instead of the MCMC sampling used in MISO, during the inference step. The VB method converts a sampling problem to an optimization problem and speeds up the estimation (Section 3.2). DEIsoM is also promising in real applications. On the HCC dataset, by PCA plotting, we find that the normal and tumor samples can be linearly separated by the estimated expression levels of PCR verified DE genes, suggesting an accurate quantification of DE isoforms in DEIsoM. Using read coverage visualization, we find that the DEIsoM KL divergence is capable of identifying isoforms whose read coverage patterns change, and does not give false positive results for isoforms with low read abundance in both conditions. This property is desirable in practice, since a low number of reads causes a large uncertainty in estimation. In DEIsoM, the posterior distributions of both conditions are close to the uniformly distributed prior if the read number is low, which reduces the KL divergence between the two conditions. However, neither Cuffdiff nor RSEMEBSeq will automatically prune such isoforms (Section 4.2, 4.3). Moreover, a great number of isoforms predicted to be DE by DEIsoM are supported by the biological literature, providing encouraging results for real applications.  However, there are still some improvements that could be incorporated into DEIsoM. First, DEIsoM builds on the approach of MISO, which considers the quantification of isoforms gene by gene. In order to handle the reads multi-mapped to different gene loci, we have also added a variant version of DEIsoM that simultaneously considers all transcript isoforms, rather than performing a gene by gene analysis. This enhancement will allow the inclusion of multiply mapped reads into the analysis. However, the KL divergence is not applicable to this version, since KL divergence measures the isoform pattern change within a gene. Second, the KL divergence as a DE evaluation method is not based on a hypothesis test, but rather on the difference of the posterior distributions of fractional isoform expression between two conditions, so it only provides a rank instead of P-values to infer 'significantly' DE genes. However, KL divergence is sensitive to the overall isoform pattern change within a gene, and more differentiable for ranking isoforms/genes than P-values, which tend to give the same rank to many genes. DEIsoM allows the estimated isoform levels to be reported as FPKM, thereby allowing P-values to be calculated by many existing differential expression analysis methods. Lastly, DEIsoM assumes a known reference genome/transcriptome and the uniform read distribution. The misannotation or the non-uniformity of the read data may compromise the estimation accuracy in DEIsoM. We are considering including the novel isoform construction and the modeling of nonuniformly distributed read data into our future versions.    6 Conclusion\r\n  We propose a hierarchical Bayesian model, DEIsoM, for detecting DE isoforms using multiple biological replicates from two conditions. DEIsoM captures the information shared across replicates, and provides fast and accurate prediction compared to alternative methods in simulations. On the HCC real dataset, the estimated expression levels of PCR verified DE genes can be used as features to separate the tumor samples from their matched normal samples in PCA plots; read coverage visualization confirms that DEIsoM KL divergence is capable of identifying DE isoforms. DEIsoM is relatively resistant, compared to alternative methods, to identifying isoforms with low read abundance in both conditions as DE. Biological literature review suggests that the DE isoforms selected by DEIsoM have high relevance to HCC.    Acknowledgements\r\n  We would like to thank Eli Lilly and company for sharing the HCC data and providing the very helpful discussions.    Funding\r\n  This work was supported by NSF CAREER Award IIS-1054903, and the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/hao-peng/DEIsoM",
    "publicationDate": "0",
    "authors": [
      "Hao Peng",
      "Yifan Yang",
      "Shandian Zhe",
      "Jian Wang",
      "Michael Gribskov",
      "Yuan Qi"
    ],
    "status": "Success",
    "toolName": "DEIsoM",
    "homepage": ""
  },
  "18.pdf": {
    "forks": 1,
    "URLs": [
      "github.com/ryusukemomota/nanatex",
      "www.med.umich.edu/lrc/coursepages/m1/anatomy2010/html/anatomytables/"
    ],
    "contactInfo": ["momo@okayama-u.ac.jp"],
    "subscribers": 3,
    "programmingLanguage": "",
    "shortDescription": "Network of Anatomical Texts",
    "publicationTitle": "Network of anatomical texts (NAnaTex), an open-source project for visualizing the interaction between anatomical terms",
    "title": "Network of anatomical texts (NAnaTex), an open-source project for visualizing the interaction between anatomical terms",
    "publicationDOI": "10.1007/s12565-017-0410-1",
    "codeSize": 458,
    "publicationAbstract": "Anatomy is the science and art of understanding the structure of the body and its components in relation to the functions of the whole-body system. Medicine is based on a deep understanding of anatomy, but quite a few introductory-level learners are overwhelmed by the sheer amount of anatomical terminology that must be understood, so they regard anatomy as a dull and dense subject. To help them learn anatomical terms in a more contextual way, we started a new open-source project, the Network of Anatomical Texts (NAnaTex), which visualizes relationships of body components by integrating text-based anatomical information using Cytoscape, a network visualization software platform. Here, we present a network of bones and muscles produced from literature descriptions. As this network is primarily text-based and does not require any programming knowledge, it is easy to implement new functions or provide extra information by making changes to the original text files. To facilitate collaborations, we deposited the source code files for the network into the GitHub repository (https://github.com/ryusukemomota/nanatex) so that anybody can participate in the evolution of the network and use it for their own nonprofit purposes. This project should help not only introductory-level learners but also professional medical practitioners, who could use it as a quick reference.",
    "dateUpdated": "2017-07-28T11:20:37Z",
    "institutions": [
      "& Ryusuke Momota",
      "Dentistry and Pharmaceutical Sciences"
    ],
    "license": "No License",
    "dateCreated": "2017-02-23T00:53:55Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     10.1007/s12565-017-0410-1   Network of anatomical texts (NAnaTex), an open-source project for visualizing the interaction between anatomical terms     Ryusuke Momota  momo@okayama-u.ac.jp  0  1    Aiji Ohtsuka  0  1    GitHub  0  1    0  &amp; Ryusuke Momota    1  Human Morphology, Okayama University Graduate School of Medicine, Dentistry and Pharmaceutical Sciences ,  2-5-1, Shikata-cho, Kita, Okayama 700-8558 ,  Japan      16  7  2017    5  4  2017     Anatomy is the science and art of understanding the structure of the body and its components in relation to the functions of the whole-body system. Medicine is based on a deep understanding of anatomy, but quite a few introductory-level learners are overwhelmed by the sheer amount of anatomical terminology that must be understood, so they regard anatomy as a dull and dense subject. To help them learn anatomical terms in a more contextual way, we started a new open-source project, the Network of Anatomical Texts (NAnaTex), which visualizes relationships of body components by integrating text-based anatomical information using Cytoscape, a network visualization software platform. Here, we present a network of bones and muscles produced from literature descriptions. As this network is primarily text-based and does not require any programming knowledge, it is easy to implement new functions or provide extra information by making changes to the original text files. To facilitate collaborations, we deposited the source code files for the network into the GitHub repository (https://github.com/ryusukemomota/nanatex) so that anybody can participate in the evolution of the network and use it for their own nonprofit purposes. This project should help not only introductory-level learners but also professional medical practitioners, who could use it as a quick reference.    Anatomical terms  Cytoscape  Network analysis  Open source       Introduction\r\n  Network analysis is an area of mathematics which examines the structure of the relationships and complex interactions among multiple components. Such analyses have been used in many fields, such as the social sciences, and have often revealed unexpected aspects of the issues and helped developing new strategies  (Manrique et al. 2016) . In biology, network analyses of molecular interactions have been carried out to find molecules or pathways that could potentially be exploited for pharmaceutical purposes  (Chen et al. 2016; Zhang et al. 2016) . Anatomical network analysis (AnNA) has recently been introduced for use in gross anatomical studies. The resulting detailed regional anatomical observations have demonstrated the modularity of the musculoskeletal system and have allowed the morphological changes that have occurred over the course of evolution to be elucidated, as well as the morphological changes associated with a particular pathological condition  (Esteve-Altava et al. 2015; Diogo et al. 2015) .  Due to the availability of advanced information technology, text-based anatomical information has become abundant. Indeed, there is so much of this information that it can overwhelm introductory-level learners of anatomy. Therefore, we attempted to integrate information on the bone and muscular interactions of the human body that is available in literature descriptions using Cytoscape, a network visualization software platform. Cytoscape was originally created to visualize biological molecular interactions, but it has since evolved to become a platform for performing any network analysis due to its user-friendly interface and the ability to extend its functionality with many apps   (Ono 2016; Bader et al. 2017; Shannon et al. 2003 ). In our analysis, we obtained a network consisting of 196 nodes with 1048 muscular interactions. Important components in the network were identified. Combined with the functions of Cytoscape, this network can be employed as a very useful learning resource and a quick graphical reference by simply adding more text information. To encourage contributions to this network and thus promote its development, we deposited the relevant files in the GitHub repository (https://github.com/ryusukemomota/nanatex).    Materials and methods\r\n  We found that the tables in the webpages of the University of Michigan Medical School were a very good resource, as they were packed with succinctly summarized anatomical bones, are shown as pink nodes. Abbreviations used to label nodes are defined in the ESM information (http://www.med.umich.edu/lrc/coursepages/m1/anatomy2010/html/anatomytables/). Therefore, we obtained the permission of Dr. Thomas Gest (Texas Tech University Health Sciences Center, Paul L. Foster School of Medicine) to use that information in this project. To prepare the data set, the tabulated data were extracted from the HTML files and transferred to Excel 2016 (Microsoft) for further manipulation. The ''Origin'' and ''Insertion'' columns were duplicated and renamed to create ''Origin1'' and ''Insertion1'' nodes, respectively. We used a simplified model in which each body part is assumed to represent a single node, so we extracted names of body parts and removed detailed descriptions of muscle attachment sites. We then unified terms by removing synonyms (e.g., ''C1 for atlas'' and ''C2 for axis''). For muscles with multiple origins/insertions such as ''rotatores'' or ''splenius,'' we visually confirmed multiple ''origin-insertion'' relationships using a MeAV Anatomie 3D system (Panasonic is indicated by both the size (small indicates a low value) and the color (which ranges from dark red, corresponding to a low value, to dark blue, corresponding to a high value) of the circle marking that node Corporation and Okayama University), and generated a new row for each ''origin-insertion'' interaction. Thus, we obtained a table with 1048 muscles defined by ''origininsertion'' interactions. The resulting xlsx file was imported and visualized by Cytoscape 3.5. We assumed that each muscle was defined by a record (row) defined by fields (columns). The fields ''Origin1,'' ''Insertion1,'' and ''Muscle'' were used as ''Source Node,'' ''Target Node,'' and ''Edge,'' respectively. The unmodified ''Origin'' and ''Insertion'' columns as well as the other columns, such as ''Innervation,'' ''Artery,'' and ''Notes,'' were also included as ''Source Attribute,'' ''Target Attribute,'' and ''Edge Attribute,'' respectively (see the Electronic supplementary material, ESM: NAnaTex.xlsx).    Results and discussion\r\n  Figure 1 shows a screenshot of the network displayed in the ''Organic'' style of yFiles Layouts. The central dense part mainly consists of vertebrae and ribs, suggesting that the bones and muscles of the trunk are tightly connected. As expected, network analyses performed with NetworkAnalyzer (release 2.7)  (Assenov et al. 2008)  revealed that this dense region is a network consisting of 43 nodes with 612 edges and a higher average number of neighbors (16.23) than that for the whole network (6.71), indicating that the bones and muscles of the trunk are tightly connected and form a closed network. According to betweenness analysis, the ilium, humerus, pubis, and fifth rib showed the highest values in the whole network (Fig. 2 and the ESM). In the trunk network alone, the nodes for the ilium, fifth rib, and the muscle between them showed the highest values, indicating that these components are important in the trunk network.  We used the same labels as listed in the Wikipedia human skeleton section. Users can access Wikipedia for more information through ''External Links.'' In addition, users can look up words using the search window, which can be used to query not only the items on the screen but also the ''Edge Attribute'' descriptions hidden in the original text. For example, if a user looks for muscles innervated by the ''radial nerve,'' the muscles of interest are highlighted on the screen (Fig. 3), as well as other nerves, arteries, and actions of the muscles. The graphical presentation of information helps users to grasp anatomical terminology; this is especially beneficial to introductory-level learners. Furthermore, since this network can be easily enhanced by adding more fields or text, it could be utilized as a platform for collaborative learning in anatomical curricula in order to promote active learning and understanding of anatomy; for instance, by encouraging students to implement new categories of related information such as diseases or physiology. As an example of how the contents of the network can be enhanced, we generated a JSON network file with information on the Foundational Model of Anatomy identifiers (FMA ID) and links to the FMA ontology data in Ontobee, a resource that integrates anatomical information based on terms and concepts (http://www.ontobee.org/ontology/FMA) ( (Rosse and Mejino 2003; Ong et al. 2017) . Currently, we are in the process of evolving this preliminary network into a more informative one with a more attractive appearance, with the aim being to start a web service. We welcome you to use it and to develop it for your own projects. To this end, we deposited the relevant files and started an open-source project called NAnaTex (Network of Anatomical Texts) in the GitHub repository. Contributions to NAnaTex from other anatomists are encouraged, as they will cause NAnaTex to evolve. Acknowledgements We are grateful to Professor Thomas R. Gest (Texas Tech University Health Sciences Center, Paul L. Foster School of Medicine) for allowing us to use the tables in our project. This work was supported by the Okayama Medical Foundation. Compliance with ethical standards Conflict of interest The author declares no conflict of interest.    ",
    "sourceCodeLink": "https://github.com/ryusukemomota/nanatex",
    "publicationDate": "0",
    "authors": [
      "Ryusuke Momota",
      "Aiji Ohtsuka",
      "GitHub"
    ],
    "status": "Success",
    "toolName": "nanatex",
    "homepage": ""
  },
  "78.pdf": {
    "forks": 2,
    "URLs": [
      "github.com/woolfson-group/",
      "pypi.python.org/pypi/isambard/"
    ],
    "contactInfo": [
      "d.n.woolfson@bristol.ac.uk",
      "chris.wood@bristol.ac.uk"
    ],
    "subscribers": 10,
    "programmingLanguage": "Python",
    "shortDescription": "Intelligent System for Analysis, Model Building And Rational Design.",
    "publicationTitle": "ISAMBARD: an open-source computational environment for biomolecular analysis, modelling and design",
    "title": "ISAMBARD: an open-source computational environment for biomolecular analysis, modelling and design",
    "publicationDOI": "10.1093/bioinformatics/btx352",
    "codeSize": 9267,
    "publicationAbstract": "Motivation: The rational design of biomolecules is becoming a reality. However, further computational tools are needed to facilitate and accelerate this, and to make it accessible to more users. Results: Here we introduce ISAMBARD, a tool for structural analysis, model building and rational design of biomolecules. ISAMBARD is open-source, modular, computationally scalable and intuitive to use. These features allow non-experts to explore biomolecular design in silico. ISAMBARD addresses a standing issue in protein design, namely, how to introduce backbone variability in a controlled manner. This is achieved through the generalization of tools for parametric modelling, describing the overall shape of proteins geometrically, and without input from experimentally determined structures. This will allow backbone conformations for entire folds and assemblies not observed in nature to be generated de novo, that is, to access the 'dark matter of protein-fold space'. We anticipate that ISAMBARD will find broad applications in biomolecular design, biotechnology and synthetic biology. Availability and implementation: A current stable build can be downloaded from the python package index (https://pypi.python.org/pypi/isambard/) with development builds available on GitHub (https://github.com/woolfson-group/) along with documentation, tutorial material and all the scripts used to generate the data described in this paper. Contact: d.n.woolfson@bristol.ac.uk or chris.wood@bristol.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-10-14T04:48:53Z",
    "institutions": [
      "University of Bristol",
      "University of Glasgow"
    ],
    "license": "MIT License",
    "dateCreated": "2016-08-10T10:37:06Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx352   ISAMBARD: an open-source computational environment for biomolecular analysis, modelling and design     Christopher W. Wood  1  2    Jack W. Heal  2    Andrew R. Thomson  2  3    Gail J. Bartlett  2    Amaurys A. Ibarra  1    R. Leo Brady  1    Richard B. Sessions  0  1    Derek N. Woolfson  0  1  2    0  BrisSynBio, University of Bristol ,  Bristol BS8 1TQ ,  UK    1  School of Biochemistry, University of Bristol ,  Bristol BS8 1TD ,  UK    2  School of Chemistry, University of Bristol ,  Bristol BS8 1TS ,  UK    3  School of Chemistry, University of Glasgow ,  Glasgow G12 8QQ ,  UK     2017   1  1  8   Motivation: The rational design of biomolecules is becoming a reality. However, further computational tools are needed to facilitate and accelerate this, and to make it accessible to more users. Results: Here we introduce ISAMBARD, a tool for structural analysis, model building and rational design of biomolecules. ISAMBARD is open-source, modular, computationally scalable and intuitive to use. These features allow non-experts to explore biomolecular design in silico. ISAMBARD addresses a standing issue in protein design, namely, how to introduce backbone variability in a controlled manner. This is achieved through the generalization of tools for parametric modelling, describing the overall shape of proteins geometrically, and without input from experimentally determined structures. This will allow backbone conformations for entire folds and assemblies not observed in nature to be generated de novo, that is, to access the 'dark matter of protein-fold space'. We anticipate that ISAMBARD will find broad applications in biomolecular design, biotechnology and synthetic biology. Availability and implementation: A current stable build can be downloaded from the python package index (https://pypi.python.org/pypi/isambard/) with development builds available on GitHub (https://github.com/woolfson-group/) along with documentation, tutorial material and all the scripts used to generate the data described in this paper. Contact: d.n.woolfson@bristol.ac.uk or chris.wood@bristol.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  Generally, the three-dimensional structures of biomolecules determine their functions. The computational design of such structuresand proteins in particular-tests and advances our understanding of biomolecular folding and assembly, and paves the way to constructing entirely new biomolecules with applications in biotechnology and synthetic biology. Here we present a new suite of computational tools, which we call ISAMBARD (Intelligent System for Analysis, Model Building And Rational Design), to aid the rational de novo design of biomolecular structures and assemblies, and for the in silico assessment of the resulting design models. The overall aims of ISAMBARD are to provide easy-to-use tools for the parametric design of such structures, and, thus, to enable a wider group of both expert and non-expert computational and experimental users to engage in the design process.  Several approaches are taken in protein design  (Huang et al., 2016; Porebski and Buckle, 2016; Regan et al., 2015; Woolfson et al., 2015) : In protein redesign, natural proteins are used as starting points and engineered to introduce desired structural, stability, or functional properties. This is guided intuitively, or, increasingly, computationally. In rational de novo protein design, chemical and physical principles, and biochemical rules of thumb for protein folding are combined to make initial designs, which are improved by iteration. In computational design, de novo sequences are built in silico onto protein backbones, which can be static or have some flexibility, to deliver multiple sequences for experimental testing.  A number of approaches to computational protein design have yielded success  (Huang et al.,2016; MacDonald and Freemont, 2016; Woolfson et al., 2015) . Initial efforts involved sequence-based redesign, where designs are generated by packing new sequences onto a backbone scaffold from a known protein structure  (Dahiyat and Mayo, 1997) . Building on this, a degree of backbone flexibility can be introduced using fragment-based design, where regions of known protein structure are combined together to form new backbone models. The most successful implementation of this method is in Rosetta  (Das and Baker, 2008) , a macromolecular modelling package, which has been central to many de novo designs including the novel fold Top7 and, more recently, de novo repeat proteins  (Doyle et al., 2015; Kuhlman et al., 2003) . Extensions of the fragment-based methodology are being actively developed  (Jacobs et al., 2016; Lapidoth et al., 2015) .  By definition, fragment-based methods are restricted, sampling only structural space observed in experimentally determined, and usually of just natural protein structures. If we are to exploit the full universe of possible protein structures, other backbone sampling methods must be pursued  (Taylor et al., 2009; Woolfson et al., 2015) . However, there are several obstacles in the way of achieving this. Most notably, the sequence and conformational spaces available to even modestly sized biomolecules are vast, and, indeed, impossible to search exhaustively. One way to reduce this complexity is to simplify the way in which biomolecular structures are described; namely, to parameterize the design target mathematically. In turn, these parametric descriptors can be used to focus the search of structural space for the backbone. Amino-acid sequences can then be tested on the resulting scaffolds, and the whole system optimized to deliver candidate solutions to a specified design problem.  Certain folds are conducive to parameterization, such as a-helical coiled coils, due to their regular structures and well-understood sequence-to-structure relationships  (Fletcher et al., 2012; Harbury et al., 1993, 1994; Woolfson et al., 2012; Woolfson, 2005) . a-Helical coiled coils are bundles of two or more a helices that invariably wrap (or supercoil) around a common axis. The helices can be arranged in parallel, antiparallel or mixed topologies, and the assemblies can be homo- or hetero-oligomers  (Lupas and Gruber, 2005) . Despite this diversity, a-helical coiled coils are the simplest and best-understood examples of geometrically regular protein structures, making them clear targets for parametric modelling and design. The original mathematical parameterization of these is from Crick  (Crick, 1953) , and has been developed since  (Offer et al., 2002) , including in CCCP  (Grigoryan and Degrado, 2011)  and CCBuilder  (Wood et al., 2014) , which are web-based applications for parametric modelling of coiled coils. These modelling methods have been applied by us and by others to design a range of a-helical coiled coils and bundles  (Grigoryan et al., 2011; Harbury et al., 1995; Huang et al., 2014; Thomson et al., 2014) .  The structural modelling methodology that we have applied to design a-helical barrels required an extension of CCBuilder, called CCScanner, which automatically fitted structural parameters for a given sequence  (Thomson et al., 2014) . However, this was a bespoke solution for the parametric modelling of coiled coils. Here, we present the ISAMBARD (Intelligent System for Analysis, Model Building and Rational Design) software package, which generalizes this modelling methodology, allowing it to be applied to the design of any parameterizable protein fold, whether all-a helix, all-b strand, mixed a/b structures, or those employing less-common secondary structures. ISAMBARD is an open-source Python package with a suite of tools for biomolecular structure analysis, protein design, model building and evaluation. ISAMBARD is modular, extendable, open source and freely available.  ISAMBARD provides a framework for atomistic model building and validation of truly de novo biomolecular structures  (Woolfson et al., 2015) . Scoring methods are built-in for assessing model quality, and optimization techniques allow rapid exploration of structural and sequence space in tractable time. Here, we demonstrate that ISAMBARD is capable of accurately modelling a range of diverse protein folds using generalized and reusable mathematical parameterizations.    2 Materials and methods\r\n  All biomolecules in ISAMBARD are represented using the AMPAL (Atom, Monomer, Polymer, Assembly, Ligand) framework. This is a formal representation of biomolecules in a hierarchical structure of lightweight Python objects. Its object-oriented implementation is intuitive to use and enables facile navigation through the protein structure in both directions, i.e. from the assembly to the atomic level and vice versa.  AMPAL objects are used in ISAMBARD to represent proteins, nucleic acids, and a more-general ligand class that is currently used for every other molecule. There are a range of tools built into these objects, which allows for straightforward structural analysis, validation and manipulation.  Figure 1 shows the structure of the AMPAL framework and its built-in inheritance pattern. This enables core functionality to be reused, making it simpler for users to create custom classes for other biomolecules.   2.1 Parametric model building\r\n  ISAMBARD has been created to aid parametric protein design by providing a general approach for modelling any parameterizable protein fold. In order to design protein folds de novo, one must choose from a set of amino acids and connect them in space according to a set of rules, in an approach analogous to that followed by a building constructor using an architect's design or specification. Therefore, we have introduced the specification object, as an extension of the AMPAL framework (Fig. 1). A specification contains instructions for building a model according to a set of input parameter values. These instructions form the parameterization of the model. Specifications can be defined at both the Polymer and Assembly level of the AMPAL framework (monospaced text indicates an ISAMBARD class). The parameters in Polymer specifications dictate how to arrange Monomers into a single chain; at the Assembly level, they detail the arrangement of Polymers with respect to each other.   2.1.1 Specifications at the polymer level\r\n  Each Residue in a Polypeptide contains an a-carbon atom, and the running average of the positions of these atoms traces a path in 3D space. Polypeptide specifications use parameters that define a path for this running average to follow. When the model is built, Residues are joined together accordingly. The paths, and therefore the Polypeptides, are described mathematically by a small number of simple parameters. For example, the Helix specification allows any type of polypeptide helix to be built, e.g. a helix, polyproline type-II helix, etc.; whereas, the HelicalHelix specification takes a Helix specification and adds a supercoil to it with input parameter values for radius and pitch of the superhelix. In this way, a path is defined along which a polypeptide segment is built. Moreover, multiple segments with different Helix and HelicalHelix specifications can readily be combined in the same design (see the Assembly specification below). As indicated, these specifications are implemented generally, such that secondary structure types including a-, collagen- (viz., polyproline type-II-) and phelices can be built along any well-defined path. It is worth reemphasizing at this point that these parameters are not reliant on structural data from natural proteins, they are built using idealized geometric models.  An alternative building-mode specification is embodied in TAPolypeptide, which generates a Polypeptide from a set of backbone torsion angles. Backbone bond lengths and bond angles can be specified if desired, otherwise default values are used  (Schulz and Schirmer, 1979) . Again, this lends itself to the design of structures that are not found in nature, but, nonetheless, are physically feasible, as they can be informed by the allowed regions of Ramachandran plots.    2.1.2 Specifications at the assembly level\r\n  Specifications at the Assembly level are relatively abstract, and are not constrained to describing a particular protein topology, architecture or even class. Three examples of specifications at the Assembly level are given in Supplementary Figures S1-S4. They describe the paths that secondary structure follows, and the same specification can be used to describe a range of folds. For example, the CoiledCoil specification can produce models of coiled coils in any oligomer state with any orientation of helices. Furthermore, the same specification can be used to describe the structure of the collagen triple helix.  Up to this point, the building process uses glycine as default residues, essentially generating a backbone-only model. Once this backbone for the target structure has been specified, side-chain atoms are modelled using SCWRL4  (Krivov et al, 2009) , which uses a backbone-dependent rotamer library and a fast anisotropic hydrogen bonding function to optimize side-chain packing.    2.1.3 Model evaluation\r\n  The main method for assessing the quality of the model uses BUFF (Bristol University Docking Engine Force Field). BUFF is a standalone implementation of the all-atom force field from BUDE (Bristol University Docking Engine)  (McIntosh-Smith et al., 2012, 2014) , which is an empirical free-energy force field originally designed to predict the free energies of binding between proteins and ligands.  BUFF is implemented with code written in C þþ and Python, with communication between these achieved by a layer of Cython  (Behnel et al., 2011) . The Cython layer allows for direct interaction with various elements of the force field using a Python interface, which is useful when prototyping design protocols, but it retains most of the speed of the original BUDE implementation. This also allows the force-field parameters to be directly accessible to the user, and modifiable for a particular application.  Other metrics are also available for assessing design quality, such as evaluating the overall geometry of the protein; for example, we have included a measure of helical strain, which assesses how far from ideal geometry a helix undergoing design is. Moreover, the modular and open nature of ISAMBARD enables and encourages users to import and apply other force fields and methods of evaluation. This is facilitated by the Python ecosystem in general, which contains a range of existing packages for protein design and modelling, such as OpenMM, PyRosetta and Modeller  (Chaudhury et al., 2010; Eastman et al., 2013; Eswar et al., 2006) .    2.1.4 Parameter optimization\r\n  The size of structural space grows exponentially with the number of parameters used to describe it. This prohibits the exhaustive exploration of space in most cases. So-called metaheuristics help address this, providing means of efficiently searching the defined parameter space to find near-optimal solutions  (Bianchi et al., 2008) . A range of metaheuristics have been implemented in ISAMBARD using modified elements of the DEAP evolutionary computation framework  (Fortin et al., 2012) , including a genetic algorithm, particleswarm optimization, differential evolution and covariance matrix adaptation evolutionary strategy. These different methods enable efficient exploration of structural space for a given specification and provide an estimate of energetic minima.  The choice of optimizer is up to the user: different optimizers will be better suited to different problems. For the examples described herein, we found that the differential evolution method performed very well. Further work to benchmark each of the optimization strategies is underway in our laboratory, and is beyond the scope of this report.  Once a fold has been parameterized, minimal human intervention is required: the optimizer fits a broad range of parameter values from the specification and delivers the best models according to the user-defined fitness function. For protein design, this is usually an all-atom scoring function, but any metric can be applied by the user.     2.2 Specification accuracy testing\r\n  To test the robustness of models produced using ISAMBARD, several protein folds were parameterized. The geometric parameterizations were tested by rebuilding natural structures that exhibited a wide range of parameters. During the rebuild, we used the root-mean-square deviation (RMSD) between the experimentally determined structure and models produced to drive the parameter optimization. This process validates whether the simple geometric parameterization has the capacity to recreate accurately observed examples of the protein fold, and thus lends confidence to modelling de novo structures. Three classes of protein were modelled: a-helical coiled coils, collagen/collagen-like peptides, and Ankyrin-like repeat proteins.  We used the differential evolution optimizer in ISAMBARD to fit the parameters for a given sequence. The scoring metric used was the RMSD between the target structure and the model as calculated  Interface angle range ( ) 20 to 20 20 to 20 20 to 20 20 to 20 by the McLachlan algorithm  (McLachlan, 1982)  as implemented in the program ProFit (Martin, A.C.R., http://www.bioinf.org.uk/soft ware/profit/).  Coiled coils were modelled using the CoiledCoil class, with the from_parameters class method, using the parameter ranges described in Table 1. Optimization was performed over 50 generations, with 20 models in each, for a total of 1020 models including the parent generation.  Collagen structures were also parameterized using the CoiledCoil class, with the tropocollagen class method. Hydroxyproline in the crystal structures was converted to proline to allow side-chain packing and structural alignment. The gross structural properties and therefore the parameterization of the fold are not affected by this change. Collagen was modelled with radii range of 1.55.5 A˚ ; pitches in the range of 25-105 A˚; unrestricted interface angles; a z-shift range for each helix of 0.0-6.2 A˚ staggered relative to each other; and a rotational offset -30 to 30 for each helix. Optimization was performed over 50 generations, with 30 models in each, for a total of 1530 models including the parent generation.  Models of Ankyrin-like peptides were built using the HelixPair class to generate the repeating unit and the Solenoid class to apply helical symmetry. The repeating unit was modelled with radii in the range of 0.0-6.0 A˚ , z-shifts in the range of -6.0 to 6.0 A˚ , unrestricted helical rotation, in-plane rotations in the range -45 to 5 and out-ofplane rotation range 90 -270 . Optimization was performed over 50 generations, with 50 models in each, for a total of 2550 models including the parent generation. The optimized repeating unit was used to model the solenoid with a radius range of 25.0-45.0 A˚, rise per repeats in the range 2.0-18.0 A˚ , unrestricted twist range. The repeat unit was allowed unrestricted rotation during optimization. Optimization was performed over 100 generations, with 40 models in each, for a total of 4040 models including the parent generation.  The solenoid model of the TAL effector protein bound to DNA was built using the same base method described above, however the Solenoid class was given radii in the range 10.0-30.0 A˚ , rise per repeat values in the range 2.0-18.0 A˚ , unrestricted twist range. The repeat unit was allowed unrestricted rotation during optimization. Optimization was performed over 50 generations, with 20 models in each, for a total of 2040 models including the parent generation. The model of DNA was built using the DNADuplex class, and manually aligned, using tools included in ISAMBARD, with the solenoid to match the phase of the DNA and protein model. The final model was aligned with the experimentally determined structure, using ProFit, based solely on the protein region. 2.2.1 RMSD100 In order to compare the quality of fit across a range of individual protein structures of different sizes, we calculated the RSMD100 value  (Carugo and Pongor, 2001)  using the following equation: RMSD100 ¼     3 Results\r\n  3.1 Specifications in ISAMBARD accurately recreate natural structures using parametric models We tested our generalized parametric modelling in ISAMBARD by rebuilding a range of natural structures. The protein folds selected were a-helical coiled coils, collagen triple helices and Ankyrin-like repeats, as these are readily parameterizable and are of interest to the protein design and broader communities  (Huang et al., 2014; Jalan et al., 2014; Parmeggiani et al., 2015; Plu¨ ckthun, 2015; Thomson et al., 2014) . Figure 3 shows that each of these folds have been successfully captured in ISAMBARD through two specifications: CoiledCoil and Solenoid.   3.1.1 Coiled coils\r\n  The Crick equations  (Crick, 1953)  provide a parametric description of a-helical coiled coils. Previously, these have been successfully implemented for model building and protein design  (Grigoryan and Degrado, 2011; Harbury et al., 1995, 1998; Huang et al., 2014; Offer and Sessions, 1995; Ra\u20acmisch et al., 2015; Thomson et al., 2014; Wood et al., 2014) . Coiled-coil modelling has been implemented differently in ISAMBARD, using a more-general approach where the mathematics describing secondary structure is separated from that that describes the overall quaternary structure. This is vital for the modularity and re-usability of the parameterizations, and allows a wide array of different protein folds to be described using the same fundamental tools. Distinct secondary structure types are defined using the same specifications at the Polymer level. The Assembly level is independent of the Polymer-level specification, and so can be applied to different secondary structures types to yield different protein folds. For example, the CoiledCoil specification is used to model both a-helical coiled coils and collagens (see below). To test if the CoiledCoil specification accurately generated the degrees of freedom observed in experimentally determined X-ray crystal structures of coiled coils, the following selection of parallel coiled-coil assemblies was recreated in ISAMBARD.  We searched the CCþ database for non-redundant, homomeric, parallel coiled coils in oligomer states ranging from 2 to 5  (Testa et al., 2009) , requiring that each structure contain at least 45 residues in order to apply the RMSD100 normalization function  (Carugo and Pongor, 2001) . This yielded 113 structures for rebuilding in ISAMBARD (Fig. 2).  The structural optimizer was initialized with the CoiledCoil specification, the amino-acid sequence and the oligomeric state of the structure being rebuilt as well as the three structural parameters (radius, pitch and uCa, Supplementary Fig. S2), which were optimized.  For each of the 113 structures, the values for each of the 3 parameters converged within 1020 models. The overall modelling accuracy was excellent, with a mean backbone RMSD of 0.64 A˚ (r ¼ 0.24 A˚ , n ¼ 113). This shows that the parameterization contained in the CoiledCoil specification is sufficient to accurately model coiled coils, even though it describes the assembly using only 3 structural parameters, none of which need to be derived in the first instance from existing protein structures. This is an improvement over modelling with CCBuilder  (Wood et al., 2014) , which gave an average backbone RMSD of 0.74 A˚ (r ¼ 0.45 A˚ , n ¼ 113) for the same selection of coiled coils, and compares favourably with alternative coiled-coil modelling methodologies  (Grigoryan and Degrado, 2011; Wood et al., 2014) . Thus, in our experience, the CoiledCoil specification in ISAMBARD is now the most accurate tool available for building parametric models of coiled coils.    3.1.2 The collagen triple helix\r\n  The level of abstraction in the CoiledCoil specification means that it can be used directly to build models of collagen. This is because the gross geometry of collagen is similar to a coiled-coil trimer, although each component helix is a polyproline type-II helix rather than an a helix. An additional structural parameter, z-shift, is required to describe relative offset of the component helices along the long axis of the collagen molecule, which creates a leading and a lagging strand  (Shoulders and Raines, 2009) .  A set of 9 representative, high-resolution crystal structures of collagen and collagen-like peptides was selected from the PDB and then their structures modelled using ISAMBARD. The parameterization accurately captured the backbone of the structures, with a mean backbone RMSD100 score of 1.31 A˚ (r ¼ 0.44 A˚, n ¼ 9) (Fig. 3, Supplementary Fig. S5, Supplementary Table S1). The difference between the best model and the worst was narrow, for example, RMSD100 score of 1.08 A˚ (3pob) and 1.57 A˚ (1cag).  The mean score was higher than for coiled-coil trimers, which had a mean value of 0.50 A˚ , (r ¼ 0.20 A˚ , n ¼ 41). This is most likely due the overall flexibility of the collagen fold due to the broader energy well of the polyproline type-II helix  (Kuster et al., 2015) . Further on this, the poorest areas of alignment were found at the N and C termini of the component polypeptides, where fraying of the X-ray crystal structures of the collagen fibres occurred. This is not observed to the same extent in coiled coils, and cannot easily be captured by parametric models. However, these models are still very accurate, and, to our knowledge, this is the only general method available for easily and rapidly generating atomistic models of the collagen triple helix backbone. The facile exploration of the collagen structural space through ISAMBARD may prove to be useful and complementary to existing methods of automated computational design of collagen fibres, which use a combination of discrete sequence-based models and geometric information from natural collagen fibres  (Xu et al., 2010, 2011) .    3.1.3 Ankyrin-like repeat proteins\r\n  Ankyrin-repeat proteins were selected as representative examples of a solenoids as there are several experimentally determined structures. Furthermore, recent designs of artificial Ankyrin-like repeat proteins, with a range of structural and functional diversity provide benchmark comparisons for our modelling  (Boersma and Pl u¨ckthun, 2011; Brunette et al., 2015; Parmeggiani et al., 2015; Pl u¨ckthun, 2015) . Models generated by ISAMBARD could form the basis of structural analysis of putative designs in attempts to create new Ankyrin-like repeat proteins with specific functions.  The models of a solenoids were built in two stages. Initially, the repeating unit of two short a helices was defined with the HelixPair specification and optimized for a given sequence, and then helical symmetry was applied with the Solenoid specification (Fig. 1).  Generally, in the Solenoid function, the repeating unit is built on a plane relative to a reference axis. The positions of the helices are described independently using 5 parameters: axis distance, z-shift, uCa, splay and off-plane rotation (Supplementary Fig. S3). As these parameters are independent, it is possible to create the same relative positions using different parameter values. Helical symmetry is applied to the repeating unit by defining the radius, twist per repeat, rise per repeat and the handedness of the solenoid. The repeating unit also has rotational freedom, needed to ensure that it remains oriented correctly relative to the helical axis (Supplementary Fig. S4).  Regular, parameterizable regions of a set of 9 representative high-resolution crystal structures of Ankyrin-like proteins were modelled using ISAMBARD (Fig. 3, Supplementary Fig. S6, Supplementary Table S2). The parameterization captured the conformation of the reference structures very effectively, with all RMSD100 scores below 1.5 A˚ , comparing favourably with the collagen-like peptides. Indeed, for 7 of the 9 structures, the RMSD100 was lower than 0.64 A˚ , the mean score for coiled coils.  This specification is the most complex of all those discussed herein, and required 7 parameters in total, compared to 3 for the parallel coiled coils and 4 for the collagen triple helix. Despite this, the models minimized in a similar time frame (4040 models, 10 minutes on a single core of a desktop computer). This demonstration of the quality of the differential evolution optimizer is certainly encouraging for modellers of even more-complicated folds and/or broader classes of protein folds.  Loops are crucial for the function of Ankyrin-like repeat proteins, and while it is not possible to model these regions parametrically, there are tools included in ISAMBARD, such as TAPolypeptide, that allow these to be modelled explicitly, by specifying a list of backbone torsion angles. Furthermore, once the backbone has been generated, the loop regions could be added to the model using one of a range of existing methods  (de Bakker et al., 2003; Bender et al., 2016; Choi and Deane, 2009; Fiser et al., 2000) . 3.2 Different elements can be combined to generate complex models Whilst ISAMBARD has been developed for parametric modelling of protein structures, most of its tools have been made as general as possible to enable their application to other biomolecules. To demonstrate this, we developed a straightforward specification for building parametric models of DNA, and used this in combination with the Solenoid specification to generate a model of a TAL effector bound to a DNA duplex. We used the rebuilding protocol to construct a model that recreates a known crystal structure (3v6t; Fig. 4).  The TAL-effector protein was constructed first, using the optimal parameter values for the Solenoid specification. With the protein model in hand, a DNA duplex was constructed using the DNADuplex specification (Fig. 1), which builds a DNA duplex based on sequences of its strands. The final model was created by rotating and translating the DNA object to bring it into phase with the TAL-effector (TALE) model using tools for geometric manipulation included in the ISAMBARD package (and built into BaseAmpal). The overall alignment of the parameterizable protein region of the TALE in Figure 4 with its model has a backbone RMSD of 1.03 A˚ (RMSD100 ¼ 0.79 A˚ ).     4 Conclusion\r\n  We have described ISAMBARD, a framework that provides a generalized approach to in silico parametric design and optimization of de novo biomolecular structure. We have shown that parametric modelling of proteins is an effective way to reduce the overall structural space that would otherwise prevent atomistic modelling, or at least make it a lengthy process for users. Even for models that require a relatively large number of parameters, as in the case of the solenoid proteins, it is possible to optimize the structure readily using the metaheuristics methods build into ISAMBARD.  The generic design of tools in ISAMBARD allows users to define their own parameterizations that are either completely novel, or composites of existing parameterizations. This focus on modularity makes it readily adaptable and extendable by the user. This ethos has been applied at all levels of the software design, enabling any user familiar with the project to extend and contribute to the code base. Indeed, we have benefitted from the modular approach: due to the model building generality, most of the tools required to model the collagen triple helix and a solenoids already existed in ISAMBARD before efforts began to parameterize these folds.  Currently, specifications are defined manually and then explored using automated optimization strategies. However, it is possible that these parametric models could be determined automatically, and we anticipate that future versions will have features to do this using machine learning strategies trained on structural data gathered using the analysis tools in ISAMBARD.  Our approach is complementary to other design and modelling suites, such as Rosetta and Modeller  (Chaudhury et al., 2010; Das and Baker, 2008; Eswar et al., 2006) . We envisage that powerful protein-design pipelines could be generated by combining ISAMBARD with these packages along with other tools for atomistic simulation such as OpenMM  (Eastman et al., 2013) . Indeed, this would be facilitated by the availability of Python-based front-ends for these software suites.  More generally, the parameterized fold is not required to have any basis in a naturally observed protein fold. Thus, while most state-of-the-art protein design packages require some element of information from natural structures, ISAMBARD provides a starting point for going into the 'dark matter of protein fold space'  (Taylor et al., 2009; Woolfson et al., 2015) .    Acknowledgements\r\n  We thank members of the Woolfson group and Prof. Adam Nelson (University of Leeds) for helpful discussions.    Funding\r\n  CWW is supported by the Biotechnology and Biological Sciences Research Council South West Doctoral Training Partnership. This work was supported by grants from the Biotechnology and Biological Sciences Research Council (BB/J008990/1) and the European Research Council (340764) to DNW. DNW holds a Royal Society Wolfson Research Merit Award.  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/woolfson-group/ISAMBARD",
    "publicationDate": "0",
    "authors": [
      "Christopher W. Wood",
      "Jack W. Heal",
      "Andrew R. Thomson",
      "Gail J. Bartlett",
      "Amaurys A. Ibarra",
      "R. Leo Brady",
      "Richard B. Sessions",
      "Derek N. Woolfson"
    ],
    "status": "Success",
    "toolName": "isambard",
    "homepage": ""
  },
  "90.pdf": {
    "forks": 2,
    "URLs": ["github.com/RUBi-ZA/MD-TASK,"],
    "contactInfo": ["o.tastanbishop@ru.ac.za"],
    "subscribers": 1,
    "programmingLanguage": "Python",
    "shortDescription": "Tool suite for analysing molecular dynamics trajectories using network analysis and PRS",
    "publicationTitle": "MD-TASK: a software suite for analyzing molecular dynamics trajectories",
    "title": "MD-TASK: a software suite for analyzing molecular dynamics trajectories",
    "publicationDOI": "10.1093/bioinformatics/btx349",
    "codeSize": 33178,
    "publicationAbstract": "Summary: Molecular dynamics (MD) determines the physical motions of atoms of a biological macromolecule in a cell-like environment and is an important method in structural bioinformatics. Traditionally, measurements such as root mean square deviation, root mean square fluctuation, radius of gyration, and various energy measures have been used to analyze MD simulations. Here, we present MD-TASK, a novel software suite that employs graph theory techniques, perturbation response scanning, and dynamic cross-correlation to provide unique ways for analyzing MD trajectories. Availability and implementation: MD-TASK has been open-sourced and is available for download from https://github.com/RUBi-ZA/MD-TASK, implemented in Python and supported on Linux/Unix.Contact: o.tastanbishop@ru.ac.za",
    "dateUpdated": "2016-11-29T17:45:24Z",
    "institutions": [
      "Sabanci University",
      "Rhodes University"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-11-15T08:29:38Z",
    "numIssues": 6,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx349   MD-TASK: a software suite for analyzing molecular dynamics trajectories     David K. Brown  1    David L. Penkler  1    Olivier Sheik Amamuddy  1    Caroline Ross  1    Ali Rana Atilgan  0    Canan Atilgan  0    O\u20ac zlem Tastan Bishop  1    0  Faculty of Engineering and Natural Sciences, Sabanci University ,  Tuzla 34956, Istanbul ,  Turkey    1  Research Unit in Bioinformatics (RUBi), Department of Biochemistry and Microbiology, Rhodes University ,  Grahamstown 6140 ,  South Africa     2017   1  1  4   Summary: Molecular dynamics (MD) determines the physical motions of atoms of a biological macromolecule in a cell-like environment and is an important method in structural bioinformatics. Traditionally, measurements such as root mean square deviation, root mean square fluctuation, radius of gyration, and various energy measures have been used to analyze MD simulations. Here, we present MD-TASK, a novel software suite that employs graph theory techniques, perturbation response scanning, and dynamic cross-correlation to provide unique ways for analyzing MD trajectories. Availability and implementation: MD-TASK has been open-sourced and is available for download from https://github.com/RUBi-ZA/MD-TASK, implemented in Python and supported on Linux/Unix.Contact: o.tastanbishop@ru.ac.za       -\r\n  *To whom correspondence should be addressed. Associate Editor: Alfonso Valencia    1 Introduction\r\n  Molecular dynamics (MD) is used to understand the movement of atoms of macromolecules in a simulated cell environment. MD simulations produce trajectories depicting the motions of atoms by presenting the atomic coordinates at specified time intervals, allowing for investigation into changes over time. Traditionally, these trajectories are used to analyze macromolecule dynamics by calculating well-established measures such as root mean square deviation, root mean square fluctuation (RMSF), radius of gyration and energy-based approaches including the molecular mechanics/Poisson Boltzmann surface area (MM/ PBSA) and the molecular mechanics/generalized Born surface area (MM/GBSA)  (Kollman et al., 2000) .  Applications of MD simulations are broad, ranging from determining the stability of macromolecular complexes to understanding allosteric behavior of proteins. Although the measures mentioned above are informative, sometimes additional approaches are required. For example, changes in residue interaction networks (RINs) have been investigated in the context of mutation analysis  (Bhakat et al., 2014; Doshi et al., 2016; Brown et al., 2017) . Another method, perturbation response scanning (PRS), following MD simulations, is used to identify residues important for controlling conformational changes  (Atilgan and Atilgan, 2009) .  Although traditional measures are incorporated into MD programs, they can be limited depending on the research questions. Hence, individual research labs often write custom scripts to further analyze trajectories. This might be challenging to some, especially if it requires mathematical knowledge and complex scripting. To serve this need, we present MD-TASK, an easy-to-use tool suite with detailed documentation, for analyzing MD trajectories. MD-TASK includes what we call dynamic residue network (DRN) (which combines the RINs of the frames in an MD trajectory) analysis, PRS, and dynamic cross-correlation (DCC), none of which are found in commonly used MD packages.    2 Materials and methods\r\n   2.1 Implementation\r\n  MD-TASK was developed using Python for Linux/Unix-based systems. Various non-standard Python libraries, including NumPy, SciPy, Matplotlib  (Hunter, 2007) , MDTraj  (McGibbon et al., 2015)  and NetworkX, were used in the suite. Thus, MD-TASK supports any formats the underlying Python libraries support including .binpos (AMBER), LH5 (MSMBuilder2), PDB, XML (OpenMM, HOOMD-Blue), .arc (TINKER), .dcd (NAMD), .dtr (DESMOND), hdf5, NetCDF (AMBER), .trr (Gromacs), .xtc (Gromacs), .xyz (VMD) and LAMMPS. Further, the igraph package for R was used to generate residue contact maps.    2.2 Network analysis\r\n  RINs can be analyzed using graph theory. In a RIN, each residue in the protein is a node in the network. An edge between two nodes exists if the Cb atoms (Ca for Glycine) of the residues are within a user-defined cut-off distance (usually 6.5-7.5 A˚ ) of each other. MD-TASK constructs a DRN and uses this to calculate the changes in betweenness centrality (BC) and average shortest path (L) to residues over the trajectory.   2.2.1 Betweenness centrality\r\n  BC is a measure of how important a residue is for communication within a protein. The BC of a node is equal to the number of shortest paths from all nodes to all others that pass through that node. MD-TASK uses an implementation of Ulrik Brandes algorithm in the NetworkX library for calculating BC  (Brandes, 2001) .    2.2.2 Average shortest path\r\n  For a given residue, L is the sum of the shortest paths to that residue, divided by the total number of residues less one. MD-TASK uses a custom algorithm in the NetworkX library for quickly calculating the shortest path between all residues  (NetworkX Developers, 2017) . The average shortest path describes how accessible a residue is within the protein.    2.2.3 Residue contact map\r\n  Residue contact maps are generated by monitoring the interactions of a residue throughout a simulation, yielding a network diagram with the residue of interest [e.g. single nucleotide polymorphism (SNP)] at the center, and residues that it interacts with arranged around it. Edges between the residue of interest and the other residues are weighted based on how often the interaction exists.     2.3 Perturbation response scanning\r\n  Given the atomic coordinates for initial and target states, together with an equilibrated MD trajectory of the initial structure, the algorithm performs a residue-by-residue scan of the initial conformation, exerting external forces on each residue, and records the subsequent displacement of other residues using linear response theory and a variance-covariance matrix obtained from the MD trajectory  (Atilgan et al., 2012) . The quality of the predicted displacement is assessed by correlating the predicted and experimental difference between the initial and target states. This results in a correlation coefficient for each residue, where a value close to one implies good agreement with the known experimental change. Residues whose perturbation invokes a conformational displacement closest to the target structure are reported as hot residues.    2.4 Dynamic cross-correlation\r\n  DCC is calculated using the following formula:  Dri Drj  Cij ¼ qffiffiffiDffiffiffirffiffii2ffiffiffiffi rffiDffiffiDffiffiffiffirffij2ffiffiffiEffiffi with Dri the displacement from the average position of atom i, and hi the time average over the whole trajectory  (Di Marino et al., 2015) . MD-TASK generates a heat-map depicting the DCC between the Ca atoms of selected frames in a trajectory to identify relative residue movements.     3 Performance\r\n  The results of a performance test are presented in Table 1. Tests were conducted on the 'example_small.dcd' trajectory, a 599 residue, 2000 frame trajectory provided in the 'example' sub-directory of the MD-TASK Github repository. Tools were set to iterate through the trajectory at 100 frame intervals and then executed 10 times. These executions were timed using the Linux 'time' utility, which includes the time to start up the Python interpreter, providing an accurate measure of what users will experience in practice. An average time was then calculated for each tool. The PRS script used 50 random forces per residue.  Tests were conducted on a PC running Ubuntu 16.04 with an Intel Core i5-6300U CPU with 4 logical cores running at 2.4 GHz and 8 GB RAM.    4 Applications\r\n  The network measures, BC and L, have been used in minimized protein structures  (Ozbaykal et al., 2015)  for Alanine scanning. One suggested use, here, is SNP analysis over MD simulations. The network analysis scripts of MD-TASK were applied to analyze reninangiotensinogen system  (Brown et al., 2017) . The data indicated that combination of RMSF values with network analysis can be informative. Further, a SNP analysis protocol combining traditional MD analysis tools with DRN is proposed in a recent review article  (Brown and Tastan Bishop, 2017) .  PRS identifies single residues playing an active role in protein conformational changes between an initial and target state  (Atilgan and Atilgan, 2009) . Perturbative methods are instrumental in uncovering different protein functions such as the effect of pH on the distribution of conformations of calmodulin  (Atilgan et al., 2011)  and ferric binding protein  (Atilgan and Atilgan, 2009) . By disclosing nonevident relationships, PRS was used to suggest new experiments to explore allosteric communication, e.g. in HSP70  (Penkler et al., 2017) .  As an example of MD-TASK outputs, we used HIV protease. Sequence of closed conformation of crystal structure (4ZIP) was used as wild type (WT) target sequence to model its structure in open conformation by means of homology modeling. Two major (V32I, I47V) and one minor (V82I) mutations were then introduced, also using homology modeling, to create the open conformation mutant structure. In both cases 1TW7 was used as the template. Network analysis was performed for 40 ns MD runs for both WT and mutant proteins in open conformation (Fig. 1A). DCC was calculated for the mutant protein (Fig. 1B). For PRS (Fig. 1C), an equilibrated 20 ns section of the mutant trajectory was used. The PDB structure, 3S54, which represents the closed conformation with the same mutations, was used as the end state during PRS calculations. Residues having the highest change in reachability (highest DL) during the course of the trajectory are 46-56, comprising the flap. This property gives HIV-protease the flexibility to expand the active site cavity and diminish the effect of inhibitors while staying functional  (Martin et al., 2017) . The peaks in DBC correspond to active site residues 25-27, supporting the hypothesis that high BC positions are responsible for interdomain communication  (Ozbaykal et al., 2015) . While these properties are similar in both WT and mutant, the neighborhood structure of position 47 slightly shifts, as shown in the residue contact maps. DCC (Fig. 1B) demonstrates that the intrachain motions are highly correlated, while the motions of the chains with respect to each other are anticorrelated (residues 1-99 versus 100-198). The PRS results (Fig. 1C) display that there are no single residues whose perturbation directly leads to the conformational change between the equilibrated WT structure at the 20 ns snapshot and the triple mutant, as the maximum correlations are 0.6. Nevertheless, highest correlations span residues 60-72 implying an allosteric communication between this region and the flap motions. The specific residues are mapped onto the protein structure in Figure 1D. Figure 1 summarizes how MD-TASK provides a means to analyze the trajectories, and gives a bird's eye view of various factors that may be effective in the dynamics of a protein.    5 Conclusion\r\n  MD simulations have become an important tool in structural bioinformatics. Here, we present a new tool suite for analyzing MD trajectories using DRN analysis, PRS, and DCC. To the best of our knowledge, MD-TASK is the first downloadable tool suite for analysis of different properties along MD simulations not commonly found in other MD packages.    Acknowledgements\r\n  We thank Tandac Furkan Guclu and Gizem Ozbaykal for helpful discussions.    Funding\r\n  This work is supported by the National Institutes of Health Common Fund under grant number U41HG006941 to H3ABioNet, and by the National Research Foundation (NRF), South Africa, [grant number 93690]. The content of this publication is solely the responsibility of the authors and does not necessarily represent the official views of the funders.  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/RUBi-ZA/MD-TASK",
    "publicationDate": "0",
    "authors": [
      "David K. Brown",
      "David L. Penkler",
      "Olivier Sheik Amamuddy",
      "Caroline Ross",
      "Ali Rana Atilgan",
      "Canan Atilgan",
      "O\u20ac zlem Tastan Bishop"
    ],
    "status": "Success",
    "toolName": "MD-TASK",
    "homepage": ""
  },
  "65.pdf": {
    "forks": 2,
    "URLs": [
      "github.com/hcji/KPIC2.■",
      "github.com/hcji/KPIC2"
    ],
    "contactInfo": [
      "hongmeilu@csu.edu.cn.",
      "zhangzhimin@csu.edu.cn."
    ],
    "subscribers": 1,
    "programmingLanguage": "R",
    "shortDescription": "KPIC2: An Effective Framework for Mass Spectrometry-Based Metabolomics Using Pure Ion Chromatograms",
    "publicationTitle": "KPIC2: An Effective Framework for Mass Spectrometry-Based Metabolomics Using Pure Ion Chromatograms",
    "title": "KPIC2: An Effective Framework for Mass Spectrometry-Based Metabolomics Using Pure Ion Chromatograms",
    "publicationDOI": "10.1021/acs.analchem.7b01547",
    "codeSize": 5506,
    "publicationAbstract": "Distilling accurate quantitation information on metabolites from liquid chromatography coupled with mass spectrometry (LC-MS) data sets is crucial for further statistical analysis and biomarker identification. However, it is still challenging due to the complexity of biological systems. The concept of pure ion chromatograms (PICs) is an effective way of extracting meaningful ions, but few toolboxes provide a full processing workflow for LCMS data sets based on PICs. In this study, an integrated framework, KPIC2, has been developed for metabolomics studies, which can detect pure ions accurately, align PICs across samples, group PICs to identify isotope and potential adducts, fill missing peaks and do multivariate pattern recognition. To evaluate its performance, MM48, metabolomics quantitation, and Soybean seeds data sets have been analyzed using KPIC2, XCMS, and MZmine2. KPIC2 can extract more true ions with fewer detecting features, have good quantification ability on a metabolomics quantitation data set, and achieve satisfactory classification on a soybean seeds data set through kernel-based OPLS-DA and random forest. It is implemented in R programming language, and the software, user guide, as well as example scripts and data sets are available as an open source package at https://github.com/hcji/KPIC2.",
    "dateUpdated": "2017-08-15T10:12:12Z",
    "institutions": ["Central South University"],
    "license": "No License",
    "dateCreated": "2016-12-07T05:25:33Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Anal. Chem.     10.1021/acs.analchem.7b01547   KPIC2: An Effective Framework for Mass Spectrometry-Based Metabolomics Using Pure Ion Chromatograms     Hongchao Ji  0    Fanjuan Zeng  0    Yamei Xu  0    Hongmei Lu  0    Zhimin Zhang  0    0  College of Chemistry and Chemical Engineering, Central South University ,  Changsha 410083 ,  PR China     2017   89  7631  7640    16  6  2017    27  4  2017     Distilling accurate quantitation information on metabolites from liquid chromatography coupled with mass spectrometry (LC-MS) data sets is crucial for further statistical analysis and biomarker identification. However, it is still challenging due to the complexity of biological systems. The concept of pure ion chromatograms (PICs) is an effective way of extracting meaningful ions, but few toolboxes provide a full processing workflow for LCMS data sets based on PICs. In this study, an integrated framework, KPIC2, has been developed for metabolomics studies, which can detect pure ions accurately, align PICs across samples, group PICs to identify isotope and potential adducts, fill missing peaks and do multivariate pattern recognition. To evaluate its performance, MM48, metabolomics quantitation, and Soybean seeds data sets have been analyzed using KPIC2, XCMS, and MZmine2. KPIC2 can extract more true ions with fewer detecting features, have good quantification ability on a metabolomics quantitation data set, and achieve satisfactory classification on a soybean seeds data set through kernel-based OPLS-DA and random forest. It is implemented in R programming language, and the software, user guide, as well as example scripts and data sets are available as an open source package at https://github.com/hcji/KPIC2.       -\r\n  H chromatography (GC) or liquid chromatography (LC) igh-resolution mass spectrometry (MS) coupled with gas plays an important role in metabolomics.1,2 In particular, LCMS is a more flexible technique, which can analyze metabolites, including those that are difficult in vaporization. However, difficulties remain in processing large-scale LC-MS data sets of metabolites due to their complexity. For example, it is estimated there are over 1,000 kinds of metabolites in human serum,3 and above 4,000 or 5,000 to 25,000 for plants.4 Moreover, background ions and random noises make the quantification even more challenging. Therefore, it is essential to develop reliable label-free strategies to handle large-scale metabolite data sets.  Previous efforts to address these challenges in LC-MS data sets have lead to many famous publicly available tools, for example XCMS,5,6 MZmine,7,8 MetAlign,9,10 OpenMS,11,12 etc. All of these pieces of software can accept raw LC-MS data files and analyze the imported data set with chemometrics algorithms. They all provide the whole pipeline for processing LC-MS data sets. Typically, a pipeline of processing LC-MS data includes preprocessing, feature detection, alignment and grouping, normalization, and further pattern recognition.5,13 Among them, feature detection is the fundamental procedure to extract useful information from raw data. Since an LC-MS data set can be seen as a three-way array, the common strategy is to reduce the dimension. Total ion chromatography, base peak chromatography, and extracted ion chromatography are all based on this strategy. Detecting peaks in extracted ion chromatography, which is used in XCMS, MZmine, MetAlign, and OpenMS, is the most widely used method for feature extraction. However, binning to extracted ion chromatograms (EICs) introduces additional steps such as baseline correction, smoothing, and peak deconvolution. Meanwhile, it may split a feature into adjacent bins.5 To circumvent the drawbacks of binning, another method called centWave14 is integrated into XCMS, which locates the region of interest (ROI) based on the relative mass differences. However, the relative mass differences of the adjacent ions in a chromatographic peak are proportional to the intensities of ions. Meanwhile, they may also vary in different mass spectrometers.15,16 Moreover, MZmine2 implemented a 2D peak detection method named GridMass,17 which generates a grid of equally spaced probes covering the entire chromatographic area and makes each probe explore a rectangular region around it to find a local maximum until no higher values exist within the exploring rectangle. Hence, the local maximums are defined as features.  Besides, new programs such as Finnee,18 FeatureFinderMetabo,19 SMART,20 and iMet-Q21 have also been published to improve the quality, efficiency, and/or usability in analyzing the LC-MS data set. Finnee provides various functions for visualization, and uses different strategies to handle profile and centroid MS mode. FeatureFinderMetabo detects ion traces and its isotopes at the same time to improve the robustness and avoid the individual deisotoping step. SMART integrates the statistical analysis methods, addressing the challenges of statistical analysis in large-scale data. iMet-Q focuses on providing a user-friendly interface.  A novel concept entitled pure ion chromatogram (PIC) has been introduced recently, which is particularly suitable for the centroid high-resolution LC-MS data set. A PIC means a length of chromatogram consisting of only one response intensity each scan, and the response values are supposed to come from the detection of a single ion. There are already many feature extraction algorithms based on this concept, including TracMass,22 massifquant,23 TracMass2,24 and PITracer.16 They can take full advantage of high-resolution mass spectrometry and track ions from scan to scan. TracMass and massifquant detect pure ion traces with a Kalman filter, even though using a Kalman filter is time-consuming. On the other hand, TracMass2 and PITracer use the adjusted mass differences of the adjacent ions as standard, which are significantly faster. KPIC25 is a PIC extraction method based on k-means clustering, which avoids estimating the mass difference tolerance of ions and reduces the number of split signals. However, methods such as TracMass2, PITracer, and KPIC do not make full use of information on the intensity of ions. Furthermore, some tools provide a deisotoping method (such as FeatureFinderMetabo and iMet-Q). The PIC extraction methods referred to above do not have these functions. So, external packages such as CAMERA26 should be used.  In this study, an integrated framework called KPIC2 has been developed. It goes further than KPIC, which increases the accuracy of PIC extraction by taking intensity into consideration. Besides, KPIC2 provides the entire pipeline for metabolites quantitation and pattern recognition. Testing with several metabolomics data sets, KPIC2 shows satisfactory results of feature detection, alignment, grouping, quantitation, and pattern recognition. KPIC2 is implemented in R programming language, which is available as an open source package at https://github.com/hcji/KPIC2.■    METHOD AND THEORY\r\n  The Concept of Pure Ion. The main challenging of LC-MS features extraction is to separate ions of analyte from any ions not meaningful, such as background ions and random noise. Extracting \u201cpure ion\u201d is an effective way to achieve this. The conception of pure ion is the ions that originate from the same analyte. The most reliable information to locate pure ions is that they exist with continuous scans, similar mass-to-charge ratios, and continuous changing intensities in the raw data. Thus, the common methods track ion from scan to scan and connect data points with similar m/z values. In this way, each ion trace usually contains only one kind of ion from the same analyte.  The difference of methods is how to define \u201csimilar\u201d. The relative mass difference tolerances can be affected by many factors, such as the intensities of the ions, resolution of instrument, ambient temperature, etc.27,28 The existing methods usually need the user to give such a tolerance value or estimate a constant value. KPIC2 gives up the arbitrary manner and uses the clustering tendency of m/z values of pure ions, which is a more flexible way. The tolerance range is not fixed and depends on the ions in the cluster.  The whole Framework of KPIC2 consists of the following modules: extraction of pure ion chromatograms, peak detection, alignment, grouping, and missing peak filling. Finally, a peak table consisting of retention time, accurate mass, and maximum intensities can be constructed easily for further statistical analysis and pattern recognition. The whole workflow is summarized in Figure 1. In this section, the theory of each procedure will be described in detail.    Extraction of Pure Ion Chromatograms. PIC extraction\r\n  in KPIC2 works in an iterative procedure. The raw LC-MS data set is converted into a three-column matrix containing all detected ions. These columns collect the values of retention time (rt), m/z, and intensity, respectively. The ion with the highest intensity is chosen as a landmark to start the iteration. Then the region of interest (ROI) of the landmark is determined by the following steps: the user gives a flexible m/z range (ppm of tolerance), and data points in this range are collected into the ROI. Then, the retention time of the ROI is extended toward both sides of the landmark until there are no data points within the given m/z range.  After determination of the ROI, it has to be known which ions have the same origination as the landmark ion. For the high-resolution mass spectrum, m/z is the most reliable criterion. Therefore, we calculate the values of the square of the m/z difference between ions in the ROI and the landmark, and perform an optimal k-means clustering algorithm (ckmeans.1d.dp27) on them. In this way, the cluster with the mean nearest to zero is most likely to contain the response of the same ion as the landmark. and random noise signals are usually assigned into other clusters, and the number of clusters is optimized automatically by ckmeans.1d.dp.  Ions selected in previous steps are converted into a chromatogram. In most cases, only one ion is selected in a single scan. For nonideal scans, the intensity of the chromatogram is represented by the maximum intensity of the ions. Hence, the base peak chromatograms (BPCs) on the ROI are obtained. However, the BPCs are not equal to the PIC, because interferential ions with very similar retention time and m/z may still be included.  Then exponential smoothing28 is used to estimate the intensity of the pure ion for each scan. The details are described in the Supporting Information (Figure S1). For the scans with more than one ion selected, the one with the most similar intensity to the estimated value is selected. The ultimately selected ions are combined together to construct the PIC. The schematic is shown in Figure 2.  Evaluation of PICs. The quality of the PICs shape is evaluated by three common criteria, which are Gaussian similarity, sharpness, and signal-to-noise ratio (SNR).29 These criteria can assist the user to choose optimal parameters for PIC extraction. The coefficient of determination30 (R-square) of Gaussian fitting is used for measuring the goodness of PIC fitting in the Gaussian curve, which is defined as eq 1, where [y1, ..., yn] are the intensities of the PIC and each is associated with a modeled value y1̂, ..., yn̂. The sharpness is defined by eq 2, where n is the total data point number and p is the peak apex index. The SNR is obtained by peak detection in wavelet space. Higher values of R-square, sharpness, and SNR indicate higher quality of PICs. Moreover, the m/z standard deviation of each PIC is also calculated. Smaller standard deviation means the points constructing the PIC are more compact in the m/z dimension, which also indicates the PIC has higher quality.  R2 = 1 −  n ∑i=1 (yi − yi ̂)2  n ∑i=1 (yi − y ̅ )2  p sharpness = ∑  yi − yi−1 + ∑n−1 yi − yi+1 i=2 yi−1 i=p yi+1  Peak Detection. Multiscale peak detection (MSPD31) has been used to detect the peaks in pure ion chromatograms. MSPD transforms signals into wavelet space via a continuous wavelet transform (CWT) with a Mexican hat wavelet and determines peak location based on the information given by ridges, valleys, and zero-crossings in wavelet space. It is better than the previous peak detection algorithm MassSpecWavelet,32 which only used the ridges information in wavelet space. Furthermore, the peak width is also estimated by enhanced SNR derivative calculation.33 ( 1 ) ( 2 )  Alignment. KPIC2 takes full advantage of the profile-based method for alignment. The alignment algorithm first constructs a moving window sliding across the m/z axis, which ranges from the minimum m/z value of the extracted PIC to the maximum one. In LC-MS analysis, deviation of the m/z dimension is much smaller than the RT dimension, and PICs of various samples with m/z values in the window tend to be oneto-one corresponding. Hence, they are aligned with the following procedures: ( 1 ) The PICs of each sample in the window are combined into a pseudochromatogram. ( 2 ) The PAFFT algorithm34,35 (Peak alignment by fast Fourier transform) is used to align the pseudochromatograms across samples. ( 3 ) The retention times of relevant PICs are corrected based on the estimated deviation profile by PAFFT. The schematic is shown in Figure 3. To avoid splitting some corresponding features, the window moves half the width of itself (i.e., 100.0−100.5, 100.25−100.75, etc.). With such strategies, KPIC2 does not count on well-behaved (samples basically have only one peak in an m/z range5) corresponding features.  Grouping. This procedure has two purposes: ( 1 ) Group PICs across samples based on the retention time after alignment; ( 2 ) Group features belonging to the same substance. The first step works with an iteration procedure. Each iteration selects the most intense feature from the feature table not yet assigned to a group ID as reference. The relative distance between other features included in the specific RT and m/z tolerance window and the reference are calculated via eq 3.  RT distance = MZ distance =   RT difference\r\n    RT tolerance\r\n    MZ difference\r\n    MZ tolerance\r\n  Then the features are clustered by the HDBSCAN36,37 method. Features in the same cluster as the reference are assigned the same group ID, and they are excluded when the next iteration runs. If more than one feature of a sample is in the cluster, the one with intensity most similar to the reference is selected to assign the group ID. The iteration runs until all of the features have group IDs.  Typically, each group represents one substance. However, this is not always true for two reasons: ( 1 ) Tailed peaks caused by the centroid step of the TOF mass spectrometer; ( 2 ) Isotopic ions and the adducts. PICs caused by these two conditions are less meaningful for statistical analysis. Therefore, we use similarity of PIC of feature group to determine which clusters represent them. The similarity estimation is analogous to the work of Erny et al.38 It is evaluated by calculating the Pearson correlation coefficient between the most intense PIC in each cluster. The correlation between two profiles X and Y is defined as px,y = cov(X , Y ) σ σ  X Y where cov (X, Y) is the covariance of X and Y, and σX, σY are the variances of X and Y, respectively. The feature groups with calculated correlation over a given threshold are removed from the results.  Filling Missing Peaks. After grouping, there will always be peak groups that do not include peaks from every sample. A missing peak does not mean that the peak does not exist. It may be caused by undetection or misalignment. The missing peak filling includes two steps. First, it extends the tolerance of both m/z and RT dimensions, and researches from the peak list of each sample. Second, if there are still missing peaks, it allows feature searching from the raw data directly. Since the feature location in both the retention time and the m/z dimension is roughly limited by the detected features of most samples, we detect the missing peaks from the BPCs in the limited region of the remaining samples.  Pattern Recognition. Aiming to select important metabolites between classes, supervised classification and variable selection methods are commonly used. Before pattern recognition, KPIC2 provides normalization and scaling algorithms for pretreatment. PLS-DA39,40 (Partial Least Squares - Discriminant Analysis), OPLS-DA41,42 (Orthogonal treated PLS-DA), Kernel-based OPLS-DA43,44 and random forest45,46 methods are implemented in KPIC2 for pattern recognition and selecting biomarkers. ■     EVALUATION DATA SETS\r\n  Simulated Data Set. This data set is based on the real experimentally identified metabolites in the study by Giavalisco et al.47 We add random noise and adjust the noise level, SNR, and m/z deviation for observing the effect of these factors on ( 3 ) ( 4 ) the methods. The construction details are in the Supporting Information (Section 3).  Mixed Compound Data Set. This data set is a mixture of 48 known compounds, which is spiked in different concentrations (20, 5, 1, 0.2 μM) into methanolic extracts of Arabidopsis thaliana leaves, and analyzed by UPLC-QTOFMS. The 20 μM compound mixed solution is also detected individually to determine the retention time of each compound. This data set is provided by Neumann et al.26 as a publicly available data set from the Metabolights repository (with Web site of http://www.ebi.ac.uk/metabolights, and identifier MTBLS188), and the experimental protocol can also be found at the Web site.  Quantification Data Set. This data set is derived from a reasonable concentration gradient of a mixed solution of five standard compounds together with human plasma detected by a Shimadzu ultrahigh-performance UPLC system coupled to an ion trap−time-of-flight (IT-TOF) mass spectrometer. The data set is available under request. The experimental details are as follows:  Five LC-MS-grade standard compounds (phenylalanine, tryptophan, hippuric acid, niacin, and methionine) were purchase from Sigma-Aldrich. Each standard was prepared at a concentration of 1 mg/mL aqueous solution. Then they were mixed and diluted to standard serial solutions at 1 μg/mL, 2 μg/mL, ..., 10 μg/mL. Plasma samples were the QC samples of the male infertility study.48 400 μL of methanol was added to 100 μL of plasma sample. Then, the mixture was centrifuged at 16000 rpm for 15 min at 4 °C for deproteinization. The supernatant was evaporated to dryness under N2 gas. The dried samples were reconstituted in 100 μL of the different experimental standard solutions.  Sample analysis was performed using a Shimadzu LC/ITTOF-MS, which is equipped with an electronic spray ion source and operated in positive mode. Chromatographic separation was achieved on a Waters ACQUITY UPLC HSS C18 column (100 mm × 2.1 mm i.d., 1.7 μm.) at 40 °C. Elution buffer A was water containing 0.1% (v/v) formic acid, and elution buffer B was acetonitrile containing 0.1% (v/v) formic acid. A binary gradient elution was under the following gradient program: 5% B increased to 40% B in 6 min and to 85% B in 14 min, ramped to 100% B in 3 min and held for 7 min, then decreased to 5% in 1 min, and finally maintained at 5% B for 5 min.    Soybean Seeds Metabolomics Data Set. This is a public\r\n  untargeted metabolomics LC-MS data set of four near-isogenic soybean seed extracts. The four near-isogenic lines include single mutants (SM3, SM19) and double mutants (DM) of two MRP genes and the wild-type (WT). There were significant metabolite differences in the four classes. This study is reported by Jervis et al.,49 and the experiment details are described in the reference.    RESULTS AND DISCUSSION\r\n  In this section, we assess the performance of KPIC2 from different perspectives. First, the results obtained from a mixed compound data set are presented. The ability of extracting PICs of features is tested by the recall rate of standard compounds.  Meanwhile, the isotopic group results of several representative substances are listed compared with the theoretical isotopic pattern. Second, the results obtained from the quantification data set are presented. We examine the linearity of the response observed in spike-in experiments to measure the quantification quality of KPIC2. Then, a typical metabolomics study is ■ aAll of the methods are run on personal computer with Intel i7−3930K CPU and 32G RAM. processed with the soybean seeds metabolomics data set. It can be tested whether the metabolomics difference can be found via the framework of KPIC2. Finally, the same data sets are processed via the state-of-the-art software XCMS and MZmine2. The results were compared with that of KPIC2.  Feature Detection. Since the ground-truth of extracts of Arabidopsis thaliana leaves is unknown, the recall rate of M+H ion features of the known 48 compounds is used as evaluation criterion. The results of different parameter values related to m/ z tolerance are listed in Table 1, while other parameters are listed in Table S1. Unsurprisingly, a higher concentration of standard compounds leads to higher recall rates. The feature detection results are robust when the concentration is over 1 μM, and the recall rates are around 80%, near the value obtained from pure solution. When the concentration decreases to 0.2 μM, the recall rates drop significantly to lower values.  Isotopic Features Identification. To evaluate whether KPIC2 can group the isotopic features with the main features, we compared the grouping results of 20 μM compound mixed solution data with the theoretical isotopic pattern. Table 2 shows the results of seven compounds. We provide the substance name, the formula, the mass-charge-ratio of the monoisotopic peak and the first three isotope peaks, and the relative peak intensity (normalized to 100). The theoretical relative intensities of isotope peaks are calculated via the rcdk package (http://cran.fhcrc.org/web/packages/rcdk). It can be seen that the relative intensities of detected isotopic features are 81.25% 83.33% 83.33% 91.67% 89.58% 91.67% 87.50% 89.58% 85.40%  Concentration very similar to the theoretical values, which means KPIC2 can extract isotopic features of compounds accurately.  Quantification Linearity. Based on the quantification data set which contains a spike-in series in a complex plasma background, we can assess the linearity of the standard compounds. The data set is processed via feature detection, alignment, and grouping procedures. The detected intensities of the standard compounds features are listed in Table S2. The relationship between the analytical concentrations and their mean corresponding feature intensities is shown in Figure S2.  The correlation coefficient of each compound is over 0.98, which reveals a good linear relationship. The intercept of phenylalanine and tryptophan is not zero, which is because the two substances exist in plasma at a significant level. KPIC2 is also tested by another data set (MTBLS 234) which is obtained from a wider range of concentrations of analytes. A good linear relationship is also obtained (Table S3 and Figure S3). The results indicate that KPIC2 can achieve satisfactory quantification of analytes.  Reproducibility. Apart from the standard compounds, the algorithm also detected around 7,000 features belonging to plasma per sample of the quantification data set, and 4000 of them exist in over 50% samples. Since the human plasma samples are reconstructed by QC samples, the substances of each sample are supposed to be the same. Thus, real features of metabolites of plasma are likely to have very similar peak intensities, while peak intensities of random features (e.g., noise, contaminants) may vary tempestuously. This fact can enable us to evaluate the reliability of KPIC2 of extracting features from a complex data set. Therefore, the relative standard deviation (RSD) of peak intensities across a sample is calculated. Figure 4 shows the number of features in each RSD range. Over 60% features are with RSDs less than 20%, which means more than 2500 features detected from the plasma samples have satisfactory reproducibility.  Pattern Recognition. Finding the discriminative metabolites of a different class is a common purpose of a metabolomics study. Hence, the soybean seeds metabolomics data set is used for such a typical metabolomics study in order to test whether the difference of the metabolites can be found after being processed by KPIC2. The data set is processed via feature detection, alignment, grouping, and peak filling steps, and the parameters are optimized, respectively. The result is normalized by total peak area and autoscaling. Then, two multivariate statistical methods, kernel-based OPLS-DA and random forest, are taken as representations and applied to analyze the peak list matrix.  The random forest is computed with 1000 trees and 7 predictors for each node. The classification result is shown in Table 3, and the multidimensional scaling (MDS) plot of the proximity matrix of random forest models is shown in Figure 5.  With features extracted by KPIC2, the random forest model achieves 100% success of classification, while XCMS misclassified one sample in both positive and negative mode. From the first two dimensions multidimensional scaling (MDS) plot based on the proximity matrix of KPIC2, the discrimination between classes is more obvious. However, in the MDS plot based on the proximity matrix of XCMS, some samples of SM3 and SM19 are mixed with the WT class, which is conformed to the classification confusion matrix shown in Table 3.  The number of the latent variable (LV), the orthogonal LV (OLV), and the kernel parameters of kernel-based OPLS-DA are optimized via cross-validation. The best parameters are used for building the model. The score plots are shown in Figure S4, respectively. It is obvious that samples of four near-isogenic lines can be separated. Then permutation test is used to compare the models. The result is listed in Table S4, where R2 is the variance in the measurement matrix explained, and ACC is the average classification accuracy rate of classes in crossvalidation. From the result of permutation test, the obtained models should be significantly better than any other random classification,50,51 and the higher accuracy rate indicates the higher discriminating power of the extracted data and models.  From the criterion and the result referenced above, the obtained classification models are reliable, which means the difference of metabolites can be found after treatment via KPIC2.    Comparison to Related Methods. XCMS/CAMERA and\r\n  MZmine2 are the most popular frameworks in the field. Thus, the same data sets are also processed by these two tools.  The results of the feature detection comparison of the mixed compound data set are summarized in Table 1. One can see the total features detected by the three methods are similar, while KPIC2 even detected fewer features. However, more true features of standard compounds are extracted by KPIC2. This is more obvious when the standard compounds are at a low ■ the soybean data set show that satisfactory classification can be achieved and variable importance of PLS-DA, OPLS-DA, and random forest can be used to screen biomarkers. KPIC2 is an effective framework for integrated analysis of metabolomics data sets equipped with the concept of pure ion chromatograms, which can improve the accuracy of quantification, classification, and biomarkers identification.    ASSOCIATED CONTENT\r\n  *S Supporting Information The Supporting Information is available free of charge on the ACS Publications website at DOI: 10.1021/acs.analchem.7b01547.  Construction of the data set and the results, result of the intensity estimation, standard compound concentrations versus mean feature intensities of quantification data set and MTBLS 234 data set, score plots, F-score versus noise level plots, F-score versus m/z tolerance parameter plots, parameters used in processing the mixed compound data set, peak intensities, permutation testing result, and discussion parameters used in processing the quantification data set (PDF) Filled peak lists of features reproducing in over 50% of the samples of each method (ZIP)    AUTHOR INFORMATION\r\n   Corresponding Authors\r\n  *E-mail address: hongmeilu@csu.edu.cn. *E-mail address: zhangzhimin@csu.edu.cn.    ORCID\r\n  Hongchao Ji: 0000-0002-7364-0741 Hongmei Lu: 0000-0002-4686-4491    Notes\r\n  The authors declare no competing financial interest. concentration. To make it more intuitive, Figure 6 shows the additive XICs of the standard compounds, which means add up the EICs extracted by XCMS or PICs extracted by KPIC2 into one chromatogram. It is clearly seen that KPIC2 extracted more low intensity true features than XCMS. The superiority of KPIC2 in detecting low intensity features can be explained by the fact that when clustering pure ions, noise signals can be excluded and not be considered, so the true features are not covered by noise. The only unreasonable thing is that the recall rate for the 20 μM solution and leaf of XCMS and MZmine2 is even higher than the recall rate of pure mixed solution, which may result from a false positive condition. The shortage of KPIC2 lies in the speed. Since MZmine2 is written in the Java programming language, and XCMS is written in a hybrid of C and R languages, KPIC2, written in pure R language, is not as fast as them.  The mixed solutions of standard compounds together with human plasma data set are processed by XCMS/CAMERA and MZmine2, too. The parameters are optimized based on the obtained quantification linearity of standard compounds, which means, with the selected parameters, all of the methods achieve their best quantification linearity of the standard compounds. The parameters are listed in Table S5. The filled peak lists of features reproducing in over 50% of the samples of each method are summarized in Table S6. The RSD of each feature is also calculated. As is shown in Figure 4, the ratios of detected features of KPIC2 and MZmine2 in each RSD range are similar and most of the RSDs are lower than 30%, which means they both have good reproducibility in their detected features. At the same time, KPIC2 detects more features with small RSD values, which indicates that KPIC2 exhibits better sensitivity while maintaining the reproducibility of the exclusive features. Meanwhile the ratio of features with RSD over 30% detected by KPIC2 is less than XCMS and MZmine2, which indicates better reproducibility in this data set.  Robustness. It is known interference factors of data sets, such as noise level, background ions, and m/z deviations, may be varied; thus, every method has several parameters to be optimized in order to suit different conditions. Unfortunately, it is not easy to know the best parameters. So a broad optional range of parameter selection suited for a data set is needed. Therefore, a simulated plants metabolites data set is designed. We can observe the performance of methods when the interference factors of a data set and the parameters of a method vary. The construction of the data set and the results and discussion are in the Supporting Information (Figures S5, S6, and S7).     CONCLUSION\r\n  In this study, a novel framework, KPIC2, has been developed for analyzing a high-resolution LC-MS data set. It can extract pure ion chromatograms from raw data, align them across samples based on the profiles, group PICs to identify isotope and potential adduct PICs, and fill missing peaks. Moreover, KPIC2 provides quantitative workflow, novel pattern recognition methods and variable importance for identifying biomarkers. One can see from the result of the MM48 data set that KPIC2 has superiority in detecting low concentration compounds and can extract truer ion features with fewer detected features. The features of KPIC2 have good ability on the quantification data set. The pattern recognition methods have been implemented in KPIC2 for further pattern recognition and biomarkers identification, and the results of ■ ■ ■    ACKNOWLEDGMENTS\r\n  This work is financially supported by the National Natural Science Foundation of China (Grant nos. 21375151, 21305163, and 21675174) and Fundamental Research Funds for the Central Universities of Central South University (No. 2016zzts247).    ",
    "sourceCodeLink": "https://github.com/hcji/KPIC2",
    "publicationDate": "0",
    "authors": [
      "Hongchao Ji",
      "Fanjuan Zeng",
      "Yamei Xu",
      "Hongmei Lu",
      "Zhimin Zhang"
    ],
    "status": "Success",
    "toolName": "KPIC2",
    "homepage": ""
  },
  "95.pdf": {
    "forks": 5,
    "URLs": [
      "github.com/vibbits/phyd3/",
      "github.com/vibbits/phyd3"
    ],
    "contactInfo": [
      "klaas.vandepoele@ugent.vib.be",
      "michiel.vanbel@ugent.vib.be"
    ],
    "subscribers": 2,
    "programmingLanguage": "JavaScript",
    "shortDescription": "Phylogenetic tree viewer based on D3.js",
    "publicationTitle": "PhyD3: a phylogenetic tree viewer with extended phyloXML support for functional genomics data visualization",
    "title": "PhyD3: a phylogenetic tree viewer with extended phyloXML support for functional genomics data visualization",
    "publicationDOI": "10.1093/bioinformatics/btx324",
    "codeSize": 2937,
    "publicationAbstract": "Motivation: Comparative and evolutionary studies utilize phylogenetic trees to analyze and visualize biological data. Recently, several web-based tools for the display, manipulation and annotation of phylogenetic trees, such as iTOL and Evolview, have released updates to be compatible with the latest web technologies. While those web tools operate an open server access model with a multitude of registered users, a feature-rich open source solution using current web technologies is not available. Results: Here, we present an extension of the widely used PhyloXML standard with several new options to accommodate functional genomics or annotation datasets for advanced visualization. Furthermore, PhyD3 has been developed as a lightweight tool using the JavaScript library D3.js to achieve a state-of-the-art phylogenetic tree visualization in the web browser, with support for advanced annotations. The current implementation is open source, easily adaptable and easy to implement in third parties' web sites. Availability and implementation: More information about PhyD3 itself, installation procedures and implementation links are available at http://phyd3.bits.vib.be and at http://github.com/vibbits/phyd3/. Contact: klaas.vandepoele@ugent.vib.be or michiel.vanbel@ugent.vib.be Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-10-11T17:49:43Z",
    "institutions": [
      "Ghent University",
      "VIB Bioinformatics Core",
      "VIB-UGent Center for Plant Systems Biology"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-10-18T09:19:54Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx324   PhyD3: a phylogenetic tree viewer with extended phyloXML support for functional genomics data visualization     Łukasz Kreft  2    Alexander Botzki  2    Frederik Coppens  1  3    Klaas Vandepoele  0  1  3    Michiel Van Bel  1  3    0  Bioinformatics Institute Ghent, Ghent University ,  9052 Ghent ,  Belgium    1  Department of Plant Biotechnology and Bioinformatics, Ghent University ,  9052 Ghent ,  Belgium    2  VIB Bioinformatics Core ,  VIB, 9052 Ghent ,  Belgium    3  VIB-UGent Center for Plant Systems Biology ,  9052 Ghent ,  Belgium     2017   1  1  2   Motivation: Comparative and evolutionary studies utilize phylogenetic trees to analyze and visualize biological data. Recently, several web-based tools for the display, manipulation and annotation of phylogenetic trees, such as iTOL and Evolview, have released updates to be compatible with the latest web technologies. While those web tools operate an open server access model with a multitude of registered users, a feature-rich open source solution using current web technologies is not available. Results: Here, we present an extension of the widely used PhyloXML standard with several new options to accommodate functional genomics or annotation datasets for advanced visualization. Furthermore, PhyD3 has been developed as a lightweight tool using the JavaScript library D3.js to achieve a state-of-the-art phylogenetic tree visualization in the web browser, with support for advanced annotations. The current implementation is open source, easily adaptable and easy to implement in third parties' web sites. Availability and implementation: More information about PhyD3 itself, installation procedures and implementation links are available at http://phyd3.bits.vib.be and at http://github.com/vibbits/phyd3/. Contact: klaas.vandepoele@ugent.vib.be or michiel.vanbel@ugent.vib.be Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Janet Kelso    1 Introduction\r\n  The construction, visualization and interpretation of phylogenetic trees are instrumental for biological analysis in multiple fields, including evolutionary biology, genetics and comparative genomics. Recently published software tools allow for the use of interactive elements and comprehensive analytics in association with these trees by means of the latest web technologies (He et al., 2016; Kuraku et al., 2013; Letunic and Bork, 2016; Smits and Ouverney, 2010). The phyloXML standard (Han and Zmasek, 2009) is widely supported in those tools to visualize the evolutionary trees as well as associated data.  A comparison of the most popular phylogenetic tree viewers reveals that a feature rich open source tool that (i) supports extended charting, (ii) is easily adaptable and (iii) can be seamlessly implemented in third party websites, is currently not available (see Supplementary Data). To this end, we extended the phyloXML standard with several new elements in order to accommodate common annotation datasets and developed a lightweight phylogenetic tree viewer using the D3.js JavaScript library (http://d3js.org).    2 Materials and methods\r\n  The commonly used phyloXML standard has been extended with various elements of the complex type as permitted by the current phyloXML XSD scheme. The newly created elements &lt;taxonomies&gt; and &lt;domains&gt; are used to specify taxonomy and domain colours, descriptions and links. In order to reference between the &lt;phylogeny&gt; element of the current standard and the new elements, the &lt;code&gt; and &lt;name&gt; sub-elements are used for the &lt;taxonomy&gt; and the &lt;domain&gt; element, respectively. Colours are represented as HEX values (e.g. 0xRRGGBB). Furthermore, graphs like a pie, a binary and a multi-bar chart as well as a heat map or a boxplot can be displayed next to the leaf nodes by using the newly defined &lt;graph&gt; elements.  The pie chart and binary chart types can also be associated with internal nodes of the phylogenetic tree. Referencing of the chart values to the nodes is achieved by defining an &lt;id&gt; tag in the &lt;clade&gt; element corresponding to the 'for' attribute in &lt;value&gt; elements.  A complete graph specification consists of the following child elements: 1. name: This element contains the graph name that will be displayed in a detailed node information popup. 2. legend: For each value series of your data a legend field has to be specified. This legend consists of the field name (which will be displayed to the user in the legend and info popup), the colour that will be used to draw this series (for all graph types besides heat map) and an additional symbol shape (e.g. circle) for the binary graph. For the heat map a gradient specification has to be applied according to the specifications of the ColorBrewer2 project (http://colorbrewer2.org).3. data: Data values for the graph have to be defined for each clade id for which a graph is displayed. They have to be sorted according to how the series are defined in the respective legend fields. Existing tags or properties from the phylogeny element can be used by referencing them via the 'tag' and 'ref' attributes of the data element.    3 Features\r\n  Using the D3.js JavaScript library, PhyD3 is implemented as a flexible and lightweight tool allowing for the display of interactive and complex phylogenetic trees in a web-based environment without security-based limitations and without the need for external plugins. The implementation is fast and responsive to user interaction with the tree elements' display parameters being easily changed through user-friendly access controls (Fig. 1).  PhyD3 offers a number of key features with respect to tree nodes and graphs visualization. Basic information (like node and taxonomy names, branch length and support values) can be displayed next to the respective nodes with detailed information (including link outs to external URLs) being shown in an info box per user request. The display of the information is flexible according to the users' preferences (e.g. all content, or only internal or external node annotations). To support a clearly annotated data presentation and better readability, leaf nodes can be lined up to match the furthest node and text can be scaled.  Furthermore, nodes can be annotated with structural and numerical information, displayed in the form of various graphs: protein domain information or read-count data can for example easily be visualized next to each node. PhyD3 provides controls to scale and filter this information to support easy analysis. Numerical annotations are presented using multiple bar charts, pie charts, binary charts or boxplot charts. Users can easily change the size and scale of graphs to match their needs.  Additionally, a number of convenience features were introduced in PhyD3: tree nodes are automatically hidden when they do not fit in the space between nodes to prevent overlapping of text and graphs; tree colours can be inverted for greater readability; node texts can be coloured according to node taxonomy. Extra display options include the capabilities to swap tree nodes within a sub-tree as well colouring options for these sub-trees.  Lastly, PhyD3 provides import and export tools to facilitate greater interoperability. Using the import tool users can supply trees in Newick and phyloXML formats, with optional numerical data, which can be easily converted to the extended phyloXML format with graph annotations. Export capabilities provide the conversion of the current tree visualization into vector graphics (SVG format) and bitmaps (PNG format), as well as the extended phyloXML data itself. A comparative overview of the main features of most popular phylogenetic tree viewers is given in Supplementary Table S1. A full feature list is described in Supplementary Table S2 (Fig. 1).    4 Conclusions and further information\r\n  We have developed an extension of the phyloXML standard to include graph specifications and datasets as extensions of the XSD scheme as well as a phylogenetic tree viewer PhyD3, using the JavaScript D3.js library. Thanks to the phyloXML extensions, the PhyD3 viewer efficiently visualizes extensively annotated phylogenetic tree data using modern web browsers. The software, full documentation and demo material are available at http://phyd3.bits.vib.be and http://github.com/vibbits/phyd3.  Conflict of Interest: none declared. Han,M.V. and Zmasek,C.M. (2009) phyloXML: XML for evolutionary biology and comparative genomics. BMC Bioinformatics, 10, 356.  He,Z. et al. (2016) Evolview v2: an online visualization and management tool for customized and annotated phylogenetic trees. Nucleic Acids Res., 44, W236-W241.  Kuraku,S. et al. (2013) aLeaves facilitates on-demand exploration of metazoan gene family trees on MAFFT sequence alignment server with enhanced interactivity. Nucleic Acids Res., 41, W22-W28.  Letunic,I. and Bork,P. (2016) Interactive tree of life (iTOL) v3: an online tool for the display and annotation of phylogenetic and other trees. Nucleic Acids Res., 44, W242-W245.  Smits,S.A. et al. (2010) jsPhyloSVG: a javascript library for visualizing interactive and vector-based phylogenetic trees on the web. Plos One, 5, e12267.    ",
    "sourceCodeLink": "https://github.com/vibbits/phyd3",
    "publicationDate": "0",
    "authors": [
      "Łukasz Kreft",
      "Alexander Botzki",
      "Frederik Coppens",
      "Klaas Vandepoele",
      "Michiel Van Bel"
    ],
    "status": "Success",
    "toolName": "phyd3",
    "homepage": ""
  },
  "52.pdf": {
    "forks": 0,
    "URLs": ["github.com/tianhe2/gromacs-mic"],
    "contactInfo": [],
    "subscribers": 1,
    "programmingLanguage": "C",
    "shortDescription": "a version of GROMACS run on MIC with offload mode",
    "publicationTitle": "A CPU/MIC Collaborated Parallel Framework for GROMACS on Tianhe-2 Supercomputer",
    "title": "A CPU/MIC Collaborated Parallel Framework for GROMACS on Tianhe-2 Supercomputer",
    "publicationDOI": "10.1109/TCBB.2017.2713362",
    "codeSize": 21213,
    "publicationAbstract": "-Molecular Dynamics (MD) is the simulation of the dynamic behavior of atoms and molecules. As the most popular software for molecular dynamics, GROMACS cannot work on large-scale data because of limit computing resources. In this paper, we propose a CPU and Intel® Xeon Phi Many Integrated Core (MIC) collaborated parallel framework to accelerate GROMACS using the offload mode on a MIC coprocessor, with which the performance of GROMACS is improved significantly, especially with the utility of Tianhe-2 supercomputer. Furthermore, we optimize GROMACS so that it can run on both the CPU and MIC at the same time. In addition, we accelerate multi-node GROMACS so that it can be used in practice. Benchmarking on real data, our accelerated GROMACS performs very well and reduces computation time significantly. Source code: https://github.com/tianhe2/gromacs-mic",
    "dateUpdated": "2016-05-12T00:33:06Z",
    "institutions": [
      "Tongji University",
      "Shandong University",
      "National University of Defense Technology Changsha"
    ],
    "license": "https://github.com/tianhe2/gromacs-mic/blob/master/COPYING",
    "dateCreated": "2016-05-12T00:01:56Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     TCBB.     10.1109/TCBB.2017.2713362   A CPU/MIC Collaborated Parallel Framework for GROMACS on Tianhe-2 Supercomputer     E-mail:pengshaoliang@nudt.edu.cn  0  1    Corresponding author  0  1  2    Equal Contributors  0  1    0  Department of Computer Science, Tongji University ,  Shanghai ,  China    1  School of Computer Science, Shandong University ,  Jinan ,  China    2  Tenglilang Zhang School of Computer Science, National University of Defense Technology Changsha ,  China     2017   2713362   -Molecular Dynamics (MD) is the simulation of the dynamic behavior of atoms and molecules. As the most popular software for molecular dynamics, GROMACS cannot work on large-scale data because of limit computing resources. In this paper, we propose a CPU and Intel® Xeon Phi Many Integrated Core (MIC) collaborated parallel framework to accelerate GROMACS using the offload mode on a MIC coprocessor, with which the performance of GROMACS is improved significantly, especially with the utility of Tianhe-2 supercomputer. Furthermore, we optimize GROMACS so that it can run on both the CPU and MIC at the same time. In addition, we accelerate multi-node GROMACS so that it can be used in practice. Benchmarking on real data, our accelerated GROMACS performs very well and reduces computation time significantly. Source code: https://github.com/tianhe2/gromacs-mic    GROMACS  Molecular Dynamics Simulation   High Performance Computing  Parallel Framework       -\r\n  Wthere is a continuous demand for molecular ith the rapid advance of the pharmaceutical industry, dynamics, which is a key procedure in pharmaceutical research. GROMACS [ 1 ] is a versatile package for performing molecular dynamics, and is increasingly popular with pharmaceutical researchers for its convenience and practicality. In China, more than 20% of the computing resources in supercomputer centers are used for GROMACS simulations. Although GROMACS is highly efficient in its use of graphics processing units (GPUs) and SIMD [ 2 ], it does not meet the rapidly increasing demand of molecular dynamics, owing to the limits of the algorithms and models. It always takes millions of iterations to achieve a simulation. Furthermore, the interaction between thousands of atoms needs to be calculated at each step. Such a huge amount of computation means that computing resources can be a bottleneck in this field. As a consequence, it usually takes a few months to accomplish a simulation with GROMACS, which is an intolerable situation.  The current version GROMACS has already added SIMD to support Many Integrated Core (MIC) architectures. The SIMD data width of the core calculation is the only part that has been Figure 1 A CPU/MIC collaborated parallel framework to accelerate GROMACS in multi-steps. modified, and it is sufficient. Some methods of acceleration, such as storage modes and data alignment, are also quite useful. Although these modifications generally have little influence on efficiency, for software such as GROMACS, whose calculation amount is extremely large, a little optimization can be very helpful and valuable. If we are able to shorten the running time of each step by even tens of milliseconds, the incremental performance obtained by the optimization will be quite notable.  In this paper, we first characterize molecular dynamics simulation, GROMACS, MIC, and offload mode. We then introduce a method to accelerate GROMACS, which is carried out on a supercomputer with MIC in offload mode. Our method is divided into three steps. First, based on the process of GROMACS, we carry out the calculation of the non-bonded force on the MIC, which can reduce the load on the CPU. Next, we propose dividing the kernel computing tasks at the atomic level, and utilize the new data flow to upgrade the software, which can make room for further acceleration and lead to faster software processing. In addition, the synergistic utilization of the CPU and MIC makes full use of the computing resources on the Tianhe-2 supercomputer. We balance the task load by experiment. At last, we apply new method on muti-node to make GROMACS more powerful, which means it can deal with larger atom systems. The relationship of three steps of acceleration is shown in Figure 1.  In addition, we evaluate our method using a series of tests that are carried out on the supercomputer to compare the performance of the updated and unmodified programs.  This paper is organized as follows: Section II introduces the necessary background of molecular dynamics simulation, GROMACS and MIC. In Sections III, IV, and V, we introduce our methods which are about accelerating GROMACS. Section VI explains our evaluation methods and presents the results of our experiments. We conclude the paper in Section VIII.    II. BACKGROUND AND RELATED WORK\r\n   A. Molecular Dynamics Simulation\r\n  Molecular dynamics simulation is a method pioneered by Alder and Wainwright [ 3 ] in 1957. This extremely powerful technique involves solving the classical many-body problem in contexts relevant to the study of matter at the atomic level. Because there is no alternative approach capable of handling such a broad range of problems at the required level of detail, molecular dynamics methods have proved themselves indispensable in both pure and applied research. As a method of simulation, molecular dynamics solve micro-scale problems that traditional methods cannot explain. This simulation method obtains the macroscopic properties of a material by calculating the trajectory of microscopic molecules, thus enabling scientists to explain the macroscopic properties of material at a molecular level and modify macro properties using molecular prediction. Therefore, molecular dynamics simulation is a bridge between macro properties and micro motion. Nowadays, thousands of studies and articles relevant to molecular dynamics are published every year. In recent years, with the rapid developments in the energy, materials, mechanical and electrical industries, the large-scale computation of molecular dynamics simulations is widely demanded, and computation speed has become a bottleneck of development for many industries.    B. GROMACS\r\n  GROMACS is a versatile package for performing molecular dynamics, i.e., simulating the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have many complicated bonded interactions, but because GROMACS is extremely fast at calculating non-bonded interactions (which usually dominate simulations) many groups are also using it for research on non-biological systems, e.g.,polymers. GROMACS supports 2 all the usual algorithms expected by a modern molecular dynamics implementation [ 5 ].  The process of a GROMACS simulation can be divided into the following three stages:  (1) Initialization of the coordinates of the atomic system, topological structure of the files, balancing parameters, and external parameters.  (2) Main simulation process, which includes the following steps. Minimization of the energy of the system before the equilibrium process is balanced to avoid errors resulting in an unreasonable structure. The system is heated, the initial velocities are set up, and initial balance is achieved. Then, by means of the molecular dynamics simulation, the trajectory of each atom, looking for the minimum potential energy of the system, is calculated.  In the simulation process, the user can observe the state of the system at any time using the visualization software.  (3) After the simulation analysis, GROMACS produces a series of documents for further analysis.    C. Intel MIC\r\n  Intel MIC Architecture is a co-processorcomputer architecture developed by Intel, incorporating earlier work on the Larrabee many-core architecture, Teraflops Research Chip multicore chip research project, and Intel Single-chip Cloud Computer multicore microprocessor[ 6 ]. Intel Xeon Phi coprocessors provide up to 61 cores, 244 threads, and 1.2 tera FLOPS of performance, and they come in a variety of configurations to address diverse hardware, software, workload, performance, and efficiency requirements [ 7 ]. The offload mode of MIC is very efficient. This mode could make the computer calculate the logic module on the CPU and calculate the computing module on the MIC, making full use of computing resources.    D. Related Work\r\n  GROMACS has a highly developed source code, heavily linked modules, and deeply nested structures, which makes the offload mode hard to achieve. Furthermore, the molecular dynamics are difficult to parallelized because of the data dependencies. So far, GROMACS has been optimized with OPENMP, MPI, SIMD, and CUDA, achieving notable acceleration. As a result, it is now hard but also significant to optimize GROMACS, even just a small amount. Recently, Plotnikov [ 11 ] ran the GROMACS in native mode on a CPU and MIC with MPI. Although this could be a good approach to utilizing both the CPU and MIC at the same time, there are still some shortcomings remaining. First, we need to offload the binary file to each MIC, which is a large-scale task that takes many computing resources. Second, when we run GROMACS with large-scale atoms and molecules, the communication will become a bottleneck. For the reasons given above, the native mode is not yet a practical solution. Plotnikovonly tested the native mode using three MICs and two CPUs.  Compared to our previously presented work [ 5 ], we introduce the following new contributions in this paper: (1) We have solved problems such as write-collision and communication overhead, which are caused by using the offload mode. (2) We have designed a new method to calculate the non-bonded force using both CPUs and MICs on a single node. (3) We have implemented our method on multiple computation nodes so that we can do a larger scale simulation and solve more complex problems.  III. ACCELERATION OF GROMACS USING OFFLOAD MODE Molecular dynamics is an iterative process. In general, the system arranges millions of iterations, and every step only takes a small amount of time, sometimes even half a millisecond. In addition, we can only carry out the offload mode during one step, because the system has a strong data dependency, which means that the next step cannot start until the last one is finished. It is a challenge to arrange the offload mode in such a little time. In molecular dynamics, there are many calculations to perform in each step, and the most compute-intensive part is the non-bonded force calculation, which calculates the non-bonded forces between atoms. To calculate the force, we need to compute the interaction between almost every pair of atoms, which is at most n × n times of calculation, where n is always the number of atoms. This number can range from thousands to millions. Luckily, some interactions between atoms are negligible, but the remaining number of non-negligible interactions is still very large.  The force contains two parts, the Coulomb force and the Vander Waal's force. At each step, GROMACS calculates the two forces billions of times as quickly as possible. The code to calculate these forces is included in two files, the outer code and the inner code. The outer code iterates each atom, and the inner code finds all the atoms that interact with this atom. Hence, at each step, the outer code is called n times, and the inner code is called more frequently. The efficiency of GROMACS seriously depends on these two files, which are called kernel codes. GROMACS has many different kernel codes, and it transfers different codes according to the background to achieve the best performance.  At this stage of the optimization work, there are two key issues that can significantly affect the efficiency. The first problem we need to solve is the data transmission between the CPU and MIC. Based on the algorithm, when we calculate the non-bonded force, the essential input data are the coordinates and the charge of each atom. The size of these data is O(n). The essential output is the force of each atom, whose size is also O(n). However, in an implementation, these input and output data is insufficient. Some other information is also needed, such as atom types, atomic offsets, and some other parameters and control variables. These data also needs to be transmitted between CPU and MIC. The relationship and the data flow are shown in Figure 2.  Currently, when we need to exchange data between the CPU and MIC, we can only transfer the data that are stored in memory continuously, in a single step. That is to say, the discrete data in memory cannot be transmitted directly. Unfortunately, when we calculate the non-bonded force between atoms, a large amount of discrete data needs to be transmitted, resulting in multiple threads. For example, every thread has an output array of the non-bonded force which is a dynamic array, and is not continuous in memory. We can only use loops to achieve segment transmission, and the number of cycles equals to the number of threads. In this way, too many Instances of data transmission and logical operations occur, which leads to a large amount of time consumption to prepare and close the transmission between CPU and MIC. As a result, the extra overhead is quite large, and the transmission is very inefficient.  In order to reduce the communication overhead, it is necessary to trim the memory segment of the data. Our approach is to reconstruct the data array that needs to be transferred by allocating a larger amount of memory and then gathering the data. Although the overall size does not change, the data are continuous in memory now, and we do not need to utilize a loop anymore. We can hence transmit between the CPU and MIC just once.  In this way, we greatly reduce the time needed for communication, and we lessen the overhead caused by the repetition work, which means we overcome the disadvantage of using offload mode. However, such adjustments will also produce some additional overheads, including the time to allocate space and the extra time for memory copy. The latter can have a particularly serious impact on program's performance.  The other issue is write-collision, which can affect the efficiency when we use OpenMP to accelerate GROMACS on MIC for non-bonded force calculations between atoms. Write-collision occurs when many results are written back to the same address of the memory, including the force, energy and other information.  In the non-bonded force calculation, because of the interaction of two atoms, it is not only necessary to write the position of the current atom, but also the position of the atoms which the current one is interacting with. In this way, the workload can be reduced by half. However, there is no way to ensure data locality. The problem is that the same atom's position in the non-bonded force array can be written at the same time in different threads because of parallel computing. A write-collision is then generated, which can reduce the efficiency of the code implementation.  The ideal method is to sort the atom pairs so that we can allow each thread to have access to a segment of the non-bonded force array to avoid write-collisions among different threads. However, during the calculation, the interactions between atoms are very complex, and it is quite hard to avoid write-collision by rearranging the atom pair order. Hence, although this solution is reasonable, it is quite difficult to implement. When studying acceleration methods for GROMACS, we found that GROMACS has a certain way to solve this problem: it allocates one output structure for each thread, and every structure has a complete copy of the non-bonded force. During the thread execution process, each thread then writes the data back to its own structure. GROMACS then deals with all the structures after all the threads have completed their computation.  IV. ACCELERATION OF GROMACS WITH COLLABORATION OF  CPU/MIC  Because the offload mode accelerated version of GROMACS does not achieve sufficient performance, we go a step further. Based on the offload mode of the MIC, we design and implement a new reasonable method, which allows the CPU to collaborate with the MIC on the Tianhe-2 super computer to accelerate GROMACS. The new method utilizes a new data flow to reduce the extra overhead caused by offload mode. We also propose a new load-balancing task allocation to make full use of Tianhe-2's computing resources. In this section, we will introduce the main principles and implementation of our method.  The most difficult problem is how to reduce extra resource consumption. It is well-known that the consumption caused by the data copy is completely unnecessary. Another issue is to reduce needless communication caused by the discrete storage of non-bonded force arrays of different threads. Hence, in order to improve the efficiency, we have to find a way to completely eliminate needless data copying, which means we have to solve the problem of discrete storage of non-bonded force arrays. However, in the current data flow, arrays are copied to each thread and stored in segregated structures. Hence, if the data flow cannot be changed, we cannot notably improve the efficiency. Overall, it would be pointless to utilize offload to accelerate GROMACS because of the large amount of extra consumption caused by the current CPU data flow.  However, GROMACS achieves good performance on a GPU, and the method used by the GPU is quite similar to the offload mode of MIC, which both needs to first transmit data, then does the calculations on the co-processors and return the result to the CPU for subsequent operations. We believe that there must be some method to solve the problem of discrete storage on a MIC. In numerous versions of the GROMACS kernel code, there are some sections of codes that are reserved for software developers who need to facilitate debugging work by using a CPU for a GPU simulation. At last, we found what we need the code utilizes the GPU's data flow in a CPU environment and simulates the calculation of the GPU without the CUDA function library. It is a good solution to the problem mentioned above. However, because of the limited number of 4 threads on the CPU, the resulting performance is not very good. Luckily, this problem can be perfectly solved by the MIC, which has many cores. Hence, the solution is relatively simple: We modify the method introduced in the last section using the new data flow to reduce the extra overhead and communication consumption.  When we use both the CPU and MIC in offload mode to run GROMACS, we need to divide the tasks. This is a good way to lessen the workload on each computing unit(one MIC can be seen as one unit and two CPUs can be taken as one unit) and also a way to increase the efficiency. In the kernel calculation of the non-bonded force, we calculation the interaction between every two atoms and then record the result.The details is shown in Algorithm 1.  Algorithm 1: Kernel calculation of the non-bonded force for(atom i){// for every atom in the atomic system  for(atom j){/* select one atom j to calculate the non-bonded force betweeni and j*/ tf=cal_non_bonded(i,j); f[j]-=tf; f[i]+=tf; } } //kernel calculation program  There are many other control variables in the practical implementation of this process. Algorithm 1 is just a simplified model. In order to balance the task division, we open the outer loop in the code above. That is, we segregate atom i into several parts, and each part is assigned to a different computing unit. Using this task division, it is quite easy to adjust the workload onto different units by changing the number of times that the for loop runs. In addition, atoms, represented by i, are all completely independent, except with respect to write-collision, so this kind of task division does not produce any extra overhead, and we do not need to repeat the calculation. This is in full compliance with the requirements of task division.  The problem of write-collision remains. The solution is quite similar to the one we introduced before. We allocate a large contiguous array in each computing unit and cut it into many segments. Each thread has access to its own segment. When the calculations are complete, we run a process on each computing unit to make a single non-bonded force array which is much shorter. We then download the array to the CPU.     V. ACCELERATION OF GROMACS USING MULTI-NODES\r\n  After we complete the acceleration on a single node, the method begins to appear more practical. This approach appears with a good performance and it can be shown that it can meet the requirements of calculation in tests with a small number of atoms. However, we need to also accelerate the process across nodes not only to satisfy the calculation requirements for large numbers of atoms, but also for better extendibility. In this section, we introduce the main method, which we designed and implement for GROMACS across nodes with offload mode. In addition, we also present the challenges encountered in the implementation and their corresponding solutions.  When we perform a cross-node calculation, the task-partition in GROMACS cuts an atomic system into grids. Every computing node calculates the atom's non-bonded forces that are in its range. In this way, we can reduce that number of atoms on each node so that we can reduce the workload on each node to promote the performance. However, there are forces between different grids. In the practical calculation, the atoms need to calculate the force within a grid; at the same time, the force across the grids also needs to be calculated. These two kinds of force are called the local non-bonded force and non-local non-bonded force. Hence, when computing the force across nodes, the main challenge is that we need to do the calculation twice, local ones and non-local ones. GROMACS has dynamic load balancing and atom migration, which means that the number and type of atoms in every single grid changes all the time, so that different nodes need to communicate with each other by MPI to exchange the atom information. This process produces more communication overhead and we cannot reuse the spatial data. Hence, when we apply GROMACS across nodes, we should be careful with communication and data control. Furthermore, we need to transmit the data, local non-bonded force, and non-local non-bonded force, to the MIC in the same simulation step. The temporal relation of transmission across nodes is much more complex than before, where we just needed to upload the data to a single node. Although the number of atoms of each node is relatively reduced, the total communication overhead is not lessened simultaneously; sometimes it is even increased. In addition, the task division of GROMACS across nodes can be a great trouble and the acceleration performance will degrade when the number of nodes increases in theory. The problems above are very difficult to solve, which leads to higher requirements for us to accelerate GROMACS across nodes with offload mode.  Our solution is to design different transmission modes for different kinds of data. A part of the data that we use when calculating the non-bonded force is static, which does not change with the process of molecular dynamic simulation. This type of data can be used by both local and non-local non-bonded force calculations, so they need to be uploaded only once, at the beginning of the process. This type of data includes a structure of the atom information. The second type of data has different content for local and non-local non-bonded force calculations; correspondingly, it is stored in different positions but has the same size. When we do different calculations, we only need to move the pointer to a different memory address to transmit the data for computing. Because the size of each data array is the same, there is no need to allocate memory again. This type of data includes the atom pair information, atom position, atom charge, and other information. The third kind of data, whose size is related to atom number, varies with the process of simulation, and represents the local and non-local non-bonded force. Hence, the space it takes must be adjusted, which means that the space cannot be reused. The strategy we apply here is to make segregative room for both local and non-local non-bonded force, calling the pointer to do the data transmission. During the simulation, we need to reapply space for the atoms, which migrate across grids, with GROMACS's dynamic load balancing. Finally, the last kind of data does not exist in the original GROMACS. This part of the data, which are intermediate variables, are mainly used to achieve optimization strategies, including for some arrays of force. Its feature is that it is not only related with the atom number, which means they need reallocation in the simulation process, but its memories are completely controlled by text.  Here we will only show the fourth data transmission which can be regard as the most typical one and it is the most complicated. We summarize it in Figure 3.  The fourth transmission mode (Figure 3)is needed for additional allocations on both CPU and MIC side.    VI. PERFORMANCE EVALUATION In this section, we introduce the experiments and evaluation of our method. In addition, we present some performance results and data.\r\n   A. Platform\r\n  Our experimental platform is the Tianhe-2 supercomputer in Guangzhou Center. Tianhe2 has 16,000 computing nodes, and each node consists of two Xeon E5 12 core CPU processors with three Xeon Phi57 core MIC coprocessors. Each node has 64GB CPU memory and 8GB MIC coprocessor memory, so each node's memory is 88GB in total. Other specifications of the CPU and MIC are listed in Table 1.  Clock Frequency(GHz) VPU width(bits) L1/L2/L3 Cache(KB) Memory Size(GB) B. Evaluation of Offload Mode on a Single MIC  As mentioned before, integrated communication is a method that gathers the segregated data, which is for multiple threads, into a large and continuous array. In this way, we can complete data transmission once to reduce the time of communication, thereby reducing the overhead.  On the Tianhe2 computing platform, we set up test simulations of 200 steps on a single node with two CPUs (24 cores in total) and a single MIC card. The test includes 30,000, 100,000, and 300,000 atomic systems correspondingly with and without integrated communication. The test results are shown in Figure 4.  Figure 4, the y-axis is the time that tasks take, in seconds, and the x-axis represents the different versions of GROMACS. \u201cWithout communication\u201d refers to the core code running on a CPU without any acceleration, which is the original version of GROMACS, is the slowest. The other two optimized versions use the MIC to calculate the non-bonded force. The communication overhead is quite large, but the efficiency is greatly improved compared with the original version. It can be seen that because of the communication integration and optimization, GROMACS obtains a notably better performance in all atomic systems. In addition, the integrated communication version is the best overall.  We also evaluate the overall performance of our new program. The platform is again the Tianhe2, with two CPUs and a single MIC, and the simulation is run for 200 steps.The performance is shown in Figure 5.  In Figure 5, the y-axis represents the running time in seconds, and the x-axis represents atom numbers in the systems. We can see that the calculation performance of a single MIC is essentially the same as the performance of both CPUs, because the MIC's frequency is slower than that of the CPU and adds too much extra cost, although our methods, integrated communication and solving the write-collision, have achieved reasonable performance. We will present further results in the next section.    C. Evaluation of CPU&amp;MIC collaboration\r\n  In order to accelerate the GROMACS, we need to solve two more problems. The first one is to lessen the additional time cost, data copy overhead and communication overhead. The method uses a new data flow, which we have introduced before, to decrease the extra cost. The other one is aim to make a perfect task partition. We need to divide the task into different computing units for the CPU and MIC, to reduce the workload on each unit and then improve the performance. To clarity, GROMACS with SIMD acceleration is referred to as the \u201coriginal version,\u201d which obtains a practical performance when it runs on the Tianhe2.  To solve the first problem, we use a new data flow to speed up GROMACS, which can effectively decrease the time cost of extra communication and data transportation. In addition, it can increase the ratio of calculation. We performed 200 steps in different atomic systems in this test, and the results are shown in Figure 6.  Figure 6 shows that the new data flow for GROMACS only performs the worst, because it runs in serial. The new data flow with MIC parallel acceleration has the best performance. In addition, we can see the calculation time cost of the parallel new data flow is much less than that in the original version, which shows a good acceleration improvement.  To obtain even better performance, we need to use the CPU and MIC at the same time to do the calculations. Task allocation among CPUs and MICs is quite similar to that among MICs. In addition, the calculation on the CPU will not bring any extra overhead because there is no need to transmit the data and there is no write-collision either.  Because of the different computing abilities of the CPUs and MICs, in actual calculations, we need to change the workload on the CPU and MIC according to task type. We set 200 steps, respectively to 30,000, 100,000, and 300,000 atoms of the system, to do the test for all task allocation conditions. The performance and results are shown in Figure 7.  Figure 5 Overall system performance of off-load mode  The x-axis is the ratio of workload on CPU and MIC, the y-axis is the time to complete the task, in seconds. As can be seen in the figure, no matter what kind of atomic system is used, the efficiency changes along with the changing of the ratio of workload between the CPUs and MICs. Moreover, one of the highest efficiency is achieved. In addition, it can be seen that the peak of different atomic systems are different, which is because the ratio of the calculations differs among the three atomic systems.  Through the use of the new data flow and multi-unit optimization, GROMACS can achieve good acceleration with the offload mode. We performed the test with a 50,000 steps simulation on a single node that has two CPUs and three MICs. The test results are shown in Figure 8.  In Figure 8, the y-axis is the program's running time. As the number of atoms increases, increasing the amount of calculation, the acceleration effect is more significant. The 300,000-atomic system obtains the best speed-up compared to the original GROMACS. In addition, for all atomic systems, the CPU/MIC collaborative computing always has better efficiency.    D. Evaluation of Multi-node GROMACS\r\n  Compared to the offload GROMACS running on a single node, which we evaluated in the last section, multi-node GROMACS has more computing resources and correspondingly better efficiency; however, the acceleration effect cannot be as good as for a single node. Hence, we set 2,000 steps and 100,000 atoms to test the performance of multi-node GROMACS. The results are presented in Figure 9. Figure 8 Performance for a 50000-step simulation on a single node Figure 9 Performance of multi-node GROMACS  The y-axis represents the time, in seconds, and the x-axis represents the number of nodes. As the number of nodes increases, the speed of both the original version and accelerated version are significantly improved. However, the extra overhead of the accelerated version is also increased, decreasing the calculation ratio, so that the speed-up decreases gradually. Overall, compared to the original GROMACS, the offload mode still achieves a significant acceleration effect.     VII. CONCLUSION\r\n  Molecular dynamics simulation is a widely applied method in materials science, life science, medicine, and other fields. It utilizes Newton's classical mechanics to calculate the trajectory of microscopic particles and simulate the movement of each atom in the system through an iterative process, so that we can obtain the macroscopic properties of the atomic system. GROMACS is one of the most commonly used molecular dynamics simulation software packages. It is popular because it is easy to use and powerful. Currently, GROMACS has been through dozen times of acceleration and optimization. The current acceleration is quite good, including the OPENMP&amp;MPI and GPU&amp;SIMD acceleration. However, after the optimization, GROMACS may sometimes be limited by insufficient computing resources, because the number of iterations are often quite large, resulting in a long calculation time. In addition, we found that there is no efficient version of GROMACS running on MICs in offload mode, so we propose a new method to accelerate GROMACS this way.  Given a full understanding of the current GROMACS acceleration methods, we presented a method that implements GROMACS on MICs in offload mode in Section III. We proposed to transplant the calculation of non-bonded forces to the MIC, and utilize a large array to gather the discrete data. We tested our method and found that it achieved very good performance.  Overall, our method uses three main steps to complete the acceleration for GROMACS on MIC with offload mode, and we ensured the acceleration effect and practicability by experiments carried out on the Tianhe2. We also analyzed varieties of challenges presented by the molecular dynamics simulation algorithm and provided practical solutions. In conclusion, we can say that our new framework and method greatly improve the efficiency of GROMACS and we can guarantee the extendibility of our method.    ACKNOWLEDGMENT\r\n  We would like to appreciate Herman Berendsen's group, department of Biophysical Chemistry of Groningen University and other developers all over the world for providing the source code of GROMACS, and Prof. Hualiang Jiang, Weiliang Zhu, and Jin'an Wang from Shanghai Institute of Materia Medica, Chinese Academy of Sciences (SIMM) for providing related test data and explaining GROMACS. And we would also like to appreciate Prof. Shengzhong Feng, Zhibin Yu, and Dr. Yanjie Wei from Shenzhen Institutes of Advanced Technology (SIAT) for providing advice of algorithm and optimization. This work is supported by NSFC Grant 61272056, U1435222, and 1133005.    ",
    "sourceCodeLink": "https://github.com/tianhe2/gromacs-mic",
    "publicationDate": "0",
    "authors": [
      "E-mail:pengshaoliang@nudt.edu.cn",
      "Corresponding author",
      "Equal Contributors"
    ],
    "status": "Success",
    "toolName": "gromacs-mic",
    "homepage": ""
  },
  "22.pdf": {
    "forks": 36,
    "URLs": [
      "github.com/3DGenomes/tadbit",
      "3dgenomes.github.io/TADbit",
      "github.io/TADbit"
    ],
    "contactInfo": [],
    "subscribers": 17,
    "programmingLanguage": "Python",
    "shortDescription": "TADbit is a complete Python library to deal with all steps to analyze, model and explore 3C-based data. With TADbit the user can map FASTQ files to obtain raw interaction binned matrices (Hi-C like matrices), normalize and correct interaction matrices, identify and compare the so-called Topologically Associating Domains (TADs), build 3D models from the interaction matrices, and finally, extract structural properties from the models. TADbit is complemented by TADkit for visualizing 3D models",
    "publicationTitle": "Automatic analysis and 3D-modelling of Hi-C data using TADbit reveals structural features of the fly chromatin colors",
    "title": "Automatic analysis and 3D-modelling of Hi-C data using TADbit reveals structural features of the fly chromatin colors",
    "publicationDOI": "None",
    "codeSize": 293849,
    "publicationAbstract": "The sequence of a genome is insufficient to understand all genomic processes carried out in the cell nucleus. To achieve this, the knowledge of its three-dimensional architecture is necessary. Advances in genomic technologies and the development of new analytical methods, such as Chromosome Conformation Capture (3C) and its derivatives, provide unprecedented insights in the spatial organization of genomes. Here we present TADbit, a computational framework to analyze and model the chromatin fiber in three dimensions. Our package takes as input the sequencing reads of 3C-based experiments and performs the following main tasks: (i) pre-process the reads, (ii) map the reads to a reference genome, (iii) filter and normalize the interaction data, (iv) analyze the resulting interaction matrices, (v) build 3D models of selected genomic domains, and (vi) analyze the resulting models to characterize their structural properties. To illustrate the use of TADbit, we automatically modeled 50 genomic domains from the fly genome revealing differential structural features of the previously defined chromatin colors, establishing a link between the conformation of the genome and the local chromatin composition. TADbit provides three-dimensional models built from 3C-based experiments, which are ready for visualization and for characterizing their relation to gene expression and epigenetic states. TADbit is an open-source Python library available for download from https://github.com/3DGenomes/tadbit.",
    "dateUpdated": "2017-08-29T00:16:04Z",
    "institutions": [
      "3 Universitat Pompeu Fabra (UPF)",
      "This is a PLOS Computational Biology Software paper"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2013-08-29T12:31:54Z",
    "numIssues": 61,
    "downloads": 0,
    "fulltext": "     July      Automatic analysis and 3D-modelling of Hi-C data using TADbit reveals structural features of the fly chromatin colors     Editor: Andreas Prlic  1    UNITED STATES  1    FrancË ois Serra  0  1    Davide BauÁ  0  1    Mike Goodstadt  0  1    David Castillo  0  1    Guillaume J. Filion  1    Marc A. Marti-Renom  0  1    0  CNAG-CRG, Centre for Genomic Regulation (CRG), Barcelona Institute of Science and Technology (BIST) ,  Barcelona, Spain, 2 Gene Regulation ,  Stem Cells and Cancer Program, Centre for Genomic Regulation (CRG) ,  Barcelona ,  Spain ,  3 Universitat Pompeu Fabra (UPF) ,  Barcelona, Spain, 4 ICREA, Barcelona ,  Spain    1  This is a PLOS Computational Biology Software paper     19  7  2017   19  2017    3  7  2017    10  2  2017     The sequence of a genome is insufficient to understand all genomic processes carried out in the cell nucleus. To achieve this, the knowledge of its three-dimensional architecture is necessary. Advances in genomic technologies and the development of new analytical methods, such as Chromosome Conformation Capture (3C) and its derivatives, provide unprecedented insights in the spatial organization of genomes. Here we present TADbit, a computational framework to analyze and model the chromatin fiber in three dimensions. Our package takes as input the sequencing reads of 3C-based experiments and performs the following main tasks: (i) pre-process the reads, (ii) map the reads to a reference genome, (iii) filter and normalize the interaction data, (iv) analyze the resulting interaction matrices, (v) build 3D models of selected genomic domains, and (vi) analyze the resulting models to characterize their structural properties. To illustrate the use of TADbit, we automatically modeled 50 genomic domains from the fly genome revealing differential structural features of the previously defined chromatin colors, establishing a link between the conformation of the genome and the local chromatin composition. TADbit provides three-dimensional models built from 3C-based experiments, which are ready for visualization and for characterizing their relation to gene expression and epigenetic states. TADbit is an open-source Python library available for download from https://github.com/3DGenomes/tadbit.       -\r\n  Data Availability Statement: All required data and scripts to entirely reproduce the Serra et al 2015 dataset can be downloaded at http://www.3DGenomes.org/datasets/serra_etal (file size: ~300Mb).  Funding: The research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) / ERC grant agreement 609989, the Spanish Ministry of Economy and Competitiveness (BFU2013-47736    Introduction\r\n  Metazoan genomes are organized within the cell nucleus. At the highest level, chromosomes occupy characteristic nuclear areas or ªchromosome territoriesº, separated by inter-chromatin compartments [ 1 ]. Underneath, chromosomes have additional levels of arrangements and P) and the Human Frontiers Science Program (RGP0044). We acknowledge support of the CERCA Programme / Generalitat de Catalunya and the Spanish Ministry of Economy and Competitiveness, `Centro de Excelencia Severo Ochoa 2013-2017', SEV-2012-0208 to the CRG.  The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. organize themselves into the A and B compartments [ 2 ], which in turn are composed of Topologically Associating Domains (TADs), defined as regions of the DNA with a high frequency of self-interactions [3±5]. Determining the three-dimensional (3D) organization of such genomic domains is essential for characterizing how genes and their regulatory elements arrange in space to carry out their functions [ 6 ]. Chromosome Conformation Capture (3C) [ 7 ] and its derived methods (here referred to as 3C-based methods) are now widely used to elucidate the spatial arrangement of genomes [ 8 ]. Although the frequency of interactions between loci can be used as a proxy for their spatial proximity, 3C-based contact maps do not easily convey all the information about the spatial organization of a chromosome. This information, however, can be inferred using computational methods [ 9 ]. Here we present TADbit, a Python library for the analysis and 3D modeling of 3C-based data. TADbit takes as input the sequencing reads of 3C-based experiments and performs the following main tasks: (i) pre-process the reads, (ii) map the reads to a reference genome, (iii) filter and normalize the interaction data, (iv) analyze the resulting interaction matrices, (v) build 3D models of selected genomic domains, and (vi) analyze the resulting models to characterize their structural properties (Fig 1). TADbit builds on existing partial implementations of methods for 3D genomic reconstruction [10±20]. As a validation of the model-building module of TADbit, a systematic analysis of its limitations has shown that 3D reconstruction of genomes based on 3C-based data can produce accurate 3D models [ 21 ].  TADbit has been already shown to provide biological insights [22±24]. Here, we introduce a new application of TADbit for the modeling and analysis of 50 genomic domains of the Drosophila melanogaster genome. It was shown that the Drosophila genome consists of five distinct chromatin types determined by mapping 53 broadly selected chromatin proteins and four key histone modifications [ 25 ]. The chromatin types were labeled with colors and comprise ªblueº chromatin, enriched in Polycomb group proteins and H3K27 methylation, ªgreenº chromatin, Fig 1. TADbit flowchart. Main functions of the TADbit library from FASTQ files to 3D model analysis. TADbit accepts many input data types such as FASTQ files, interaction matrices and 3D models. A series of python functions in TADbit (Supplementary Text) allow for the full analysis of the interaction data, interaction matrices as well as derived 3D models. 2 / 17 bound by HP1 and located at peri-centromeric regions, ªyellowº and ªredº chromatin, harboring distinct classes of active genes, and ªblackº chromatin, covering more than 40% of the Drosophila genome and characterized by low occupancy of most chromatin markers. More recently, genome-wide 3C-based interaction maps in Drosophila revealed that TAD boundaries are gene-dense, highly bound by transcription factors and insulator proteins and correspond to transcribed regions [ 5, 26 ]. Moreover, it was shown that the active red and yellow chromatin types preferentially locate at TAD borders while the others preferentially locate inside TADs. This work highlighted the existence of interplay between the structural organization of genomic domains and their chromatin composition. Similar relationships have also been observed in other organisms, including mouse and human [27±30].  To further characterize the structural properties of the Drosophila chromatin types, we have used TADbit on available Hi-C data. By building 3D models of genomic domains covering more than 50 Mb of the Drosophila genome, we show that the five previously described chromatin colors are characterized by distinct structural properties. Black chromatin is a compact, dense and closed chromatin fiber. In comparison, the heterochromatic types blue and green are more open and accessible. Finally, the yellow and red types feature a loose and open chromatin, potentially accessible to proteins and transcription factors responsible for regulating resident genes. Design and implementation TADbit has been implemented as a Python library to deal with all steps to analyze, model and explore 3C-based data. With TADbit the user can map FASTQ files to obtain raw interaction binned matrices (Hi-C like matrices), normalize and correct interaction matrices, identify and compare Topologically Associating Domains (TADs), build 3D models from the interaction matrices, and finally, extract structural properties from the models. Next, we describe in more details each of the main independent tasks that can be executed with TADbit:   FASTQ quality check\r\n  The TADbit pipeline starts by performing a quality control on the raw data in FASTQ format. This quality check is similar to the tests performed by the FastQC program [ 31 ] with adaptions for Hi-C datasets (S1 Fig).    Iterative mapping\r\n  TADbit implements an iterative mapping strategy that is a slightly modified version of the original ICE method developed for the HiClib library [ 32 ]. The minimal differences with the original ICE method are the mapper used (TADbit uses GEM [ 33 ]) and a more flexible way to define the position of the iterative mapping windows, which can now be fully defined by the user.  Fragment based filtering The filtering strategy implemented in TADbit builds on previously described protocols [ 32 ] to correct all the computationally detectable experimental biases/errors. After mapping, TADbit can filter the reads depending on ten criteria (S2 Fig), which can be applied individually or as a set of filters.  Interaction matrix cleaning and normalization Once filtered, the read-pairs are binned at a user-specified resolution (bin size) depending on the average density per cell required by the analysis to be performed. However, a minimum amount of counts per bin is usually required for the normalization of the data [ 32 ]. To 3 / 17 determine the threshold amount of interactions for masking columns, TADbit proceeds in two steps. First, the columns with zero counts are removed. Second, a polynomial is fitted to the empirical distribution of the total amount of interactions per column, and the first mode of this distribution is used to define the exclusion threshold value below which columns will be removed. After the column removal, the remaining bins are further normalized to remove local genomic biases (e.g., to correct for the genomic regions with higher mappability and/or PCR amplification). The normalization procedure implemented in TADbit is a modification of the ICE balancing method implemented in the HiClib library [ 32 ]. The modification in TADbit consists simply in truncating the balancing process of the ICE normalization after an undefined number of iterations. In TADbit, and with default parameters, the ICE normalization stops when a maximum of 10% of variability between the sum of interactions in a given bin and the average over the genomic matrix is reached (this percentage of variability can be user-defined).  Comparison of interaction matrices Once normalized, the Hi-C contact matrices can be compared to estimate their degree of similarity. For this purpose, TADbit implements plotting functions (S1 Text) and two comparison scores: (i) a Spearman rank correlation between bins in two matrices at increasing genomic distances (Fig 2C) and (ii) a Pearson correlation between the first eigenvectors of each matrix (Fig 2D). Although both measures aim at identifying whether two matrices are similar or not, they have different properties. The first one is sensitive to the matrix resolution and decays as the genomic distance of the compared bins increases. The second one provides a more global comparison of the matrices and aims at identifying whether the internal correlations in the matrix (detected by its principal eigenvectors) are similar between the compared matrices.  Genome segmentation into Topologically Associating Domains (TADs) TADbit analyzes the contact distribution along the genome and subsequently segments it into its constitutive TADs, with each TAD border corresponding to a vertical slice of the Hi-C interaction matrix. TADs can be computed on the interaction matrix from a single experiment or from the matrix resulting from the merge of different experiments. To calculate the position of borders between TADs along a chromosome, TADbit employs a breakpoint detection algorithm [ 22 ] that returns the optimal segmentation of the chromosome under BIC-penalized likelihood (S2 Text). The algorithm in TADbit for segmenting the genome into TADs among others have been recently assessed [ 34 ].  Alignment of TAD boundaries TAD borders are conserved across different cell types and even across species, indicating that topological domains may play an important role in the organization of chromatin in metazoan genomes [ 3 ]. To assess whether TAD borders are conserved throughout different experiments, we implemented a multiple-experiment border alignment algorithm. Starting from different border definitions of the same genomic region, TADbit aligns each TAD to a consensus TAD list, either using the classic Needleman-Wunsch algorithm [ 35 ] or using a method based on reciprocal closest boundaries.  Three-dimensional (3D) modeling of genomic domains In TADbit, the three-dimensional (3D) models of selected genomic domains are generated by transforming the input 3C-based interaction maps into a set of spatial restraints that are later satisfied using the Integrative Modeling Platform (IMP) [ 36 ], as previously described [ 12 ]. 4 / 17 Fig 2. Hi-C interaction maps at 100 kb resolution for the entire Drosophila genome. (a) Raw, filtered and normalized genome-wide interaction maps for the BR dataset. Only after the normalization of the data, the enriched interaction between centromere regions of the Drosophila chromosomes can be observed. (b) Normalized maps for the TR1 and TR2 datasets. (c) Comparison of the normalized Hi-C maps between the three datasets at 100 kb resolution. The Spearman correlation was computed between off-diagonal regions as a function of their genomic distance. (d) Matrices of Pearson correlation coefficients of main eigenvectors from the three Hi-C datasets (that is, BR, TR1 and TR2). The data shows the expected high correlation of the top three eigenvectors [ 32 ]. (e) Genomic coverage of the mapped reads per chromosome from the SUM dataset. (f) Hi-C normalized interaction matrix at 100 kb 5 / 17 resolution for the SUM dataset. The three main eigenvectors of the normalized interaction matrix mark the position of centromeres (E1), chromosomes (E2), and chromosome arms (E3). TADbit automatically generated all the plots in the figure.  Structural clustering of the resulting 3D models To assess the structural similarity of the generated models, TADbit first structurally aligns them using a pair-wise rigid-body superposition that minimizes the Root Mean Squared Deviation (RMSD) between the superimposed conformations [ 37 ]. Then, a matrix with an allagainst-all similarity score (S3 Text) is input in the Markov Cluster Algorithm (MCL) program [ 38 ] for generating unsupervised sets of clusters of structurally related models.  Structural analysis of the resulting 3D models In this work, we have showed how TADbit could be used to model the 3D architecture of chromatin. However, we have implemented a detailed description of how to use each function implemented in TADbit and a series of structural analysis in TADbit to be applied on the generated 3D models (see online documentation and tutorials http://3dgenomes.github.io/TADbit) and outputs several measures to describe the architecture of the model.  Output and visualization of 3D models TADbit includes a simple three-dimensional model viewer using matplotlib [ [3 9], it is designed to be compatible with other visualizing tools, including TADkit (http://www.3DGenomes.org/TADkit).     Results\r\n  Chromatin interaction maps of the Drosophila melanogaster genome The TADbit pipeline starts from raw data (i.e. reads generated from a 3C-based experiment).  We downloaded SRA files from the NCBI Gene Expression Omnibus under accession number GSE38468 [ 26 ], and converted them to FASTQ files using the SRA Toolkit [ 39 ]. The dataset contained three separate Hi-C experiments [ 2 ] performed on Drosophila Kc167 cells using the restriction endonuclease HindIII, consisting of one biological replicate (SRR398921) and two technical replicates (SRR398318 and SRR398920), labeled here as ªBRº, ªTR1º and ªTR2º.  They comprised about 194, 67 and 112 million paired-end reads, respectively (Table 1). A quality check of the first million reads in each of the FASTQ file showed that the average PHRED scores [ 40 ] were higher than 25 across each of the 2x50 bp paired-end reads, which is indicative of good quality. Moreover, TADbit assessed that more than 95% of the reads had undergone digestion during the Hi-C experiment and only ~2% of the reads contained dangling ends sensu stricto (reads starting with a digested restriction site, S2 Fig). Next, the pairedend reads were aligned in TADbit to the Drosophila reference genome (dm3) using the GEM mapper [ 33 ] with a previously proposed iterative mapping strategy [ 32 ]. With this strategy, 67.0% to 77.8% of the original reads could be uniquely mapped (Table 1). After discarding those with only one mapped end, the number of mapped pairs diminished (50.2% to 63.5% of the original reads). These numbers were similar to those reported in the original experiments [ 26 ]. After mapping, the reads were further filtered as previously described [ 32 ], resulting in about 48, 24, and 41 million valid pairs (or interactions) for the BR, TR1 and TR2 experiments, respectively (Table 1). Finally, the filtered interaction maps were normalized using the iterative correction and eigenvector decomposition (ICE) procedure [ 32 ], also implemented in TADbit (Fig 2A). The resulting interaction matrices were highly correlated (Fig 2B, 2C and 2D), which 6 / 17 e z i S . d e t n e s re re v p  e O r o t e s lo E C R r o r r E s e t a c il p u D e l c ir c lf e S g n li g s n d a n  D e . s % t l u  s s ir e a r p l d a il t a n V e m i r p x e e % C d - e i  p s H ap ira e M p h t % 0 % 4 0 6 2 , 9 1 4 6 8 0 , 0 3 5 , 3 % 1 1 % 4 4 9 8 , 4 7 5 , 0 1 9 0 7 , 7 6 5 , 3 4 3 1 , 4 4 2 , 4 3 8 9 5 , 8 3 8 , 8 1 8 8 2 , 4 5 6 , 4 % 5 3 % 9 1 % 5 % 9 . 4 2 6 4 5 , 3 1 2 , 8 4 % 2 . 0 5 3 5 4 , 5 5 4 , 7 9 % 0 2 0 4 , 2 9 % 3 7 6 4 , 8 6 1 , 1 % 5 7 1 4 , 9 1 3 , 2 % 7 1 1 1 6 , 5 1 2 , 7 % 9 1 4 2 5 , 5 5 2 , 8 % 1 1 5 4 7 , 2 0 8 , 4 % 7 2 7 8 , 9 6 9 , 2 % 0 . 5 3 5 7 7 , 3 4 5 , 3 2 % 5 . 3 6 9 2 6 , 3 5 6 , 2 4 % 0 % 3 3 6 9 , 0 3 1 6 4 1 , 6 6 6 , 1 % 5 % 9 9 6 7 , 0 8 4 , 3 3 7 6 , 5 7 7 , 5 7 6 2 , 8 2 2 , 0 1 2 4 4 , 8 7 7 , 6 % 6 1 % 1 1 % 6 1 0 1 , 7 9 9 , 3 % 6 . 6 3 7 1 2 , 3 6 8 , 0 4 % 2 . 7 5 0 0 2 , 6 4 7 , 3 6 % 0 % 3 0 1 1 , 8 4 6 0 9 1 , 2 2 2 , 6 % 8 % 8 8 9 5 , 7 1 6 , 6 1 9 0 9 , 2 9 1 , 6 1 6 0 2 , 2 7 2 , 4 5 9 7 2 , 2 9 8 , 0 3 1 0 6 , 7 5 8 , 1 1 1 2 6 , 6 6 1 , 2 1 1 % 7 2 % 5 1 % 6 % 1 . 0 3 % 7 . 4 5 6 4 5 , 6 1 8 , 3 0 2 y l l  , a º in d f e  t d n n e a s ) e º r rs p i e a r p r  e d v e p O  ª p , a º M E ª( R s to d n e e s  o h l t  C o ª b ,  º in s  e d t e a  c p i  l p a p u m D  ª s  , d º a s e d r n f o e g s ir iln a g p n , ) a º  D s ª d , a º  s e r e  l d c  r e i p c p lf a e M S ª ª (  y d b e  d p  e p i  f a it m n  a y l u e q u  e iq r n e u w e s r  d e a w e t r a d th re  e e t s l  i o f th e , h )º T s .) d º a s  r e i  a R ª p ( t id  l n  a e  V m ª i ( r e g  n p i x r e e  t r l  i e f p r  e s t  f d  a a e d r e f n o i r a e m b e  r m t u a n h  t e h s t d 1 0 7 / 17 prompted us to merge the input reads into a single dataset of more than 372 million reads. The new dataset, herein referred to as ªSUMº, was also automatically filtered and normalized by TADbit (Fig 2E and 2F). The interaction map from the SUM dataset shows all the previously described features of the 3D organization of the Drosophila genome, including the chromosome arm territories, the clustering of centromeres and the infrequent interactions between telomeres.  The Drosophila genome is partitioned into TADs of different robustness Next, we generated 10 kb resolution interaction maps of the Drosophila genome to which we applied a TAD boundary detection algorithm implemented in TADbit (Design and Implementation and S2 Text). This algorithm uses a change-point detection approach inspired from methods used to identify copy number variations in CGH experiments [ 41 ]. Briefly, we use Poisson regression to find the most likely segmentation of the chromosome in m TADs and choose the value of m associated with the optimal Bayesian Information Criterion. In addition to the optimality of the solution, the main advantage of the new algorithm is the assignment of a robustness score to each TAD boundary (Design and Implementation and S2 Text). TADbit identified a total of 689 TADs with an average length of 162.8 kb (ranging from 20 kb to 1.5 Mb), representing larger TADs than previously reported [ 26 ]. Given the hierarchical organization of the genome [ 8 ], we set out to assess whether the difference was due to the identification of new borders or to the merging of the identified TADs. We downloaded the interaction matrices and the TAD borders as defined by Hou et al. [ 26 ] (here referred to as the original definition) and compared them to the borders obtained by running TADbit on these interaction matrices (Fig 3A, 3B and 3C). To this end we used the TADbit module to align multiple TAD boundaries from several experiments (Design and Implementation and Fig 3D). Overall, 81% of the borders defined by TADbit align within 20 kb of an original border when using the TADbit definition as reference (Fig 3E). The number decreases to 67% of the borders when using the original definition as a reference. By forcing TADbit to identify the same number of borders as the original definition (1,110 borders), the agreement increases to 74% within 20 kb. For comparison, the agreement of the TADbit border definitions between the three independent Hi-C experiments (BR, TR1 and TR2) is about 90%. The degree of similarity between the original and the TADbit definitions points to a variation of the algorithm sensitivity more than to discrepancies (see Fig 3D for instance). Moreover, the borders present only in the TADbit definition usually have a weak strength. Indeed, the agreement increases to 94% by comparing borders of 6 or higher strength as defined by TADbit. In summary, our results using TADbit confirm the previously described TAD level partitioning of the Drosophila genome and refine it with a confidence score. Such strength score could later be used to characterize the hierarchical organization of the genome in TADs or as an indicator of the confidence in the prediction (S3 Fig).  Automatic modeling of 50 genomic regions of the Drosophila genome Next, we used TADbit to model the 3D structure of 50 selected genomic regions of about 1 Mb each (S1 Table). It is important to note that there is not an optimal size for modeling a chromatin region. The optimal size depends on the experimental design, the underlying biological question and the computational power. The 50 regions were selected based on their chromatin colors composition [ 25 ]. The selection included the top ten regions of the genome most enriched in each of the five defined chromatin colors. Given the non-homogenous distribution of chromatin colors in the Drosophila genome, where the genome is composed of large stretches of black chromatin interspersed by shorter domains of blue, yellow and red chromatin 8 / 17 Fig 3. TAD border detection and comparison with the results from Hou et al. [ 26 ]. (a) Hi-C normalized interaction matrix at 10 kb resolution for the first 4.5 Mb of chromosome 2L in the Drosophila genome. Interactions matrix and TAD borders were obtained from published data [ 26 ]. (b) Hi-C normalized interaction matrix from the same genomic region and resolution as in panel a. The interaction counts are as previously published [ 26 ] but the TAD borders are those defined by TADbit. (c) Hi-C normalized interaction matrix from the same genomic region and resolution as in panel a. Interaction data and TAD borders are both generated by TADbit. (d) TAD border alignments between the three differently processed experimental data: borders defined in Hou et al. [ 26 ] (Hou-2012, top graph), borders defined by TADbit using the Hou-2012 matrix (mid graph), and borders and matrix determined by TADbit (bottom graph). Dark and light grey arches indicate TADs with higher and lower than expected intra-TAD interactions, respectively. TAD borders are indicated with a black arrow for the Hou-2012 defined borders and by color arrows for the TADbit identified borders. TADbit border robustness (from 1 to 10) is identified by a color gradient from blue to red. (e) Comparison of the agreement between the aligned TAD borders in the three datasets. As a reference, the horizontal grey line indicates a ±20 kb (2 bins) agreement between the biological replica (BR) and the first technical replicate (TR1) as determined by TADbit. The plots in panels a to d were automatically generated by TADbit. (green chromatin is an exception, as it is mainly found in peri-centromeric regions and on chromosome 4), finding continuous 1 Mb stretches of chromatin for the blue, yellow and red colors was not always possible (Fig 4A). For instance, the highest red coverage in a 1 Mb region of the genome was only 22%. For yellow and blue, the maximum coverage was 48% and 52%, respectively, whereas for black and green chromatin types the maximum coverage was 98% and 100%, respectively.  All the selected genomic domains yielded a Matrix Modeling Potential (MMP) score [ 21 ] ranging from 0.85 to 0.96, which is predictive of high accuracy models (S1 Table). To model the 3D structure of the 50 regions, we used as input the Hi-C interaction matrix where each 10 kb bin was represented as a spherical particle in the model. All the particles were restrained in space based solely on their measured interactions, chain connectivity and excluded volume.  The size of the spherical particles representing 10 kb was defined by the relationship 0.01nm/bp assuming the canonical 30 nm fiber [ 42 ]. However, this relationship can be modified or optimized using the ªscaleº parameter in TADbit [ 12 ]. We modeled the chromatin as a homopolymer, assuming that the space occupied by each 10 kb piece of chromatin is constant.  This strategy is necessary because of two reasons. First, the amount of free parameters needed 9 / 17 Fig 4. TADbit 3D models and structural properties. (a) Genomic coordinates, chromatin color proportions, 3D models and structural clustering for the five regions with highest coverage for each color in the Drosophila genome. The ensemble of models for cluster number 1 (the most populated cluster) for each color is represented by its centroid as a solid tube colored by its particle colors. The ensemble around the centroid is simulated by a transparent surface covering a Gaussian smooth surface 150 nm away from the centroid. Figures of 3D models were produced by Chimera [ 47 ]. The structural clustering of the 2,000 models produced per region were aligned with TADbit and clustered by structural similarity. Most modeled regions segregate into two large clusters corresponding to mirror images of each other. (b) Comparison of the input interaction Hi-C matrix to a contact map from the 2,000 built models per region, with Spearman correlation coefficient. (c) Structural properties by particle are shown for accessibility (percentage), density (bp per nanometer), interactions (number), and angle (degree). The background of the plot represents the color assigned to each of the particles in the models. TADbit automatically generated all plots. to optimize the size of each particle independently is intractable statistically and computationally. And, second, we cannot define categories of particles using information about their epigenetic state (with for example smaller heterochromatic particles), because the information about epigenetic states of the chromatin is later used to assess the quality of the 3D models.  The conversion from interactions to distances was previously published [ 12 ]. Briefly,  TADbit considers an inverse relationship between spatial distances and the corresponding 10 / 17 frequencies of interactions. Given this assumption, TADbit transforms the frequencies of interactions into spatial restraints differently for consecutive and non-consecutive particles.  Two consecutive particles are spatially restrained according to their occupancy, which corresponds to the sum of their radii. Non-consecutive particles are restrained based on empirically identified parameters that define a set of restraints, their distances and the forces applied to them. TADbit empirically identifies three optimal parameters using a grid search where a limited number of models are built for each set of parameters. The three parameters are: the proximal distance between two non-interacting particles, a lower bound cut-off to define particles that do not interact frequently and an upper bound cut-off defining particles that do interact frequently. Finally, the modeling parameters are optimized by maximizing the correlation between the contact map of the models and the input Hi-C interaction matrix (Design and Implementation and S1 Table).  All the 50 modeling exercises resulted in high correlations between the contact maps and the Hi-C interaction matrices, ranging from 0.83 to 0.93 (Fig 4B and S1 Table). Altogether, the modeled regions covered a total of 51.8 Mb of the Drosophila genome, forming the largest dataset of genomic regions modeled at 10 kb resolution (S4 Fig).  Structural properties of the Drosophila chromatin colors The generated models were automatically analyzed by TADbit to further characterize their structural properties. In particular, among the set of descriptive measures available in TADbit, we calculated four main structural properties for each particle (genomic bin) in the models.  Those included: (i) accessibility, measuring how accessible from the outside a particle is; (ii) density, measuring the local compactness of the chromatin fiber; (iii) interactions, counting the number of particles within a given spatial distance from a selected particle; and (iv) angle, measuring the angle formed by a particle and its two immediate neighbor particles. To assess whether the different occupancy of proteins and chromatin modifications defining the five colors of chromatin had an influence on the 3D structure of the genome, we assigned to each particle one of the five chromatin colors if at least 50% of the 10 kb region was covered by this chromatin type [ 25 ]. Particles with non-homogenous colors were assigned to the undefined ªwhiteº color. These four measures provided an overview of the structural properties of each color in a particle-based manner. Models with decreasing amount of black, blue and green particles resulted in less compact and regular structures compared to those enriched in blue or black particles (Fig 4C). For example, the top black region (98% black, 1% red and 1% white) had low accessibility throughout, combined with a relatively high density (interestingly, the lowest density for that region corresponds to the only red particle), high number of interactions and closed angle between particles (Fig 4C last column).  Overall, the chromatin colors resulted in distinct structural properties (Fig 5A). For example, black chromatin was the least accessible (median accessibility 26.5%), compared to green and blue (median accessibilities 34.4% and 34.3%, respectively) and to yellow and red (median accessibilities 46.5% and 51.6%, respectively). Black chromatin also featured the highest density in our models (median 212 bp/nm). This was slightly more than blue (207 bp/nm) and substantially more than green, yellow and red (182 bp/nm, 180 bp/nm, and 179 bp/nm, respectively). The chromatin type with most interactions was green (median 48.7 interacting particles within 250 nm) followed by black (45.3), yellow (43.7), blue (41.9), and red (37.9) chromatin. Finally, yellow and red chromatin featured the most extended fibers (median absolute angles 94.6Ê and 89.7Ê, respectively), compared to blue (85.3Ê), green (82.6Ê) and black (80.3Ê). Taken together, the 3D models generated by TADbit indicate that the chromatin types of Drosophila have intrinsic and distinctive structural properties. 11 / 17 Fig 5. Structural properties of the five described chromatin colors. (a) Distribution of each of the four structural properties (that is, accessibility, density, interactions, and angle) grouped by chromatin colors (including the undefined ªwhiteº color for particles of nonhomogeneous coloring). Statistical significance of the differences as computed by Tukey's `Honest Significant Difference' test (*: p &lt; 0.01, ***: p &lt; 0.001, ns: non-significant). (b) Schematic representation of the structural properties of the five colors for the Drosophila chromatin. 12 / 17  It has been shown that the five types of Drosophila chromatin not only differ in protein composition but also in biochemical properties, transcriptional activity, histone modifications, replication timing, and DNA binding factors targeting [ 25 ]. They also differ in the sequence properties and the functions of the embedded genes. Now we demonstrate that the chromatin types also have specific and distinctive structural features (Fig 5B). Importantly, these results shed light on the nature of the elusive black chromatin. Most chromatin markers are depleted in this environment, including those responsible for active repression of transcription. It is thus unclear how genes are maintained silent and why transcription factors do not bind to their consensus sequence in black chromatin. Our results suggest that part of the answer is that black chromatin is very compact and inaccessible to external factors. The high curvature of black chromatin fibers in the models suggests that those regions are intrinsically ordered or that they are compressed. The enrichment of the linker histone H1 in black chromatin may account for all these properties. The previous conception of heterochromatin was closer to green (HP1-bound) or blue (Polycomb-bound) chromatin types. Interestingly, both of them are more accessible than black chromatin, yet green chromatin has a higher number of interactions. This indicates that green chromatin, compared to black chromatin, is a more open but irregular structure where specific interactions are more plausible within a distance cut-off. In contrast, the closed and regular organization of black chromatin results in fewer likely unspecific interactions per particle. This may somehow be related to the observation that the expression of some genes translocated to HP1-bound regions tends to fluctuate, a phenomenon known as position effect variegation [ 43 ]. We speculate that genes caught in this chromatin environment may be trapped in the local entanglement and physically locked away from their enhancers. Both yellow and red chromatin exhibit the most different structural features compared with black chromatin. Their 3D models are open and accessible, which is consistent with the fact that those regions are mostly transcribed and bound by many transcription factors. However, the overall protein occupancy in red chromatin is substantially higher than in yellow chromatin, yet their overall structural properties are relatively similar. This suggests that the extraordinary occupancy observed in red chromatin is not necessarily rooted in its conformational properties, but rather in mechanisms that operate at a finer scale. Additional studies will be needed to further investigate the molecular mechanisms associated to the structural properties of the chromatin types. However, our 3D models, as well as their correlation with the epigenetic features, are a firm basis for future investigation on chromatin occupancy by proteins and it spatial organization.  Availability and future directions Here we introduced TADbit to comprehensively address all the necessary steps from FASTQ files to the full analysis of 3D models. Currently, TADbit is the only computational pipeline to cover all relevant steps [ [4 4], including: (i) read quality control and design of the mapping strategy; (ii) mapping of reads to the reference genome; (iii) interaction map filtering and normalization; (iv) interaction matrix analysis, including matrix comparison, TAD detection and TAD alignment; (v) 3D modelling of genomes and genomic domains; and (vi) 3D model analysis. Recently, several publications have emerged comparing available tools, including TADbit, for a partial list of these steps. For example, reads mapping and contacts filtering [44±46] or TAD detection [ [34, 4 4]. Unfortunately, 3D modelling assessment is practically impossible given the lack of a golden set of genomes of known structure. However, initial theoretical assessments with toy models are being produced [ [2 1]. A complete list of the computational functions implemented in TADbit is provided in the Supplementary Material (S1 Text), each of which is more deeply described in the TADbit online documentation (http://3dgenomes.13 / 17 github.io/TADbit) together with complete tutorials covering each step from sequencing data to 3D model analysis. Next, we will further expand the TADbit functionality with additional modules for meta-matrix analysis, loop detection, matrix comparison, and additional features needed to fully analyze 3C based datasets.  Supporting information S1 Fig. FASTQ quality control plots generated using the quality_plot function in TADbit. (a) Quality plots for the BR dataset. Top plot shows the PHRED score (blue line) and number of ªNº positions (black line) as a function of the sequence position in the reads. Bottom plot shows the number of undigested sites (red), dangling ends (yellow) and re-ligated sites (blue) as a function of the nucleotide position in the reads. (b) TR1 dataset. (c) TR2 dataset. (d) SUM dataset. Panels b, c, and d show the same data as described in panel a. (PDF) S2 Fig. Schematic representation of all applied filters in TADbit to remove 3C-based artifacts in the mapped reads. The filters include dangling-ends, self-circles, errors, random breaks, too short, too large, over-represented or duplicated reads. The exact definition of each of the filters can be found in the ªonline methodsº section of the manuscript. (PDF) S3 Fig. Percentage of borders of a given robustness score. Data for borders aligning within 10 kb (a) or exactly in the same bin (b). The plot on the left of the panels assesses the global sensitivity of TADbit predictions by comparing it with TAD borders ªoriginal definitionº (see main text). The plot on the right assesses the sensitivity of TADbit prediction to experimental replicates. The plots show the border agreements (in percentage) as a function of the TADbit border strength. (PDF) S4 Fig. 3D models of selected domains in the Drosophila genome. Superimposed 3D structures for selected models in cluster #1 for each of the 50 modeled domains. Models are colored by their particle chromatin type as previously defined [ [2 5]. They can be directly visualized using TADkit by visiting the Web site http://www.3DGenomes.org/datasets/serra_etal.(PDF) S1 Table. Selected 50 regions of the Drosophila melanogaster genome for modeling. Columns in the table correspond to: Chromosome, starting coordinate, end coordinate, size of the region (in Mb), index number of the first bin (10Kb bins), index number of the last bin, size of the region (in bins), fraction of the different colors (white, red, yellow, green, blue and black), MMP score [2 21 ] of the interaction matrix, correlation coefficient of the contact map of the final models and the initial interaction matrix, TADbit parameters for the modeling (including, scale, lower distance, lower cut-off, upper cut-off, and distance cut-off). Description of the parameters can be found in the main text. (PDF) S1 Text. TADbit classes and functions. Description of all functions and classes available in TADbit. (PDF) S2 Text. TAD border detection from interaction matrices. Outline of the TAD detection algorithm implemented in TADbit.  (PDF) 14 / 17 S3 Text. Structural similarity score for model clustering. Description of the similarity measure comparing two models generated with TADbit. (PDF) S4 Text. Serra et al 2015 dataset. Description of the Serra et al 2015 dataset to reproduce the data in this article. (PDF) Author Contributions   Conceptualization: FS DB GJF MAMR.\r\n    Data curation: FS MAMR.\r\n  Formal analysis: FS DB MG DC GJF MAMR.    Funding acquisition: MAMR.\r\n  Investigation: FS DB MG DC GJF MAMR. Methodology: FS DB GJF MAMR.    Project administration: MAMR.\r\n  Resources: FS DB MG DC GJF MAMR. Software: FS DB MG DC GJF MAMR.    Supervision: GJF MAMR.\r\n    Validation: FS MAMR.\r\n    Visualization: MG DC MAMR.\r\n    Writing ± original draft: FS DB MG DC GJF MAMR.\r\n    Writing ± review &amp; editing: FS DB MG DC GJF MAMR.\r\n  15 / 17 16 / 17     ",
    "sourceCodeLink": "https://github.com/3DGenomes/tadbit",
    "publicationDate": "0",
    "authors": [
      "Editor: Andreas Prlic",
      "UNITED STATES",
      "FrancË ois Serra",
      "Davide BauÁ",
      "Mike Goodstadt",
      "David Castillo",
      "Guillaume J. Filion",
      "Marc A. Marti-Renom"
    ],
    "status": "Success",
    "toolName": "TADbit",
    "homepage": ""
  },
  "35.pdf": {
    "forks": 0,
    "URLs": ["github.com/sysbiolux/FALCON"],
    "contactInfo": ["thomas.sauter@uni.lu"],
    "subscribers": 4,
    "programmingLanguage": "Matlab",
    "shortDescription": "FALCON: A Toolbox for the Fast Contextualization of Logical Networks",
    "publicationTitle": "FALCON: a toolbox for the fast contextualization of logical networks",
    "title": "FALCON: a toolbox for the fast contextualization of logical networks",
    "publicationDOI": "10.1093/bioinformatics/btx380",
    "codeSize": 10901,
    "publicationAbstract": "Motivation: Mathematical modelling of regulatory networks allows for the discovery of knowledge at the system level. However, existing modelling tools are often computation-heavy and do not offer intuitive ways to explore the model, to test hypotheses or to interpret the results biologically. Results: We have developed a computational approach to contextualize logical models of regulatory networks with biological measurements based on a probabilistic description of rule-based interactions between the different molecules. Here, we propose a Matlab toolbox, FALCON, to automatically and efficiently build and contextualize networks, which includes a pipeline for conducting parameter analysis, knockouts and easy and fast model investigation. The contextualized models could then provide qualitative and quantitative information about the network and suggest hypotheses about biological processes. Availability and implementation: FALCON is freely available for non-commercial users on GitHub under the GPLv3 licence. The toolbox, installation instructions, full documentation and test datasets are available at https://github.com/sysbiolux/FALCON. FALCON runs under Matlab (MathWorks) and requires the Optimization Toolbox. Contact: thomas.sauter@uni.lu Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2016-10-28T09:13:14Z",
    "institutions": ["University of Luxembourg"],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-09-07T05:41:18Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx380   FALCON: a toolbox for the fast contextualization of logical networks     S e´bastien De Landtsheer  0    Panuwat Trairatphisan  0    Philippe Lucarelli  0    Thomas Sauter  0    0  Systems Biology Group, Life Sciences Research Unit, University of Luxembourg ,  Belvaux ,  Luxembourg     2017   1  1  6   Motivation: Mathematical modelling of regulatory networks allows for the discovery of knowledge at the system level. However, existing modelling tools are often computation-heavy and do not offer intuitive ways to explore the model, to test hypotheses or to interpret the results biologically. Results: We have developed a computational approach to contextualize logical models of regulatory networks with biological measurements based on a probabilistic description of rule-based interactions between the different molecules. Here, we propose a Matlab toolbox, FALCON, to automatically and efficiently build and contextualize networks, which includes a pipeline for conducting parameter analysis, knockouts and easy and fast model investigation. The contextualized models could then provide qualitative and quantitative information about the network and suggest hypotheses about biological processes. Availability and implementation: FALCON is freely available for non-commercial users on GitHub under the GPLv3 licence. The toolbox, installation instructions, full documentation and test datasets are available at https://github.com/sysbiolux/FALCON. FALCON runs under Matlab (MathWorks) and requires the Optimization Toolbox. Contact: thomas.sauter@uni.lu Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  The functional characteristics of eukaryotic cells are largely determined by the properties of their regulatory networks. Notwithstanding the vast amount of biological data accumulated over the past decades, a global model of the way these networks determine the phenotypes of both healthy and diseased cells remains elusive. One goal of systems biology is to understand these networks at the highest possible functional level, for example to devise therapeutic strategies for treating patients affected by diseases like cancer.  Numerous mathematical approaches exist to optimize and train regulatory network models against steady-state experimental data  (Villaverde and Banga, 2014) . Of these, logical models  (Le Nove`re, 2015)  are of particular interest, as they are able to capture essential features of the system being modelled and generate biological insights, while requiring less prior knowledge and experimental observations than differential equation models  (Morris et al., 2010) . Some successful applications include the logical models of yeast cellcycle protein network  (Li et al., 2004) , gene regulatory networks  (Mendoza et al., 1999) , signalling networks  (Saez-Rodriguez et al., 2007) . In addition, logical models are in general more powerful than statistical models, as they incorporate the relational information embedded in the network structure, while statistical models aiming at reverse-engineering biological networks from high-throughput data implicitly consider all possible topologies  (Bansal et al., 2007) .  In logical models of systems at steady-state, nodes represent the degree of activation of the constituents of the system at equilibrium and edges represent the logical functions between nodes. These functions can be either linear or non-linear functions of the parent nodes and are combinations of the fundamental 'AND', 'OR' and 'NOT' Boolean functions.  While Binary Boolean models  (Kauffman, 1969)  only consider full activation or complete absence, more quantitative approaches, for instance, Probabilistic Boolean Networks (PBNs)  (Trairatphisan et al., 2013)  and Dynamic Bayesian Networks (DBNs)  (La\u20achdesma\u20acki et al., 2006)  can account for intermediate or continuous activation values and allow the integration of data uncertainty. These approaches are usually analyzed by Monte Carlo approaches  (Mizera et al., 2016; Trairatphisan et al., 2014) , which can be computationally demanding or non-intuitive to use. Here, we propose a tool called FALCON to efficiently contextualize logical regulatory networks based on steady-state experimental data. Our algorithm is based on DBNs and computes the expected value of the nodes by including an algebraic interpretation of the logical gates. The FALCON pipeline is shown in Figure 1.    2 Materials and methods\r\n   2.1 Modelling of logical networks\r\n  FALCON models biological regulatory systems as DBNs, which are directed graphical models defined by the set of n nodes with PXa¼ðX½tð0iÞ;Þ1Þ nwahnerdethXetðipÞdroenboatbeislittyhediis'ttrhibnuotidoenaPtðtXi mtjeXtt a1nÞd¼PQaðin¼X1tðiPÞÞð XretðpiÞ-j resents the parents of XtðiÞ. These conditional probabilities are implicitly formulated by the structure of the network. The different nodes represent the different molecules of the system, with a value corresponding to the degree to which these molecules exist in their active form (for example, phosphorylated proteins). These node values can be understood as the proportion of the molecules in the system being active, or as the probability for a randomly chosen molecule to be active at time t.  In the FALCON framework, each molecular interaction is formulated as a logical predicate associated with a weight quantifying the relative importance of that specific interaction. We model different types of biochemical interactions with two types of edges: positive and negative edges connect activators and inhibitors to their downstream targets. Hyperedges corresponding to the 'AND' and 'OR' logical operations link multiple nodes to an output node, and model the activity of protein complexes and competition, respectively. Each edge and hyperedge is associated to a weight kjðiÞ representing the relative influence of the upstream node to the downstream node. Because our modelling framework is grounded in Bayesian theory, the weights need to obey the law of total probability: for each node XðiÞ having a set jþof m activating functions, we ensure the sum of activating weights Pjmþ¼1 kjðþiÞ ¼ 1. Similarly, as weights of inhibiting interactions materialize the relative inhibition of upstream nodes, for nodes having a set j of l inhibiting functions, we ensure that 0 Pjl ¼1 kjðiÞ 1.  Given a network structure established from prior knowledge, a set of parameters (weights) and a set of experimental conditions, the steady-state of the network is computed for each of the conditions and the values of the nodes corresponding to the measured species are recorded. For each one of the conditions, the nodes of the network are initialized with random values, except for the nodes considered as inputs (external to the system) for which the value is determined by the experimental condition and kept constant. The network is then updated repeatedly by computing synchronously for each node the expected value of its probability distribution, given the value of its parent nodes and the weights associated with each interaction.  XtðiÞ ¼  Xm kðiÞPa XðiÞ ðjþÞ jþ¼1 jþ t 1 1  j ¼1 kjðiÞPa XðiÞ ðj Þ Xi t 1 Because all nodes at each update are considered as independent, the inputs values of 'AND' logical gates are multiplied. The computation of 'OR' gates follows De Morgan's law, i.e. the complement of the union of two sets is the same as the intersection of their complements. Inputs pointing to the same child node that are not members of a logical gate are summed. Table 1 summarizes the different types of interactions explicitly formulated in our framework. The algebraic formulas used for the computations can be directly derived from the conditional probability tables of the DBN formulation of the logical interactions.  The resulting dynamical system converges to a steady-state where each node value corresponds to the normalized equilibrium concentration of the activated form of the molecule in the system.    2.2 Contextualization algorithm\r\n  Objective function. To perform the contextualization of the model with experimental data, we extract from the network at steady-state the value of the nodes corresponding to the measurements, compare them with the normalized values from the experimental data and compute the mean squared error (MSE) between the estimated values and the measurements. We minimize this measure of the error by optimizing the value of the weights using a gradient-descent algorithm. To guarantee high efficiency while allowing for arbitrary degrees of recurrence in the networks, we use the interior-point method  (Waltz et al., 2004) . A scheme of the FALCON workflow is presented in Figure 1.  Rapid optimization. Using the gradient-descent optimization algorithm fmincon with interior-point method, FALCON is able to rapidly estimate the set of weights that minimizes the objective function. Random initialization of the weights is done either from a uniform distribution across the [0, 1] range, or from a truncated normal distribution centred on 0.5, depending on users' choice. Normally distributed initial values have been shown to improve learning for deep neural networks  (Glorot and Bengio, 2010)  and in our hands, increase the speed of convergence of the optimization algorithm. 2.3 Subsequent analyses on optimized logical networks Once a set of parameters has been inferred from a given topology and dataset, a series of additional analyses can be performed to gain more insight into the systems-level properties of the regulatory network being modelled as summarized in Figure 2.  Robustness of optimized parameter values. Depending on the topology of the network, the uncertainty in the measurement of some nodes can have more impact on the parameter values of the model than others. FALCON can analyze the uncertainty on inferred parameter values by sampling a user-defined number of artificial datasets based on original experimental measurements and determining the weights of the model in the light of the new data (Fig. 2a). The artificial datasets are constructed from the average experimental measurements and their associated error, assuming normally distributed residuals.  Identifiability analysis. In order to assess the identifiability of the model parameters, an approach similar to Raue et al. is applied  (Raue et al., 2009) . For each parameter, the algorithm samples the range of possible parameter values [0, 1], and re-optimizes the model under the additional constraint of this parameter being fixed to each one of the sampled values. In order to obtain the most meaningful results we sample the same number of points on both sides of the optimal value. We include the option to skip the most extreme values based on a threshold determined by the resampling analysis (red line, Fig. 2b), thereby accelerating computations. The resulting MSE profiles allow to determine which parameters are well constrained by the experimental measurements.  Interactions knockouts. FALCON allows the systematic removal of each edge in the network and provides a graphical output showing the effect on the global fitness of the model. The models are compared using the Akaike Information Criterion  (Burnham and Anderson, 2004) , which balances goodness-of-fit with model complexity (Fig. 2c). By using this additional analysis, it is possible to differentiate the crucial edges of the system from the ones that are dispensable, which can be pruned out.  Nodes knockouts. A frequent goal of systems biology analyses is to identify the crucial molecules of a regulatory network. Often performed via network topological properties (centrality measures), this identification is of particular interest in the case of target discovery efforts. FALCON allows the systematic evaluation of models in which each node is removed from the network. The comparison of these models using the Akaike Information Criterion allows to identify these crucial nodes not only from topological properties but from the effect their removal has on the behaviour of the entire system (Fig. 2d).  Differential regulation. In many real-life modelling applications, a system is studied in different contexts. For example, during a drug screen, the same signalling pathways are studied for different cell lines, or over time. One goal of systems biology is to identify differences between the contexts in the way the system is regulated. FALCON automates such analyses by optimizing identical models in parallel for multiple series of experimental conditions. Users can discover which parts of the network are activated or shut down between cell lines/time points, and this may lead to the identification of specific interventions strategies for each context (Fig. 3).     3 Pipeline and performance\r\n  FALCON is a highly efficient optimization tool that is capable of contextualizing small-to-large biological networks. For an easy input of model structure and experimental data, FALCON accepts different file formats (.txt, .xls, .xlsx, .csv) which are subsequently used to build logical models. Inference of network structure, interaction matrices and parameter constraints are fully automated, and the toolbox outputs a user-friendly summary comprising the optimized weights for the different interactions, both in text and graphical forms. To facilitate the use of our toolbox, we included a graphical user interface (GUI) to guide users through the different steps of the workflow. Users who are more comfortable with the MATLAB language can instead choose to use the provided driver script for full flexibility.  To showcase the performance of our toolbox, we provide four examples, including the replication of several studies, each presenting a particular challenge for the toolbox. The results of our tests are shown in Table 2. All computations were performed on a network, associated synthetic data and trained model are illustrated in Supplementary Figure S1.  Toy model: we demonstrate the basic functionality of FALCON on a 6-node toy model, comprising both positive and negative interactions, as well as a Boolean AND gate. The structure of this PDGF: we used FALCON to optimize a platelet-derived growth factor signalling model  (Trairatphisan et al., 2016) , comprising 30 nodes and 37 interactions (19 free parameters). The dataset was assembled from the quantification of 6 proteins by western blot analysis in HEK293 cells expressing a constitutively active form of the PDGF receptor, in the presence or absence of two types of perturbations: single-point mutations of tyrosine residues on the PDGF receptor associated with the recruitment sites of downstream signalling molecules, and kinase inhibitors. We obtained a fitting cost (MSE ¼ 0.0041) and parameter values very similar to the original study, where the tool optPBN  (Trairatphisan et al., 2014)  was used to perform the optimization, and in accordance with it, we are able to train the network with single perturbations and accurately predict the signalling profiles of combined perturbations experiments (see Supplementary Material).  Apoptosis: we replicated a modified model of a previous study in which a large Boolean model of apoptosis was used to investigate non-linear dose-effects of UV radiation on cultured hepatocytes  (Schlatter et al., 2009; Trairatphisan et al., 2014) . The model comprises 138 nodes and 160 interactions (41 free parameters). We correctly estimated apoptosis levels and the other associated experimental measures, and could draw the same conclusions as the original study concerning the importance of cross-talks, especially between Caspase 8 and NFKB (see Supplementary Material). While the original study used the software CellNetAnalyzer  (Klamt et al., 2007) , which uses a multi-value Boolean formalism and concentrates on network properties, a previous replication with the optPBN toolbox  (Trairatphisan et al., 2014)  could infer more quantitative properties, but at the expense of long computation times. Analysis of this network and data with FALCON is comparatively very fast with up to 170-fold improvement (FALCON: 76 seconds; optPBN: 4 hours 40 minutes) and we obtained a fitting cost (FALCON: MSE ¼ 0.017) comparable with the previous studies (optPBN: MSE ¼ 0.011; Schlatter et al.: MSE ¼ 0.013). In comparison, CellNetAnalyzer, using discrete Boolean modelling and only able to consider either full activation of complete inactivity of the molecules, achieves a worse fit (MSE: 0.056). The comparison of the inferred molecular states of optPBN and FALCON can be found in Supplementary Figure S6.  MAPK: we compared the performance of our tool with the software CellNOptR  (MacNamara et al., 2012; Terfve et al., 2012)  in the fuzzy logic mode (CNORfuzzy) for quantitative optimization of model states. Using the toy example provided, which is the optimized network of the DREAM4 challenge and contains 22 nodes, 36 interactions and 25 experimental conditions  (Prill et al., 2011) , we obtained a similar fitting cost with FALCON (MSE ¼ 0.036) and with CellNOptR (MSE ¼ 0.032) but with a gain of speed of about 44 times (see Table 2).    4 Discussion\r\n  We present FALCON as an alternative tool for the efficient optimization and comprehensive analysis of logical models of regulatory networks. Our modelling framework, based on DBNs, is able to determine qualitative and quantitative features of the systems being modelled. Node values, being comprised in the interval [0, 1], represent the probabilities for molecules to be in their active state at equilibrium. They can also be understood as the normalized average activities of the nodes. The computed parameters, or weights, also comprised in the interval [0, 1] and subject to the law of total probability, represent the probabilities for the designated interactions to influence downstream nodes. They can also be interpreted as the relative influences of the parent nodes on their children nodes and are useful in assessing the flow of the signal transduction.  FALCON, through its GUI, is easy to use for scientists without extensive modelling experience. FALCON is also very fast compared to similar tools based on PBNs, and surpassed CellNOptR in our test. The low computation costs make it possible to analyze the models at the systems level through a series of bundled additional analyses which allow to answer a number of biologically important questions: whether the parameter values are well constrained by the available data, how the experimental error influences the confidence in the parameter values, and which are the nodes and interactions most crucial to the behaviour of the system versus the ones that can be pruned out. Together, our results suggest that FALCON is a very useful software for rapid model exploration, especially for large networks and large datasets.  Compared to the popular package CellNOptR, the FALCON pipeline is faster in contextualizing a small graphical model with quantitative data. The inferred parameters are also more intuitively understandable as the relative strength of the interactions, while CellNOptR combines linear and Hill's equations in a way that does not encourage direct interpretation. This relative complex formulation, together with the multiple concurrent formalisms proposed and the increased computational cost suggest reserving this tool for more complex tasks, while FALCON is better adapted for exploratory studies of larger networks and datasets.  Future development of the FALCON toolbox will include full compatibility with established model representation formats (SBML-Qual, Bio-PAX), and the conversion of the toolbox to other languages, like R, Python and C þþ. One particular aspect that we regard as highly interesting is the use of FALCON to explore model topologies in a large-scale, systematic way to uncover previously unknown mechanisms in regulatory networks.  In terms of applications, we demonstrated that FALCON is applicable to model signal transduction networks and could easily be extended to study other biological regulatory systems. We envision that FALCON has the potential to be widely adopted by the computational biology community, including biologists with limited programming experience.    Acknowledgements\r\n  We would like to acknowledge Dr Jun Pang, Dr Andrzej Mizera, Prof. Dr Dagmar Kulms, Greta del Mistro and Dr Thomas Pfau for the fruitful discussions and suggestions on the modelling and analytical pipelines.    Funding\r\n  This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 642295 (MEL-PLEX) and the Luxembourg National Research Fund (FNR) within the projects MelanomSensitivity (BMBF/BM/7643621) and ALgoReCEll (INTER/ANR/15/11191283).  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/sysbiolux/FALCON",
    "publicationDate": "0",
    "authors": [
      "S e´bastien De Landtsheer",
      "Panuwat Trairatphisan",
      "Philippe Lucarelli",
      "Thomas Sauter"
    ],
    "status": "Success",
    "toolName": "FALCON",
    "homepage": ""
  },
  "48.pdf": {
    "forks": 5,
    "URLs": [
      "github.com/pascalkieslich/mousetrap-os#",
      "github.com/pascalkieslich/mousetrap-os",
      "github.com/pascalkieslich/mousetrap-os#installation",
      "osdoc.cogsci.nl/manual/timing/",
      "pascalkieslich.github.io/mousetrap/",
      "github.com/pascalkieslich/mousetrap-resources,",
      "github.com/pascalkieslich/mousetrap-os#examples"
    ],
    "contactInfo": ["kieslich@psychologie.uni-mannheim.de"],
    "subscribers": 2,
    "programmingLanguage": "Python",
    "shortDescription": "Mouse-tracking plugin for OpenSesame",
    "publicationTitle": "Mousetrap: An integrated, open-source mouse-tracking package",
    "title": "Mousetrap: An integrated, open-source mouse-tracking package",
    "publicationDOI": "10.3758/s13428-017-0900-z",
    "codeSize": 1888,
    "publicationAbstract": "Mouse-tracking - the analysis of mouse movements in computerized experiments - is becoming increasingly popular in the cognitive sciences. Mouse movements are taken as an indicator of commitment to or conflict between choice options during the decision process. Using mouse-tracking, researchers have gained insight into the temporal development of cognitive processes across a growing number of psychological domains. In the current article, we present software that offers easy and convenient means of recording and analyzing mouse movements in computerized laboratory experiments. In particular, we introduce and demonstrate the mousetrap plugin that adds mouse-tracking to OpenSesame, a popular general-purpose graphical experiment builder. By integrating with this existing experimental software, mousetrap allows for the creation of mouse-tracking studies through a graphical interface, without requiring programming skills. Thus, researchers can benefit from the core features of a validated software package and the many extensions available for it (e.g., the integration with auxiliary hardware such as eye-tracking, or the support of interactive experiments). In addition, the recorded data can be imported directly into the statistical programming language R using the mousetrap package, which greatly facilitates analysis.",
    "dateUpdated": "2017-09-11T09:47:55Z",
    "institutions": [
      "University of Mannheim",
      "University of Koblenz-Landau",
      "Schloss Ehrenhof Ost",
      "Max Planck Institute for Research on Collective Goods",
      "Pascal J. Kieslich"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2015-10-20T13:49:26Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     10.3758/s13428-017-0900-z   Mousetrap: An integrated, open-source mouse-tracking package     Pascal J. Kieslich  kieslich@psychologie.uni-mannheim.de  0  1  2  3    Felix Henninger  0  1  2  3    0  Center for Doctoral Studies in Social and Behavioral Sciences, University of Mannheim ,  Mannheim ,  Germany    1  Cognitive Psychology Lab, University of Koblenz-Landau ,  Landau ,  Germany    2  Experimental Psychology, School of Social Sciences, University of Mannheim, Schloss Ehrenhof Ost ,  D-68131 Mannheim ,  Germany    3  Max Planck Institute for Research on Collective Goods ,  Bonn ,  Germany    4  Pascal J. Kieslich     Mouse-tracking - the analysis of mouse movements in computerized experiments - is becoming increasingly popular in the cognitive sciences. Mouse movements are taken as an indicator of commitment to or conflict between choice options during the decision process. Using mouse-tracking, researchers have gained insight into the temporal development of cognitive processes across a growing number of psychological domains. In the current article, we present software that offers easy and convenient means of recording and analyzing mouse movements in computerized laboratory experiments. In particular, we introduce and demonstrate the mousetrap plugin that adds mouse-tracking to OpenSesame, a popular general-purpose graphical experiment builder. By integrating with this existing experimental software, mousetrap allows for the creation of mouse-tracking studies through a graphical interface, without requiring programming skills. Thus, researchers can benefit from the core features of a validated software package and the many extensions available for it (e.g., the integration with auxiliary hardware such as eye-tracking, or the support of interactive experiments). In addition, the recorded data can be imported directly into the statistical programming language R using the mousetrap package, which greatly facilitates analysis.    Mouse-tracking  Experimental design   Software  Response dynamics  Process tracing   OpenSesame  Python       Introduction\r\n  Mouse-tracking - the recording and analysis of mouse movements in computerized experiments - is becoming an increasingly popular method of studying the development of cognitive processes over time. In mouse-tracking experiments, participants typically choose between different response options represented by buttons on a screen, and the position of the mouse cursor is continuously recorded while participants move towards and finally settle on one of the alternatives  (Freeman &amp; Ambady, 2010) . Based on the theoretical assumption that cognitive processing is continuously revealed in motor responses  (Spivey &amp; Dale, 2006) , mouse movements are taken as indicators of commitment to or conflict between choice options during the decision process  (Freeman, Dale, &amp; Farmer, 2011) .  Mouse-tracking was first introduced as a paradigm in the cognitive sciences by Spivey, Grosjean, and Knoblich (2005). In their study on language processing, participants received auditory instructions to click on one of two objects (e.g., \u201cclick the candle\u201d). A picture of the target object was presented together with a picture of a distractor that was either phonologically similar (e.g., \u201ccandy\u201d) or dissimilar (e.g., \u201cdice\u201d). Participants' mouse movements were more curved towards the distractor if it was phonologically similar than if it was dissimilar, suggesting a parallel processing of auditory input that activated competing representations.  Following  Spivey et al. (2005) , mouse-tracking has been used to gain insight into the temporal development of cognitive processes in a growing number of psychological domains, such as social cognition, decision making, and learning  (for a review, see Freeman et al., 2011) . More recently, researchers have extended the initial paradigm, combining mouse-tracking with more advanced methods. For example, mouse-tracking has been used in conjunction with eye-tracking to study the dynamic interplay of information acquisition and preference development in decision making under risk  (Koop &amp; Johnson, 2013) . In an experiment with real-time interactions between participants, mouse-tracking uncovered different degrees of cognitive conflict associated with cooperating versus defecting in social dilemmas  (Kieslich &amp; Hilbig, 2014) . As these examples show, an increasing number of researchers with different backgrounds and demands are using mouse-tracking to study cognitive processes. As a tool, mouse-tracking is increasingly combined with other methods to build complex paradigms and to integrate data across sources, leading to a richer understanding of cognition.  So far, many researchers conducting mouse-tracking studies have built their own experiments manually in code  (e.g., Koop &amp; Johnson, 2013; Scherbaum, Dshemuchadse, Fischer, &amp; Goschke, 2010) . These custom implementations were often one-off solutions tailored to a specific paradigm, and accompanied by custom analysis code to handle the resulting data specifically and exclusively. Researchers have spent considerable effort and technical expertise building these codebases.  As an alternative, other researchers have used MouseTracker  (Freeman &amp; Ambady, 2010) , a stand-alone program for mousetracking data collection and analysis. Its ability to build simple experiments relatively quickly and design the mouse-tracking screen via a graphical user interface, as well as its integrated analysis tools have made mouse-tracking studies accessible to a broader range of researchers. However, researchers choosing MouseTracker lose the flexibility that general-purpose experimental software provides, in particular the ability to implement complex experimental designs within a single tool (involving, e.g., individually generated stimulus material, real-time communication between participants, and/or the inclusion of additional devices for data collection). In addition, many experimental software packages provide a graphical user interface not only for the design of single trials but of the entirety of the experimental procedure. Finally, most experimental software offers a scripting language so that its built-in features can be customized and extended. Although MouseTracker is free of charge (as citation-ware), the source code is not openly available and thereby not open to extensions and customization, limiting its features to those provided by the original authors. Moreover, MouseTracker is only available for the Windows operating system.  Going beyond custom implementations and stand-alone software solutions, there is a third option, namely providing modular components that extend existing experimental software. By building on the user-friendliness and flexibility of these existing tools, complex and highly customized experiments can be created easily, often without resorting to code. By using established open data formats for storage of mouse trajectories alongside all other data, preprocessing and statistical analyses are possible in common analysis frameworks such as R  (R Core Team, 2016) .  In this article, we present the free and open-source software mousetrap that offers users an easy and convenient way of recording mouse movements. Specifically, we introduce a plugin that adds mouse-tracking to OpenSesame  (Mathôt, Schreij, &amp; Theeuwes, 2012) , a general-purpose graphical experiment builder. Together, these offer an intuitive, graphical user interface for creating mouse-tracking experiments that requires little to no further programming. Users can thus not only draw upon the extensive built-in functionality of OpenSesame for designing stimuli and controlling the experimental procedure, but also on additional plugins that extend it further, adding for example eyetracking functionality  (using PyGaze; Dalmaijer, Mathôt, &amp; Van der Stigchel, 2014)  and real-time interaction between participants (using Psynteract; Henninger, Kieslich, &amp; Hilbig, in press). Yet further customization is possible through Python inline scripts. Like OpenSesame, mousetrap is available across all major platforms (Windows, Linux, and Mac).  In summary, mousetrap provides a flexible, extensible, open mouse-tracking implementation that integrates seamlessly with the graphical experiment builder OpenSesame and can be included by drag-and-drop in any kind of experiment. Its open data format allows users to analyze the data with a software of their choice. In particular, the recorded data can be imported directly into the statistical programming language R using the mousetrap package  (Kieslich, Wulff, Henninger, Haslbeck, &amp; Schulte-Mecklenbeck, 2016) , which allows users to process, analyze, and visualize the collected mousetracking data.  In the following, we provide a brief introduction to mousetrap in combination with OpenSesame, and demonstrate how a mouse-tracking experiment can be created, what the resulting data look like, and how they can be processed and analyzed. In doing so, we create an experiment based on a classic mouse-tracking study by Dale, Kehoe, and Spivey (2007). In this study, participants' mouse movements are recorded while they classify exemplars (specifically: animals) into one of two categories; for example, a participant might be asked to classify a cat as mammal or reptile. The central independent variable in this paradigm is the typicality of the exemplar for its category: Exemplars are either typical members of their category, as above, or they are atypical (e.g., a whale), in that that they share both features with the correct (mammal) and a competing category (fish). The central hypothesis tested in this paradigm is that there should be more conflict between response options when classifying an atypical exemplar, and that mouse movements should therefore deviate more towards the competing category for atypical as compared to typical exemplars.    Building a mouse-tracking experiment\r\n  In the following, we provide a brief tutorial for building a mouse-tracking experiment with mousetrap, demonstrating the plugin's major features as we do so. Our final result will be a simplified version of Experiment 1 by  Dale et al. (2007) . This study incorporates many features of a typical mousetracking study: participants are presented with simple stimuli (here only a single word) in a forced-choice design with two response alternatives (one of which represents the correct response). Besides, a within-participants factor (typicality) is manipulated with a directed hypothesis regarding its influence on mouse movements.    Plugin installation and overview\r\n  Mousetrap depends on OpenSesame (version ≥ 3.1.0), which is available free of charge for all major operating systems from http://osdoc.cogsci.nl/, where it is also documented in depth.Mousetrap itself is available from GitHub (https://github.com/pascalkieslich/mousetrap-os), and is added to OpenSesame as a plugin.1 The plugin includes built-in offline help and documentation for all features. Additional online resources are available from the GitHub repository, which offers extensive documentation and several example experiments, including the one built in the following (https://github.com/pascalkieslich/mousetrap-os#examples).  OpenSesame provides a graphical user interface through which users can create a wide range of experiments without programming. The building blocks of OpenSesame experiments are different items, from which an entire experiment can be assembled by dragand-drop. For example, one might use a sketchpad item to present a visual stimulus, a keyboard_response or mouse_response item to record key presses or mouse clicks in response to the stimulus, and a logger item to write the collected data into a log file. Where desired, Python code can be included in an experiment using inline_script items to add further functionality. All of these items can be organized into sequences to run multiple items in direct succession and loops to repeat the same items multiple times (with variations). In a typical mouse-tracking experiment, a loop may contain the list of different stimuli that are presented in different trials, while a sequence contains all the items that are needed for each trial.  The items provided by the mousetrap plugin allow users to include mouse-tracking in any experiment using the same drag-and-drop operations and with the same ease. As OpenSesame provides two different ways of 1 Information on installing the plugin is provided at https://github.com/pascalkieslich/mousetrap-os#installation building displays, the mousetrap plugin contains two corresponding items: the mousetrap_response and the mousetrap_form item. Both provide comparable mousetracking functionality, but differ in the way the stimulus display is designed.  The mousetrap_response item tracks mouse movements while the stimulus display is provided by another item typically by a sketchpad item that offers a graphical user interface for stimulus design. The mousetrap_response item then monitors the cursor position and registers button clicks.  In comparison, the mousetrap_form item extends the builtin OpenSesame form_base item to provide both a visual display as well as mouse-tracking. The visual content (e.g. text, images, and buttons) can be specified directly from within the item using a simple syntax and positioned on a user-defined grid.  Both the mousetrap_response and the mousetrap_form can be used without writing Python code. For even more flexibility, both items provide corresponding Python classes which can be accessed directly from code. Examples as well as documentation for these are provided online.    Creating a mouse-tracking trial\r\n  Figure 1 shows the structure of our example experiment. In the beginning of the experiment, a form_text_display item labelled \u201cinstructions\u201d is included to explain the task to participants. Next, a loop item called \u201cstimuli\u201d is added, which repeats the same sequence of items in each trial while varying the exemplars and response categories in random order (this data, along with additional metadata, is entered in the loop in tabular format - see bottom right of Fig. 1, where each row corresponds to one stimulus and the associated response options).  A simple way to create a mouse-tracking trial via the graphical user interface is to use a sketchpad item to create the visual stimulus display and a subsequent mousetrap_response item to track the mouse movements while the sketchpad is presented. Before creating the individual items, the overall experiment resolution should be set to match the resolution that will be used during data collection, because sketchpad items run at a fixed resolution and do not scale with the display size. As mouse-tracking experiments are normally run in full-screen mode, the experiment resolution will typically correspond to the display resolution of the computers on which the experiment will be conducted.  The trial sequence itself begins with a form_text_display item that contains a start button in the lower part of the screen, as is typical for mouse-tracking experiments  (Freeman &amp; Ambady, 2010) . Participants start the stimulus presentation by clicking on this button, which also ensures that the start position of the cursor is comparable across trials. Using a form_text_display item is the most basic way of implementing response item that collects the participant's response and tracks cursor movements, and a logger item to save the data into the logfile. On the right, the details of the loop are visible. The design options at the top configure the loop such that each stimulus is presented once in random order, and the table at the bottom contains the actual stimulus data for four trials, namely the exemplar and response categories to be shown on screen, the correct response, and the experimental condition for inclusion in the dataset a start screen because it provides a ready-made layout including some adaptable instruction text and a centered button which can be used to start the trial. Further customization of the start screen is possible, for example, by instead using an additional sketchpad - mousetrap_response combination (as was done in the experiment reported below; see also the online example experiment without forms).  The start item is followed by a sketchpad that defines the actual stimulus (Fig. 2). In the most general terms, a typical mouse-tracking task involves the presentation of a stimulus (e.g., a name or picture of an object), and several buttons. In the current study, the buttons correspond to different categories, and the participant's task is to indicate which category the presented exemplar (i.e., the name of the animal as text) belongs to by clicking on the corresponding button.  The most important part of the mouse-tracking screen is the exemplar that is to be categorized. It is added to the sketchpad using a textline element which allows for creating formatted text. To vary the presented text in each trial and insert the data from the loop (cf. Fig. 1), the corresponding variable name can be added in square brackets.  Creating button-like elements on a sketchpad item consists of two steps. First, the borders of the buttons are drawn using rect elements. Next, the button labels are inserted using textline elements (again using the variable names from the loop in square brackets). When designing the buttons, a symmetrical layout is desirable in most cases. Importantly, all buttons should have the same distance from the starting position of the mouse. Typically, the buttons are placed in the corners of the screen so that participants can easily reach them without risking overshooting the button, yet the distance between buttons is maximal.  As the tracking of mouse movements should start immediately when the sketchpad is presented, the duration of the sketchpad is set to 0 and a mousetrap_response item is inserted directly after the sketchpad in the trial sequence (see Fig. 1, where the mousetrap_response item is labelled \u201cget_response\u201d). Because the mousetrap_response item is separated from the stimulus display, the number of buttons2 as well as their location and internal name need to be provided (see Fig. 3). In our case, and indeed for the majority of experiments, the buttons correspond to the rectangles added to the sketchpad earlier. Thus, the appropriate values for x and y coordinates as well as width and height can be copied from the element script, which can be accessed by double-clicking on its border (Fig. 2). In addition to the coordinates, each 2 The mousetrap_response item supports up to four buttons. More can be added by using the mousetrap_form item or by defining buttons in Python code. button receives a name argument that will be saved as response when a participant clicks within the area of the corresponding button. We recommend using the text content of the button for this purpose (e.g., name = [CategoryLeft] for the left button in Fig. 2).  When the response options are named, a correct response can be defined by adding the corresponding button's name in the respective field. OpenSesame will then automatically code the correctness of the response (as 1 or 0) in the variable labelled correct, which is included in the data for later analysis and can also be used to provide feedback during the study. As with the labels, the correct response in each trial is determined based on the variables specified in the loop (variable   CategoryCorrect, cf. Fig. 1).\r\n  In addition to logging the correctness of a single response, OpenSesame's global feedback variables (e.g., the overall accuracy) can be updated automatically by selecting the each button can be accessed by double-clicking on the respective rectangle: the script corresponding to the left button is shown in the pop-up window. The x, y, w, and h arguments define the left and top coordinates of the rectangle and its width and height. They can be copied and pasted into the mousetrap_response item (cf. Fig. 3) to define the buttons corresponding option, which makes it easy to, for example, pay participants contingent on their performance. In the current experiment, participants are provided with feedback on their performance on the last screen of the experiment through this mechanism.  T h e c u r s o r p o s i t i o n i s r e c o r d e d a s l o n g a s t h e mousetrap_response item is active. The interval in which the positions are recorded is specified under logging resolution in the item settings (see Fig. 3). By default, recording takes place every 10 ms (corresponding to a 100-Hz sampling rate). The actual resolution may differ depending on the performance of the hardware (but has proven to be very robust in our studies, see example experiment below and software validation in the Appendix).  Finally, a logger item is inserted at the end of the trial sequence (see Fig. 1). This item writes the current state of all variables to the participant's log file, which will later be used Fig. 3 Settings of the mousetrap_response item: The topmost settings define the number of buttons used, as well as their position (using the arguments from the rect element script, cf. Fig. 2) and internal name (the button label that was defined in the stimuli loop, cf. Fig. 1). The correct answer can be specified in the Correct button name option to make use of OpenSesame's feedback capabilities. If desired, the mouse cursor can be reset to exact start coordinates at tracking onset. Optionally, a timeout (in ms) can be specified to restrict the time participants have to give their answer. The boundary setting can be used to terminate data collection if the cursor crosses a specified vertical or horizontal boundary on the screen. Additional options concern the possibility to restrict the mouse buttons available for responding, the immediate display of a warning if cursor movement is not initiated within a given interval, and the adjustment of the logging resolution, that is, the interval between subsequent recordings of the cursor position in the analysis. The variable inspector can be used to monitor the current state of the variables in the experiment if it is run from within OpenSesame. The central mouse-tracking data recorded through mousetrap items is stored in variables starting with timestamps, xpos, and ypos.     Alternative implementation using forms\r\n  As mentioned above, mousetrap also provides an alternative way of implementing mouse-tracking via the mousetrap_form item. In contrast to the mousetrap_response item, the display is defined directly within the item by using a form. Forms are a general item type which is used throughout OpenSesame. They place content (which is referred to as \u201cwidgets\u201d and can include labels, images, buttons and image buttons) on a grid, which allows forms to scale with the display resolution. Forms do not provide a graphical interface, but instead use a simple syntax to define and arrange the content.  In the current example, a mousetrap_form could replace both the \u201cpresent_stimulus\u201d sketchpad and the \u201cget_response\u201d mousetrap_response item. Assuming a grid with 16 columns and 10 rows, a visual stimulus display similar to Fig. 2 can be created as follows: widget 6 7 4 2 label text = \" [Exemplar]\" widget 0 0 4 2 button text = \" [CategoryLeft]\" widget 12 0 4 2 button text = \" [CategoryRight]\" The numbers in the example define the position and extent of each widget on the grid, followed by the type of element and its specific settings. The additional mouse-tracking sett i n g s a r e l a r g e l y i d e n t i c a l t o t h e s e t t i n g s o f t h e mousetrap_response item (see Fig. 3 and the online example experiment demonstrating a mousetrap_form).    Methodological considerations\r\n  With the basic structure of the experiment in place, the stimulus display designed and the mouse-tracking added, the experiment would now be ready to run. However, some additional methodological details should be given consideration. Mouse-tracking studies in the literature differ in many methodological aspects, depending on the implementation and researchers' preferences. We can provide no definitive recommendations, but we aim to cover most common design choices and their implementation using the mousetrap plugin in the following  (see also Fischer &amp; Hartmann, 2014; Hehman, Stolier, &amp; Freeman, 2015, for recommendations regarding the setup of mouse-tracking experiments) .   General display organization\r\n  One general challenge is the design of the information shown during the mouse-tracking task. Because mouse movements should reflect the developing commitment to the choice options rather than information search, the amount of new information that participants need to acquire during tracking should be minimized. At the same time, some information must be withheld until tracking begins, so that participants develop their preferences only during the mouse-tracking task and not before.  To some degree, this also represents a challenge for the current example experiment, where in addition to the name of the exemplar, the information about the two response categories needs to be acquired.  Dale et al. (2007)  solved this by presenting the response categories for 2,000 ms at the beginning of each trial, even before the start button appeared. The experiment sketched above can be adapted to implement this procedure by including an additional sketchpad in the beginning of the trial sequence that presents only the buttons and their labels for a specified duration. This procedure was also used in the experiment reported below.  Note that other mouse-tracking studies have used an alternative approach by presenting the critical information acoustically  (e.g., Spivey et al., 2005) . One advantage of this approach is that it prevents any artifacts that might be caused from reading visually presented information. This approach can be implemented easily in OpenSesame, for example, by inserting a sampler item between the sketchpad and the mousetrap_response item.    Starting position\r\n  As previously discussed, it is often desirable to have a comparable starting position of the cursor across trials, as is achieved through the start button in our experiment. However, this method only leads to generally comparable, but not identical starting positions across trials. Though the start coordinates can be aligned during the later analysis, the cursor position can also be reset to exact coordinates by the experimental software before the tracking starts. This can be achieved by checking the corresponding option, and the start coordinates can be specified as two integers (indicating pixel values in the sketchpad metric where \u201c0;0\u201d represents the screen center). These values are usually chosen to correspond to the center of the start button, so that the jump in position is minimized (the mousetrap_response item by default uses start coordinates that correspond to the center of the button on a form_text_display item).    Movement initialization\r\n  In many mouse-tracking studies, participants are explicitly instructed to initiate their mouse movement within a certain time limit  (as described by Hehman et al., 2015)  while other studies refrain from giving participants any instructions regarding mouse movement  (e.g., Kieslich &amp; Hilbig, 2014; Koop &amp; Johnson, 2013) . If such an instruction is given, compliance will typically be monitored and participants may be given feedback. The mousetrap items provide several ways of implementing this. The items automatically compute the initiation_time variable that contains the time it took the participant to initialize any mouse movement in the trial. This variable can be used to give feedback to the participant after the task, for example, by conditionally displaying a warning message if the initiation time is above a predefined threshold. Alternatively, it is also possible to display a warning message while the mouse-tracking task is running. In this case, the time limits and the customized warning message can be specified in the item settings (see Fig. 3). We recommend not using this second option during the actual mouse-tracking task to avoid distracting participants. However, it might be useful in initial practice trials.  Going beyond a mere a priori instruction to initiate movement quickly, some studies have also used a more advanced procedure implementing a dynamic start condition  (e.g., Dshemuchadse, Scherbaum, &amp; Goschke, 2013; Frisch, Dshemuchadse, Görner, Goschke, &amp; Scherbaum, 2015) . In these studies, the critical stimulus information was presented only after participants crossed an invisible horizontal boundary above the start position, ensuring that movement had already been initiated. A dynamic start condition can be implemented by including an additional sketchpad and mousetrap_response item specifying an upper boundary for tracking in the item settings (see corresponding online example experiment).  In a first attempt to assess the influence of the starting procedure on mouse-tracking measures, Scherbaum and  Kieslich (2017)  compared data from an experiment using such a dynamic start condition to a condition in which the stimulus was presented after a fixed delay. While results showed that theoretically expected effects on trial-level mouse-tracking measures (i.e., trajectory curvature) were reliably found in both conditions, effects on within-trial continuous measures were stronger and more temporally distinguishable in the dynamic start condition. This was in line with generally more consistent and homogeneous movements in the dynamic start condition.  Another alternative to ensure a quick initialization of mouse movements is to restrict the time participants have for giving their answer. This time limit can be specified (in ms) in the corresponding option in the item settings (Fig. 3).3    Response indication\r\n  An additional methodological factor that varies across mouse-tracking studies is the way participants indicate their response. While many studies require participants to click on the button representing their choice  (e.g., Dale et al., 2007; Koop, 2013) , in other studies merely entering the area corresponding to the button with the cursor is sufficient  (e.g., Dshemuchadse et al., 2013; Scherbaum et al., 2010) . Both options are available in mousetrap (see Fig. 3). If a click is required, the (physical) mouse buttons that are accepted as a response indication can be specified. By default, mouse clicks are 3 However, introducing a time limit might also induce time pressure which might lead to other (undesired) effects. required and both left and right mouse clicks are accepted.    Counterbalancing presentation order\r\n  A final consideration should be given to potential position effects: So as not to introduce confounds between response alternatives and the position of the corresponding button, the mapping should be varied across trials and / or across participants. This is especially important if the response alternatives stay constant across trials  (which is often the case in decision making studies, e.g., Kieslich &amp; Hilbig, 2014; Koop, 2013) . In the current study, the position of the correct response and the foil (left vs. right) should be varied. This can be done statically by varying their order across trials (see Fig. 1). To go further, the position of response options can be randomized at run time using OpenSesame's advanced loop operations (as was done in the experiment reported below, see also shuffle_horiz online example experiment).     Data collection\r\n  After creating the mouse-tracking experiment, it should be tested on the computers that will later be used to collect the data. We also recommend importing and analyzing selfcreated test data to check that all relevant independent and dependent variables have been recorded, and to check the logging resolution (see below). When preparing the study for running in the lab, a number of methodological factors need to be considered.  As noted in the previous section, mouse-tracking experiments should be run in full screen mode at the maximum possible screen resolution. The OpenSesame Run program, which is included with OpenSesame, can run the experiment without having to open it in the editor, making the starting process more efficient, and hiding the internal structure, conditions, and item names from participants.  In addition, the mouse sensitivity settings of the operating system should be checked and matched across laboratory computers, in particular the speed and acceleration of the cursor relative to the physical mouse (these settings cannot be influenced directly from within OpenSesame). There is currently no single setting applied consistently across studies in the literature, and the settings used in the field are often not reported. Presumably, the settings will often have been left to the operating system defaults (under Windows 7 and 10, medium speed with acceleration) or speed will have been reduced deliberately and acceleration turned off  (as recommended by Fischer &amp; Hartmann, 2014) .  When preparing the laboratory, it should be ensured that participants have enough desk space to move the mouse. In this regard, we have found it useful to move the keyboard out of the way and design the experiment so that participants can complete the entire experiment by using only the mouse. Additionally, heretofore largely unexplored factors concern the handedness of participants, the hand used for moving the mouse, and their interplay. Some authors go as far as to recommend including only right-handed participants  (Hehman et al., 2015) . We would recommend assessing the handedness of participants, as well as the hand actually used for moving the mouse in the experiment.  In general, we would like to stress the importance of documenting mouse-tracking studies in sufficient detail, both so that fellow researchers can replicate the experiment and so that potentially differing findings between individual mouse-tracking studies can be traced back to differences in their methodological setup. Ideally, each of the degrees of freedom sketched above should be documented, as well as the specifics of the lab computers (especially screen resolution and mouse sensitivity settings). It is also very useful to provide a screenshot of the actual mousetracking task. Finally, to give interested colleagues the opportunity to explore the specific details of the task setup, it is also useful to provide them directly with the experiment files. This is particularly easy if mouse-tracking experiments are created in OpenSesame with the mousetrap plugin, as OpenSesame is freely available for many platforms. OpenSesame also provides the option to automatically save experiments on the Open Science Framework and share them with other researchers.    Example experiment\r\n  Having built and tested the experiment, enterprising colleagues could begin with the data collection immediately. We have done exactly this, and have performed a replication of Experiment 1 by   Dale et al. (2007 ). In doing so, we aimed to assess the technical performance of the plugins (especially with regard to the logging resolution), to demonstrate the structure of the resulting data and how they can be processed and analyzed, and to replicate the original result that atypical exemplars lead to more curved trajectories than typical exemplars. The exact experiment that was used in the study (with German material and instructions) and a simplified but with regard to the task identical version (with English example material and instructions) can be found online at https://github.com/pascalkieslich/mousetrap-resources, as can the raw data and analysis scripts.    Methods\r\n  We used the 13 typical and 6 atypical stimuli from Dale et al.'s Experiment 1  (see Table 1 in Dale et al., 2007)  translated to German. Participants first received instructions about their task and completed three practice trials. Thereafter, the 19 stimuli of interest were presented in random order. Participants were not told that their mouse movements were recorded, nor did they receive any specific instructions about moving the mouse.  Each trial began with a blank screen that was presented for 1,000 ms. After that, the two categories were displayed for 2,000 ms in the top left and right screen corners (the order of the categories was randomized at run time), following the procedure of the original study. Next, the start button appeared in the bottom center of the screen, and participants started the trial by clicking on it. Directly thereafter (the cursor position was not reset in this study), the to-be-categorized stimulus word was displayed above the start button and participants could indicate their response by clicking on one of the two categories (see Fig. 2).  The experiment was conducted full screen with a resolution of 1,680 × 1,050 pixels. Laboratory computers were running Windows 7, and mouse settings were left at their default values (acceleration turned on, medium speed). Cursor coordinates were recorded every 10 ms.  The experiment was conducted as the second part in a series of unrelated studies. Before the experiment, we assessed participants' handedness using the Edinburgh Handedness Inventory  (EHI; Oldfield, 1971) . We used a modified version of the EHI with a five-point rating scale on which participants indicated which hand they preferred to use for ten activities (-100 = exclusively left, −50 = preferably left, 0 = no preference, 50 = preferably right, 100 = exclusively right) and included an additional item for computer mouse usage.  Participants were recruited from a local student participant pool at the University of Mannheim, Germany, and paid for their participation (the payment was variable and depended on other studies in the same session). Participants were randomly assigned to either an implementation of the study using the mousetrap plugin in OpenSesame (N = 60, 39 female, mean age = 22.2 years, SD = 3.5 years) or another implementation (a development version of an online mouse-tracking data collection tool) not included in the current article. Participants' mean handedness scores based on the original EHI items indicated a preference for the right hand for the majority of participants (50 of 60 participants had scores greater than 60), no strong preference for eight participants (scores between −60 and 60) and preference for the left hand for two participants (below −60). Interestingly, all participants reported using a computer mouse preferably or exclusively with the right hand, as indicated by the newly added item.    Data preprocessing\r\n  In the following section, we focus on a simple but frequently applied comparison of (aggregate) mouse trajectory curvature.4 In doing so, we will go through all analysis steps from loading the raw data to the statistical tests in the statistical programming language R  (R Core Team, 2016) . The complete analysis script is shown in Fig. 4.  The libraries required for the following analyses can be installed from CRAN using the following command: install.packages(c(\"readbulk\",\"mousetrap\")). T h e r e a f t e r , b o t h l i b r a r i e s a r e l o a d e d u s i n g library(readbulk) and library(mousetrap) respectively. We will only touch upon the most basic features of both; additional library-level documentation can be accessed with the command package?mousetrap (or online at http://pascalkieslich.github.io/mousetrap/), and help for specific functions is available by prepending a question mark to any given command, as in ?mt_ import_mousetrap.  OpenSesame produces an individual comma-separated (CSV) data file for each participant. Because there is a single logger item in the experiment that is repeated with each trial, every line corresponds to a trial. Different variables are spread across different columns. For our purposes, the most important columns are those containing the mouse-tracking data, namely the columns beginning with timestamps, xpos, and ypos. These columns contain the interval since the start of the experiment in milliseconds, and the x and y coordinates of the cursor at each of these time points. The position coordinates are given in pixels, whereby the value 0 for both x and y coordinates corresponds to the center of the screen and values increase as the mouse moves toward the bottom right.  As a first step after opening R (or RStudio), the current working directory should be changed to the location where the raw data is stored (either using setwd or via the user interface in RStudio). To read the data of all participants into R, we suggest the readbulk R package  (Kieslich &amp; Henninger, 2016) , which can read and combine data from multiple CSV files into a single dataset. Readbulk provides a specialized function for OpenSesame data (read_opensesame). Assuming that the raw data is stored in the subfolder \u201craw_data\u201d of the working directory, we can combine all individual files into a single data.frame using read_opensesame(\"raw_data\").  Next, the raw data are filtered so that only the trials of interest are retained. Specifically, all trials from the practice phase are excluded. Besides, we determined which trials were solved correctly using the correct variable, which was automatically set by the mousetrap_response item. The accuracy 4 These analyses differ from the more elaborate analyses in the original article by  Dale et al. (2007) , which we have omitted for reasons of brevity. We provide an R script for replication of the original analyses online. in the current study was 88.9% for atypical and 95.4% for typical trials - results comparable to those in the original study. Following Dale et al., only the correctly completed trials were kept for the analyses.  For preprocessing and analyzing mouse-tracking data, we have developed the mousetrap R package  (Kieslich et al., 2016) . A detailed description of the package and its functions is provided elsewhere  (Kieslich, Wulff, Henninger, Haslbeck, &amp; Schulte-Mecklenbeck, 2017) . In the following, we will focus on the most basic functions needed for the present analyses.  As a precondition for further analysis, the raw data must be r e p r e s e n t e d a s a m o u s e t r a p d a t a o b j e c t u s i n g t h e mt_import_mousetrap function. This function will automatically select the mouse-tracking data columns from the raw data5 and transform their contents into a data structure amenable to analysis.  Next, several preprocessing steps ensure that the data can be aggregated within and compared meaningfully between conditions. Trajectories are remapped using mt_remap_symmetric which ensures that every trajectory starts at the bottom of the coordinate system and ends in the top left corner (regardless of whether the left or the right response option was chosen). Because the mouse cursor was not reset to a common coordinate at the start of tracking, mt_align_start is needed to align all trajectories to the same initial coordinates (0, 0). Trajectories are then typically time-normalized so that each trajectory contains the same number of recorded coordinates regardless of its res p o n s e t i m e ( S p i v e y e t a l . , 2 0 0 5 ) . To t h i s e n d , mt_time_normalize computes time-normalized trajectories using a constant (but adjustable) number of time steps of equal length (101 by default, following Spivey et al.).  Several different measures for the curvature of mouse trajectories have been proposed in the literature  (Freeman &amp; Ambady, 2010; Koop &amp; Johnson, 2011) . One frequently used measure is the maximum absolute deviation (MAD). The MAD represents the maximum perpendicular deviation of the actual trajectory from the idealized trajectory, which is the straight line connecting the trajectories' start and end points.6 The MAD and many additional trial-level measures can be calculated using the mt_measures function.7 These measures are then typically aggregated per participant for each l e v e l o f t h e w i t h i n - p a r t i c i p a n t s f a c t o r. F o r t h i s , mt_aggregate_per_subject can be used (see Fig. 4). 5 In case that more than one mousetrap item is included in the experiment, the names of the columns need to be provided explicitly using the corresponding arguments. 6 If this maximum deviation occurs in the direction of the non-chosen option (i.e., \u201cabove\u201d the idealized trajectory), it receives a positive sign, otherwise a negative sign. 7 This function uses the raw trajectories by default to avoid the (unlikely) possibility that relevant spatial information gets lost during time normalization. In the current sample, the MAD values based on the raw trajectories and on the time-normalized trajectories correlate to .9999. Fig. 4 R script for replicating the main data preparation and analysis steps. First, the individual raw data files are merged and read into R. They are then filtered, retaining only correctly solved trials from the actual task. Next, the mouse-tracking data are imported and preprocessed by remapping all trajectories to one side, aligning their start coordinates and computing trial-level summary statistics (such as the maximum absolute deviation, MAD). The MAD values are aggregated per participant and condition, and compared using a paired t-test. Finally, the trajectories are time-normalized, aggregated per condition, and visualized    Data quality check\r\n  To check whether the intended logging resolution was actually met, mt_check_resolution can be used to compute the achieved interval between logs. Across all recorded mouse positions in all trials that entered the following analyses, 99.4% of the logging intervals were exactly 10 ms, corresponding to the desired logging resolution. An additional 0.5% of intervals were shorter than 10 ms, due to the fact that every click in the experiment leads to an immediate recording of the current cursor position, even outside of the defined logging interval. Finally, 0.1% of logging intervals were greater than 10 ms, of which 76.2% lagged by 1 additional ms only. Overall, the mean timestamp difference was 9.98 ms (SD = 0.43 ms).  A more comprehensive technical validation of the mousetrap plugin is reported in the appendix. Extending a procedure by  Freeman and Ambady (2010) , we used external hardware  (Henninger, 2017)  to generate known movement patterns from the start button to one of the response buttons. An analysis of the recorded cursor positions revealed that almost every change in position was captured on the raw coordinate level, and that the recorded positions and derived trial-level measures almost perfectly corresponded to their expected values.    Results\r\n  A quick first visual impression of the effect of the typicality manipulation on mouse movements can be obtained by inspecting the aggregate mouse trajectories. Specifically, mt_plot_aggregate can be used to average the timenormalized trajectories per condition (first within and then across participants) and to plot the resulting aggregate trajectories (Fig. 5). In line with the hypothesis by Dale et al., the aggregate response trajectory in the atypical condition showed a greater attraction to the non-chosen option than the trajectory in the typical condition.  To statistically test for differences in curvature, the average MAD values per participant and condition can be compared. In line with the hypothesis and the visual inspection of the aggregate trajectories, the MAD was larger in the atypical (M = 343.8, SD = 218.6) than in the typical condition (M = 750 ) x p (e500 t a n i d r o o c y250  Condition  Atypical  Typical 0 −600 −400 −200 x coordinate (px) 0 Fig. 5 Average time-normalized trajectories per experimental condition 172.2, SD = 110.8). This difference was significant in a paired t-test, t(59) = 6.73, p &lt; .001, and the standardized difference of dz = 0.87 represented a large effect.  The analyses just described give an initial impression of what mouse-tracking data look like. While we have provided a first simple test of our basic hypothesis, the analysis has barely scratched the surface of what is possible with this data (and what can be realized using the mousetrap package). Specifically, we have skipped a number of important preprocessing and analyses steps that are standard procedure in mouse-tracking studies, such as the inspection of individual trials to detect anomalous or extreme mouse movements  (Freeman &amp; Ambady, 2010)  and analyses to detect the presence of bimodality  (Freeman &amp; Dale, 2013) .8 The original article our study was based upon also contains many more analyses  (see online supplementary material for a replication of the analyses by Dale et al., 2007, based on the current dataset) .  Several more advanced analyses methods and measures have also been proposed, such as velocity and acceleration profiles, spatial disorder analyses via sample entropy, or the investigation of smooth versus abrupt response competition via distributional analyses  (see, e.g., Hehman et al., 2015, for an overview) . Many of these methods and measures are implemented in the mousetrap R package, and are described and explained in the package documentation. We discuss elsewhere in detail the methodological possibilities and considerations when processing and analyzing mouse-tracking data, as well as their implementation in mousetrap  (Haslbeck, Wulff, Kieslich, Henninger, &amp; Schulte-Mecklenbeck, 2017; Kieslich et al., 2017) .    Discussion\r\n  In this article, we presented the free and open-source software mousetrap that offers users easy and convenient means of recording mouse movements, and demonstrated how a simple experiment can be built and analyzed. Specifically, we introduced mousetrap as a plugin that adds mouse-tracking to the popular, open-source experiment builder OpenSesame, allowing users to create mouse-tracking experiments via a 8 A simple bimodality analysis can be conducted by computing bimodality coefficients (BC). Following  Freeman and Ambady (2010) , we z-standardized MAD values per participant and computed the BC separately for the atypical and the typical condition. In both conditions, the BC was higher than the recommended cutoff (.555), BCTypical = .608, BCAtypical = .593, indicating a bimodal distribution. To analyze whether the difference in MAD between typicality conditions remained significant after excluding outliers, we excluded all trials with |zMAD| &gt;1.50 and repeated the main analyses (for details, see online supplementary material). As in the complete dataset, aggregate MAD was significantly higher in the atypical than in the typical condition, p &lt; .001. Note, however, that more advanced and comprehensive alternative analyses are available  (Kieslich et al., 2017) . graphical user interface. To demonstrate the usage of mousetrap, we created and replicated a mouse-tracking experiment by  Dale et al. (2007) , and analyzed the resulting data using the mousetrap R package. In line with the original hypothesis and results, we found that mouse trajectories displayed greater curvature towards the competing response option for atypical compared to typical exemplars. Naturally, we have only been able to discuss the most salient decisions in the construction of mouse-tracking experiments. However, where possible, we have noted the additional degrees of freedom and design choices, and sketched their implementation.  Mousetrap offers an alternative to the two major ways mouse-tracking studies are currently implemented. First, researchers have built custom code-based implementations of mouse-tracking for specific paradigms. These custom-built experiments can be flexibly tailored to the individual researchers' needs, but their implementation requires extensive programming skills, and paradigms are often cumbersome to adapt to new tasks. Secondly, researchers have relied on MouseTracker  (Freeman &amp; Ambady, 2010) , a specialized experimental software for building mouse-tracking experiments and analyzing the resulting data. While this software has made mouse-tracking studies accessible to more researchers by providing a visual interface for designing the mouse-tracking screen and recording the mouse movements, it forgoes the flexibility and many useful features of general-purpose experimental software (such as the option to define the structure of the experiment itself via a graphical user interface, or to directly include a scripting language for customization and run time adaptation).  Aiming to combine the advantages while avoiding the disadvantages of both approaches, mousetrap extends the general purpose graphical experiment builder OpenSesame (Mathôt et al., 2012). Thereby, it allows users to easily create mousetracking experiments via a graphical interface without requiring programming skills. In addition, it makes available the many useful features of OpenSesame, such as a user-friendly interface for designing the structure of the experiment and implementing advanced randomizations, the support for diverse audiovisual stimuli, an open data format, extensibility via Python scripts, and cross-platform availability.  While mouse-tracking is a frequently used method for assessing response dynamics  (Koop &amp; Johnson, 2011) , it should be noted that other methods are also available, such as the use of remote controllers  (e.g., a Nintendo Wii Remote, cf. Dale, Roche, Snyder, &amp; McCall, 2008)  or the direct recording of hand movements  (via a handle, e.g., Resulaj, Kiani, Wolpert, &amp; Shadlen, 2009, or using a motion capture system, e.g., Awasthi, Friedman, &amp; Williams, 2011) . Another approach that might become more important in future research is the tracking of finger (or pen) movements via touchscreens  (e.g., Buc Calderon, Verguts, &amp; Gevers, 2015; Wirth, Pfister, &amp; Kunde, 2016)  due to the increasing availability of tablets and smartphones. The mousetrap plugin could be extended to implement the latter approach in OpenSesame.  With mousetrap, we hope to make mouse-tracking accessible to researchers from many different fields, and thereby to enable them to gain insights into the dynamics of cognitive processes. Given the fast-paced development of the mousetracking method, we hope that our modular and open approach will help users to implement the increasingly complex designs, to combine mouse-tracking with other process tracing methods such as eye-tracking, and to apply the method in fields where only few mouse-tracking studies have been conducted so far, such as behavioral economics with real-time interactive experiments. Similarly, we hope that the open data format and the close link to open analysis tools such as those demonstrated herein will make the manifold methods of analyzing mouse-tracking data widely available.  Acknowledgments We thank Anja Humbs for testing a development version of the mousetrap plugin for OpenSesame, Monika Wiegelmann and Mila Rüdiger for collecting the data for the example experiment, and Arndt Bröder and Johanna Hepp for helpful comments on an earlier version of this manuscript. This work was supported by the University of Mannheim's Graduate School of Economic and Social Sciences funded by the German Research Foundation    Appendix\r\n    Software validation\r\n  To validate the data collection procedure, we extended the procedure employed by  Freeman and Ambady (2010) , who simulated and processed artificial mouse trajectories. We used external hardware  (Henninger, 2017)  to generate two known movement patterns that connected the start and the top left response button: either a diagonal line, or a triangular path leading only upward at first, and then left towards the response button (Fig. 6). The validation experiment was built in OpenSesame (version 3.1.6, using the legacy backend9) using the mousetrap_response item (version 1.2.1). The screen layout and mouse-tracking settings were identical to the example experiment reported in the main article (cf. Figs. 2 and 3). The study was run on a laboratory terminal with modest hardware (Windows 7 Professional, on an Intel Pentium Dual-Core running at 3 GHz with 4 GB RAM).  In the following simulations, we ventured to perform a strict test of the software: First, to test the performance of the data collection procedure under heavy load, we simulated 9 OpenSesame provides other backends with superior temporal accuracy. However, we used legacy in our simulations and the example experiment, as it is generally more stable, especially when using forms, which are often used when designing mouse-tracking experiments. More information on general benchmark results for OpenSesame can be found at http://osdoc.cogsci.nl/manual/timing/ rapidly changing cursor coordinates. Specifically, in all simulations, the cursor position was updated at the logging resolution (10 ms) to assess whether data is recorded correctly when the cursor position changes as fast as data are collected. On each update, the cursor moved to the next integer pixel location on its path, that is, both one pixel up and one left for 800 px for the diagonal trajectory, or first one pixel upwards for 800 px and then one left for 800 px for the triangular path. The trial was started by a (simulated) click on a start button, which initiated the display of the response buttons, and ended with a mouse click on the left response button, with pauses of 110 ms before movement initiation and 100 ms between the end of movement and the simulated response. This means that the time between the start and end click was 8,210 ms for the diagonal path and 16,210 ms for the triangular path. Second, we validate the resulting data at the lowest possible level, that is, using the raw trajectory coordinates of each individual (simulated) trial. In scientific practice, standard mousetracking analyses will compensate for imperfect measurement to some degree because mouse trajectories are typically timenormalized and analyses are based on aggregate statistics.  For both the diagonal and the triangular path, we simulated 1,000 trials. The resulting data files were read into R and processed and analyzed using the mousetrap R package  (Kieslich et al., 2016) . All data and analyses scripts can be found at https://github.com/pascalkieslich/mousetrap-os# validation.  To determine the temporal alignment between the external hardware and the data recorded by the mousetrap_response item, we performed several analyses (based on the absolute timestamps recorded in OpenSesame): After the click on the start button, the screen with the response buttons was displayed with an average delay of 6.9 ms (SD = 0.7 ms) in both simulations. Mouse-tracking started after an additional delay of 0.7 ms (SD = 0.5 ms). This means that, on average, 7.6 ms passed between a click on the start screen and tracking onset on the next screen. Taking this delay into account, the observed tracking durations10 in both simulations matched the expected value very closely, with an average duration of 8202.9 ms (SD = 0.9 ms) for the diagonal simulation, and an average duration of 16203.1 ms (SD = 0.9 ms) for the triangular simulation.  Next, we assessed whether the specified logging resolution was met, using the mt_check_resolution function to compute the time interval between subsequent recorded cursor positions. In the diagonal simulation, the mean interval was 10.0 ms (SD = 0.3 ms) matching the intended logging interval. Specifically, 99.86% of the logging intervals were exactly 10 Tracking durations can be obtained via the response_time variable stored in OpenSesame or by using the RT variable computed from the timestamps using the mt_measures function of the mousetrap R package. Both approaches lead to identical results. 10 ms, corresponding precisely to the desired logging resolution. An additional 0.12% of intervals were shorter than 10 ms, due to the fact that each click led to an immediate recording of the current cursor position, even before the end of a logging interval (and because logging was not exactly synchronized with simulated cursor movements and clicks). Finally, 0.02% of logging intervals were greater than 10 ms, of which 99.3% lagged by 1 additional ms only. Similar results were obtained in the triangular simulation, in which the mean timestamp difference was 10.0 ms (SD = 0.2 ms) and where 99.92% of the logging intervals were exactly 10 ms, 0.06% were shorter, and 0.02% longer (of which 94.4% lagged by 1 ms only).  To gain a first visual impression of the data, all raw trajectories were plotted separately for the two simulations. As can be seen in Fig. 6, all trajectory shapes were perfectly aligned within each simulation and no anomalous positions were recorded.  Missed position changes due to lags in the logging interval can be identified simply by computing the distance between two adjacent cursor positions recorded in each trial. These are expected to be either 0 px for a period where the cursor did not move along the respective dimension or 1 px along one (for the triangular simulation) or both (for the diagonal simulation) dimensions for a period with movement. Any value greater than 1 px indicates a missed change in position. In the diagonal simulation, 99.9995% of the subsequently recorded positions were either 0 px or 1 px apart for both x and y coordinates - the remaining 0.0005% differed by 2 px along either dimension, indicating that a single movement was missed. In the triangular simulation that involved changes in x coordinate only for the first, and y coordinate only for the second half of the trial, for the x coordinates, 99.9949% of the distances were either 0 px or 1 px, and 0.0051% were 2 px indicating that a single movement was missed (in only a single additional case were two changes in position missed). For the y coordinates, 99.9953% of the distances were either 0 px or 1 px, and 0.0047% were 2 px.  To assess the accuracy of the recorded cursor position at each point during the trial, we computed its expected position for each set of recorded coordinates (based on the known path generated by the external hardware, and taking into account the average tracking onset). We then computed Pearson correlations between the observed and the expected position separately for the x and y coordinates. In the diagonal simulation, the correlation was .99999999996 for both x and y coordinates, and the expected and observed position were identical in 99.9995% of cases (and differed by 1 px for the remaining cases). In the triangular simulation, the correlation was .999999993 for the x coordinates, and .999999995 for the y coordinates. For the x coordinates, the observed and expected position were identical in 99.8994% of cases (and differed by 1 px for all remaining cases except one, where it differed by 2 px). For the y coordinates, the observed and expected position were identical in 99.9298% of cases (and differed by 1 px for the remainder).  Table 1 Expected values, observed mean and standard deviation for selected mouse-tracking measures per simulation Expected M SD  Diagonal MAD MAD maximum absolute deviation, AUC area under curve, AD average deviation.  In the diagonal simulation, Ms for MAD and AD were &lt; 9*10-14 and SD for AD was &lt; 3*10-20  Finally, we computed a number of mouse-tracking indic e s b a s e d o n t h e r a w t r a j e c t o r y d a t a , u s i n g t h e mt_measures function. The descriptive statistics for a selection of the measures can be found in Table 1. In line with the expected measures based on the predetermined paths, the maximum absolute deviation (MAD), area under curve (AUC) and average deviation (AD) were 0 for the diagonal simulation and did not vary between trials. For the triangular simulation, the MAD always met the expected value of 565.69 px (which is the height of a right-angled triangle where both legs have a length of 800 px) and the AUC was always 320,000 px2 (which corresponds exactly to the area of the previously described triangle). The AD values were on average also as expected (M = 279.01 px) with a minor variation between trials (SD = 0.02 px) because the AD takes every logged coordinate value into account and is therefore most sensitive to variations therein.  In sum, with regard to both logging resolution and measured coordinates, the mousetrap plugin for OpenSesame captures the raw mouse trajectory extremely well. It should be noted that the current validation was performed under even stricter conditions than those used in the validation of another software package  (Freeman &amp; Ambady, 2010) : in the current simulation, the cursor was updated at a higher rate (every 10 ms instead of 30 ms) and more fine-grained analyses were used, focusing on exact raw trajectories instead of averaged data. When applied to actual data, even the remaining minute discrepancies will most often be negligible given that mouse-tracking analyses usually interpolate the raw trajectories to some extent (e.g., through timenormalization) and analyze trial summary statistics such as the measures reported above. Thus, we are confident that our software will perform reliably under most conditions.    ",
    "sourceCodeLink": "https://github.com/pascalkieslich/mousetrap-os",
    "publicationDate": "0",
    "authors": [
      "Pascal J. Kieslich",
      "Felix Henninger"
    ],
    "status": "Success",
    "toolName": "mousetrap-os",
    "homepage": ""
  },
  "1.pdf": {
    "institutions": ["Princeton University"],
    "URLs": [
      "github.com/Singh-Lab/DifferentialMutation-Analysis",
      "github.com/SinghLab/Differential-Mutation-Analysis",
      "gdc.cancer.gov/"
    ],
    "contactInfo": ["mona@cs.princeton.edu"],
    "fulltext": "     Przytycki and Singh Genome Medicine     10.1186/s13073-017-0465-6   Differential analysis between somatic mutation and germline variation profiles reveals cancer-related genes     Pawel F. Przytycki  0  1    Mona Singh  mona@cs.princeton.edu  0  1    0  Department of Computer Science, Princeton University ,  Princeton, NJ 08544 ,  USA    1  Lewis-Sigler Institute for Integrative Genomics, Princeton University ,  Princeton, NJ 08544 ,  USA     2017   9    7  8  2017    2  2  2017     A major aim of cancer genomics is to pinpoint which somatically mutated genes are involved in tumor initiation and progression. We introduce a new framework for uncovering cancer genes, differential mutation analysis, which compares the mutational profiles of genes across cancer genomes with their natural germline variation across healthy individuals. We present DiffMut, a fast and simple approach for differential mutational analysis, and demonstrate that it is more effective in discovering cancer genes than considerably more sophisticated approaches. We conclude that germline variation across healthy human genomes provides a powerful means for characterizing somatic mutation frequency and identifying cancer driver genes. DiffMut is available at https://github.com/SinghLab/Differential-Mutation-Analysis.    Cancer  Whole-exome sequencing  Somatic mutations  Germline variation  Cancer driver genes       Background\r\n  Large-scale cancer genome sequencing consortia, such as TCGA [ 1 ] and ICGC [ 2 ], have provided a huge influx of somatic mutation data across large cohorts of patients. Understanding how these observed genetic alterations give rise to specific cancer phenotypes represents a major aim of cancer genomics [ 3 ]. Initial analyses of cancer genomes have revealed that numerous somatic mutations are usually observed within each individual and yet only a subset of them is thought to play a role in tumor initiation or progression [ 4 ]. Further, such analyses have shown that somatic mutations in cancer are highly heterogeneous, with each individual presenting a distinct set of mutations across many genes [ 3, 4 ]. As a result, computational methods are necessary for analyzing cancer genomics datasets in order to uncover which of the many observed altered genes are functionally important in cancers [ 5 ].  Perhaps the most commonly applied approach to identify cancer-related genes is to analyze a cohort of individuals and find the genes in which somatic mutations frequently occur [ 6, 7 ]. However, gene-specific characteristics, such as length, replication timing, and expression, all play a role in any given gene's propensity for acquiring mutations [ 4, 5, 7, 8 ]. Thus, a gene's frequency of mutation is typically compared to a background mutation rate, computed across either the entire gene or a specific genomic region, that represents how frequently we would expect that gene to be mutated by chance alone; only genes with mutation rates significantly higher than background mutation rates are predicted to be relevant for cancer [ 8-12 ]. Background mutation rates have been estimated based upon a variety of data, including silent mutation frequency [ 11, 12 ], mutational frequencies per nucleotide contexts (e.g. CG dinucleotides) [ 9 ], and known gene-specific characteristics [ 8, 10 ], as well as combinations of these features as inferred using machine learning techniques [ 13 ]. A high background mutation rate in a gene is indicative of that gene's propensity to accumulate mutations, thereby suggesting that mutations within it are more likely to be neutral [ 11 ].  Here we introduce a new framework, differential mutation analysis, that uncovers cancer genes by comparing the mutational profiles of genes across cancer genomes with their natural germline variation profiles across healthy individuals. We hypothesize that if a gene is less constrained with respect to variation across the healthy population, it may also be able to tolerate a greater amount of somatic mutation without experiencing a drastic detrimental functional change. Our rationale is that the propensity of a gene to acquire neutral mutations is likely subject to many of the same gene specific characteristics (e.g. length) regardless of whether these mutations occur in germline cells or somatic cells [ 6, 14 ]. Furthermore, genomic breakpoints tend to be shared across genomic samples leading to instability and mutations in the same regions in both somatic and germline cells [ 15 ]. Thus, we propose that just as differential gene expression analysis in cancer studies identifies genes that are differentially expressed between cancer samples and normal samples, so differential mutation analysis can reveal genes that are differentially mutated between cancer genomes and the genomes of healthy individuals. While genes that are found to be differentially expressed are thought to reflect functional differences in regulation [ 16 ], we propose that genes that are differentially mutated are candidate cancer \u201cdriver\u201d genes.  We present a fast and simple method for differential mutational analysis. Our approach leverages large-scale human variation data from the 1000 Genomes project [ 17 ] and identifies genes whose mutational profiles across cancer genomes are enriched compared to their relative variability across healthy populations. Previously, natural variation data have been used to interpret mutations found in the genomes of individuals with a disease of interest [ 12, 18-20 ]. For example, mutations that fall in highly polymorphic sites are frequently assumed not to play a significant role in disease [ 12, 18, 19 ]. Furthermore, genic regions with a high ratio of rare variants to common ones have been found to be more intolerant to functional variation and thus changes within them are more likely to be responsible for inherited diseases [ 20 ]. Somatic mutations that fall into such regions can also have a large functional impact [ 18, 19 ]. Moreover, pergene rare variant frequency has been used to prioritize cancer genes and distinguish tumor samples from normal samples [ 21 ]. In contrast to these earlier approaches that consider allelic frequencies at individual sites to help elucidate the impact of mutations, our work introduces the idea of comparing the variability of a gene across a healthy population with its mutational profile across a cancer cohort in order to determine whether it is likely to be relevant for cancer.  Our method for identifying genes differentially mutated in cancer does not rely on any parameter fitting or machine learning and obviates the need to integrate the large amounts of external covariate data that many other methods rely on [ 7 ]. Our method runs in minutes and outperforms considerably more sophisticated and timeconsuming approaches for uncovering cancer genes. We therefore posit that germline variation information can serve as a robust background for characterizing somatic mutations revealed by cancer genome sequencing studies and that differential mutation analysis is an intuitive yet highly efficacious framework for discovering cancer driver genes.    Methods\r\n   Method overview\r\n  We have developed a method, DiffMut, that evaluates each gene for differential mutation when comparing cancer and healthy cohorts. Our approach is entirely based on somatic mutations and germline variation, without any additional parameters (Fig. 1). Briefly, for a cancer type of interest, we first count, for each individual, the number of non-silent single nucleotide mutations found in the exons of each gene. Similarly, we use the 1000 Genomes sequencing data to count, for each individual, how many variants appear in each gene. We define a variant as any nucleotide that differs from the most common one across the healthy cohort. For each individual, we then rank normalize the mutation or variant counts across genes so that each gene is assigned a score between 0 and 1 that reflects the relative number of mutations or variants that fall within it. Next, for each gene, we aggregate its mutation and variation scores across healthy and cancer cohorts separately, resulting in a set of normalized variation scores as well as a set of normalized mutation scores. We use these sets to build a pair of histograms estimating the density of mutation and variant normalized scores. The first represents the gene's ranks among all genes with respect to somatic mutation across a cancer genome cohort; the other represents its ranks with respect to germline variation across a healthy cohort. In order to uncover whether a gene has a mutational profile that is more extreme for cancer than healthy cohorts, we compute the difference between the two distributions using a modification of the classic Earth Mover's Distance [ 22 ], which we refer to as a unidirectional Earth Mover's Difference (uEMD). A key advantage of an EMD-based score is that it measures the cost of transforming one distribution into another by considering the shapes of the two distributions in addition to the differences between the constituent values. Genes with higher uEMD scores have normalized cancer mutation scores that tend to be larger than their normalized variation scores. Thus, we rank all genes by their uEMD scores, considering higher ranking genes to be more likely to be functionally related to a given cancer type, and compute a supporting empirical q-value at each uEMD score [ 23 ].    Processing cancer exome mutations\r\n  We downloaded all level 3 cancer somatic mutation data from The Cancer Genome Atlas (TCGA) [ 1 ] that was available as of October 1, 2014. This consisted of 75 Mutation Annotation Format (MAF) files across 24 cancer types. We then mapped point mutations based on their provided location in the human reference genome to all known human proteins in NCBI's annotation release 104 whose amino acid sequences matched nucleotide sequences from the human reference genome build 37 patch 10 (GRCh37.p10) [ 24 ]. Mutations were classified as missense if they changed the encoded amino acid, nonsense if they changed an amino acid into a stop codon, and silent if they had no effect on the protein sequence. For each gene, we selected only the longest known isoform, which left us with 19,460 protein isoforms that uniquely mapped to genes. In cases where the MAF file was annotated to an earlier release of the human reference genome, we used the liftOver tool [ 25 ] to convert genomic locations to build 37. For each of the 24 cancer types, we selected the MAF file with the most mapped non-silent mutations (with the exception of those files processed by Canada's Michael Smith Genome Sciences Centre which excluded nonsense mutations) in order to have the largest number of mutations without mixing mutations from different processing pipelines (see Additional file 1: Section A for mutation counts for each cancer type).    Processing natural human variants\r\n  We downloaded all phase 3 whole-genome variant calls from the 1000 Genomes Project (released May 2, 2013) [ 17 ] and mapped them uniquely to the longest isoform for each gene as described above. This resulted in 960,408 variant sites over 2504 healthy individuals, of which 578,002 contained missense variants, 11,543 contained nonsense variants, and 370,974 contained silent variants (note that a single variant site can yield missense, silent, or nonsense variations in different individuals). For each variant site, each individual is given a score of 0, 1, or 2 depending upon whether the variant is absent, heterozygous, or homozygous relative to the most commonly observed allele in the population. Variants in the Y chromosome were excluded and variants in male X chromosomes were always marked as homozygous.    Rank normalizing mutations and variation counts per individual\r\n  For each individual with cancer, we counted the number of mutations that were found in each gene in their cancer genome. Similarly, for each individual included in the 1000 Genomes Project, we counted the sum of variant scores for each gene, as described above. Next, for each individual, we rank normalized their mutation or variation counts across all genes. To do so, each gene was first assigned a rank equal to the number of genes it had a greater count than. All ranks were then divided by the total number of genes. This generated a score between 0 (no observed mutation or variation in the gene for the given individual) and 1 (the gene has the most observed mutation or variation for the given individual) for each gene, per individual.    Computing uEMD per gene\r\n  After rank normalization as described above, each gene has two sets of scores: one for all cancer samples and one for all healthy samples. We compare the histograms corresponding to these sets of scores using a unidirectional version of the EMD. In general, EMD is a measure of the distance between two probability distributions based on how much probability density or \u201cdirt\u201d must be \u201cmoved\u201d for the two distributions to match. EMD has been used, for example, in pattern recognition contexts such as measuring the difference between two images [ 22 ]. In order to compute how often and by how much mutation scores exceed variation scores for each gene, we created a uEMD that only measures the amount of \u201cdirt\u201d that must be moved downward from the first distribution (mutation data) to the second (variation data) but ignores \u201cdirt\u201d that would be moved the other way. In practice, we compute uEMD for a gene g by constructing histograms for both sets of scores for that gene in 100 evenly spaced bins between 0 and 1. Then, starting from the highest bin, we count the fraction of cancer mutation scores that fall in that bin and subtract the fraction of natural variant scores that fall in that bin. Next, we move the surplus or deficit fraction of mutations to the next bin but only add any surplus to a running total for uEMD. We repeat this process for all bins or until all mutations have been accounted for. This process can equivalently be expressed by the formula 1 uEMDg ¼ X max where Mb,g is the fraction of mutations in bin b for gene g and Nb,g is the same for variants. For a fixed number of bins, computing uEMD scores for all genes is done in linear time in the number of genes.    Test for correlation with known covariates\r\n  We tested for correlation between our per-gene uEMD scores and gene length, DNA replication time, global expression level, and chromatin state, as these covariates have been previously shown to correlate with non-silent mutation rate [ 8 ]. We computed length as the total number of bases in the longest isoform of a gene. The other three covariates were downloaded from the Cancer Genome Analysis (CGA) group [ 8 ] and were computed as described there. In each case, for each cancer type, we computed the Spearman correlation between the uEMD scores and the given measure for mutated genes.    Evaluation\r\n  To evaluate our gene rankings, we downloaded three curated lists of known cancer genes: the list of known cancer genes in the Cancer Gene Census (CGC) from COSMIC [ 26 ], the list of \u201cdriver genes affected by subtle [point] mutations\u201d from Vogelstein et al. [ 3 ], and the pan-cancer list of significantly mutated genes from Kandoth et al. [ 27 ]. We filtered the CGC list to only those related to somatic point mutations. We split the CGC and Vogelstein list into oncogenes and tumor suppressor genes (TSGs) as classified by each, respectively. This resulted in 202 genes in the CGC list, 47 of which are oncogenes and 52 of which are TSGs; 125 in the Vogelstein list, 54 of which are oncogenes and 71 of which are TSGs; and 137 in the Kandoth list. With respect to any list of known cancer genes, we used two methods to assess overall performance. First, since any list of known cancer genes is incomplete, we examined what fraction of top-ranking genes by our method was in the given list of genes across varying ranking cutoffs. This gave us a general idea of how enriched cancer genes were in that list. Second, to evaluate the enrichment for cancer genes across the full spectrum of predictions, we measured the area under the precision-recall curve (AUPRC) using the perfMeas package for R [ 28 ]. Note that in either case, high-scoring genes found by any method that are not in the list of known cancer genes may, in fact, correspond to newly discovered genes with functional roles in cancers. For each test, we used the list of known cancer genes as positive examples and removed known cancer genes that are implicated for other reasons from the set of negatives. Specifically, we removed all the genes we filtered out from the CGC list from the list of negatives as well as any genes that are labeled as cancer genes in any of the lists we consider. Furthermore, we removed oncogenes from the list of negatives when testing TSGs and vice versa. We applied both measures to the list of per-gene uEMD scores for each of the 24 cancer types. In evaluations against MutSigCV [ 8 ], the method developed by Youn and Simon [ 11 ], OncodriveCLUST [ 29 ], OncodriveFML [ 30 ], and MADGiC [ 10 ], we always ran these programs using default parameters on the same MAF file we used for our method. We ran FunSeq2 [ 19 ] by submitting identical MAF files to their web server using default parameters.    Computing supporting q-values\r\n  To evaluate whether the uEMD of a gene is significant, we test whether it has a significantly higher uEMD than would be expected if each patient's mutations were randomly distributed across genes. In particular, we generate randomized data as follows. First, for each individual, we randomly permute their mutation ranks across genes. Next, we use this randomized data to compute a full set of \u201cdecoy\u201d uEMDs; that is, for each gene, we compute the uEMD between the distribution of randomized mutation ranks for that gene and its distribution of normalized variation counts across the healthy population. For each score threshold, we then compute a false discovery rate (FDR) by computing the ratio between (1) the number of decoy uEMDs at least as large as the threshold and (2) the number of genes with uEMD at least as large as the threshold when using the actual somatic mutation data. For each gene, we use its uEMD score to obtain an FDR, and a q-value is obtained by taking the minimum FDR for a score at least as small. This is a conservative method for controlling the FDR [ 23 ]. In practice, we repeat the randomization process five times and estimate the FDR for each gene by taking an average over these randomizations.     Results\r\n   Identifying cancer driver genes by differential mutation analysis\r\n  We applied our method to all 24 cancer types sequenced in TCGA using all non-silent mutations (Additional file 1: Section A). Unlike many other methods, we do not remove hypermutated samples and do no additional pruning of genes. We evaluated our method by examining whether the CGC list of known cancer driver genes, as curated by COSMIC [ 26 ], is enriched among genes with high uEMD scores. First, since no list of known cancer genes is complete, we examined what fraction of top ranking genes by our method was in the list of known cancer genes. Across all 24 cancer types, we find that a high fraction of the top-scoring genes are, in fact, known cancer genes (Fig. 2a). Indeed, genes that are significantly differentially mutated (q-value &lt; 0.1) are enriched for cancer genes (Additional file 1: Section B). As a control, we repeated this analysis using silent somatic mutations. Since silent mutations do not change protein products, we do not expect that differential mutation analysis will be predictive of cancer genes in this scenario [ 3 ]. As anticipated, we do not see an enrichment for cancer genes among genes that are the highest scoring using only silent mutation data (Fig. 2a), with only one cancer gene found with q-value &lt; 0.1 across all 24 cancer types (Additional file 1: Section B).  To evaluate the enrichment for cancer genes across the full spectrum of predictions of our method, we also measured the AUPRC. To quantify the improvement in enrichment, we computed the log2 fold change in AUPRC between uEMD scores produced by non-silent mutations vs silent mutations (Fig. 2b, left). Next, we tested the rankings generated by our method against ranking genes by how frequently they are mutated per base of exon, a baseline method for finding cancerrelated genes [ 12 ]. We found that in terms of AUPRC our method consistently outperformed mutation rate across all cancer types (Fig. 2b, right).    Differential mutation analysis outperforms prior frequency-based methods in identifying cancer genes\r\n  We evaluated DiffMut's uEMD scores against gene rankings generated by MutSigCV [ 8 ], which is the de-facto standard method for detecting cancer driver genes based on somatic mutations, as well as the method developed by Youn and Simon [ 11 ], OncodriveCLUST [ 29 ], OncodriveFML [ 30 ], and MADGiC [ 10 ]. We chose these methods for evaluation because, like differential mutation analysis, they only require the user to specify a MAF file as input, in contrast to methods such as MuSiC [ 9 ], which require raw sequencing reads. Despite the relative simplicity of our method, it outperformed MutSigCV for 23 of the 24 cancer types in ranking cancer genes, as judged by AUPRC as described above (Fig. 3, left). Of particular note, DiffMut showed a fourfold improvement in AUPRC over MutSigCV in predicting cancer genes based on somatic mutations in breast cancer (BRCA). Further, DiffMut outperformed Youn and Simon's method and OncodriveCLUST in all 24 cancer types, MADGiC on all 12 types we could run that program on, and OncdodriveFML on 19. Overall, we dominate most competing methods over the full length of the precision recall curve, both on the 24 individual cancers and in pan-cancer analysis (Additional file 1: Section C).  We also performed several other evaluations of our method. First, we tested the log2 fold change in AUPRC of DiffMut vs the other methods up to only 10% recall; we obtained similar results, suggesting good performance in the top range of predictions (Additional file 1: Section D). Second, we considered the cancer-specific driver genes identified in the CGC; while these sets of genes are too small for meaningful AUPRC computations, we found that for each cancer type, the cancer-specific genes were generally ranked higher than other known cancer genes (Additional file 1: Section E). This implies that DiffMut preferentially selects cancer-specific genes rather than repeatedly identifying the same set of genes across cancer types. Third, we evaluated our method on the curated lists of cancer genes described by Vogelstein et al. [ 3 ] and Kandoth et al. [ 27 ] and obtained similar results (Additional file 1: Section F). Fourth, we performed runtime analysis of our method and found that it is typically significantly faster than previous approaches; for example, when run on the BRCA dataset, DiffMut is 30 times faster than MutSigCV, even when run on a less powerful machine (Additional file 1: Section G). Finally, we confirmed that uEMD scores do not correlate with known covariates (Additional file 1: Section H). We conclude our general evaluation of how well DiffMut identifies known cancer genes by noting that the performance of all these methods, including our own, can likely be improved by additional curation and processing [ 31 ]; however, our goal was to perform an automated, large-scale comparative analysis on identical mutation files without any further optimizations or gene or patient pruning.    Differential mutation analysis can separately identify oncogenes and tumor suppressor genes\r\n  The list of known cancer genes from the Cancer Gene Census is divided into oncogenes and TSGs, due to the well-established significant biological differences between the two. While oncogenes drive cancer growth with specific functional mutations, TSGs inhibit growth when functioning normally. It is therefore thought that TSGs can be easily disrupted by nonsense mutations [ 3 ]. Because of this fundamental biological difference between TSGs and oncogenes, we decided to analyze missense and nonsense mutations separately. As expected, when using only missense mutations, we are better able to predict oncogenes; and when using only nonsense mutations, we are much better able to predict TSGs. The vast majority of the time, our method is better able to detect oncogenes and TSGs than the five methods to which we compare (Fig. 3 middle and right). We see similar results using the set of oncogenes and TSGs described by Vogelstein et al. (Additional file 1: Section F). Thus, our approach allows us to enrich for specific subtypes of cancer driver genes while other methods have not been shown to readily make this distinction.    Differential mutation analysis reveals that many long genes with high mutation rates in cancers are also highly variable across natural populations\r\n  Olfactory receptors and some extraordinarily long genes (including the muscle protein TTN, the membrane associated mucins MUC4 and MUC16, and the nuclear envelope spectrin-repeat protein SYNE1) have high mutation rates, but it has been proposed that mutations within them are unlikely to play causal roles in cancers [ 8 ]. In support of this, of the 372 olfactory receptor genes found in the HORDE database [ 32 ], none are found to be significantly differentially mutated (q-value &lt; 0.1) in 23 of the 24 cancer types we analyzed, and only one is found to be differentially mutated in the last cancer type. In contrast, the five other tested methods often do not show the same under enrichment for olfactory receptor genes among their lists of predicted driver genes (Additional file 1: Section I). Similarly, of the ten longest genes with above average mutation rates, none are implicated by differential mutation across any of the 24 cancer types (Additional file 1: Section I). That is, while these genes have a high mutation rate for their length, they also vary naturally at a higher rate. Although the functions of some of these genes are not fully known, and some may, in fact, be cancer related, their relationship to the disease is likely complex and so they are not expected to be implicated by somatic mutation alone [ 8 ]. Thus, differential mutational analysis provides a powerful yet simple approach to eliminate genes that have high somatic mutation rates but are found to be highly variable across human populations.    Differential mutation analysis proposes new cancer driver genes\r\n  Although many of the genes found to be differentially mutated are known cancer genes, high-scoring genes not in the list of known cancer genes may, in fact, correspond to newly discovered genes with functional roles in cancers. For example, two genes that we found to be significantly differentially mutated, TRPS1 and ZNF814, both contain numerous mutations in and near their DNA-binding zinc finger domains. Across all the samples in TCGA, we observed 103 missense mutations of a single nucleotide in ZNF814, indicating that it may be an oncogene by the definition presented in Vogelstein et al. [ 3 ]. TRPS1, on the other hand, contains 18 nonsense and 228 missense mutations across its exons, suggesting that it may be a TSG. It has previously been reported that TRPS1 plays a role in cancer development [ 33 ], and that higher levels of TRPS1 improved survival [ 34 ]. Similarly, CDH10 contains 20 nonsense and 319 missense mutations and, in agreement with our results, has previously been identified as a potential TSG in colorectal cancer and lung squamous cell carcinoma [ 35, 36 ]. Other differentially mutated genes such as EIF1AX have been reported by previous studies [ 37, 38 ] but are absent from the gold standards we used. A full list of genes that were not already included in our lists of positives but show significant differential mutation across the 24 cancer types can be found in Fig. 4.     Discussion\r\n  We have shown that natural germline variation data serve as a powerful source of information for discovering cancer driver genes. This one type of data allowed us to develop a fast (Additional file 1: Section G) and simple non-parametric method for detecting cancer driver genes with higher precision than currently used methods without the use of any extraneous covariate data. In the future, alternate approaches to uncover genes differentially mutated between cancer and healthy cohorts can be developed based upon the increasing availability of data and may yield even better performance. Encouragingly, we observe that the power of our current differential mutation analysis method increases as more tumor samples are sequenced (Additional file 1: Section G), thereby suggesting that further cancer genome sequencing will increase the predictive power of our framework.  As larger numbers of healthy human genomes are sequenced and germline variation data become more abundant, our approach can likely be improved via explicit modeling of population structure. Indeed, many variant sites may be stable within subpopulations. For example, sub-Saharan African populations exhibit a great deal of natural variation relative to European populations [ 39 ]. Ashkenazi Jewish populations, on the other hand, show less genetic variation [ 40 ] and, significantly, show genetic predisposition to some types of cancer [ 41 ]. In order to account for this, in the future, variants could be counted only when they differ within the appropriate subpopulation.  Another benefit of further sequencing would be an increase in the density of observed mutations and variants. Currently, there are only enough data to glean differential mutation on a whole-gene level. However, with denser annotation it may be possible to score smaller regions of genes such as known functional domains. For example, HLA genes, which are highly variable, all have very low differential mutation scores. However, much of this is due to natural variation within specific genic regions. In the future, it may be possible to evaluate regions such as these separately to determine whether mutations in other less variable parts of genes are important in cancers.  While this work introduces the idea of detecting cancer-relevant genes by identifying those that are differentially mutated between cancer cohorts and healthy populations, natural variation has previously been used to measure the impact of specific mutations. Cancer mutations that fall directly onto variant sites are often discarded [ 12 ] and some somatic mutations that fall into regions with a high ratio of rare variants to common ones can have a large functional impact [ 18 ]. Previous approaches have aimed to find such mutations across patients with the goal of identifying mutations that drive each patient's cancer [ 19 ]. Although these previous approaches are not designed to identify cancer driver genes and do not perform well at this task (Additional file 1: Section F), identifying driver mutations is a challenging parallel task and a potential direction for further work with differential mutation analysis.  Thus far, we have only shown the power of differential mutation in identifying individual genes that may play a role in cancer. However, it is well understood that cancer is a disease of pathways [ 3, 4 ]. Thus, an especially promising avenue for future work is in performing differential mutation analysis at the pathway level. In particular, gene-set and pathway analyses can be performed by examining how germline variation accumulates across entire sets of genes and assessing whether there is evidence of differential mutation at that level as well. Differential mutation analysis could also potentially be integrated into network-based approaches that do not require known pathway annotations but instead uncover novel cancer pathways [ 42, 43 ].  Finally, similar to other methods for detecting cancer driver genes, differential mutation analysis is likely to benefit from domain-specific knowledge. For example, in melanomas there are a large number of C to T mutations that are the result of ultraviolet radiation [ 6, 8 ]. Because these mutations occur in a much higher abundance than other mutations, they dominate the mutational signal. We therefore hypothesize that it may be beneficial to look at specific types of mutations for some cancers. Further improvements on other cancer types are also likely to be possible by explicitly considering mutational context. Similarly, in cancer types where non-point mutations (such as copy number variation, insertions, or deletions) play a larger role than somatic mutation, incorporating additional knowledge on these mutation types from both cancer and natural variation data will broaden our ability to predict cancer-related genes.    Conclusions\r\n  Despite somatic mutations and germline variants being subject to a different set of evolutionary pressures [ 7 ], we propose that genes observed to have numerous variants across the population are able to accumulate more somatic mutations without experiencing a drastic functional change. While we presented a method that directly leverages this idea and have shown that it is highly effective in identifying cancer-related genes, it is likely that even more powerful predictors of cancer driver genes could be obtained by integrating natural variation data with other information. In conclusion, we propose that akin to the prominent role of differential expression analysis in analyzing cancer expression datasets, differential mutation analysis is a natural and powerful technique for examining genomic alteration data in cancer studies.    Additional file\r\n  Additional file 1: A file containing all additional figures and tables. Section A shows the total mutation counts for all the cancer types we analyzed. Section B shows the enrichment for cancer genes among differentially mutated genes when using non-silent mutations but not when using silent mutations. Section C shows the full precision-recall curves and the areas under them. Section D shows the log-fold change in AUPRC when only computing the area up to 10% recall. Section E shows how known cancer-specific genes are ranked relative to all known cancer genes. Section F shows the performance of our method against other methods when evaluated using several different lists of known cancer genes. Section G shows the fast runtime of our method and how the power of our method increases with more tumor samples. Section H shows that our method's ranking of genes does not correlate with known covariates. Section I shows DiffMut's lack of enrichment for olfactory receptors and extraordinarily long genes among its top-ranked genes. (PDF 1097 kb) Abbreviations AUPRC: Area under the precision-recall curve; CGC: Cancer Gene Census; TCGA: The Cancer Genome Atlas; TSG: Tumor suppressor gene; uEMD: Unidirectional Earth Mover's Distance Acknowledgements Not applicable.  Funding This work is partly supported by the NIH (CA208148 to MS), an NSF Graduate Research Fellowship (to PFP), and the Forese Family Fund for Innovation. These funding bodies played no role in the design of the study, the collection, analysis, and interpretation of the data, or in the writing of the manuscript. Availability of data and materials The datasets supporting the conclusions of this article were derived from public domain resources. TCGA data are downloadable from https://gdc.cancer.gov/ and 1000 Genomes data from http://www.internationalgenome.org/. The code for DiffMut, our Differential Mutation Analysis method, is written in R, is platform independent, and is available for download at https://github.com/Singh-Lab/DifferentialMutation-Analysis. The only required input is a Mutation Annotation Format (MAF) file. The resulting output is a list of genes with their uEMD scores and supporting q-values. The code can optionally be run specifically on only missense or nonsense mutations to uncover oncogenes and TSGs separately. Authors' contributions PFP and MS designed the study. PFP performed the analysis. PFP and MS wrote manuscript. Both authors read and approved the final manuscript. Ethics approval and consent to participate Not applicable.  Consent for publication Not applicable.  Competing interests The authors declare that they have no competing interests.    Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "publicationTitle": "Differential analysis between somatic mutation and germline variation profiles reveals cancer-related genes",
    "title": "Differential analysis between somatic mutation and germline variation profiles reveals cancer-related genes",
    "publicationDOI": "10.1186/s13073-017-0465-6",
    "publicationDate": "0",
    "publicationAbstract": "A major aim of cancer genomics is to pinpoint which somatically mutated genes are involved in tumor initiation and progression. We introduce a new framework for uncovering cancer genes, differential mutation analysis, which compares the mutational profiles of genes across cancer genomes with their natural germline variation across healthy individuals. We present DiffMut, a fast and simple approach for differential mutational analysis, and demonstrate that it is more effective in discovering cancer genes than considerably more sophisticated approaches. We conclude that germline variation across healthy human genomes provides a powerful means for characterizing somatic mutation frequency and identifying cancer driver genes. DiffMut is available at https://github.com/SinghLab/Differential-Mutation-Analysis.",
    "authors": [
      "Pawel F. Przytycki",
      "Mona Singh"
    ],
    "status": "Success",
    "toolName": "DifferentialMutation-Analysis"
  },
  "71.pdf": {
    "forks": 2,
    "URLs": ["github.com/TheJacksonLaboratory/CloudNeo"],
    "contactInfo": ["jeff.chuang@jax.org"],
    "subscribers": 4,
    "programmingLanguage": "HTML",
    "shortDescription": "CWL implementation of CloudNeo: A cloud pipeline for identifying patient-specific tumor neoantigens ",
    "publicationTitle": "CloudNeo: a cloud pipeline for identifying patient-specific tumor neoantigens",
    "title": "CloudNeo: a cloud pipeline for identifying patient-specific tumor neoantigens",
    "publicationDOI": "10.1093/bioinformatics/btx375",
    "codeSize": 15094,
    "publicationAbstract": "Summary: We present CloudNeo, a cloud-based computational workflow for identifying patientspecific tumor neoantigens from next generation sequencing data. Tumor-specific mutant peptides can be detected by the immune system through their interactions with the human leukocyte antigen complex, and neoantigen presence has recently been shown to correlate with anti T-cell immunity and efficacy of checkpoint inhibitor therapy. However computing capabilities to identify neoantigens from genomic sequencing data are a limiting factor for understanding their role. This challenge has grown as cancer datasets become increasingly abundant, making them cumbersome to store and analyze on local servers. Our cloud-based pipeline provides scalable computation capabilities for neoantigen identification while eliminating the need to invest in local infrastructure for data transfer, storage or compute. The pipeline is a Common Workflow Language (CWL) implementation of human leukocyte antigen (HLA) typing using Polysolver or HLAminer combined with custom scripts for mutant peptide identification and NetMHCpan for neoantigen prediction. We have demonstrated the efficacy of these pipelines on Amazon cloud instances through the Seven Bridges Genomics implementation of the NCI Cancer Genomics Cloud, which provides graphical interfaces for running and editing, infrastructure for workflow sharing and version tracking, and access to TCGA data. Availability and implementation: The CWL implementation is at: https://github.com/TheJacksonLaboratory/CloudNeo. For users who have obtained licenses for all internal software, integrated versions in CWL and on the Seven Bridges Cancer Genomics Cloud platform (https://cgc.sbgenomics.com/, recommended version) can be obtained by contacting the authors. Contact: jeff.chuang@jax.org Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-10-13T06:30:41Z",
    "institutions": [
      "University of Connecticut Health",
      "The Jackson Laboratory for Genomic Medicine",
      "The Jackson Laboratory"
    ],
    "license": "Apache License 2.0",
    "dateCreated": "2016-11-14T02:15:17Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx375   CloudNeo: a cloud pipeline for identifying patient-specific tumor neoantigens     Preeti Bais  1    Sandeep Namburi  1    Daniel M. Gatti  2    Xinyu Zhang  1    Jeffrey H. Chuang  0  1    0  Department of Genetics and Genome Sciences, University of Connecticut Health ,  Farmington, CT 06032 ,  USA    1  The Jackson Laboratory for Genomic Medicine ,  Farmington, CT 06030 ,  USA    2  The Jackson Laboratory ,  Bar Harbor, ME 04609 ,  USA     2017   1  1  3   Summary: We present CloudNeo, a cloud-based computational workflow for identifying patientspecific tumor neoantigens from next generation sequencing data. Tumor-specific mutant peptides can be detected by the immune system through their interactions with the human leukocyte antigen complex, and neoantigen presence has recently been shown to correlate with anti T-cell immunity and efficacy of checkpoint inhibitor therapy. However computing capabilities to identify neoantigens from genomic sequencing data are a limiting factor for understanding their role. This challenge has grown as cancer datasets become increasingly abundant, making them cumbersome to store and analyze on local servers. Our cloud-based pipeline provides scalable computation capabilities for neoantigen identification while eliminating the need to invest in local infrastructure for data transfer, storage or compute. The pipeline is a Common Workflow Language (CWL) implementation of human leukocyte antigen (HLA) typing using Polysolver or HLAminer combined with custom scripts for mutant peptide identification and NetMHCpan for neoantigen prediction. We have demonstrated the efficacy of these pipelines on Amazon cloud instances through the Seven Bridges Genomics implementation of the NCI Cancer Genomics Cloud, which provides graphical interfaces for running and editing, infrastructure for workflow sharing and version tracking, and access to TCGA data. Availability and implementation: The CWL implementation is at: https://github.com/TheJacksonLaboratory/CloudNeo. For users who have obtained licenses for all internal software, integrated versions in CWL and on the Seven Bridges Cancer Genomics Cloud platform (https://cgc.sbgenomics.com/, recommended version) can be obtained by contacting the authors. Contact: jeff.chuang@jax.org Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  Mutations in tumor genomes create specific peptide changes that can be recognized by the immune system and influence sensitivity to immunotherapy  (van der Most et al., 1996; van Rooij et al., 2013) . The mechanism of action involves binding of native major histocompatibility complex (MHC) class I and II molecules, a.k.a. human leukocyte antigen (HLA) complex I and II molecules, to the novel peptide sequences that result from protein-changing somatic mutations in cancer cells. Cells presenting these neoantigens are recognized as foreign by T-cells, which then selectively destroy them. With the arrival of new next generation sequencing platforms, it has become possible to interrogate the genomes of patient tumors and computationally predict T-cell reactivity against putative mutationderived neoantigens  (Schumacher et al., 2015)  by estimating the binding of MHC class I molecules to each new peptide sequence.  Several bioinformatics tools are routinely used to predict tumor neoantigen-MHC class I binding from sequencing data. For example, HLAMiner  (Warren et al., 2012)  and Polysolver  (Shukla et al., 2015)  are software tools that can predict patient-specific HLA classes I and II typing from RNA sequencing data, and netMHCpan (Nielsen et al., 2016) predicts HLA-peptide binding. Prior studies in cancer immunotherapy have successfully used these tools to predict the efficacy of immuno-oncological therapies in a patient-specific manner  (Rizvi et al., 2015; Van Allen et al., 2015) , demonstrating the importance of making such methods easily available to the general research community. However, the cost of developing and maintaining the bioinformatics infrastructure to perform this type of analysis is substantial. In particular, research groups are generating increasing amounts of custom sequencing data or investigating massive consortium datasets such as The Cancer Genome Atlas  (Weinstein et al., 2013) , for which data transfer and scalability of computing can be significant obstacles to analysis on local compute clusters. To resolve these problems, we have developed a cloudbased analysis pipeline for tumor neoantigen detection.    2 Description\r\n  We developed the CloudNeo pipeline on the Seven Bridges cloud platform as part of the National Cancer Institute's Cancer Genomics Cloud [http://www.cancergenomicscloud.org/] (CGC), which uses Docker containers to execute the tasks in the workflow. Briefly, CloudNeo takes a vcf file (for mutations) and bam file (for HLA typing) as inputs and then outputs HLA binding affinity predictions for all mutated peptides (see Supplementary Fig. S1). A first input to CloudNeo is a list of non-synonymous mutations in vcf file format. There are multiple somatic mutation calling pipelines that can be used to generate and filter this vcf file  (Alioto et al., 2015) , including several which are available through the CGC. The genomic variants are translated into amino acid changes using the VEP tool (McLaren et al., 2010) and a custom R script that we have created called Protein_Translator. The output of the custom tool is a list of Namino-acid-long peptide sequences in a fasta format, such that the single peptide change is in the middle of the N-mer. In parallel, Protein_Translator generates another fasta file for the homologous N-mers with no peptide mutation. Users have options to calculate the HLA types using either HLAminer or Polysolver. Six HLA types are predicted, namely the top two predictions for each of HLA-A, HLA-B and HLA-C. The final step in the pipeline is the NetMHCpan tool, which uses the HLA types and the N-mer mutant peptide sequences to calculate the binding affinities for potential neoantigens. Affinities between the two HLA-A, two HLA-B, and two HLA-C molecules and each of the ([N/2]þ1)mer peptide subsequences within the N-mers are computed. The output of the pipeline is a list of peptide subsequences along with the MHC binding affinity scores for each of the six HLA types. Similar results are generated for the homologous unmutated peptide sequences as a comparison.  To test this pipeline, we analyzed 23 melanoma tumor samples (Hugo et al., 2016) as described earlier using both the HLAminer and Polysolver versions of the pipeline. We then predicted neoantigens based on criteria of strong mutant-MHC binding affinity (NetMHCpan score &lt; 500), non-zeroexpression of the transcript containing the mutation, and lack of strong affinity between the non-mutated sequence and the MHC (NetMHCpan score for the non-mutant sequence 500). For each sample we merged the set of neoepitopes predicted across the six HLA types. The neoepitope load ranged from 0 to 1244 with an average of 107.89 using the HLAminer version of the pipeline. For the Polysolver version of the pipeline, the same filtering criteria were used and the neoepitope load was from 0 to 1417 with an average load of 133.53. The differences in the two pipeline results were due to differing HLA type predictions by Polysolver and HLAminer. 16 HLAtype predictions by the tools overlapped with each other, and there were 102 unique HLA predictions from Polysolver and 122 unique predictions from HLAminer. While our HLA type predictions were based on RNAseq data, CloudNeo can also use DNA data as inputs for HLA calling. The average wall time required to run the pipeline for a given tumor on CGC was 8 h and 2 min for the HLAminer version and 7 h and 25 min for the Polysolver version (see Supplementary Material 'Pipeline Performance').    3 Discussion\r\n  Other recent methods, such as (Hundal et al., 2016), are similar to CloudNeo in providing a computational pipeline for neoantigen prediction. However, to our knowledge CloudNeo is the only such pipeline that has been developed for cloud computing. This allows users to realize advantages of cloud analysis, including massive computing scalability and access to large datasets on the CGC such as TCGA, as these can be reached without downloading to a local server. This cloud approach also makes CloudNeo easy to match to time and budget restrictions on demand, providing a flexible computational approach for the research community. A version of the CloudNeo pipeline is openly available at the Github site as a Common Workflow Language (CWL) implementation that can be run using Rabix (Kaushik et al., 2016), allowing for running on systems including AWS, Google Compute Engine and Azure. Licenses for academically licensed software (HLAminer and NetMHCpan) must be obtained by users, but simple instructions to do so are provided at the Github site. Users with licenses can also contact the authors to request a version with all software integrated. Full versions are available either in CWL or as a workflow on the Seven Bridges implementation of the CGC. The CGC version is recommended, as this provides additional functionality including graphical interfaces for running and editing, simple workflow sharing and version tracking, improved calling of multiple cloud instances, and access to TCGA data. Full details and docs are at https://github.com/TheJacksonLaboratory/CloudNeo.    Acknowledgement\r\n  We thank G. Kaushik for assistance with the CGC platform.    Funding\r\n  J.H.C. was supported by the National Cancer Institute of the National Institutes of Health under awards [R21CA191848] and supplement [R21CA19184801A1S1]. Research was also partially supported by the National Cancer Institute under award [P30CA034196]. D.M.G. was supported by National Institute of General Medical Sciences under award [R01 GM07068308].  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/TheJacksonLaboratory/CloudNeo",
    "publicationDate": "0",
    "authors": [
      "Preeti Bais",
      "Sandeep Namburi",
      "Daniel M. Gatti",
      "Xinyu Zhang",
      "Jeffrey H. Chuang"
    ],
    "status": "Success",
    "toolName": "CloudNeo",
    "homepage": ""
  },
  "84.pdf": {
    "forks": 46,
    "URLs": [
      "pachterlab.github.io/sleuth",
      "github.com/pachterlab/sleuth_paper_analysis",
      "www.nature.com/reprints/index.html"
    ],
    "contactInfo": ["lpachter@caltech.edu"],
    "subscribers": 36,
    "programmingLanguage": "R",
    "shortDescription": "Differential analysis of RNA-Seq",
    "publicationTitle": "d ifferential analysis of rna -seq incorporating quanticfiation uncertainty",
    "title": "d ifferential analysis of rna -seq incorporating quanticfiation uncertainty",
    "publicationDOI": "10.1038/nmeth.4324",
    "codeSize": 22533,
    "publicationAbstract": "We describe sleuth (http://pachterlab.github.io/sleuth), a method for the differential analysis of gene expression data that utilizes bootstrapping in conjunction with response error linear modeling to decouple biological variance from inferential variance. sleuth is implemented in an interactive shiny app that utilizes kallisto quanticfiations and bootstraps for fast and accurate analysis of data from rna -seq experiments.",
    "dateUpdated": "2017-10-13T18:50:02Z",
    "institutions": [
      "Berkeley",
      "Division of Biology and Biological Engineering",
      "University of Iceland"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2015-02-13T18:44:52Z",
    "numIssues": 40,
    "downloads": 0,
    "fulltext": "     10.1038/nmeth.4324   d ifferential analysis of rna -seq incorporating quanticfiation uncertainty     Harold Pimentel  0    Nicolas L Bray  4    Suzette Puente  1    Páll Melsted  3    Lior Pachter  lpachter@caltech.edu  2    0  Department of Computer Science, University of California, Berkeley ,  Berkeley, California ,  USA    1  Department of Statistics, University of California, Berkeley ,  Berkeley, California ,  USA    2  Division of Biology and Biological Engineering ,  Recivd 27 Janu R;y ceptda 4 M;ya published onlie 5    3  Faculty of Industrial Engineering ,  Mechanical Engineering and Computer Science ,  University of Iceland ,  Reykjavík ,  Iceland    4  Innovative Genomics Institute and Department of Molecular &amp; Cell Biology, University of California, Berkeley ,  Berkeley, California ,  USA     2017    We describe sleuth (http://pachterlab.github.io/sleuth), a method for the differential analysis of gene expression data that utilizes bootstrapping in conjunction with response error linear modeling to decouple biological variance from inferential variance. sleuth is implemented in an interactive shiny app that utilizes kallisto quanticfiations and bootstraps for fast and accurate analysis of data from rna -seq experiments.       -\r\n  Many methods have been developed for differential analysis of RNA-seq data1. Some of these methods are designed to translate models developed for microarray analysis2, while others are based on models tailored to RNA-seq1,3-5. Differential analysis of RNAseq experiments requires careful assessment of gene expression variability from a few replicate samples to identify biologically relevant expression differences between conditions6. There is ongoing debate on even basic questions such as how to measure gene abundances1, whether there is sufficient power to test for differences in abundance of individual isoforms7, and how to best utilize biological replicates1. Part of the reason for this uncertainty is the lack of agreed-upon standards for testing and benchmarking RNA-seq methods. In most cases, accuracy claims are based on simulations of read counts from distributions assumed in the models, rather than on simulations of raw reads2,5,6,8,9. Such readcount-based simulations typically discount the effects of ambiguously mapping reads and fail to capture both the possibilities for and challenges of isoform-specific differential analysis.  Here we describe a novel approach to RNA-seq differential analysis and a comprehensive benchmarking framework with broader scale and scope than have previously been published. Our approach is implemented along with interactive visualization software that provides crucial transparency in assessing results, and it offers users a convenient tool for exploratory data analysis. Throughout the paper we use the name 'sleuth' to refer to both our statistical method and the software application.  sleuth relies on variance decomposition to identify biological differences in transcript or gene expression (Fig. 1). While using a standard strategy of shrinkage to stabilize variance estimates from few samples2,6, sleuth is able to leverage recent advances in quantification10 to obtain error estimates that can be used to decouple biological variance from inferential variance before shrinkage (Fig. 1a). Variance decomposition is important because of the diversity of variance estimates across genes that arise when quantifying abundances. In one example (Fig. 1b,c), DESeq2 and voom were run on the same data with featureCounts summaries, and these tools identified a gene as differentially expressed at a false discovery rate (FDR) threshold of 0.10 (reported FDR 8.81 × 10−21 and 5.56 × 10−10, respectively); whereas sleuth did not find differences between conditions to be significant (reported FDR 0.156) because of the high inferential variance. Some methods have attempted to utilize estimates of quantification errors6,11,12, but these methods are limited by long run times and lack of robustness to ambiguously mapping reads. By leveraging kallisto's10 rapid quantification and variance estimation, sleuth overcomes these issues and provides a statistically rigorous, flexible, and efficient tool for RNA-seq analysis.  To test its performance, we compared sleuth with other widely used methods on both simulated and biological data. Our simulation was based on two experimental conditions with three replicates each (see Online Methods). We simulated biological variance (dispersion) according to the negative binomial count model used by DESeq2 (ref. 9). To accurately assess performance, each simulation was repeated 20 times. All programs except sleuth used quantifications inferred from genome alignments (see Online Methods). Full details of the simulation experiments are in Supplementary Note 1.  sleuth displayed higher sensitivity than Cuffdiff 2 (ref. 12), DESeq6, DESeq2 (ref. 9), EBSeq7, edgeR8, voom2; and sleuth displayed log-fold change13 in the FDR range of usual interest (0-10%) and beyond, up to FDR 0.3 (Fig. 2; Supplementary Figs. 1 and 2; and Supplementary Note 1). As expected, our simulations found that DESeq2 has more power than DESeq at all relevant FDRs, and that the naïve ranking of genes by log-fold change produces poor results. To control for the effect of different filtering strategies, we ran all programs on a common filtered set of genes and showed that sleuth maintains its sensitivity advantage (Supplementary Note 1). Finally, to show the benefit of directly estimating inferential variance, we also demonstrated that sleuth outperforms models in which inferential variance is assumed to be Poisson or zero (Supplementary Note 1).  Since FDR control is fundamental for identifying differentially expressed genes in experiments with few replicates, we carefully b 00000152253 rrsseadpebae123 Gd0 S le N a E cs    Biological Inferential\r\n  . d e v r e s e r s t h g i r l l A . e r u t a N r e g n i r p S f o t r a p , . c n I , a c i r e m A e r u t a N 7 1 0 2 © 200 100 0.00 c f igure 1 | Overview of sleuth. (a) sleuth models different sources of variance to predict differentially expressed transcripts and genes. Biological variance (biol. var.) results from differences in RNA content between replicates and from stochastic biochemistry during library preparation, while inferential variance arises from random sequencing and computational analysis of reads. See Online Methods for description of terms. (b) Results for an example gene after running kallisto on RNAseq data from Trapnell et al.12 generated from human lung fibroblasts transfected with scrambled siRNA (scramble condition) and HOXA1 siRNA (HOXA1KD condition). DESeq2 and voom identify the gene as differentially expressed, but high inferential variance causes sleuth to find no difference. Red dots, point estimates. Blue dots, results for bootstrap samples to assess inferential variance. (c) The between-sample raw variance leads to a small estimated biological variance that fails to account for uncertainty introduced when quantifying the samples. examined the accuracy with which methods self-report their FDR. Other than sleuth and voom, all methods underestimated their FDR; several methods reported an FDR of 0.01, when the true FDR was greater than 0.1. While sleuth overestimates the FDR, this error is conservative; i.e., fewer genes are reported, yet they are highly enriched for being differentially expressed. 700 Number of genes DE  To test the accuracy of FDR estimation on biological data, we repeated an experiment from the DESeq2 paper9 (see Online Methods). Using the Bottomly data set14, which contains more than ten replicates from two mice strains, we randomly selected two sets of three samples (in sets of 20 replicates) and used differential expression results from the remaining samples as the 'truth' while ensuring that batch types were equally represented across replicates. Each method was tested to see how well it could (i) recapitulate its own results using a smaller data set and (ii) control the FDR, as assessed by comparing to its own results in the remaining high-replicate samples. As in the simulation, sleuth and voom demonstrate a superior ability to estimate their FDR accurately (Fig. 3a).  Using our simulated data, we also performed a consistency experiment from the DESeq2 paper9 to test whether methods produce similar results with less data. Results from simulated data recapitulate the results from biological data, thus validating the reliability of the consistency experiment (Fig. 3b).  Additionally, we performed a negative-control experiment comparing two groups of randomly selected female Finnish samples from the GEUVADIS data set15, for which no biologically meaningful differential expression is expected between groups. sleuth and voom found very few false positives, whereas other methods generated many (sleuth and voom are the only methods with a median of less than 5 false positives at all FDR ranges tested at both the gene and isoform level, whereas the next best method has 95; Supplementary Figs. 3 and 4).  While RNA-seq is standard for gene-level differential analysis, there has been debate about its suitability and power at the isoform level. We and others previously demonstrated that isoform-level differential analysis can highlight interesting differential splicing and promoter usage12, but the significance and reliability of such results have been contested13.  To examine the power and accuracy of isoform-level differential analysis, we repeated the gene-level analysis at the transcript level (Fig. 2b) using kallisto quantifications as the input for each program. We confirm previous findings that the increased testing    Method\r\n  sleuth voom Cuffdiff 2 DESeq2 edgeR DESeq EBSeq GLFC  LFC 900 0.06 800 Number of transcripts DE b 700 600 ty0.04 500 iiitsn 400 v eS0.02 300 0.00 200 100 0.00 0.05 0.10 0.15 False discovery rate 0.20 0.25 0.05 0.20  0.25 0.10 0.15 False discovery rate f igure 2 | Sensitivity and false discovery rates of differential expression methods. (a,b) Sensitivity versus FDR curve for each program on simulated data ('effect from experiment' simulation; see Online Methods), showing the ranking of all genes (a) or transcripts (b) passing its filter. Circles, triangles and squares represent rankings at an FDR of 0.01, 0.05, and 0.10, respectively. Ideally, each symbol would lie directly above the corresponding symbol on the x-axis indicating true FDR. Error bars, 2 s.d. (n = 20). Each isoline represents an indicated number of genes (or transcripts) that are called differentially expressed; intersection with a curve indicates a program's performance when looking at that number of top-ranked genes. FDR lines were averaged over 20 replications of the simulation. . d e v r e s e r s t h g i r l l A . e r u t a N r e g n i r p S f o t r a p , . c n I , a c i r e m A e r u t a N 7 1 0 2 © eFDR = 0.01 eFDR = 0.05 eFDR = 0.10 required for isoform-level analysis decreases sensitivity in comparison to that of gene-level analysis. However, we find that sleuth can still control the FDR at the isoform level while calling many differentially expressed isoforms. Interestingly, while the power to discover differentially expressed features is lower, our simulations show that, at a given FDR, the total number of differential features detected is fairly similar to that when performing gene-level analysis (see isolines in Fig. 2; Supplementary Note 1). Moreover, for simulations in which isoform abundances change independently between conditions, we find that sleuth outperforms other methods. The same is also true for the correlated-effect simulation (see Online Methods, Supplementary Note 1). In addition, we tested BitSeq5 on a single sample, as its run time was prohibitive on the entire simulation set. We found BitSeq performed well overall, although sleuth outperformed BitSeq when the true FDR was less than 0.12 (Supplementary Note 1). To demonstrate that the improvement of sleuth's performance arises from its model rather than from its use of kallisto's quantifications, we ran sleuth for one replicate of our simulation using RSEM quantifications for the original data along with manually performed bootstraps, and we saw almost identical performance (Supplementary Figs. 5 and 6).  We also used tximport16 to test the result of using kallisto quantifications to estimate gene abundances for differential analysis with other programs, and we found that sleuth remained superior to other methods (Supplementary Figs. 7 and 8).  Our results show that by accounting for uncertainty in quantifications, sleuth is more accurate than previous approaches at both the gene and isoform levels. Crucially, the estimated FDRs reported by sleuth are well controlled and reflect the true FDRs, making the predictions of sleuth reliable and useful in practice.  The sleuth workflow was designed to be simple, interpretable, and fast. The model was chosen in part for its tractability, and the Shiny visualization framework was chosen for its portability (Supplementary Note 2). The modularity of the algorithm also makes it easy to explore improvements and extensions, such as analysis of more general transcript groups and different shrinkage and normalization schemes to improve performance. As a result, when coupled with kallisto, which has dramatically reduced run times for quantification based on the use of pseudoalignment, sleuth is a quick, accurate, and versatile tool for the analysis of RNA-seq data. . d e v r e s e r s t h g i r l l A . e r u t a N r e g n i r p S f o t r a p , . c n I , a c i r e m A e r u t a N 7 1 0 2 © methods Methods, including statements of data availability and any associated accession codes and references, are available in the online version of the paper.  Note: Any Supplementary Information and Source Data files are available in the online version of the paper. a ckno Wledgments H.P. and L.P. were partially supported by NIH grant nos. R01 DK094699 and R01 HG006129. We thank D. Li, A. Tseng, and P. Sturmfels for help with implementing some of the interactive features in sleuth. author contributions H.P. led the development of the sleuth statistical model and was assisted by S.P., N.L.B., P.M., and L.P. The method comparison and testing framework was designed by H.P., N.L.B., P.M., and L.P. The interactive sleuth live software was designed and implemented by H.P., as was the sleuth R package. H.P. automated production of the results. H.P., N.L.B., P.M., and L.P. analyzed results and wrote the paper. comPeting financial interests The authors declare no competing financial interests. r eprints and permissions information is available online at http://www.nature.com/reprints/index.html. Publisher's note: springer n ature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. . d e v r e s e r s t h g i r l l A . e r u t a N r e g n i r p S f o t r a p , . c n I , a c i r e m A e r u t a N 7 1 0 2 © online methods Model. We consider an additive response error model in which the total between-sample variability has two additive components, (i) 'biological variance' that arises from differences in expression between samples as well as from variability due to library preparation and (ii) 'inferential variance,' which includes differences arising from computational inference procedures in addition to measurement 'shot noise' arising from random sequencing of fragments. The model is an extension of the general linear model where the total error has two additive components. Given a design matrix x, we assume a general linear model for the (unknown) abundance Yti of transcript t in sample i in terms of fixed-effect parameters beta and 'noise' epsilon.  Yti | xi = xiT bt + eti ( 1 )  ( 1 )  While the Yti are not directly observed, (normal) perturbations of Yti constitute the observed random variables Dti:  Dti | yti = yti + zti ( 2 )  ( 2 )  With some further assumptions, one can derive that the Dti values are normally distributed (see Supplementary Note 2) and the variance, which is the key to performing differential analysis, can be interpreted as the sum of biological (εti) and inferential (ξti) variance.  Testing for differential expression. In comparing samples to identify differentially expressed genes or transcripts, sleuth applies the likelihood ratio test-where the full model contains labels for the samples, and the reduced model ignores labels.  Underlying the test is an estimate of the variances V(Dti), where t ranges over the transcripts and i over the samples. The estimate for V(Dti) used in sleuth is  Vˆ (Dti ) = max(st2,sˆt2) + tˆt2 ( 3 )  ( 3 ) where tˆt2 is the estimate of the inferential variance obtained from bootstrapping, σˆ t2 the raw biological variance, and st2 a shrinkage-based estimator of the biological variance. For details of how the individual variance estimates were obtained, see Supplementary Note 2.  Simulations. A null distribution for transcript abundances was learned from the largest homogeneous population in the GEUVADIS data set, 59 samples of lymphoblastoid cell lines derived from Finnish females15, a proxy for a homogeneous set of samples. We estimated transcript-level abundances with kallisto, then we estimated parameters for negative binomial distributions (using the Cox-Reid dispersion estimator) to model count distributions using DESeq2.  After the null distribution was constructed, expression features (isoforms or genes, depending on the type of simulation) were chosen to be differentially expressed. Transcripts with less than five estimated counts on average across the GEUVADIS samples were marked as too rare to be simulated as differentially expressed. A gene was assumed to pass the filter if at least one of its constituent transcripts passed the filter. In each simulation, 20% of the features that passed the filter were chosen to be differentially expressed at random. If the simulation had unequal size factors, random size factors were chosen from the set {1/3, 1, 3} such that the geometric mean equaled 1, similar to the simulation procedure in DESeq2. However, unlike the DESeq2 simulation procedure, our size factors were chosen at random. Counts were generated from the negative binomial distribution, after which reads were simulated using the RSEM simulator4. This resulted in about 30 million 75-base-pair paired-end reads per sample for a total of 13.8 billion reads overall (see tables in Supplementary Note 1 for exact counts). Three types of simulations were performed.  Independent effect simulation. Isoforms across the transcriptome were chosen to be differentially expressed at random. The simulations were generated with equal size factors. Effect sizes were chosen from a truncated normal distribution, such that the minimum absolute fold change for differential transcripts or genes was 1.5.  Correlated effect simulation. Genes (instead of isoforms) were randomly chosen to be differentially expressed. A direction (sign) for each effect size was chosen at random, then all the effects were simulated from a truncated normal with minimum absolute fold change 1.5. The simulation used random unequal size factors generated as described above.  Effect from experiment. To mimic the types of changes seen in real experiments, fold changes were learned from Trapnell et al.12 from the set of transcripts that either DESeq2 or sleuth found to be differentially expressed at FDR 0.05. Genes were chosen at random to be differentially expressed. The null mean counts were used to determine the rank of each transcript relative to its parent gene. These ranks were matched between the Trapnell data set, and the null distribution was learned from the GEUVADIS data set.  Self-consistency experiment. In order to validate whether methods would produce similar results with less data, we performed an experiment similar to those of Love et al.9. For each iteration, we randomly selected three samples from condition C57BL/6J and three samples from condition DBA/2J and ran each tool.  The 'truth' set was established by using the remaining samples to identify differentially expressed genes or transcripts using that program. For each FDR level (0.01, 0.05, 0.10), we compared the results from the smaller data set with those of the larger data set for each tool. The FDR was then computed and plotted in Figure 3.  Data processing and software notes. Sleuth may be downloaded at http://pachterlab.github.io/sleuth. For isoform analyses, all quantification was performed using kallisto version 0.42.4. For gene-level analyses, HISAT2 was used to align reads to human genome GRCh38 and mouse genome GRCm38 for all programs other than sleuth (which used kallisto). Quantifications for Cuffdiff 2 were performed using Cufflinks. Remaining quantifications were done using featureCounts. Ensembl release 80 was used for human analyses, and release 84 was used for mouse analyses.  The following R programs were used to compile the results: sleuth 0.28.1, BitSeq 1.16.0, DESeq 0.24.0, DESeq2 1.12.0, EBSeq 1.12.0, edgeR 3.14.0, and limma-voom 3.28.2. When testing programs at the isoform level, kallisto 0.42.4 was used to obtain quantifications. Cuffdiff 2.21 was used with alignments from HISAT2 2.0.1 (ref. 17). Subread (featureCounts) 1.5.0 (ref. 18) was used . d e v r e s e r s t h g i r l l A . e r u t a N r e g n i r p S f o t r a p , . c n I , a c i r e m A e r u t a N 7 1 0 2 © with alignments from HISAT2 to get raw gene counts. BitSeq was provided alignments from Bowtie 1.1.2 (ref. 19). All analyses in the paper are fully reproducible through the Snakemake system20, available at https://github.com/pachterlab/sleuth_paper_analysis and in the Supplementary Software.  Data availability statement. The Bottomly data set is available at the NCBI Gene Expression Omnibus (GSE26024, accession nos.  SRR099223-SRR099243). The Trapnell et al. data set (Fig. 1b) is available at the NCBI Gene Expression Omnibus (GSE37704, accession nos. SRR493366-SRR493371). The GEUVADIS data set is available at the European Nucleotide Archive (accession no. ERP001942). nature methods  ERRATA Erratum: Differential analysis of RNA-seq incorporating quantification uncertainty In the version of this article initially published, the final term in equation ( 2 ) in the Online Methods was incorrectly specified as xti. The correct term is ti. Also, the two callouts to Supplementary Note 3 in the Online Methods section were incorrect and should have referred to Supplementary Note 2. These errors have been corrected in the HTML and PDF versions of the article.    ",
    "sourceCodeLink": "https://github.com/pachterlab/sleuth",
    "publicationDate": "0",
    "authors": [
      "Harold Pimentel",
      "Nicolas L Bray",
      "Suzette Puente",
      "Páll Melsted",
      "Lior Pachter"
    ],
    "status": "Success",
    "toolName": "sleuth",
    "homepage": "http://pachterlab.github.io/sleuth"
  },
  "41.pdf": {
    "forks": 2,
    "URLs": ["github.com/jtaghiyar/kronos"],
    "contactInfo": ["sshah@bccrc.ca"],
    "subscribers": 1,
    "programmingLanguage": "Python",
    "shortDescription": " A workflow assembler for cancer genome analytics and informatics",
    "publicationTitle": "Kronos: a workflow assembler for genome analytics and informatics",
    "title": "Kronos: a workflow assembler for genome analytics and informatics",
    "publicationDOI": "10.1093/gigascience/gix042",
    "codeSize": 110,
    "publicationAbstract": "Background: The field of next-generation sequencing informatics has matured to a point where algorithmic advances in sequence alignment and individual feature detection methods have stabilized. Practical and robust implementation of complex analytical workflows (where such tools are structured into \u201cbest practices\u201d for automated analysis of next-generation sequencing datasets) still requires significant programming investment and expertise. Results: We present Kronos, a software platform for facilitating the development and execution of modular, auditable, and distributable bioinformatics workflows. Kronos obviates the need for explicit coding of workflows by compiling a text configuration file into executable Python applications. Making analysis modules would still require programming. The framework of each workflow includes a run manager to execute the encoded workflows locally (or on a cluster or cloud), parallelize tasks, and log all runtime events. The resulting workflows are highly modular and configurable by construction, facilitating flexible and extensible meta-applications that can be modified easily through congfiuration file editing. The workoflws are fully encoded for ease of distribution and can be instantiated on external systems, a step toward reproducible research and comparative analyses. We introduce a framework for building Kronos components that function as shareable, modular nodes in Kronos workflows. Conclusions: The Kronos platform provides a standard framework for developers to implement custom tools, reuse existing tools, and contribute to the community at large. Kronos is shipped with both Docker and Amazon Web Services Machine Images. It is free, open source, and available through the Python Package Index and at https://github.com/jtaghiyar/kronos.",
    "dateUpdated": "2017-09-29T21:55:38Z",
    "institutions": [
      "University of Toronto",
      "Simon Fraser University",
      "British Columbia Cancer Agency",
      "University of British Columbia",
      "Ontario Institute for Cancer Research (OICR)"
    ],
    "license": "MIT License",
    "dateCreated": "2015-12-08T02:31:30Z",
    "numIssues": 2,
    "downloads": 0,
    "fulltext": "     10.1093/gigascience/gix042   Kronos: a workflow assembler for genome analytics and informatics     M. Jafar Taghiyar  2  3    Jamie Rosner  2    Diljot Grewal  2  3    Bruno M. Grande  1    Radhouane Aniba  2  3    Jasleen Grewal  1    Paul C. Boutros  0  4    Ryan D. Morin  1    Ali Bashashati  2  3    Sohrab P. Shah  sshah@bccrc.ca  2  3    0  Department of Medical Biophysics, University of Toronto ,  101 College Street, M5G 1L7 Toronto, ON ,  Canada    1  Department of Molecular Biology and Biochemistry, Simon Fraser University ,  8888 University Drive, V5A 1S6 Burnaby, BC ,  Canada    2  Department of Molecular Oncology, British Columbia Cancer Agency ,  675 West 10th Ave, V5Z 1L3 Vancouver, BC ,  Canada    3  Department of Pathology and Laboratory Medicine, University of British Columbia ,  2211 Wesbrook Mall, V6T 2B5 Vancouver, BC ,  Canada    4  Ontario Institute for Cancer Research (OICR) ,  661 University Avenue, M5G 0A3 Toronto, ON ,  Canada     2017     7  6  2017    7  3  2017    6  6  2017     Background: The field of next-generation sequencing informatics has matured to a point where algorithmic advances in sequence alignment and individual feature detection methods have stabilized. Practical and robust implementation of complex analytical workflows (where such tools are structured into \u201cbest practices\u201d for automated analysis of next-generation sequencing datasets) still requires significant programming investment and expertise. Results: We present Kronos, a software platform for facilitating the development and execution of modular, auditable, and distributable bioinformatics workflows. Kronos obviates the need for explicit coding of workflows by compiling a text configuration file into executable Python applications. Making analysis modules would still require programming. The framework of each workflow includes a run manager to execute the encoded workflows locally (or on a cluster or cloud), parallelize tasks, and log all runtime events. The resulting workflows are highly modular and configurable by construction, facilitating flexible and extensible meta-applications that can be modified easily through congfiuration file editing. The workoflws are fully encoded for ease of distribution and can be instantiated on external systems, a step toward reproducible research and comparative analyses. We introduce a framework for building Kronos components that function as shareable, modular nodes in Kronos workflows. Conclusions: The Kronos platform provides a standard framework for developers to implement custom tools, reuse existing tools, and contribute to the community at large. Kronos is shipped with both Docker and Amazon Web Services Machine Images. It is free, open source, and available through the Python Package Index and at https://github.com/jtaghiyar/kronos.    genomics  workflow  pipeline  reproducibility       Background\r\n  The emergence of next-generation sequencing (NGS) technology has created unprecedented opportunities to identify and study the impact of genomic aberrations on genome-wide scales. Data generation technology for NGS is stabilizing, and exponential declines in cost have made sequencing accessible to most research and clinical groups. Alongside progress in data generation capacity, a myriad of analytical approaches and software tools have been developed to identify and interpret relevant biological features. These include computational methods for raw data preprocessing, sequence alignment and assembly, variant identification, and variant annotation. However, major challenges are induced by rapid development and improvement of analytical methods. This makes construction of analytical worklfows a near dynamic process, creating a roadblock to seamless implementation of linked processes that navigate from raw input to annotated variants.  As a consequence, robust analysis and continuous iterative improvements in the analysis of large sets of sequencing data remain labor intensive and costly and require considerable analytical expertise. As best practices (e.g., [ 1 ]) remain a moving target, software systems that can rapidly adapt to new (and optimal) solutions for domain-specific problems are necessary to facilitate high-throughput comparisons.  Several tools and frameworks for NGS data analysis and workflow management have been developed to address these needs. Galaxy [ 2 ] is an open, web-based platform to perform, reproduce, and share analyses. Using the Galaxy user interface, users can build analysis workflows from a collection of tools available through the Galaxy Tool Shed [ 3 ]. The Taverna suite [ 4 ] allows the execution of workflows that typically mix web services and local tools. Tight integration with myExperiment [ 5 ] gives Taverna access to a network of shared workoflws, including NGS data processing.  Although the current workflow management systems such as Galaxy are great for routine bioinformatics tasks, the development of customized tools and workflows is not convenient, and experienced bioinformaticians commonly work at a lower programming level and write their own workflows in scripting languages such as Bash, Perl, or Python [ 6 ]. A number of lightweight workflow management tools have been specifically developed to simplify scripting for these target users, including Ruffus [ 7 ], Bpipe [ 8 ], and Snakemake [ 9 ]. Common Workflow Language [  10 ] is another similar tool that has roots in GNU make and aims to build portable workflows across a variety of platforms by using a set of standard specification to define wrappers around command line tools as well as creating nested workflows. While these workflow management tools reduce development overhead, users still need to write a substantial amount of routine code to create their own workflows, maintain the existing ones, replace subsets of workflows with new ones, and run subsets of existing workflows.  To further facilitate the process of creating workflows, Omics-Pipe proposed a framework to automate best practice multi-omics data analysis workflows based on Ruffus [  11 ]. It offers several preexisting workflows and reduces the development overhead for tracking the run of each workflow and logging the progress of each analysis step. However, it remains cumbersome to create a custom workflow with Omics-Pipe as users need to manually write a Python script for the new workflow by copy/pasting a specific header to the script and writing the analysis functions using Ruffus decorators. The same applies when adding or removing an analysis step to an existing workflow.  We introduce a highly flexible open source Python-based software tool (Kronos) that enables bioinformatics developers, i.e., bioinformaticians who develop workflows for analyzing genomic data, to quickly create a workflow. It uses Ruffus [  7 ] as the underlying workflow management system and adds a level of abstraction on top of it, which significantly reduces programming overhead for workflow development and provides a mechanism to represent a workflow by a top-level YAML configuration ifle.  Kronos is shipped with Docker and Amazon Machine Images to further facilitate its use locally on high performance computing clusters and in the cloud infrastructures. A number of workflows for the analysis of single human genomes and cancer tumour-normal pairs following best analysis practices accompany Kronos and are freely available.    Results\r\n  Kronos creates modular workflows that can be easily updated by editing their corresponding configuration file. Each module in the workflow corresponds to a component, which is a wrapped command line tool (i.e., described in more detail later). As shown in Fig. 1, users can create a workflow from a set of existing components by following the 3 steps listed below (referred to as Steps 1, 2, and 3 in the remainder of this paper). Section 2 of Additional ifle 1 provides an example of how to make a variant calling worklfow.  Step 1. Given a set of existing components, create a conifguration file template by running the following Kronos command: kronos make˙congfi [list of components] -o &lt;output name&gt; where [list of components] refers to the component names that we aim at using in our workflow.  Step 2. In the configuration file template, specify the order by which the components in the workflow should be run. This does not require programming skills and is merely textbased.  Step 3. Create the workflow by running the following Kronos command with the configuration file as its input: kronos init -y &lt;config file.yaml &gt; -o &lt;workflow name&gt;  The output is an executable Python script that runs the workflow. Depending on its corresponding configuration file, the script is encoded to automatically parallelize eligible tasks, provide pause/resume functionality, make unique run IDs, make the desired output directory tree, submit jobs to cluster or run them locally, and log the events.   Kronos components\r\n  A component is a wrapper around a command line tool that encapsulates all the required programming. The purpose of components is to modularize workflows with reusable building blocks that require minimal development. As shown in Section 1 of Additional file 1, the number of lines of codes for making a new component is very small. The simple development instructions eliminate, e.g., the need to use Ruffus decorators, input/output management using regex expressions, and complicated dependency management in the code that can easily become very complex with the number of tasks in a workflow. Furthermore, a large workflow can be divided into a set of small components that results in a much faster and more manageable workflow development. Kronos also provides a command for making component templates that helps develop a new component in a few minutes.  All command line tools, such as a simple copy command or a complicated single nucleotide variant (SNV) caller, can be wrapped as Kronos components. Regardless of how complicated they are, their corresponding components have a standard directory structure composed of specific wrappers and subdirectories. The wrappers are also independent of the programming language used for developing the command line tool.  The components should be developed prior to making the workflow. However, since they are individually and independently developed and due to their reusability, the initial preparation of a component happens only once, and various workflows can use the already developed component.    Kronos configuration file\r\n  Kronos workflows are represented by a YAML configuration file. For a given set of components, the Kronos make config command generates a configuration file template that is mostly preiflled with default values. For each input component, there is a corresponding section with a unique name in the configuration file called task. Users should use these sections to specify the order by which each task in the workflow should be run (Step 2 of creating a workflow). This can be done by a simple convention called IO-connection. An IO-connection is basically a pair of values comprising a task name and 1 of its parameters. It determines which task should be followed by the current task and is specified as an argument to 1 of the parameters of the current task. For example, in the following configuration file, (' TASK 1 ', 'out file') is an IO-connection that makes TASK 2 follow TASK 1 , i.e., the input to the parameter in file of TASK 2 comes from the parameter out file of TASK 1 .  TASK 1 : out file: 'foo.txt' TASK 2 : in file: (' TASK 1 ', 'out file')  The run options for each task are also set in the configuration ifle, including granular resource requests such as free memory or the number of CPUs, running locally or on cluster, running with parallelization, pause/resume functionality, etc.  A configuration file has the following blocks (see Additional ifle 1: Fig. S1): system-specific, which captures the system-dependant requirements of the workflow (such as the paths to the local installations) and includes the GENERAL and PIPELINE INFO sections; user-specific, which contains the input files and arguments and includes the SHARED and SAMPLES sections workflow-specicfi, which defines the connection between the components in the workflow. Task sections related to each component are in this group.  This design has the following advantages: (i) if users want to rerun the same workflow for various sets of input files and arguments, they would only need to update the user-specific sections. This prevents inadvertent changes in the flow of the workflow when changing the inputs; and (ii) the segregation of system-specific information from the rest of the sections enables users to run a workflow practically anywhere. In other words, by simply updating the system-specific sections with proper values, the requirements of the workflow can be observed on any machine.    Kronos workflows\r\n  Each workflow made by Kronos is a directed acyclic graph (DAG) of components where every node in the graph corresponds to a task section in the configuration file. Task sections can independently be added, removed, or replaced in the configuration file (Fig. 3). Therefore, to add, remove, or replace a component in the workflow or equivalently a node in the DAG, users simply need to change the corresponding task section in the configuration file and run the command in Step 3. As a result, the workflows are highly modular and maintaining them is as easy as updating the configuration file without having to rewrite the workflow. Finally, a workflow can be run by simply running the Python workflow script using the command line as depicted in Fig. 4.    Kronos features and benefits\r\n  Full details of how to use each of the following features can be found in the software documentation.   Parameter sweeping\r\n  It is sometimes desired to run a particular tool or algorithm with various sets of parameters in order to select the parameter set with superior performance for a given problem. For example, a user may want to find the proper model parameters (such as mapping quality and base quality thresholds) for a variant calling tool to accurately detect single nucleotide variants. Kronos provides a mechanism for this purpose where users can specify all different sets of input arguments (or parameters) in the SAMPLES section of the configuration file. In this case, running Step 3 creates a number of intermediate workflows, each for 1 set of input arguments, along with the main workflow. When running the main workflow, Kronos runs the intermediate worklfows in parallel, each on one set of the input arguments. We have provided a variant calling workflow with parameter sweeping functionality in Section 3 of Additional file 1 to demonstrate this feature.    Tool comparison\r\n  In bioinformatics, it is often required to compare the performance of 2 or more algorithms or compare a new analysis tool to the existing ones to select the 1 that best fits the particular goals of a project. For example, it is often helpful to evaluate the performance of different variant calling algorithms [ 12 ]. The modularity of the workflows generated by Kronos facilitates the comparison of different algorithms and tools. For this purpose, as shown in Fig. 3, the user can simply replace a task section corresponding to an analysis tool with another task section corresponding to another similar tool and run Step 3.    Automatic parallelization and merge\r\n  Most of the recent tools developed in the bioinformatics field are parallelizable or have the potential to run in parallel. However, the majority of these tools are shipped without the builtin functionality and require the users to manually break the analysis into smaller analyses. For example, many variant calling algorithms are capable of running on user-specified coordinates of the genome but are not shipped with parallelization functionality. However, a user can analyze whole genome sequencing data chunk by chunk in parallel with the caveat of manually scripting the parallelization steps. Due to the cumbersome nature of manual parallelization, many users might avoid running the tools in parallel, which considerably increases the runtime of the analysis. To resolve this issue, Kronos automatically parallelizes tasks in the workflow if feasible. Then, it aggregates the outputs of all child tasks and merges them if necessary.    Reproducible workflows\r\n  The configuration file and components of a workflow are portable.  Therefore, users can readily duplicate a workflow elsewhere by only running the kronos init command in Step 3. To show this functionality, we have included an example of a workflow that performs somatic variant calling on whole genome data of a breast cancer case using the Strelka algorithm [ 13 ] and generates a number of plots based on Strelka calls (Fig. 5). Detailed stepby-step instructions to reproduce this figure are in Section 3 of Additional file 1. It should be noted that Kronos workflows can be duplicated elsewhere but the user would still need to manage tool installations and dependencies.    Cloud support\r\n  The massive scale of genomic data justifies a move to the cloud for storage and analyses in order to minimize cost and handle the ebb and flow of computational demands. Kronos' flexibility addresses the emerging need for rapid deployment of analysis workflows in the cloud. Several command line tools exist for managing fleets of compute nodes on cloud platforms such as Amazon Web Services, including StarCluster, CfnCluster, and Elasticluster. A guide on the creation and management of a cloud cluster using StarCluster software and deployment of Kronos is provided in the online documentation, and an Amazon Machine Image is provided for convenience.    Controlled pause/resume by breakpoints\r\n  When running a workflow, certain blocks of the workflow may need to run multiple times, e.g., to tune a particular parameter of a component or to inspect the results of the previous tasks in the workflow before the next tasks are triggered. Analogous to the debuggers, Kronos provides users with breakpoints to perform a controlled pause/resume action.  In addition, with the breakpoint mechanism, users can break the flow of a workflow into several subworkflows and run each part on a different machine or cluster. In other words, once a breakpoint happens, i.e., 1 subworkflow is complete, the main workflow can be transferred to a different machine and it will pick up running from where it left off on the previous machine, provided that all the intermediate files are present. For example, a workflow can contain a component as its last step that loads the final results to a local database that can be reached only from a specific IP or machine. In this case, the user can run the workflow on a powerful computing node or a cluster with a breakpoint set for the component prior to the last component, i.e., database loader in this example. Once the breakpoint is applied, the user can resume the workflow on the other machine, so that the results can be loaded to the local database.    Forced dependency\r\n  Often in a workflow, a task requires the output of the previous one. As explained earlier, Kronos handles this explicit dependency by IO-connection. However, sometimes a task might need strelka strelka to wait for 1 one or more other steps in the workflow to finish although there are no explicit IO-connections between them. For example, when 2 tasks intend to write results in the same file, one needs to make sure that both tasks do not run at the same time. Another example would be a variant calling algorithm (e.g., GATK) that accepts a bam file as input. However, it also expects the index of the bam file to be present in the same directory as the bam file. If the index is created in 1 of the previous tasks in the workflow, then the current task that needs the bam file and its index would depend implicitly on the other task that creates the index file. In this case, a mechanism is required to force the variant calling task to wait until the index file is ready. Kronos provides a forced dependency feature to overcome this problem (see Additional file 1: Fig. S2).    Results directory customization\r\n  It is desirable to have full control of the structure of the results directory when running a workflow. With Kronos, users can readily determine the structure of the results directory in the configuration file. This provides easy file management for the users. Figure 4 shows an example of the tree structure of the results directory generated for a workflow.    Boilerplates\r\n  Users can use this feature to insert a command or a script into the begining of the command used to run a task in a workflow. This is particularly useful for setting up the environments using the Environment Modules package [ 14 ]. It also provides a means to run preprocessing steps for a specific task prior to running the task itself.    Keywords\r\n  There are several specific keywords that users can use in the configuration file that will be automatically replaced by proper values in runtime. This enables users to customize the paths and file names based on the workflow-specific values in runtime such as run-ID, workflow name, or sample ID.      Workflows\r\n  We have developed a number of standard genome analysis workflows using Kronos. These workflows utilize many of the Kronos features introduced earlier and are publicly available.   Alignment workflow\r\n  This workflow accepts paired-end FASTQ files as input and aligns them using the Burrows-Wheeler aligner [ 15 ]. It also sorts the aligned bam file, flags the duplicates, indexes the file, and generates statistics for the final bam file.    Germline variant calling workflow\r\n  This workflow is an implementation of the best practices guide established by the Broad Institute [ 1 ] applied to variant discovery using haplotypecaller. In short, it runs the Bowtie2 aligner, creates targets using GATK RealignerTargetCreator, and calls SNVs and indels using GATK.    Copy number estimation workflow\r\n  HMMCopy is a suite of tools for copy number estimation of whole genome sequencing data [ 16 ]. This workflow takes a bam ifle as an input and estimates the copy number with GC and mappability correction using HMMCopy. It also segments and classifies the copy number profiles with a robust Hidden Markov Model.    Somatic variant calling workoflw\r\n  This workflow takes a pair of tumour/normal bam files as inputs and detects the somatic SNVs and indels using the Strelka algorithm [ 13 ], annotates the resulting VCF files using SnpEff [ 17 ], and flags the variants observed in 1000 genomes and dbSNP databases.    RNA-seq analysis workflow\r\n  This workflow aligns RNA-seq FASTQ files using STAR aligner [ 18 ], followed by Cufflinks, which assembles transcriptomes from RNA-seq data and quantifies their expression [  19 ].     Conclusions\r\n  A foundation for rapid and reliable implementation of genomic analysis workflows is an essential need as a myriad of potential applications of genomics (ranging from personalized cancer therapies to monitoring the evolution and spread of infectious diseases) are projected to produce massive amounts of genomic data in the next few years. We have developed Kronos to address this need by expediting workflow development. It minimizes the tedious process of writing code by transforming a YAML configuration file into a Python script and manages its execution. Given a set of premade components, constructing a worklfow by Kronos does not need programming skills as the user only needs to fill out specific sections of the configuration file. Making components still requires programming. However, their development time and effort is minimal given their design structure. They also provide a powerful and highly flexible framework for bioinformatics developers to fully customize their workflows with reusable modules.  A number of standard genomic analysis workflows and their building components that have been made by Kronos accompany this software and are available to the public. Kronos has been developed for genomics applications, but it can be readily utilized in other scientific and nonscientific ifelds.  The configuration file and components of a Kronos worklfow are portable. This is a step toward reproducible research; however, it should be noted that while Kronos workflows can be duplicated elsewhere, the user would still need to manage tool installations and dependencies. For fully reproducible research, a Docker image of the whole workflow or the environment is perhaps more plausible. Kronos is complementary to other efforts for reproducible research. For example, in order to unify representation of workflow definitions and tool wrappers, the Common Workflow Language working group [  10 ] and the Workfow Description Language [ 20 ] offer specifications that enable data scientists to describe analysis tools and workflows that are human-readable, easy to use, portable, and support reproducibility. It would be beneficial for workflow management tools to adopt these representation standards once they are agreed upon in the field.  In conclusion, this work provides a framework toward rapid integration of new (and optimal) genomic analysis advances in high-throughput studies. The flexibility, customization, and modularity of Kronos make it an attractive system to use in any high-throughput genomics analysis endeavour. We expect that Kronos will provide a foundational platform to accelerate toward the need to standardize and distribute NGS workflows in both clinical and research applications.    Additional files\r\n   Additional file 1 \u2014 Supplementary information\r\n  The supplementary information is in pdf format and expalins (a) how to make a component, (b) how to make a workflow, (c) how to run a workflow, and (d) Fig. S1 and Fig. S2.     Abbreviations\r\n  DAG: directed acyclic graph; NGS: next-generation sequencing; SNV: single nucleotide variant.    Availability and requirements\r\n  Project name: Kronos  Project home page: https://github.com/jtaghiyar/kronos Operating system(s): Linux, Windows, Mac OS Programming language: Python 2.7.5 Other requirements: Ruffus, PyYaml License: MIT    Availability of supporting data\r\n  Snapshots of the code can be found in the GigaScience repository, GigaDB [ 21 ].    Acknowledgements\r\n  The authors would like to thank Shadrielle Melijah G. Espiritu and Andre Masella for their feedback on the manuscript/software. This project has been supported by funding from Genome Canada/Genome British Columbia (grant No. 173CIC), the Natural Science and Engineering Research Council of Canada (grant No. RGPGR 488167-2013), and Terry Fox Research Institute - Program Project Grants (grant No. 1021).    Competing interests\r\n  The authors declare that they have no competing interests.    Author contributions\r\n  J.T. developed the software, wrote the documentation, and contributed to manuscript writing. J.R. assisted in developing part of the logger and a few of the helper functions for the software, testing software features, and providing feedback on the manuscript. D.G. developed a number of pipelines accompanying the manuscript, tested the software, and provided feedback on the software features and manuscript. B.G. deployed and tested Kronos in the cloud, wrote the documentation for cloud deployment, tested software, and provided feedback on the software features and manuscript. R.A. developed the germline variant calling workflow and provided feedback on the manuscript. J.G. tested and provided feedback on the software. P.B. provided feedback on the manuscript. R.M. provided resources, supervised testing Kronos in the cloud, and provided feedback on the manuscript. A.B. contributed to the design and development of the software. A.B. and S.S. co-supervised, provided intellectual contributions to the work, and contributed to manuscript writing. A.B. and S.S. are joint senior authors.    ",
    "sourceCodeLink": "https://github.com/jtaghiyar/kronos",
    "publicationDate": "0",
    "authors": [
      "M. Jafar Taghiyar",
      "Jamie Rosner",
      "Diljot Grewal",
      "Bruno M. Grande",
      "Radhouane Aniba",
      "Jasleen Grewal",
      "Paul C. Boutros",
      "Ryan D. Morin",
      "Ali Bashashati",
      "Sohrab P. Shah"
    ],
    "status": "Success",
    "toolName": "kronos",
    "homepage": ""
  },
  "9.pdf": {
    "forks": 0,
    "URLs": [
      "github.com/IcarPA-TBlab/nrc,",
      "hub.docker.com/r/tblab/nrc/",
      "github.com/IcarPA-TBlab/nrc"
    ],
    "contactInfo": ["antonino.fiannaca@icar.cnr.it"],
    "subscribers": 1,
    "programmingLanguage": "Python",
    "shortDescription": "nRC: non-coding RNA Classifier",
    "publicationTitle": "nRC: non-coding RNA Classifier based on structural features",
    "title": "nRC: non-coding RNA Classifier based on structural features",
    "publicationDOI": "10.1186/s13040-017-0148-2",
    "codeSize": 634,
    "publicationAbstract": "Motivation: Non-coding RNA (ncRNA) are small non-coding sequences involved in gene expression regulation of many biological processes and diseases. The recent discovery of a large set of different ncRNAs with biologically relevant roles has opened the way to develop methods able to discriminate between the different ncRNA classes. Moreover, the lack of knowledge about the complete mechanisms in regulative processes, together with the development of high-throughput technologies, has required the help of bioinformatics tools in addressing biologists and clinicians with a deeper comprehension of the functional roles of ncRNAs. In this work, we introduce a new ncRNA classification tool, nRC (non-coding RNA Classifier). Our approach is based on features extraction from the ncRNA secondary structure together with a supervised classification algorithm implementing a deep learning architecture based on convolutional neural networks. Results: We tested our approach for the classification of 13 different ncRNA classes. We obtained classification scores, using the most common statistical measures. In particular, we reach an accuracy and sensitivity score of about 74%. Conclusion: The proposed method outperforms other similar classification methods based on secondary structure features and machine learning algorithms, including the RNAcon tool that, to date, is the reference classifier. nRC tool is freely available as a docker image at https://hub.docker.com/r/tblab/nrc/. The source code of nRC tool is also available at https://github.com/IcarPA-TBlab/nrc.",
    "dateUpdated": "2017-08-17T19:52:34Z",
    "institutions": [],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2017-01-19T15:10:57Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Fiannaca et al. BioData Mining     10.1186/s13040-017-0148-2   nRC: non-coding RNA Classifier based on structural features     Antonino Fiannaca  antonino.fiannaca@icar.cnr.it    Massimo La Rosa    Laura La Paglia    Riccardo Rizzo    Alfonso Urso     2017   10  2  19    24  7  2017    5  4  2017     Motivation: Non-coding RNA (ncRNA) are small non-coding sequences involved in gene expression regulation of many biological processes and diseases. The recent discovery of a large set of different ncRNAs with biologically relevant roles has opened the way to develop methods able to discriminate between the different ncRNA classes. Moreover, the lack of knowledge about the complete mechanisms in regulative processes, together with the development of high-throughput technologies, has required the help of bioinformatics tools in addressing biologists and clinicians with a deeper comprehension of the functional roles of ncRNAs. In this work, we introduce a new ncRNA classification tool, nRC (non-coding RNA Classifier). Our approach is based on features extraction from the ncRNA secondary structure together with a supervised classification algorithm implementing a deep learning architecture based on convolutional neural networks. Results: We tested our approach for the classification of 13 different ncRNA classes. We obtained classification scores, using the most common statistical measures. In particular, we reach an accuracy and sensitivity score of about 74%. Conclusion: The proposed method outperforms other similar classification methods based on secondary structure features and machine learning algorithms, including the RNAcon tool that, to date, is the reference classifier. nRC tool is freely available as a docker image at https://hub.docker.com/r/tblab/nrc/. The source code of nRC tool is also available at https://github.com/IcarPA-TBlab/nrc.    ncRNA  Classification  Structural features  Deep learning       Background\r\n  During the last decade, research has shown a growing interest in non-coding RNA (ncRNA). They are small non-coding sequences with the potential to have a functional role in many biological processes and diseases [ [ 1] by acting through the regulation of gene expression [ [2- 5]. Different classes of ncRNA have been identified, differing from each other by nucleotide sequence length, folding and function. The most well-known ncRNAs are structural RNA belonging to ribosomal RNA (rRNA) and transfer RNA (tRNA), both involved in translation events [ [ 6]. Another interesting class of ncRNA are microRNAs (miRNAs), 18-24 nucleotide long regulative RNA molecules [ [7- 9]. They can behave as tumour suppressors or oncogenes depending on which target they act upon by altering the standard molecular mechanisms in which their targets are involved [ [1 0]. In particular, they interact with target genes through a direct binding to complementary sequences © The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated. leading to either mRNA degradation or translational suppression [1 11 ]. The final result is the inhibition of the protein product. A miRNA can be considered as an oncogene if its amplification or overexpression down-regulates tumour suppressors or other genes involved in cell differentiation, thereby contributing to cancer formation by stimulating proliferation, angiogenesis, and invasion; whereas the ncRNA molecule will be considered as a tumor-suppressor if it will cause a decrease in oncogene expression [1 12 ].  Other ncRNA classes are small nuclear RNAs (snRNA), long non-coding RNAs (lncRNA), silencing RNA (siRNA), riboswitches and internal ribosome entry sites (IRES) [ 13 ]. The small nucleolar RNA (snoRNA) molecules, belonging to the snRNA class, participate to post-transcriptional modifications of rRNA, together with small nucleolar ribonucleoproteins (snoRNPs) with whom they are complexed. Dong and colleagues [ 14 ] reported a disruption of these RNA molecules in different conditions and cancer diseases [ 14, 15 ], they also identified snoRNA U50 as an important factor in the development and/or progression of breast cancer. The lncRNAs are ncRNA longer than 200 nucleotides. Recent works evidence a dysregulated expression pattern of lncRNAs in cancer samples that may be used as independent predictors of patient outcomes [ 16, 17 ]. Riboswitches are another class of ncRNA. They are structured non-coding RNA domains that selectively bind metabolites and control gene expression. They can act without the support of proteins, which strengthens the hypothesis of their important role in the regulatory machine [ 18 ].  Because of the large number and functions of different ncRNA, their proper identification and classification are a new challenging bioinformatics scenario. Indeed, considering the low percentage of the \u201cdiscovered ncRNAome\u201d and the lack of knowledge about these non-coding molecules, their classification could help biologists and clinicians in understanding the molecular mechanisms of this regulatory machine. This also implies a need to re-state the principles of basic therapeutic strategies.  The aim of the first works about ncRNA classification was to discriminate between coding and non-coding sequences. To this purpose some bioinformatics tools employ support vector machine (SVM) models [ 19 ]: CONC and CPC are prediction tools based on SVM that classify transcripts according to features belonging to coding products [ 20, 21 ]. Another interesting classification method, proposed by Lertampaiporn and colleagues [ 22 ], uses a hybrid Random Forest (RF) algorithm combined with a logistic-regression model that realises a feature-based discrimination among various ncRNAs. The recent discovery of a \u201cPandora box\u201d full of a multitude of different biologically functional ncRNA, opened the way to develop resources able to discriminate the different classes of ncRNAs. Various approaches have been applied such as RNA-CODE [ 23 ], based on the alignment of short reads, or others based on multifeature extraction and full-sequence analysis such as RNAcon [ 24 ] and GraPPLe [ 25 ]. These last methods, in particular, use graph properties (both local and global) of predicted secondary RNA structures together with machine learning algorithms. Their main feature is to identify and extract graph properties that can reflect the functional information of different classes of RNAs. To the best of our knowledge, the RNAcon algorithm currently represents the state-of-the-art classifier of ncRNA classes based on structural features and machine learning techniques. RNAcon considers 20 graph features obtained from the predicted RNA secondary structure and adopts an RF classifier [ 26 ].  In this paper, we present nRC (non-coding RNA Classifier), a novel method for the classification of ncRNA sequences belonging to different classes. Our approach uses the structural features extracted from ncRNA secondary structure, rather than the primary structure since it has been demonstrated that the structure of ncRNAs can provide relevant information about their biological functions and therefore their class type [ 27 ]. Moreover, we adopted a supervised classification algorithm implementing a deep learning (DL) architecture based on convolutional neural networks (CNNs) [ 28 ]. DL represents a successful paradigm for big data analysis, giving a relevant contribution to several fields of medicine and bioinformatics [ 29 ]. For instance, the use of DL architectures for the prediction of genomic sequences allows improving the performance of the other standard machine learning methods [ 30, 31 ].  In particular, CNNs have been successfully adopted for image classification [ 32 ] because they can extract significant features from images at different abstraction levels. Recently, CNNs have also been applied to DNA sequence classification [ 33 ] with good results, due to their capability to extract meaningful features even from sequences of symbols. The combination of both structural features and a DL architecture allows us to reach classification scores that outperform other similar classification methods based on secondary structure features and machine learning algorithms like the random forest (RF) [ 26 ] and naive Bayes (NB) [ 34 ] classifiers.    Methods\r\n  Proposed method In this section, we introduce the proposed approach for the classification of ncRNA sequences. We classify ncRNA sequences by exploiting a set of discriminative substructures extracted from RNA secondary structures. Starting from a dataset composed of ncRNA fasta sequences belonging to different non-coding classes, we first predict the secondary structure of each sequence (Fig. 1). Then, we identify as features all the discriminative frequent sub-structures extracted from predicted ncRNA secondary structures. Finally, a supervised classification algorithm is trained using as input a ncRNA sequence vs. sub-structures boolean matrix. Each step of the proposed approach, corresponding to a box in Fig. 1, is detailed in the next subsections. ncRNA training dataset To create a consistent and statistically meaningful ncRNA dataset, we followed the approach proposed by Panwar et al. [ 24 ], and by Childs et al. [ 25 ]. Similar to those studies, we downloaded the ncRNA sequences from the latest version of the Rfam database, release 12 [ 35 ]. The Rfam repository represents one of the most complete collections of manually curated RNA sequences, including sequence alignments, annotation and consensus secondary structures. We selected the following 13 ncRNA classes: miRNA, 5S rRNA, 5.8S rRNA, ribozymes, CD-box, HACA-box, scaRNA, tRNA, Intron gpI, Intron gpII, IRES, leader, riboswitch. As will be further explained below, we chose these classes to allow a comparison as fair as possible with RNAcon tool. According to Rfam hierarchical organisation among the selected ncRNA classes (Fig. 2), 5S rRNA and 5.8S rRNA belong to rRNA class; CD-box, HACA-box and scaRNA belong to snRNA/snoRNA class; Intron gpI and Intron gpII belong to Intron class. Generally speaking, the leaves of the hierarchical tree represent ncRNA classes used in this study. According to Rfam database, there are three main functional categories for non-coding RNA sequences, i.e. gene, intron or cis-regulatory element. Considering those ncRNA classes, we built a dataset composed of 20% non-redundant sequences obtained by using the CD-HIT tool [ 36 ], as done by Panwar et al. [ 24 ]. Finally, to create a balanced dataset, we randomly selected 500 sequences for each ncRNA class, except IRES for which there are only 320 available sequences, to obtain 6320 ncRNA sequences. ncRNA secondary structure prediction Since the ncRNA dataset reports sequences in fasta format, it does not contain information about the secondary structure of non-coding RNA sequences. As aforementioned in the previous section, the secondary molecular structures can provide a major key for elucidating the potential functions of RNAs and, consequently, could help us to predict if a ncRNA sequence belongs to the same class. For this reason, just as the RNAcon approach, we choose to exploit the IPknot tool [ 37 ] for predicting the secondary structure of ncRNA. This tool takes into account all the most important topologies in RNA secondary structures and can provide good predictions in terms of both speed and accuracy with respect to other RNA structure prediction methods [ 37 ]. To the best of our knowledge, IPknot is one of the best pseudoknot-free secondary structure prediction tools, since it uses less memory and runs much faster than the other tools, without loss of accuracy [ 38 ]. In our study, the most of ncRNA sequences, such as 5S rRNA, tRNA and miRNA, are pseudoknotfree [ 39 ]. Figure 3 shows how IPknot can predict a complex secondary structure. As a result, this tool produces a dot-parenthesis format file (representing a graph) for each input sequence.   Discriminative sub-structure selection\r\n  Each dot-parenthesis format file prepared in the previous step can be read as an undirected labelled graph representing the RNA sequence, in which vertices are nucleotides and edges are bonds between two nucleotides. As mentioned before, we have 6320 graphs belonging to 13 ncRNA classes. Of course, we can reasonably suppose there is a sort of similarity among the sequences (graphs) that belong to a particular class. Our hypothesis is that frequent sub-structures (sub-graphs) can act as local features for describing ncRNA sequences because they are probably correlated with the molecular function and, thus, they can be used to identify classes of similar non-coding RNAs.  In this context, the selection of molecular sub-structures can be solved in terms of frequent sub-graphs having a certain minimum \u201csupport\u201d in a given set of graphs, where the term support identifies the number of graphs containing a sub-graph. To find these sub-graphs, we adopted the molecular substructure miner (MoSS) algorithm [ 40 ], which implements a depth-first search strategy. The support expressed as a percentage value is the MoSS parameter that specifies the minimum frequency which a sub-structure must occur to be reported. In any case, since the search of frequent sub-graphs in a set of graphs can produce a very large number of features, the advanced pruning techniques implemented in the MoSS algorithm allows us only to obtain closed frequent sub-graphs. A sub-graph is closed only if its support (i.e., the number of graphs that contain this sub-graph) is higher than the support of all the search tree super-graphs containing this sub-graph. Also, the MoSS algorithm lets the user set the m minimum and the n maximum size the sub-structures must have to be taken into account. In the field of molecular compounds, a similar approach was applied to find potential candidates in drug discovery processes [ 41, 42 ].  As an example, Fig. 4 shows a search tree (starting from an adenine nucleotide as a seed) created by the MoSS algorithm, when the input is a list of graphs (such as those reported in the top of the figure). In this figure, a sub-graph, i.e. a node of the search tree, is highlighted with a green 'T' shape area in both predicted secondary structure and the search tree. That means that this sub-graph is a support for the ncRNA sequence at the top of the figure; if it is also a support for a certain user-determined percentage of input sequences and its super-graph has a lower support, it can be considered as a feature of a ncRNA dataset.  Outcomes of the MoSS algorithm are both the list of closed frequent sub-graphs and, for each graph, the list of its closed sub-graphs. Given g graphs (ncRNA sequences) and s sub-graphs (frequent sub-structures), it is possible to define a Boolean matrix A(g, s) where the element (i, j) is set to 1 when sub-structure j is contained in an ncRNA sequence i.    Classification with deep learning architecture\r\n  A machine learning classifier requires a hand-crafted feature selection task to obtain the best representation of the input patterns; this step is crucial for the performances of the classifier. Automatic feature selection is one of the key results of the so-called deep learning neural networks [ 28, 43, 44 ]. Le Cun and colleagues demonstrated that feature selection could be obtained from neural network training [ 32 ]. The proposed model, called convolutional neural network, was constituted by a set of layers based on convolutional filters and average pooling layers, followed by a multi-layer perceptron. Nowadays CNN networks are often used for image classification, in these applications the first layers of the network are trained to recognise features constituted by edges or colour details that are assembled to create more sophisticated features used as image descriptors [ 28 ]. These image descriptors represent the input of the last, fully connected layers of the network, that implement the classifier.  In the nRC system, each position of the vector obtained by the MoSS subsystem indicates the presence or absence of a structural configuration (Fig. 4). Even if the input vectors are binary and the vector components are not in a particular order, bit configurations can still be used as useful features and assembled to build new, more sophisticated patterns that a CNN can exploit. The neural network used in the nRC system is made by two convolutional layers, C1 and C2, followed by two fully connected layers (Fig. 5). The first convolutional layer C1 of the network learns to recognise features constituted by n1 groups of these binary values. The dimension of the convolutional kernels or filters in this first layer should be enough to capture interesting patterns but is upper limited by the computational time. In this work kernels from 3 to 8 were tried and k = 5 was used because represents a compromise between length and computational load. The kernels are floating point vectors adjusted during training phase by the learning algorithm. Considering that the input vectors are binary, then an upper limit for the number of kernels n1 is due to the total number of configurations that can be obtained with k bits. If k = 5, then the kernels can be 31 at most (excluding the configuration with all zeros values). To maintain a manageable training time we choose to use a n1 = 10 kernel for the first stage. The second convolutional layer of the network has kernels of the same dimension (k = 4) and n2 = 20.  A CNN, like the one used in this work, is usually considered \u201cdeep\u201d if compared with the commonly used multilayer perceptrons that usually have three layers (input, hidden, and output). If the input pattern is a vector x ∈ L and the layer C1 uses a set of n1 kernels w of dimension k (w ∈ k), the convolution output will be a set of n1 vectors fi ∈ L−k+1.  fi = wi x + bi i = 1, 2, . . . , n1 where indicates the convolution operator and bi is an offset parameter. A logistic function is a non-linear function applied to the output in the proposed application. The output vectors are reduced using the max-pooling operation with a pool of dimension two so that the resulting output vectors will be (L − K + 1)/2. The max pooling layer compresses the input representation from C1 layer and allows to obtain a more dense representation of the input data. The C2 layer has the same structure but operates with a multi-dimensional input, the output of the C2 layer is constituted by a set of n2 vectors fj j = 1, 2, . . . , n2 given by: fj =  wj,l xl + bj j = 1, 2, . . . , n2. l (1) (2)  The fj vectors are rearranged in a single vector y, containing the features extracted from the input pattern. This vector is the input to a fully connected multi-layer perceptron, with only one hidden layer. The whole network is trained using the stochastic gradient descent algorithm and is implemented in Python using the Theano framework [ 45, 46 ].    Implementation details\r\n  According to the introduced pipeline, we integrated the following publicly available algorithms: Ipknot (release 0.0.2), MoSS (release 2.13) and Theano (release 0.8.2). The docker image is based on the operating system Linux Centos (release 7.2.1511). Java (release 1.8.0) and Python (release 2.7.5) were the languages used to implement the nRC tool.     Results\r\n  In this section, we presented the classification results obtained by our classification pipeline. We performed two kinds of experiments: in the first one, we tested nRC tool using a ten-fold cross-validation scheme to find the best configuration in terms of a number of structural features and parameters of the CNN model. In the second one, we validated the best models obtained during the testing phase by considering an independent dataset, downloaded from Rfam database, and consisting of 2600 sequences, not used in the training phase, belonging to the same 13 ncRNA classes as the training dataset. That validation procedure assured us that there is not overfitting with regards to both feature extraction and the learning of the CNN. We introduced both the number of local features and the statistical measures used for testing procedures. Then, since we want to demonstrate that in the proposed pipeline a deep learning architecture can outperform standard classification techniques, we compared the CNN algorithm with 4 of the most knowns supervised classifiers. Moreover, to test our method against RNAcon tool, the state-of-the-art technique for classification of ncRNA sequences, we introduced an independent validation dataset. Finally, we discussed the obtained results.   Testing procedures\r\n  To evaluate our method for classification of ncRNA sequences, we developed a testing procedure considering different values of the minimum and maximum size of the frequent subgraph fragments extracted by the MoSS tool. Each configuration of those parameters gave, in fact, a different number of structural features used by the classification algorithms. We considered five different configurations, with sub-fragment sizes ranging from two to six, because we are interested in considering local features. The chosen size produced a different number of input features: in particular, we obtained only a few features (about 250) up to many features (about 6000) with regards to the number of sequences in our dataset, i.e. 6320. The min and max size of the MoSS sub-graphs are from 2/4 to 3/6 and the corresponding number of features range from 250 to 6483 (Table 1).  Classification performances have been computed using a ten-fold cross-validation procedure in terms of accuracy, precision, sensitivity, specificity, F-score and MCC. These statistical measures are defined in Table 2.    Comparison among CNN and other machine learning algorithms\r\n  Our proposed classifier based on DL architecture has been compared with four stateof-the-art feature-based algorithms: NB [ 34 ], RF [ 26 ], k nearest neighbour (kNN) [ 47 ] and support vector machine (SVM) [ 19 ]. All these algorithms were run using the Weka 3.6.13 platform [ 48 ]. As done for the CNN parameters, the one introduced in \u201cClassification with deep learning architecture\u201d section, we made several trials with different parameter values to establish, for each classification algorithm, the configuration that gave the best performances in terms of evaluation criteria. In detail, on the default algorithm configurations in the Weka platform, we set the following parameters: NB with kernel estimator option, RF with 100 trees and seed = 10, kNN with K = 3, SVM with gamma = 0.01 and cost = 10. As regards the CNN, the kernel size is k = 5 for both first and second layer; the pool size is 2 for both layers; the number of kernels is n1 = 10 for the first layer and n2 = 20 for the second layer. In the fully connected layer, the number of hidden units (columns of M1 and rows of M2) was 500. We did the first comparison to consider how the accuracy scores change according to the five different numbers of the input features (see Table 1). Our DL approach reaches the highest score of about 74.7% when considering the 6443 features (Fig. 6). The second best classifier is the SVM, with a max accuracy score of about 67.36% when considering 1258 features. The remaining three classifiers did not provide satisfying results.  Considering the results obtained with all the classifier algorithms and all the performances indexes, we found that the CNN network results have the lowest standard deviation for all the measures (Table 3). Moreover, the value of all the performance indices increases with the number of input features (Table 4).  During the evaluation procedure, we also compared the execution time among the classification algorithms used in this study. As regards the training phase, the CNN algorithm is significantly more time consuming with respect to the other algorithms, a it is based on DL architecture. Fortunately, in the most case, the classification model is trained only once, so that users can exploit it for classifying new sequences. Conversely, as regards the testing phase, i.e. the classification of new sequences, the CNN is the second fastest TP are true positives, TN are true negatives, FP are false positives, FN are false negatives algorithms just behind the RF. We report the average execution time taken to test classification models in Table 5. All experiments were carried out on a Windows 10 PC, with Intel i7 2.8 GHz CPU and 8 GB RAM.    Validation procedure\r\n  To further validate our proposed method we performed another classification experiment using an independent dataset whose elements have never been seen by the classifier during the learning phase. We downloaded the validation dataset from Rfam database and is composed of 2600 sequences belonging to the same 13 ncRNA classes as in the original dataset (200 sequences per class). To be more precise, we wanted to demonstrate that both the feature space of size 6443 and the CNN model, learned with the whole training dataset, can generalise the ncRNA class predictions, thus avoiding overfitting. To do that, we first predicted the secondary structure of validation sequences through IPknot, then we represented the sequences of the validation dataset in the same feature space created during the training phase (Fig. 7); finally we evaluated the best CNN model (see Table 4, fourth row) trained with the whole training dataset predicting the ncRNA classes of the validation dataset. The classification results confirm the robustness of the nRC tool with unknown data (Table 6).    Comparison between nRC and the RNAcon tool\r\n  As explained at the end of the \u201cBackground\u201d section, the RNAcon tool is the reference classifier of ncRNA sequences that consider structural features and machine learning algorithms. In particular, RNAcon extracts 20 local and global graph properties from the ncRNA predicted secondary structures, and it makes classification using the RF algorithm. Because our proposed method also considers structural features, the frequent sub-graphs, a machine learning classifier, i.e. the DL convolutional network, we made a direct comparison of our results with the ones provided by RNAcon. We used the RNAcon web service available at http://crdd.osdd.net/raghava/rnacon/, and we made the comparison considering the validation dataset because it represents an independent dataset for both tools. In particular, we removed from the validation dataset the sequences belonging to the scaRNA class, obtaining this way a set of 2400 sequences, because they are not present in the training dataset of RNACon. Our method outperforms RNAcon, doubling its performances according to accuracy and sensitivity scores when 6443 input features are considered (Fig. 8).     Discussion\r\n  Because it has been proved that structural properties of the secondary structure of RNA molecules can provide specific information of the biological function of different ncRNA classes [ 27 ], we presented a classifier that works on a feature set representing frequent fragments of the RNA molecular structure. That representation, coupled with a classifier based on a DL architecture, allowed us to obtain the best scores when compared to other machine learning algorithms and the RNAcon tool. To analyse in detail the performances of our method, we produced the confusion matrix (Fig. 9), so that it is possible to inspect which ncRNA classes our approach better predicted. That confusion matrix has been obtained putting together the single confusion matrices produced at the end of each fold during the testing procedure. For example, we noticed that Intron_gpI and Intron_gpII classes are predicted with a sensitivity score of about 95%, whereas miRNA, IRES and HACA-box classes reached sensitivity and precision score of about 50%. We highlighted in red some situations that will need further investigation in the future. For example, the most misclassified miRNAs (9%) are predicted as HACA-box, correspondingly, 8.4% of HACA-box are predicted as miRNA. The same situation happened to the scaRNA class, with 11.4% misclassified as HACA-box, which in turn is predicted as scaRNA in 13% of cases.  As evidence, there is a misrepresentation of some ncRNA classes. As for the CD-box and HACA-box, both classes belong to the same main class group, i.e. they all are snoRNA (see Fig. 2). Even though they have a different global secondary structure, they could share local sub-structures, in fact, they are both involved in the chemical modification of the RNA classes rRNA, tRNA and snRNA after transcription, hypothesising a common link between their function and their structural sub-features. In particular, CD-box RNAs guide methylation events and HACA-box RNAs guide pseudouridylation of the RNA target [ 49 ]. Another RNA class belonging to snoRNA class is scaRNAs. The scaRNAs are involved in the modification of RNA polymerase II transcribed spliceosomal RNAs, and they are also defined as composite HACA- and CD-box RNAs, because their conserved domains are the typical motifs of both HACA-box and CD-box [ 50 ]. Moreover, similar sub-structures could be found in both miRNAs and some snoRNA, since recent reports have indicated that, despite the differences in size and secondary structure, a human snoRNA and a protozoan snoRNA are associated with Argonautes, processed into small RNAs, and can function as miRNAs [ 51, 52 ].  As mentioned before, to confirm there is not overfitting with regards to both feature extraction and the learning of the CNN, another confusion matrix (Fig. 10), has also been computed for the experiment with the independent validation dataset. Once again, we noticed the same behaviour as in the previous case, with a similar trend with regards to classification mistakes, such as the miRNA-snoRNA (CD-box and HACA-box) and CD-box-HACA-box misclassifications.  All these evidence let us hypothesise that all these classes of ncRNAs have some shared sub-features on the other analysed ncRNA classes. Because our approach considers these sub-structures as local features, the misclassification among some of the ncRNA classes could be explained by those shared features. Concluding, therefore, in spite of the overall good performances of our classification approach, we need to carry out some further analysis for the ncRNA classes whose sensitivity and a precision score was about 50%. A deeper investigation would allow us to increase the classification scores and to try understanding if and what are the relations between RNA sub-classes, considering, for example, global features as well.    Conclusions\r\n  In this work, we introduce nRC (noncoding RNA Classifier), a new tool for the classification of non-coding RNA sequences. Three steps are the basis of the proposed method: the prediction of ncRNAs secondary structures, the extraction of frequent substructures as features and the classification of known ncRNA classes. To implement these processes, we used the IPknot algorithm to predict RNA secondary structures with pseudoknots, the MoSS decision tree pruning algorithm to obtain sub-structures, and a deep learning network architecture, namely a convolutional neural network, as a supervised classifier. Differently to other existing ncRNA classification approaches, we (i) created a ncRNAs vs. local topological features Boolean matrix as input data and (ii) adopted a DL architecture for classification. To demonstrate the effectiveness of the proposed approach, we first compared the proposed classifier with four of the most well-known classification algorithms, i.e. RF, NB, kNN and SVM, and then we compared our method with the RNAcon tool, that is the literature reference classifier of ncRNA sequences. Experiments have also been carried out using an independent validation dataset. In both tests, we demonstrated the advantages of using our approach on other strategies, obtaining the highest scores in terms of five different statistical measures, i.e. accuracy, sensitivity, specificity, precision, F-score and MCC. In particular, results demonstrated the proposed method outperformed the state-of-the-art RNAcon approach, doubling its performance in terms of accuracy and sensitivity.  As future work, we are working to train a classification model with much more ncRNA sequences, also belonging to some other well studied ncRNA classes, such as piwi-interacting RNA (piRNA) [ 53 ] and circular RNA (circRNA) [ 54 ]. In addition, to improve classification performances, we are planning to test some new secondary structure prediction tools, like those proposed in [ 55, 56 ]. Finally, we aim at creating a publicly available web service for the classification of unlabelled non-coding RNA sequences.  Abbreviations CNN: Convolutional neural network; DL: Deep learning; IRES: Internal ribosome entry site; kNN: K nearest neighbour; lncRNA: Long non-coding RNA; miRNA: MicroRNA; MoSS: Molecular sub-structure miner; NB: Naive Bayes; ncRNA: Non-coding RNA; nRC: Non-coding RNA classifier; RF: Random forest; rRNA: Ribosomal RNA; tRNA: Transfer RNA; siRNA: Silencing RNA; snoRNA: Small nucleolar RNA; snoRNP: Small nucleolar ribonucleoproteins; snRNA: Small nuclear RNA; SVM: Support vector machine Acknowledgments Not applicable.  Funding The publication costs for this article were funded by the CNR Interomics Flagship Project CUP B81J12000980001 \u201cDevelopment of an integrated platform for the application of \u201comic\u201d sciences to biomarker definition and theranostic, predictive and diagnostic profiles\u201d.  Availability of data and materials The source code of nRC tool is freely available at https://github.com/IcarPA-TBlab/nrc, moreover, since the proposed tool requires several dependencies to be installed, a docker image has been released at https://hub.docker.com/r/tblab/nrc/. The datasets generated and analysed during the current study are available in the TBLAB repository, http://tblab.pa.icar.cnr.it/public/nRC/paper_dataset/. Authors' contributions AF: project conception, implementation, experimental tests, discussions, assessment, writing. MLR: project conception,implementation, experimental tests, discussions, assessment, writing. LLP: project conception, writing, assessment, discussions. RR: project conception, implementation, discussions, assessment, writing. AU: project conception, discussions, assessment, writing, funding. All authors read and approved the final manuscript. Ethics approval and consent to participate Not applicable.  Consent for publication Not applicable.  Competing interests The authors declare that they have no competing interests.    Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/IcarPA-TBlab/nRC",
    "publicationDate": "0",
    "authors": [
      "Antonino Fiannaca",
      "Massimo La Rosa",
      "Laura La Paglia",
      "Riccardo Rizzo",
      "Alfonso Urso"
    ],
    "status": "Success",
    "toolName": "nrc",
    "homepage": ""
  },
  "92.pdf": {
    "forks": 0,
    "URLs": [
      "github.com/NDBL/ADDIT",
      "www.ieee.org/publications_standards/publications/rights/index.html"
    ],
    "contactInfo": [
      "achakrabarty@seas.harvard.edu",
      "semrich@nd.edu"
    ],
    "subscribers": 2,
    "programmingLanguage": "Python",
    "shortDescription": "Imputation tool for missing genotype data in model and non-model species",
    "publicationTitle": "Highly Accurate and Efficient Data-Driven Methods For Genotype Imputation",
    "title": "Highly Accurate and Efficient Data-Driven Methods For Genotype Imputation",
    "publicationDOI": "10.1109/TCBB.2017.2708701",
    "codeSize": 5935,
    "publicationAbstract": "-High-throughput sequencing techniques have generated massive quantities of genotype data. Haplotype phasing has proven to be a useful and effective method for analyzing these data. However, the quality of phasing is undermined by the presence of missing information. Imputation provides an effective means of improving the underlying genotype information. For model organisms, imputation can rely on an available reference genotype panel and a physical or genetic map. For non-model organisms, which often do not have a genotype panel, it is important to design an imputation technique that does not rely on reference data. Here, we present ADDIT (Accurate Data-Driven Imputation Technique), which is composed of two data-driven algorithms capable of handling data generated from model and non-model organisms. The non-model variant of ADDIT (referred to as ADDIT-NM) employs statistical inference methods to impute missing genotypes, whereas the model variant (referred to as ADDIT-M) leverages a supervised learning-based approach for imputation. We demonstrate that both variants of ADDIT are more accurate, faster, and require less memory than leading state-of-the-art imputation tools using model (human) and non-model (maize, apple, grape) genotype data. Software Availability: The source code of ADDIT and test data sets are available at https://github.com/NDBL/ADDIT",
    "dateUpdated": "2017-04-19T07:29:27Z",
    "institutions": ["University of Notre Dame"],
    "license": "No License",
    "dateCreated": "2017-04-19T07:25:35Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     TCBB.     10.1109/TCBB.2017.2708701   Highly Accurate and Efficient Data-Driven Methods For Genotype Imputation     Olivia Choudhury  0    Student Member  0    Ankush Chakrabarty  achakrabarty@seas.harvard.edu  0    Member  0    Scott J. Emrich  semrich@nd.edu  0    Senior Member  0    0  O. Choudhury and S.J. Emrich are with the Department of Computer Science and Engineering, University of Notre Dame ,  Notre Dame, IN, 46556 , USA     2017   2708701    2  12  2016     -High-throughput sequencing techniques have generated massive quantities of genotype data. Haplotype phasing has proven to be a useful and effective method for analyzing these data. However, the quality of phasing is undermined by the presence of missing information. Imputation provides an effective means of improving the underlying genotype information. For model organisms, imputation can rely on an available reference genotype panel and a physical or genetic map. For non-model organisms, which often do not have a genotype panel, it is important to design an imputation technique that does not rely on reference data. Here, we present ADDIT (Accurate Data-Driven Imputation Technique), which is composed of two data-driven algorithms capable of handling data generated from model and non-model organisms. The non-model variant of ADDIT (referred to as ADDIT-NM) employs statistical inference methods to impute missing genotypes, whereas the model variant (referred to as ADDIT-M) leverages a supervised learning-based approach for imputation. We demonstrate that both variants of ADDIT are more accurate, faster, and require less memory than leading state-of-the-art imputation tools using model (human) and non-model (maize, apple, grape) genotype data. Software Availability: The source code of ADDIT and test data sets are available at https://github.com/NDBL/ADDIT    Genotype imputation  single nucleotide polymorphisms (SNPs)  next-generation and high-throughput sequencing   machine learning  big data       INTRODUCTION\r\n  H quencing, whole exome sequencing, and genome  IGH-throughput techniques like whole genome sewide single nucleotide polymorphism (SNP) microarrays are generating huge volumes of genotype data. To associate phenotypes such as disease susceptibility with underlying genotypes, there has been a rapid growth of phasingbased inference formalisms. Phasing is particularly useful in genome-wide association studies (GWAS) [ 1 ] to infer linked alleles on a chromosome. Other downstream analyses include identifying recombinant breakpoints [ 2 ], deducing history of human demographics [ 3 ], and modeling cisregulation of gene expression [ 4 ].  Missing genotype data is a major hindrance to phasing. This is a result of inherent shortcomings of the underlying techniques that generate such data. Previous efforts have shown that genotype imputation can improve phasing quality in genetic association studies by up to 10% [ 5 ]. Although we focus on imputation as a precursor to phasing, secondary benefits of formulating data-driven imputation methods include generating higher fidelity genetic maps and improved metagenomic analysis (see for example, [ 6 ], [ 7 ]).  A majority of the existing imputation methods require a panel of reference genotypes and a physical or genetic map. The absence of such a reference panel, as in non-model organisms, makes the problem of imputation and phasing much more difficult using currently available software. We address this gap with a lightweight framework, referred to as ADDIT-NM, for fast and accurate imputation in nonmodel organisms that relies only on the underlying statistics of the genotype data. A preliminary version of ADDIT-NM has been reported in [ 8 ]. We also demonstrate that the model organism specific variant of ADDIT, referred to as ADDITM, can extract available information in the reference panels via supervised learning to significantly improve imputation accuracy. We perform an extensive, comparative numerical study of ADDIT against the leading imputation tools, such as Beagle [ 9 ], IMPUTE2 [ 10 ] (for model and non-model organisms) and LinkImpute [ 11 ] (for non-model organisms). The comparison results are compiled using real data of varying sizes, varying proportions of missing genotypes, and varying sizes of reference panels (for model organisms). In these comparisons, ADDIT consistently outperforms the other tools in terms of speed, memory, and accuracy.  Our primary contributions include: (i) the formulation of data-driven, lightweight imputation algorithms for both model and non-model organisms with high speed and accuracy; (ii) the incorporation of both local and global information by utilizing adaptive windows and trust metrics; (iii) the exploitation of adjacent genotype data to significantly expedite imputation under certain conditions; (iv) the employment of multi-class supervised learning algorithms to extricate information from reference panels of model organisms to enhance the imputation process.  The rest of the paper is organized as follows. In Section 2, we present the relevant literature and explain current limitations. Our proposed method is discussed in detail in Section 3, with subsections devoted to the implementation of ADDIT-NM and ADDIT-M for non-model and model organisms, respectively. Results from a comparative study with state-of-the-art imputation algorithms are reported in Section 4 using real data from multiple non-model organisms and a model organism (human). We conclude and discuss future work in Section 5. 2    RELATED WORK\r\n  Prior work on genotype imputation generally considered either related or unrelated samples. In individuals containing blocks of shared haplotypes, the authors in [ 2 ] proposed a method called identity-by-descent (IBD) to impute missing values. For closely related populations of small size with large number of samples, methods such as [ 12 ], [ 13 ] have demonstrated effectiveness via long-range phasing.  For unrelated samples with strongly linked polymorphisms, Clark's algorithm [ 14 ] is one of the earliest approaches for inferring haplotypes. To relax the inherent assumption of tight linkage, the principle of Expectation Maximization (EM) is leveraged in [ 15 ]. However, EMbased methods are effective only when high quality prior models are available for training. They are also computationally prohibitive for large-scale genotype data. More recent methods take into consideration the fact that new haplotypes are derived from older haplotypes via mutation and recombination [ 16 ].  Based on these observations, widely used phasing tools, such as IMPUTE2 [ 17 ], generate approximate coalescent models and hidden Markov models (HMMs) from genotypes for subsequent stochastic EM-based algorithms. PHASE [ 18 ] employs Markov chain MonteCarlo (MCMC) algorithm to explore possible combinations of haplotypes [ 19 ]. The combinatorial explosion inherent in MCMC limits the applicability of this tool to small datasets [ 20 ]. FastPHASE [ 21 ], a faster variant of PHASE, implements a parsimonious clustering of haplotypes and is more amenable to smaller sample sizes. In large datasets, the algorithm uses subsets of haplotypes, resulting in performance degradation. To overcome this challenge in large samples, Beagle [ 22 ] employs haplotype clustering at individual loci to compute transition probabilities in HMM.  For model organisms with reference panels, existing tools ( [ 17 ], [ 23 ], [ 24 ]) extricate information from the reference panel to generate an HMM model for phasing and imputation. An imputation algorithm based on a variant of knearest neighbor interpolation employed in LinkImpute [ 11 ] has been demonstrated to perform well in a myriad of heterozygous populations; however, it is usually sluggish when the dataset size is increased, and exhibits limited accuracy.  Machine learning methods have been used in genetic analysis [ 25 ] and the genotype imputation problem has been explored using artificial neural networks [ 26 ]. Although the authors in [ 27 ] report an extensive comparison of modern biclassification methods to genotype imputation, for general populations, more than two alleles is more viable for imputation. For more general multi-class classification solutions, we refer the reader to [ 28 ]-[ 30 ]. The authors in [ 31 ] demonstrate the effectiveness of reduced-feature models in comparison with the two common missing value treatments: missing data handling via oracle/discarding, and missing data handling via imputation, on a suite of benchmark data sets. We design ADDIT-M based on reduced-feature models as it restricts the selection of training samples to a small neighborhood of the value to be imputed, thereby preserving local distribution properties. In this section we present a detailed description of our proposed ADDIT algorithm. For ADDIT-NM, the imputation framework is segregated into multiple steps (Steps 1-5), and justification for each step is provided in section 3.1. Note that the algorithm in Section 3.1 is described in detail in [ 8 ].  For ADDIT-M, we present a windowed, multi-class supervised learning-based imputation algorithm in section 3.2.  Let N be the number of samples in a given population and M0 be the total number of missing genotypes in the entire population. Let Gjq denote the genotype at the jth position of the qth sample. Here, q 2 f1; : : : ; N g and j 2 f1; : : : ; M g. Let Iq be the set of potential imputable genotypes at Gjq. We denote a window centered at Gjq by W(Gjq; d), where d is the window length and the window contains (d 1)=2 elements on either side of the central element Gjq. We use Hamming distance to measure the similarity between two windows of identical length.  For example, the Hamming distance between '111000' and '100010' is three. Let jAj represent the cardinality of a set A.  A list of important notation/symbols used in the subsequent discussion can be found in Table 1. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCBB.2017.2708701, IEEE/ACM  A −1       +1 Quick Impute  Trusted Candidates 1 2 3 4 5 6 7 8 9 Step 1: Quick imputation using immediate neighbors Suppose Gjq is a missing genotype. We begin by performing a quick impute (QI) step by imputing the central value to either of its two neighboring alleles if Gjq 1 = Gjq+1; this is described in Algorithm 1 and illustrated in Figure 1A.  The QI step is effective in substantially reducing the search  Step 2: Similarity computation for each missing genotype For a missing value Gjq in the query sample, we create a query window of length d, denoted W(Gjq; d). We construct windows of identical length centered at the jth position of each of the remaining population samples, which we call sample windows, denoted W(Gjk; d), where k 2 f1; : : : ; q 1; q + 1; : : : ; N g.  Algorithm 2 Similarity computation for each missing genotype Require: M missing genotypes with distinct neighbors 1: for each missing Gjq with distinct neighbors do 2: Construct query window W(Gjq; d) 3: for each k 2 f1; : : : ; N g except q do 4: if Gjk is not missing data then 5: Cq add W(Gjk; d) to the set of candidate windows  end if end for for i = each candidate window in Cq do  similarity score using (1) 6: 7: 8: 9: i 10: end for 11: end for 12: return set of candidate windows for M missing geno  M types, fCqgq=1 13: return set of similarity scores for all candidate windows,  M f qgq=1 We now exclude those sample windows that have miss  j ing data at Gk. The subset of sample windows with no missing data at Gjk is hereby referred to as the candidate set Cq for the missing genotype Gjq (Figure 1B). Each sample window is given a similarity score using i = d  Step 3: Similarity threshold of candidate windows Consider a histogram of similarity scores for the collection of candidate windows denoted as H( ). A maximum likelihood  min = arg max H( ) is used to compute a similarity threshold, the procedure for which is provided in Algorithm 3.  Algorithm 3 Similarity threshold of candidate windows Require: fCqgqM=1; f qgq=1 from Algorithm 2  M 1: for r = 1 to d 1 do 2: H( ) frequency of windows in all candidate sets  M fCqgq=1 with similarity score r 3: end for 4: min arg max H( ) 5: return Similarity threshold, min (2)  It is important to note that the value of min is computed considering all candidate windows of the M missing genotypes remaining after Step 1. The inherent globality in our formulation offers various advantages: First, it avoids bias induced by local candidate windows with low similarity scores. For example, suppose that local candidate windows for a particular missing genotype have similarity scores between 2 and 7, with window length d = 11. Also consider that the entire sample population contains windows (with identical d) of similarity scores between 2 and 10 with most windows having scores &gt; 6. If we compute min based on local candidates only, then mloicnal is (say) 4. However, using the globally-derived similarity threshold min, we obtain global = 6 because the global sample population has more min candidate windows with higher similarity scores. If we were to impute a value based on mloicnal, then we would enable windows with lower similarity scores to have an effect on the decision, which is undesirable, as it is likely to recommend an erroneous imputed value. Instead, using mgloinbal filters out these low-similarity candidates, resulting in more accurate imputation. Finally, our method avoids diluting information; using candidate windows retains relevant local information with embedded global trends of these data.  This improves imputation accuracy while lowering required computation.  We will next leverage the notion of the similarity threshold to categorize candidate windows as trusted or untrusted.  Step 4: Adaptive classification of trusted candidates For the qth query window we construct a set of trusted candidates, denoted Tq. A candidate window is said to be a trusted candidate if its similarity score is at least the similarity threshold min (as shown in Figure 1C). Note that Tq cannot be empty for the choice of min in (2). This claim can be proven by contradiction, given that there is at least one candidate window for a given window length d &gt; 2.  Suppose Tq is empty for a given min := arg max H( ).  This implies that there is no candidate window whose similarity score is at least min. Clearly, this is a contradiction because the set of candidate windows is not empty, so at least one candidate window must have similarity score min in order to ensure that it is the maximizer of the histogram H( ). Therefore, Tq must be non-empty for this choice of min.  We refer to this method as adaptive because it allows each Tq to have a variable number of trusted candidate windows, unlike existing frameworks such as those employing knearest neighbor algorithms. This is advantageous because it exploits only the most similar windows for subsequent imputation. To take into account both window similarity and repetitiveness of the central allele, we introduce the following priority-based weighting scheme.  Algorithm 4 Adaptive classification of trusted candidates Require: Set of candidates fCqgqM=1 with similarity values f qgqM=1, obtained in Algorithm 2; similarity threshold, min 1: for each missing genotype Gjq do 2: Cq set of candidate windows for Gjq 3: for each candidate window in Cq do 4: similarity score of candidate window 5: if min then 6: Tq add candidate window to the set of  trusted candidates 7: end if 8: end for 9: end for 10: return All M sets of trusted candidates, fTqgqM=1 Step 5: Priority-based Imputation scheme Recall that Iq is the set of potential imputable genotypes at Gjq. For each imputable genotype iq 2 Iq, we assign decision weights based on two criteria: (i) the frequency of iq at the central element over all trusted candidate windows in Tq; and (ii) the similarity score of trusted candidate windows containing iq in the central position (refer to Figure 1D). The window similarity decision weight !s indicates the reliability of Tqiq for imputing the missing genotype with iq. The allele frequency decision weight !f signifies the likelihood of iq, even if the corresponding trusted candidates have low similarity scores with respect to the query window. The decision weights are designed to handle potential bias towards highly frequent genotypes found in trusted candidates with low similarity scores. Mathematically, the decision weights are written as: !f (iq) = !s(iq) =  Fiq ; jTqj 1 where Fiq is the frequency of iq at the central position of the trusted candidates, Tqiq Tq is the set of trusted candidates with iq in the central position, and  H is the Hamming distance between the query window and each window W(Gjk; d) 2 iq  We categorize the values of !f (iq) and !s(iq) as high, medium, or low. For this categorization, we use data in the histogram obtained in Step 3 as follows. We first eliminate all windows with similarity scores below min as in Step 3.  Thus, the lowest allowable similarity score is min, which motivates us to classify &lt; min=d as low. Let max be the highest similarity score on the histogram with non-zero frequency. Then we classify high as &gt; max=d. Note that max=d &lt; 1 since the maximal similarity is d 1. All decision weights in the range [ min=d; max=d] are considered medium. For each imputable genotype iq, we determine its priority level P(iq) using the rules in Table 2.  Within Table 2, the priorities are set such that higher weights are given to imputable genotypes supported by highly similar trusted candidates irrespective of !f . This is motivated by the fact that, in haplotypes, we expect highly similar samples over local regions to exhibit identical inheritance of genotypes. If the trusted candidates have medium similarity, then we check !f . This is because the trusted candidates cannot be completely relied upon to generate a correct imputed genotype. Instead, we also rely on a likelihood-based estimate embedded into !f . For an imputable genotype with medium !s, if its corresponding !f is high, then that holds more priority than medium or low !f . We will assign even less priority to the genotypes that have high or medium frequencies with low !s for similar reasons as discussed above. Finally, we will assign the least priority to a genotype if its supporting trusted candidates are of low similarity and low frequency. For such cases, we investigate the remaining genotypes in Iq.  We impute the genotype iq with the highest priority level. If there is a clash of priorities, either value can be imputed. This is written mathematically as: ^iq = arg min P(iq); iq2Iq  (4) where ^iq is the imputed genotype. The pseudo-code for this step is presented in Algorithm 5. 3.2   ADDIT-M: Imputation for model organisms\r\n  Step 1: Construction of training and truth sets from reference panel Let Ntrain be the number of training samples collected to train a supervised learning machine L, and Rkj denote the jth position of the kth reference sample. Recall that Gjq is the missing value in the query sample at the jth position, and d is a positive integer that denotes the number of features of the training set.  For each Gjq, we begin by constructing a truth set, j j Struth 2 RNtrain , and training set, Strain 2 RNtrain (d 1): these will be used by a classifier L to inform the imputation j process. The truth set Struth is constructed using Ntrain 1 2 3    4 5 6  A   Training Set, train\r\n    Truth Set, truth\r\n  Learn ℒ B Predict ℒ    Test Set, test\r\n  Imputed Value  Truth Set Training Set  Test Set  C Algorithm 5 Priority-based imputation scheme Require: Set of trusted candidates Tq for query window  W(Gjq; d) Require: Set of imputable genotypes Iq 1: for each imputable genotype iq 2 Iq do 2: iq trusted candidates with iq in the central  Tq position 3: !f (iq); !s(iq) decision weights using eqn. (3) 4: Categorize !f and !s as 'high', 'medium', or 'low' 5: P(iq) priority level according to Table 2 6: end for 7: ^iq using eqn. (4) 8: return Imputation decision, ^iq genotypes from the reference panel that do not have missing data at the jth position, that is:  j Struth = h j R1  j R2  j i RNtrain :  The training set construction (feature selection) is more involved. One cannot select a training set containing reference data with indices belonging to a window of length d centered at Rkj for k = 1; 2; : : : ; Ntrain, because the corresponding testing set (a window of length d centered at Gjq) could contain missing data, which may result in low-quality predictions. Instead the training set is selected from the reference panels corresponding to indices in the neighborhood of Gjq that do not contain missing values. This can be written more rigorously as where fmigid=11 is a set of indices representing d 1 nearest neighbors to Gjq containing no missing values. The corresponding test set is given by  j Stest = Gqm1  Gqm2  Gqmd 1 : The formation of the training, testing, and truth set is illustrated in Figure 2A.  Step 2: Imputation based on identical truth values j Clearly, if all the labels in the truth set Struth are identical, there is no need to train the classifier L. In such a case, the j imputed value is the label in Struth.  Step 3: Quick imputation This is an optional step, which performs effectively for data exhibiting a low degree of double recombination in adjacent positions. The implementation of this step has been previously discussed in Step 1 of Section 3.1.  Step 4: Imputation via multi-class supervised learning If the conditions in the earlier steps are not satisfied, this j implies that the truth set Struth contains more than one j unique label. In fact, Struth could contain multiple labels; for example, three labels if the genotypes are encoded with f0; 1; 2g. The learning machine L is, therefore, referred to as a multi-class learning machine [ 32 ]. The multi-class classifier j L learns from the training set Strain and the corresponding j multi-class truth values Struth, and can consequently be j used to predict the value of Gjq using the test set Stest. This procedure is illustrated in Figure 2B-C. 4      RESULTS AND DISCUSSION\r\n   Testing ADDIT-NM\r\n  4.1.1 Data Acquisition We test ADDIT-NM on three benchmark plant datasets considered in [ 11 ] (see Table 3 for details). In summary, we use genotype by sequencing (GBS) data from members of the grape genus Vitis generated by Illumina Hi-Seq and mapped to its reference genome [ 33 ], [ 34 ]. Some SNPs based on missing values, heterozygosity, and minor allele frequency (MAF) as in [ 11 ] are discarded. A similar apple dataset generated from members of the genus Malus is acquired from the 1995 accession from the US Department of Agriculture repository in Geneva, NY. The samples are double-digested with restriction enzymes and sequenced with Illumina Hi-Seq. The reads are mapped to the reference genome of Malus domestica version 1.0 [ 35 ]. Similar to the above grape data, variants are also filtered. Finally, we consider a large maize (corn) dataset available at the International Maize and Wheat Improvement Center [ 36 ] to verify the scalability of our proposed algorithm. For this dataset, a pre-processing stage eliminates bi-allelic SNPs with &lt; 20% missing data, minor allele frequencies (MAF) of &gt; 1%, and samples with &gt; 20% missing values. 4.1.2 Comparative Analysis We implement ADDIT-NM and compare its performance with contemporary imputation algorithms such as Beagle 3.3.2, LinkImpute, and IMPUTE2. Performance metrics used for this comparison include (i) percentage of genotype imputation errors; (ii) runtime; and (iii) memory usage. The results of this comparative study are tabulated in Table 3. It is clear that ADDIT-NM significantly outperforms the competition. For example, the genotype errors of grape and maize imputation are less than half the minimum of the errors produced by the other methods. The runtime of ADDITNM is consistently small, at times an order of magnitude smaller than the corresponding runtimes of Beagle and/or LinkImpute. Importantly, this large speed-up does not result in prohibitive use of memory. This is demonstrated by a 2-3 order-of-magnitude reduction of memory usage in comparison with Beagle or LinkImpute, and significantly less memory (around half or less) as IMPUTE2.  As discussed in [ 20 ], Beagle is more accurate than IMPUTE2 for large sample sizes. IMPUTE2 implements prephasing, wherein genotypes are first phased and then haplotypes are imputed. This reduces runtime and memory usage at the cost of accuracy. LinkImpute requires similar runtime as Beagle, although it has slightly higher accuracy. It incurs high computational overhead since it uses a genome-wide similarity search based on k-nearest neighbor imputation (kNNi) [ 37 ]. Contrary to these, ADDIT-NM relies on an adaptive number of reliable trusted candidate windows, which helps in increasing imputation accuracy. It also can significantly reduce runtime and memory use via an initial pruning of the search space that we call quick imputation. Unlike Beagle and IMPUTE2, we do not require a large genotype panel, which further reduces the lookup time and memory required and makes ADDIT-NM applicable to less studied organisms. 4.1.3 Effectiveness of Quick Imputation A major reason for the computational efficiency of ADDITNM is due to the quick imputation step. To illustrate the performance of each imputation stage (that is, quick versus priority-based imputation), we refer the reader to Figure 3. We observe that the number of quick imputes (QI) is significant for each dataset. However, the corresponding number of quick impute errors (QI Error) are small. For the grape, apple, and maize datasets, 1 out of 1487 (&lt; 0:1%), 210 out of 6326 (&lt; 4%), and 168 out of 6078 (&lt; 3%) genotypes, respectively, are incorrectly imputed in the QI stage. Figure 3 also contains information regarding the number of priority imputations (PIs) and their corresponding imputation errors (PI Error). For the real datasets, the proportion of PI Errors is low, ranging from &lt; 10% in apple and grape, to &lt; 17% in maize. This trend suggests that for these plant data, the QI step can be exploited because it combines high computational speeds along with a relatively lower imputation error rate.  This is further supported by comparing imputation performance of ADDIT-NM with and without the QI step (Table 4). We observe that the maximum memory used for both the configurations are identical for all datasets, and the error percentage is comparable; a minuscule increase in the number of errors is noted when the quick impute step is skipped. The most noteworthy result obtained from this investigation is the runtime differences: the lack of the quick impute step results in higher execution time for each dataset, particularly for larger datasets, as expected. 4.2    Testing ADDIT-M\r\n  We also test the performance of ADDIT-M on human model organism data. We use the multi-class support vector machine (MC-SVM) as an exemplar supervised learning algorithm. The MC-SVM is implemented via Python's scikit-learn module. The rationale behind choosing the SVM as our supervised learning method is its ability to handle high-dimensional data using the kernel-trick, its efficiency with smaller-sized training sets [ 29 ], and its effectiveness in the imputation problem, as reported in the comparative study [ 27 ]. 4.2.1 Data Acquisition For testing ADDIT-M, we obtain genotype data of phase 3 human chromosome 20 from the 1000 Genomes Project [ 38 ]. This data comprises 2504 individuals from 26 populations. We select a subset comprising 8 populations (GBR, TSI, CHS, STU, GIH, LWK, CHB, IT) and randomly mask 1%, 2%, and 5% of the data for subsequent imputation. We further use 75%, 90%, and 95% of the remaining phase 3 data as reference panels for running Beagle and IMPUTE2 and as the training set for the learning algorithm of ADDIT-M. Fig. 3. Illustration of the number of quick imputations (QI) in ADDIT-NM: blue, quick imputation error (QI Error): green, priority-based imputations (PI): dark red, and priority-based imputation error (PI Error): purple, for the non-model organisms: grape, apple, and maize. 4.2.2  Comparative Analysis To test our proposed ADDIT-M imputation algorithm, we again consider the overall imputation error percent, total runtime, and maximum memory used. For our experiments, since IMPUTE2 required at least 130 computation time higher than Beagle (also shown in [ 9 ]), we only consider Beagle for comparison with the now optional QI step of ADDIT-M (see previous section) turned off.  The results of our comparative study is tabulated in Table 5(A) and (B). In Table 5(A), we demonstrate the effect of increasing the proportion of missing values. We randomly fix 75% of the data as the reference panel containing no missing data, and mask the remaining data by 1%, 2%, and 5%. We note that ADDIT-M consistently requires less memory (about 1/4th) and demonstrates speedups of two orders of magnitude. Additionally, the overall imputation error percent is considerably lower for ADDIT-M. In Table 5(B), we demonstrate results for 5% missing data when the training set size varies amongst 95%, 90% and 75%. As expected, decreasing the number of training samples worsens the performance of the supervised learning algorithm: the total error percent for ADDIT-M gradually increases from 1.9% to 2.8%. Note that the error percent of Beagle exhibits a more accelerated increase with reduction of training size relative to our proposed approach. 4.2.3 When should we use QI? As mentioned before, the effectiveness of QI is most pronounced when the adjacent alleles exhibit a low degree of double recombination. Although ADDIT-M with QI completes roughly 7% faster on the human data obtained from [ 38 ], it does perform worse (see Table 6) using a 75% training set and 5% missing data. In Figure 4, we illustrate the distribution of imputation errors over the three decisionmaking steps of ADDIT-M: the identical truth value (Step 2), QI (optional Step 3), and supervised learning based imputation (Step 4). Since we have a high level of trust in the reference genotype panel, we give Step 2 the highest priority in terms of determining the imputed value. Thus, the percentage of values imputed in Step 2 remains unaltered with and without QI. It is clear from the figure that the QI step only affects the other 70% of the missing values: specifically 17% of the missing genotypes are eligible for QI. Of these 17%, 10% are imputed incorrectly. The SVM performance, both with and without QI, are very similar and exhibit 4% imputation error (this is because the training samples are identical for both runs). It follows that for these data QI performs relatively worse as compared to using available reference data. Thus we do not recommend the QI step unless adjacent alleles exhibit low degrees of double recombination.  Missing (%) 1 2 5 95 90 75 Training Size (%)  Method  Beagle ADDIT-M  Beagle ADDIT-M  Beagle ADDIT-M Method  Beagle ADDIT-M  Beagle ADDIT-M  Beagle  ADDIT-M (B) Fixed Missing Genotypes (5%) 4.2.4 Importance of Multi-class Supervised Learning We believed that using a supervised learning algorithm would enhance imputation accuracy. As a result, we expect that Beagle and ADDIT-M will outperform ADDIT-NM by exploiting the information embedded in the reference genotype panel. This is indeed the case, as deduced from Table 7. Among the two imputation tools for model organisms, ADDIT-M outperformed Beagle in terms of imputation accuracy, runtime, and memory. A considerable subset of the query data was filtered for identical truth (IT) and quick impute (QI)-based deduction, that lead to accurate and expedited imputation. For the remaining set of missing values, the supervised learning approach enabled accurate imputations. The results presented in Table 7 show that for model organisms, utilizing genotype information in reference panel, as in the case of Beagle and ADDIT-M, provide more accurate imputations.     5 CONCLUSIONS\r\n  Genotype imputation is an essential precursor for improving the quality of haplotype phasing in applications like genome-wide association studies. Although model organisms can resort to available reference genotype panel for imputation, the problem becomes more challenging for nonmodel organisms that lack such reference data. Here, we present accurate and efficient window-based data-driven approaches for imputation of missing genotypes in both model and non-model organisms. We test our proposed methods on real datasets of non-model and model organisms, including humans. For varying sizes of data, proportions of missing genotypes, and sizes of training samples, our method consistently performs better than the leading tools like Beagle, IMPUTE2, and LinkImpute.  Although the multiclass classifier approach used in ADDIT-M generated accurate imputations, there still remains a scope to investigate other data-driven supervised learning approaches in Step 4 of section 3.2. One can further analyze the performance of our imputation tools when plugged in to different phasing algorithms. Finally, a natural extension of genotype imputation is the devlopment of an accurate haplotype phasing mechanism for downstream analysis. In this regard, one can employ a graph-based phasing approach [ 39 ] or further explore sophisticated hidden Markov model-based algorithms.    ACKNOWLEDGMENTS\r\n  This work was supported by the Eck Institute for Global Health (EIGH) PhD fellowship to Choudhury and University support provided to Emrich as director of bioinformatics. We also thank the anonymous reviewers for their thought-provoking questions and improving suggestions.  10 1545-5963 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.  Olivia Choudhury is a doctoral candidate and Eck Institute for Global Health (EIGH) Fellow at the Department of Computer Science and Engineering, University of Notre Dame, IN. She received a B.Tech. in Computer Science and Engineering from the West Bengal University of Technology, Kolkata, India. She is interested in bioinformatics, high performance computing, predictive modeling, and data-driven methods.  Ankush Chakrabarty is a postdoctoral fellow at the School of Engineering and Applied Sciences, Harvard University, Cambridge, MA. He received a B.E. with first-class honors in Electrical Engineering at Jadavpur University, Kolkata, India, and received his Ph.D. in Automatic Control at the School of Electrical and Computer Engineering at Purdue University, West Lafayette, IN.  He is interested in nonlinear systems, unknown input observers, biomedical control, and datadriven methods.  Scott J. Emrich received the BS degree in biology and computer science from Loyola College in Maryland and the PhD degree in bioinformatics and computational biology from Iowa State University. His research interests include computational biology, bioinformatics and parallel computing, including arthropod genome analysis with applications to global health and ecology.  He is a member of the IEEE Computer Society.    ",
    "sourceCodeLink": "https://github.com/NDBL/ADDIT",
    "publicationDate": "0",
    "authors": [
      "Olivia Choudhury",
      "Student Member",
      "Ankush Chakrabarty",
      "Member",
      "Scott J. Emrich",
      "Senior Member"
    ],
    "status": "Success",
    "toolName": "ADDIT",
    "homepage": ""
  },
  "33.pdf": {
    "forks": 0,
    "URLs": [
      "www.ebi.ac.uk/GOA",
      "dongjunchung.github.io/bayesGO/",
      "www.ncbi.nlm.nih.gov/pubmed",
      "www.yeastgenome.org/"
    ],
    "contactInfo": ["chungd@musc.edu"],
    "subscribers": 1,
    "programmingLanguage": "R",
    "shortDescription": "bayesGO: Bayesian ontology fingerprint, a statistical framework for biomedical literature mining ",
    "publicationTitle": "A statistical framework for biomedical literature mining",
    "title": "A statistical framework for biomedical literature mining",
    "publicationDOI": "10.1002/sim.7384",
    "codeSize": 138,
    "publicationAbstract": "In systems biology, it is of great interest to identify new genes that were not previously reported to be associated with biological pathways related to various functions and diseases. Identification of these new pathwaymodulating genes does not only promote understanding of pathway regulation mechanisms but also allow identification of novel targets for therapeutics. Recently, biomedical literature has been considered as a valuable resource to investigate pathway-modulating genes. While the majority of currently available approaches are based on the co-occurrence of genes within an abstract, it has been reported that these approaches show only sub-optimal performances because 70% of abstracts contain information only for a single gene. To overcome such limitation, we propose a novel statistical framework based on the concept of ontology fingerprint that uses gene ontology to extract information from large biomedical literature data. The proposed framework simultaneously identifies pathway-modulating genes and facilitates interpreting functions of these new genes. We also propose a computationally efficient posterior inference procedure based on Metropolis-Hastings within Gibbs sampler for parameter updates and the poor man's reversible jump Markov chain Monte Carlo approach for model selection. We evaluate the proposed statistical framework with simulation studies, experimental validation, and an application to studies of pathway-modulating genes in yeast. The R implementation of the proposed model is currently available at https://dongjunchung.github.io/bayesGO/. Copyright © 2017 John Wiley & Sons, Ltd.",
    "dateUpdated": "2017-04-17T06:40:21Z",
    "institutions": [
      "Medical University of South Carolina",
      "The University of Texas Health Science Center at Houston"
    ],
    "license": "No License",
    "dateCreated": "2017-04-14T15:33:07Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     10.1002/sim.7384   A statistical framework for biomedical literature mining     Dongjun Chung  chungd@musc.edu  0    Andrew Lawson  0    W. Jim Zheng  1    0  Department of Public Health Sciences, Medical University of South Carolina ,  Charleston, South Carolina ,  U.S.A    1  School of Biomedical Informatics, The University of Texas Health Science Center at Houston ,  Houston, Texas ,  U.S.  A. Charleston, South Carolina ,  U.S.A     2017   4  17    3  6  2017    23  6  2016     In systems biology, it is of great interest to identify new genes that were not previously reported to be associated with biological pathways related to various functions and diseases. Identification of these new pathwaymodulating genes does not only promote understanding of pathway regulation mechanisms but also allow identification of novel targets for therapeutics. Recently, biomedical literature has been considered as a valuable resource to investigate pathway-modulating genes. While the majority of currently available approaches are based on the co-occurrence of genes within an abstract, it has been reported that these approaches show only sub-optimal performances because 70% of abstracts contain information only for a single gene. To overcome such limitation, we propose a novel statistical framework based on the concept of ontology fingerprint that uses gene ontology to extract information from large biomedical literature data. The proposed framework simultaneously identifies pathway-modulating genes and facilitates interpreting functions of these new genes. We also propose a computationally efficient posterior inference procedure based on Metropolis-Hastings within Gibbs sampler for parameter updates and the poor man's reversible jump Markov chain Monte Carlo approach for model selection. We evaluate the proposed statistical framework with simulation studies, experimental validation, and an application to studies of pathway-modulating genes in yeast. The R implementation of the proposed model is currently available at https://dongjunchung.github.io/bayesGO/. Copyright © 2017 John Wiley &amp; Sons, Ltd.    Bayesian hierarchical model  biological pathway  gene ontology  literature search  ontology ifngerprint       1. Introduction\r\n  System biology aims to develop computational and mathematical models to describe complex biological systems, by considering molecules, cell, and organisms as biological components constituting this system. In system biology, the study of interaction between these biological components is of main interest as it allows to understand the dynamics, resulting in various functions and behaviors of the system. Biological pathway is the de facto functional unit in system biology, and a pathway is defined as a set of genes interacting to make aggregate function. For example, the mitogen-activated protein kinase (MAPK) signaling pathway consists of 267 genes in the human genome and is involved in various cellular functions, including cell proliferation, differentiation, and migration. More importantly, mutations of genes in this pathway have been reported to be involved in the development of multiple types of cancers. Hence, potential cancer treatments might be developed by reversing or blocking undesirable behaviors of the genes in this pathway. As implied in this example, discovering new genes that potentially modulate a pathway of interest does not only promote understanding of pathway regulation mechanism but also provide novel targets for therapeutics.  In spite of its importance, the identification of new pathway-modulating genes remains challenging because pathways often consist of many components, some pathways are only ambiguously or improperly defined, and the biological links between a pathway and genes influencing its activity may not be obvious. Hence, various genomic and genetic studies have been implemented to discover interaction among genes. For example, genetic interaction and co-expression patterns can be identified from genome-wide association studies and RNA-seq studies, respectively, while various relationships between a regulator and its target genes can be inferred with ChIP-seq and CLIP-seq experiments at transcriptional and translational levels, respectively. However, these approaches are limited as they focus on only one specific aspect of biology while two genes can be linked through various biological relationships such as genetic association, epigenetic association, protein-protein interaction, among others. Although some approaches like YeastNet [ 1 ] tried to overcome this limitation by combining the results from multiple sources to identify novel gene interactions, effective integration of these results is not a trivial task. More importantly, its performance can be affected by the quality of individual data sets [ 2 ].  Recently, biomedical literature has been considered as a valuable resource to investigate relationship among genes. Especially, PubMed (http://www.ncbi.nlm.nih.gov/pubmed), developed and maintained by the National Center for Biotechnology Information at the National Institutes of Health, is a free and popular resource for biomedical literature comprising over 24 million citations. In contrast to the approaches described in the previous paragraph, biomedical literature is not restricted to certain aspects of biology, and relationship among genes is comprehensively characterized throughout the history of biomedical research. However, effective mining of valuable information from biomedical literature still remains a challenging task. Currently, the majority of approaches mining biomedical literature [ 3, 4 ] are based on the assumption that two genes are functionally related if they appear together within the same biomedical literature (Figure 1A). However, the utility of these approaches is limited by the fact that 70% of PubMed abstracts contain information about only a single gene (only about 83,000 PubMed articles are annotated with two or more human genes while over 160,000 annotated with only a single gene). In addition, information in abstracts is often utilized deterministically while uncertainty in discovery of gene-gene relationship is not taken into account properly.  In order to address these limitations of currently available methods, we propose a novel statistical framework for biomedical literature mining. Some key features of the proposed statistical framework can be summarized as follows. First, we address the limitation of co-occurrence-based approaches by utilizing the idea of ontology fingerprint . This approach does not only allow to identify indirect relationship among genes using gene ontology (GO) information but also effectively extract information from biomedical big data and summarize them to manageable size. Second, the proposed Bayesian hierarchical model effectively identifies hidden pathway structure and addresses redundancy and correlation among GO terms. Third, our poor man's reversible jump Markov chain Monte Carlo (RJMCMC) approach gracefully addresses the model selection problems introduced for identification of gene and GO term clusters. Finally, the proposed Metropolis-Hastings within Gibbs sampler allows the computationally efficient implementation of posterior inference.  This paper is structured as follows. In Section 2, we propose a novel statistical framework for biomedical literature mining, based on the idea of ontology fingerprint and a Bayesian hierarchical model. In Section 3, we evaluate the proposed method with simulation studies. In Section 4, we further evaluate the proposed method with experimental validation and apply it to biomedical literature mining studies of pathway-modulating genes in yeast. In Section 5, we discuss our future research directions.    2. Methods\r\n   2.1. Ontology fingerprint\r\n  Recently, we developed the concept of ontology fingerprint [ 5 ] for biomedical literature mining to overcome the limitation of traditional approaches based on co-occurrence of genes in an abstract (Figure 1A). The key idea of ontology fingerprint is to consider indirect relationships between genes in biomedical literature mining. Specifically, we consider two genes to be related to each other if each of these two genes are described for similar biological functions in its corresponding abstract (Figure 1B). However, in such approaches, it is possible that two functionally identical genes can be described with similar but slightly different terms. Hence, it is critical to have a systematic way to describe functions of genes while their vocabulary is rich enough to reflect complexity of biology. Based on this rationale, we use the terms compiled in the Gene Ontology (GO) database (http://geneontology.org) for biomedical literature mining because it provides a set of structured, precisely defined, common, and controlled vocabularies to describe the role of genes and their products in any organism.  After we search abstracts using genes and GO terms, a hypergeometric test is performed on each pair of gene and GO term to determine the degree of enrichment of GO terms for a gene, where each abstract is considered as an observation in the space of literature (Figure 1C). Specifically, we calculate the hypergeometric p-value for the pair of i-th gene and t-th GO term as  (Nt )( N−Nt ) Yti = 21 Nti (NN)i−Nti  Ni + ∑ r&gt;Nti (Nt)(N−Nt) r Ni−r (N ) Ni where N is the total number of abstracts, Ni is the number of abstracts associated with i-th gene, Nt is the number of abstracts associated with t-th GO term, and Nti is the number of abstracts associated with both t-th GO term and i-th gene. Note that the mid p-value approach was used in this calculation to address the discreteness of the hypergeometric distribution. This approach does not only effectively extract information from abstracts and reduce search space to the manageable size but also allow to take into account complexity in discovery of these relationships, for example, specificity of description in an abstract, abstractness of GO terms, among others. Finally, the ontology fingerprint for a gene is defined as a set of GO terms over-represented in the abstracts linked to the gene, along with the enrichment p-values corresponding to these terms from hypergeometric tests (Figure 1D). While these enrichment p-values have great potential to improve biomedical literature mining, effective utilization of this information is not trivial.  In our previous work [ 5 ], we considered a correlation-like measure to quantify relationships among genes. However, this measure was not based on a data generative model, and as a result, it was not easy to interrogate its properties and measure its uncertainties in inference for relationship among genes. In addition, inference procedures were laborious and inefficient because inference steps were not integrated. Specifically, relationships among genes were first determined using a hierarchical model, and then, gene clusters were identified by applying existing clustering algorithms to the results from this fitted model. Finally, functions of each gene cluster were investigated by checking which GO terms are enriched for these genes. In order to effectively address these challenges, in this paper, we propose a novel Bayesian hierarchical model described in Section 2.2. We note that our previous work [ 5 ] only constitutes the basis for mining and preprocessing of biomedical literature. The proposed Bayesian hierarchical model to analyze these biomedical literature mining results is the unique contribution of this paper. We also note that our approach is not equivalent to a meta-analysis. Specifically, meta-analysis aims to combine evidences for genes (e.g., p-values) across literature. In contrast, the proposed approach rather considers each abstract as a single observation and ignores evidence for genes provided in each literature.  This paper was directly motivated by the study of identifying pathway-modulating genes for yeast using the PubMed literature. In this study, we downloaded the 12/30/2009 version of PubMed abstracts in XML format and processed them to extract the PubMed ID and the text of each abstract. We extracted GO terms and their descriptions from the 12/20/2009 version of the GO database compiled by the Gene Ontology Consortium. We used a total of 46,524 abstracts linked to at least one yeast gene and 7258 GO terms linked to these abstracts, to analyze 5983 yeast genes. In order to avoid unspecific association between genes and GO terms, we used only the abstracts annotating at most 100 genes and also excluded general GO terms. Then, a hypergeometric test was performed on each pair of yeast gene and GO term (Figure 1C).  We summarized the p-values from these hypergeometric tests as a matrix, where rows and columns correspond to GO terms and genes, respectively. We note that multiple testing adjustment is not a critical issue here because we do not use p-values to make conclusions and multiple testing adjustment essentially only affects the emission distribution modeling. Based on this rationale, we used the original p-values instead of adjusted ones in the modeling. We also note that there are many missing cells in this matrix because if we do not observe a pair of a GO term and a gene in abstracts, then we do not have corresponding p-value at all. For example, 60% cells are missing even when we considered 100 rows (GO terms) and 116 columns (genes) with least number of missing cells (Section 4.2). One approach to handling these missing cells is to consider them as missing observations in the model, estimate (impute) them, and utilize them for the inference. However, in our modeling below, we ignore these missing cells instead of imputing them because of the following two reasons. First, accurate inference for these missing cells are not of main interest because they correspond to uninteresting gene-GO term pairs, and these cells have only limited information to improve clustering of genes and GO terms. Second, we can speed up our inference multiple times faster by ignoring these missing cells, as it reduces the number of parameters significantly. Based on this rationale, we consider only the non-missing gene-GO term pairs as observations in the hierarchical model described below.    2.2. Bayesian hierarchical model\r\n  In this section, we describe a Bayesian hierarchical model to identify unreported pathway-modulating genes and facilitate functional interpretation of these genes, using the biomedical literature mining data described in Section 2.1. From the hypergeometric test described in Section 2.1, we have the p-value of the hypergeometric test (Yti) for the gene i = 1, · · · , G and the GO term t = 1, · · · , T . For modeling purposes, we take a probit transformation of these p-values, which facilitates easier data visualization and modeling [ 6 ], that is, Zti = Φ−1 (Yti), where Φ (⋅) is the cumulative distribution function for standard normal distribution. We assume that G genes constitute unobserved K gene clusters based on the exploratory analysis of data (Figure S1a) and genes in the same cluster are considered to be related to similar functions. Note that here we assume that a gene can belong to only a single gene cluster. In addition, there is also strong correlation among GO terms (Figure S1b), mainly because gene ontology is designed as a directed acyclic graph (DAG) structure [ 7 ]. In order to take this into account, we further assume that T GO terms constitute V unobserved GO term clusters. Note that in theory, we can model the relationship among GO terms as a DAG, as gene ontology is originally designed. However, in our application, accurate estimation of the relationship among GO terms is not of main interest, and it sufifces to utilize the relationship among GO terms to improve interpretation of gene clustering results. Moreover, modeling the relationship among GO terms as a DAG will increase the computational burden significantly, which is not desirable when we consider its role in our model. Based on this rationale, we model the relationship among GO terms as clusters instead of a DAG. We denote the membership of i-th gene to a gene cluster as Mi and the membership of t-th GO term to a GO term cluster as Lt, where Mi ∈ {1, · · · , K} and Lt ∈ {1, · · · , V }. Finally, in order to reflect the fact that each gene cluster can be described with a set of related GO terms, we introduce a binary indicator for the enrichment of t-th GO term for i-th gene, denoted as Eti, where Eti = 1 if t-th GO term is enriched for i-th gene and Eti = 0 otherwise.  The main distribution hierarchy of the proposed model is as follows.  (Zti|Eti = 1,  i1,  i1) ∼ N( i1, 1∕ i1), (Zti|Eti = 0,  i0,  i0) ∼ N( i0, 1∕ i0), (Eti| , Lt, Mi) ∼ Bernoulli( LtMi ), (Mi| ) ∼ Categorical ( 1, · · · ,  K ) , (Lt| ) ∼ Categorical ( 1, · · · ,  V ) , for t = 1, · · · , T and i = 1, · · · , G. Note that here we assume K and V to be known. In Section 2.3, we will discuss how we implement data-driven selection of K and V using the poor man's reversible jump Markov chain Monte Carlo approach.     Emission distribution for hypergeometric test p-values: Conditional on the enrichment status (Eit),\r\n  we model the emission distribution for the probit-transformed p-value from hypergeometric test for t-th GO term and i-th gene (Zti), using a mixture of Gaussian densities. Specifically, if t-th GO term is not enriched for i-th gene (Eti = 0), we assume that (Zti|Eti = 0,  i0,  i0) ∼ N( i0, 1∕ i0), for t = 1, · · · , T and i = 1, · · · , G. Note that this corresponds to the empirical null distribution approach [ 6 ], which is often employed in genomic applications such as differential gene expression analysis. On the other hand, when t-th GO term is enriched for i-th gene (Eti = 1), we assume that (Zti|Eti = 1,  i1,  i1) ∼ N( i1, 1∕ i1). Note that here we consider gene-specific distributions of enriched p-values because we expect that there might be huge variation in degree of study (e.g., number of abstracts describing each gene) among genes, which results in significantly different shapes of distributions for p-values. This was also confirmed in our exploratory data analysis.  Enrichment of GO terms for genes: Conditional on the cluster index for i-th gene (Mi) and the cluster index for t-th GO term (Lt), we model the enrichment of t-th GO term for i-th gene (Eti) using Bernoulli trials. In our application, it is reasonable to assume that genes belonging to the same gene cluster can be described with similar GO terms because genes in the same pathway are involved in similar biological processes. On the other hand, it is also reasonable to assume that GO terms in the same GO term cluster are associated with similar set of genes because these GO terms describe similar biological processes that characterize functions of a pathway (Figure S1b). We incorporate such information about enrichment structure into the model by making a single distribution shared among gene-GO term pairs belonging to the same gene and GO term clusters. Based on this ratioale, we model the enrichment of t-th GO term for i-th gene as (Eti| , Lt, Mi) ∼ Bernoulli( LtMi ), for t = 1, · · · , T and i = 1, · · · , G.   2.3. Model selection\r\n  One of the challenging issues in clustering problems is how to determine unknown number of clusters. In this paper, we model the clustering of genes and the clustering of GO terms using the poor man's Reversible Jump Markov Chain Monte Carlo (RJMCMC), which is a special case of Metropolised Carlin and Chib algorithm [ 8, 9 ]. This approach often avoids complicated component split and combine procedures, which are challenging in the application of RJMCMC. On the other hand, this method still retains advantages of RJMCMC, such as the data-driven model selection within posterior sampling and the reflection of uncertainty due to model selection on inference.  Gene clustering: We first describe the modeling of membership of i-th gene to a gene cluster. Let us denote the maximum possible number of gene clusters as Kmax while K be the effective number of gene clusters, that is, K ≤ Kmax. In other words, we assume that there are K number of gene clusters while we consider up to Kmax number of gene clusters. Then, we generate the cluster index for i-th gene (Mi) as follows.  ( 1∗, · · · ,  K∗max | 0) ∼ Dirichlet( 0, · · · ,  0), ( k| ) ∼ Bernoulli( ),  k k∗  ,  k = ∑Kmax  k\u2032 k∗\u2032  k\u2032=1 (Mi| ) ∼ Categorical ( 1, · · · ,  Kmax ) , for i = 1, · · · , G and k = 1, · · · , Kmax. In this framework,  ∗ can be interpreted as relative proportion of k genes in k-th cluster while  k indicates whether k-th cluster participates in the model or not. Then, final value for proportion of genes in k-th cluster ( k) is calculated using only the clusters that participate in the model ( k = 1).  GO term clustering: We model the membership of t-th GO term to a GO term cluster in a similar way. Again, let us denote the maximum possible number of GO term clusters as Vmax and the effective number of GO term clusters as V, that is, V ≤ Vmax. Then, we generate the cluster index for t-th GO term (Lt) as follows.  ( 1∗, · · · ,  ∗  Vmax | 0) ∼ Dirichlet( 0, · · · ,  0), ( v| ) ∼ Bernoulli( ),  v v∗  ,  v = ∑Vmax  v\u2032 v∗\u2032  v\u2032=1 (Lt| ) ∼ Categorical ( 1, · · · ,  Vmax ) , for t = 1, · · · , T and v = 1, · · · , Vmax.  Determination of Kmax and Vmax: As Kmax and Vmax determine only upper bounds for the numbers of gene clusters and GO term clusters, it suffices to set them large enough so that K &lt; Kmax and V &lt; Vmax. In our experience of analyzing real datasets, we found that a good rule of thumb is to set Kmax = 0.1G and Vmax = 0.1 T. Moreover, it is also straightforward to check whether Kmax or Vmax are not large enough by monitoring K and V values across the MCMC iterations. Specifically, if Kmax or Vmax are set too low, K and V frequently hit their upper bounds (i.e., K = Kmax or V = Vmax), and in this case, we can simply increase Kmax and Vmax values and re-run the MCMC.    2.4. Posterior inference\r\n  Key quantities: The proposed Bayesian framework provides key quantities that allow to identify novel pathway-modulating genes and facilitate easy interpretation of these genes. First, the number of gene clusters (pathways) and the number of GO term clusters can be inferred from Pr(∑  k|⋅) and k Pr(∑v  v|⋅), respectively. Genes modulating the same pathway can be identified using Pr(Mi|⋅), while Pr(Lt|⋅) can be used to identify a group of GO terms describing similar functions. Finally, function for a pathway can be investigated by checking the pattern of Pr(Eti|⋅).  Prior specification: We consider semi-conjugate priors for the emission distributions and a conjugate prior for the enrichment status, that is,  i1 ∼ N ( 1, 1∕ 1),  i1 ∼ Gamma (2, 1),  i0 ∼ N ( 0, 1∕ 0),  i0 ∼ Gamma (2, 1), and  vk ∼ Beta ( 01,  02) while we restrict the range of  i0 to (−5, 0) to avoid identiafibility issues. Note that the range (−5, 0) in the probit scale corresponds to (2.87e − 7, 0.5) in the scale of original p-values, which is wide enough to cover centers of background p-values. Hyperpriors are specified in weakly informative ways, that is,  1 ∼ N(0, 1002),  1 ∼ Gamma (2, 1),  0 ∼ N(0, 1002),  0 ∼ Gamma (2, 1),  01 ∼ Gamma (0.1, 0.1),  02 ∼ Gamma (0.1, 0.1),  ∼ Beta (1, 1),  ∼ Beta (1, 1),  0 ∼ Gamma (0.1, 0.1), and  0 ∼ Gamma (0.1, 0.1). Sensitivity analysis in Section 3 in the supplementary materials indicates that the posterior inference results are only marginally affected by misspecification of priors.  Posterior sampling: We carried out the posterior sampling with the mixed Gibbs-Metropolis algorithm. We summarize the algorithm as follows.  1. For i = 1, · · · , G and t = 1, · · · , T, update Eti from its Bernoulli full conditional. 2. For i = 1, · · · , G, update Mi from its categorical full conditional. 3. For t = 1, · · · , T, update Lt from its categorical full conditional. 4. For v = 1, · · · , Vmax, update  v from its Bernoulli full conditional. 5. For k = 1, · · · , Kmax, update  k from its Bernoulli full conditional. 6. For v = 1, · · · , Vmax and k = 1, · · · , Kmax, update  vk from its Beta full conditional. 7. Update  from its Beta full conditional. 8. Update  from its Beta full conditional.  9. For i = 1, · · · , G, update  i1 from its Normal full conditional. 10. For i = 1, · · · , G, update  i1 from its Gamma full conditional. 11. For i = 1, · · · , G, update  i0 from its Normal full conditional. 12. For i = 1, · · · , G, update  i0 from its Gamma full conditional.  13. Update  1 from its Normal full conditional. 14. Update  1 from its Gamma full conditional. 15. Update  0 from its Normal full conditional. 16. Update  0 from its Gamma full conditional.  17. Update ( ∗,  ∗,  0,  0,  01,  02) using a Metropolis-Hastings algorithm.  Sections 1 and 2 in the supplementary materials provide complete details about the posterior inference and implementation, including full conditional distributions. We note that most of the parameter updates (steps 1-16) can be efficiently implemented using Gibbs samplers based on explicit forms of full conditional distributions. For example, even our sub-optimal JAGS implementation of the proposed model to analyze 74 genes and 112 GO terms in Section 4.1 took only 80 min for the MCMC updates of two chains, each of 30,000 iterations, using a single 2.10-GHz CPU. For all the analysis results in Sections 3 and 4, the burn-in period of 20,000 samples with two chains was used to evaluate the convergence of the chains, and after the burn-in, a sample of 10,000 iterations was obtained as the converged posterior sample. We evaluated convergence of two chains with Gelman-Rubin statistics [ 10 ] and visual inspection of trace plots. Diagnostics plots for the real-data applications in Section 4 can be found in Figures S5-S8. No significant violation of convergence was detected in these diagnostics. Finally, we evaluated the proposed model using the posterior predictive checking based on the real-data applications in Sections 4.1 and 4.2, and the results (Figure S9) indicate that the proposed model nicely fits the real data.     3. Simulation studies\r\n  We first evaluated the proposed Bayesian hierarchical model using simulation studies based on 100 genes and 100 GO terms. Specifically, we considered the following four simulation settings, which mimic various cases that occur in the real data such as ones in Sections 4.1 and 4.2. True gene clusters (Mi), GO term clusters (Lt), and enrichment matrix (Eti) corresponding to these four settings are depicted in Figures 2A,C and 3A,C.  \u2022 Simulation setting #1 (Figure 2A): Four gene clusters have their own GO term clusters, where sizes of gene and GO term clusters are generated from multinomial (100, (0.25, 0.25, 0.25, 0.25)). \u2022 Simulation setting #2 (Figure 2C): Three gene clusters have their own GO term clusters and there also exist genes and GO terms without association. Sizes of gene and GO term clusters (three clusters and a group of background genes/GO terms) are generated from multinomial (100, (0.25, 0.25, 0.25, 0.25)). \u2022 Simulation setting #3 (Figure 3A): Three gene clusters have their own GO term clusters with various cluster sizes, where sizes of gene and GO term clusters are generated from multinomial (100, (0.5, 0.25, 0.25)). \u2022 Simulation setting # 4 (Figure 3C): Three gene clusters have their own GO term clusters while one additional GO term cluster is shared across these three gene clusters. Sizes of gene clusters are generated from multinomial (100, (0.33, 0.33, 0.33)) while sizes of GO term clusters are generated from multinomial (100, (0.25, 0.25, 0.25, 0.25)).  Given the settings previously, we generated probit-transformed p-values (Zti) from N(0, 1) if Eti = 0 (i.e., background) and from N( i1, 1∕ i1) if Eti = 1 (i.e., signal) for i = 1, · · · , 100 and t = 1, · · · , 100, where  i1 ∼ U(−5, −1) and  i1 ∼ U(0.1, 0.3) for i = 1, · · · , 100. In addition, we also considered the simulation data generated with  i1 ∼ U(−10, −5) and  i1 ∼ U(0.5, 1) to evaluate the effect of signal-tonoise ratios (Figures S2a,c and S3a,c).  Overall, the proposed model could successfully identify true gene clusters (Mi) and GO term clusters (Lt) with high confidence while also recovering the enrichment matrix ( Eti) that is almost identical to the true one even when signal-to-noise ratio is relatively weak (Figures 2B,D and 3B,D; see Figures S2b,d and S3b,d for the settings with higher signal-to-noise ratio). We note that our second simulation study has especially practical implication because it corresponds to the case that some uninformative genes and GO terms are also included in the data because of insufficient prior knowledge. In this case, the proposed model identified four gene clusters ( Mi) and four GO term clusters (Lt), where genes without associated GO terms constitute one cluster and similarly, GO terms without associated genes constitute one cluster (Figure 2D). This can be practically useful property because it provides easy post-processing guideline. Specifically, if we identify gene or GO term clusters without strong enrichment after we fit the proposed model, we can simply throw them away as a group instead of checking each gene or GO term individually.  During this study, we recognized that applications of biclustering algorithms to the matrix of probittransformed p-values could potentially be alternative solutions to identify gene and GO term clusters because biclustering algorithms aim to cluster rows and columns simultaneously. In order to investigate this possibility, we studied popular biclustering algorithms, including the Cheng and Church algorithm [ 11 ] and the Plaid model [ 12 ], all of which are implemented in the R package biclust. However, biclustering methods actually turned out to be sub-optimal solutions for this problem because of the following two reasons. First, while many real biomedical literature datasets, such as ones considered in Sections 4.1 and 4.2, have significant amount of missing cells in the matrix of probit-transformed pvalues, most biclustering algorithms are not tailored for a dataset with large proportion of missing cells. Second, even when we assume that there are no missing cells at all, these biclustering algorithms failed to identify correct gene and GO term clusters for the simulation datasets previously (Figure S4). This is mainly because these algorithms tried to identify a block that can explain a large proportion of matrix after reordering rows and columns. Specifically, the Cheng and Church algorithm identifies blocks by minimizing the averaged squared errors while the Plaid model identifies blocks by minimizing the sum of squared errors. Hence, these methods essentially prefer to identify a larger block with smaller within variance. However, in many real biomedical literature datasets such as ones considered in Sections 4.1 and 4.2, signal components (GO terms associated with genes) usually have larger variances than background components (GO term-gene pairs without association) while background components also correspond to a large proportion in the data matrix. As a result, biclustering algorithms prefer to identify blocks corresponding to backgrounds rather than signals. In contrast, the proposed model does not suffer from these issues because it takes into account such background and signal structure. We also considered various transformations of p-values (e.g., Yti, 1 − Yti, Φ−1 (Yti), and Φ−1 (1 − Yti)) and confirmed that this conclusion still remains valid. Finally, from the practical point of view, it is often not trivial to use these biclustering algorithms because these methods have multiple tuning parameters, which are not easy to tune and highly data specific. In contrast, the proposed statistical model does not require such tuning and the only data-specific parameters are the maximum numbers of gene and GO term clusters, which are relatively easy to specify. In summary, the proposed model does not only outperform popular biclustering approaches but also provide practical benefits compared with them, such as easier tuning and natural handling of missing values.    4. Application to the yeast PubMed data\r\n   4.1. Experimental validation\r\n  As an additional evaluation of the proposed model in addition to the simulation studies, in this section, we implemented an experimental validation study for the proposed model using yeast (Saccharomyces cerevisiae). Specifically, we utilized the experimental validation data from [ 5 ], which provides 30 sphingolipid pathway genes selected by the sphingolipid expert (\u201cknown genes\u201d), 14 genes that were experimentally validated to be associated with the sphingolipid pathway (\u201cpositive control genes\u201d), and 30 genes that were experimentally shown not to be associated with the sphingolipid pathway (\u201cnegative control genes\u201d) in yeast. We considered these 74 genes here, while 14 and 30 genes in the second and third groups are considered as positive and negative controls, respectively. Then, we considered 112 GO terms that appeared most often with these 74 genes in the analysis.  We first evaluated GO term clustering performance. Based on the median number of GO term clusters (∑  v = 6; Figure S10a), we post-processed GO term clustering results by assuming six clusters v (Section 2 in the supplementary materials) and the clusters 1 to 6 have 13, 5, 59, 12, 11, and 12 GO The first block of table shows the gene clustering results compared with the known genes, the positive control genes, and the negative control genes. The second block of table shows the results after the filtering step (removing genes that were assigned to a cluster less than 90% of MCMC iterations (maxk P(Mi = k|⋅) ≤ 0.9) from each cluster). terms, respectively (Figure S11a). When we removed GO terms that were assigned to a cluster less than 90% of MCMC iterations (maxv P(Lt = v|⋅) ≤ 0.9) from each cluster, the GO term clusters 1-6 have 2, 4, 17, 0, 6, and 9 GO terms, respectively (Table S1). Essentially, the proposed model could effectively group GO terms describing similar biological functions, that is, ceramide metabolic processes (cluster 1), sphingolipid metabolic processes (cluster 2), steroid metabolic processes and positive regulation of metabolic processes (cluster 3), fatty acid metabolic processes (cluster 5), and transferase activities (cluster 6). Furthermore, this result indicates that incorporating GO terms that are not well associated with genes of interest into the analysis does not degrade performance of the proposed model, and such GO terms can be easily detected based on the patterns of GO term clustering results. This again confirms our observation in our second simulation study with background genes.  We next evaluated the gene clustering performance for the 74 genes we considered. Based on the median number of gene clusters (∑  k = 4; Figure S10b), we post-processed gene clustering results k by assuming four clusters and the clusters 1-4 have 10, 19, 15, and 30 genes, respectively (Figure S11b; Table I). When we removed genes that were assigned to a cluster less than 90% (maxk P(Mi = k|⋅) ≤ 0.9) from each cluster, the clusters 1-4 have 1, 9, 5, and 11 genes, respectively (Table I). The known genes were assigned to the gene clusters 2-4, while all the positive control genes were assigned to the gene cluster 4. Negative control genes were almost equally distributed across four clusters, but most of them were removed after the filtering step. Moreover, no known or positive control genes were assigned to the gene cluster 1 to which one thirds of the negative control genes were assigned. This result indicates that the proposed model has power to distinguish the negative control genes from the known and the positive control genes. As it is interesting for the known genes (i.e., the genes that are considered to belong to the same biological pathway, according to the expert) to make multiple clusters, we further studied whether there are any differences between known genes in clusters 2, 3, and 4. Specifically, we checked the pathway \u201csphingolipid metabolism\u201d reported in the Saccharomyces Genome Database (http://www.yeastgenome.org/), the most well-established pathway database for yeast genome (Figure S12). The known genes assigned to the gene cluster 2 are specifically associated with the two units, \u201cdihydrosphingosine\u201d and \u201cphytosphingosine,\u201d while the known genes assigned to the gene cluster 3 are essentially involved in all the steps in this pathway. The known genes assigned to the gene cluster 4 are specifically associated with the two units, \u201cserine palmitoyltransferase\u201d and \u201cceramide synthase.\u201d This also explains why all the positive control genes belong to cluster 4 because these genes were experimentally validated using myriocin, which is known to specifically inhibit serine palmitoyltransferase and ceramide. This result shows that the proposed model has sensitivity to separate out biologically distinct gene clusters that could potentially be missed even by human experts.  In order to further characterize three gene clusters, we investigated the GO terms associated with each gene cluster (Eti; Figure 4A). First of all, Figure 4B shows that the GO term cluster 2 (sphingolipid pathway) is specifically associated with the known genes while there were almost no abstract describing the positive and negative control genes with the GO terms in cluster 2. This makes good sense because known genes were chosen based on prior knowledge about association with sphingolipid pathway, while positive and negative control genes were not previously reported to be associated with sphingolipid pathway. Moreover, Figure 4B shows that there are multiple subgroups of genes within the known genes with respect to their associations with GO term clusters 1, 5, and 6. On the one hand, the known genes were assigned to the gene clusters 2-4, and all of these three gene clusters were associated with the GO term cluster 2 (sphingolipid pathway). On the other hand, the gene cluster 2 is associated with the GO term cluster 5 (fatty acid metabolic processes), the gene cluster 3 is associated with the GO term cluster 1 (ceramide metabolic processes), and the gene cluster 4 is associated with both the GO term clusters 1 (ceramide metabolic processes) and 6 (transferase activities). This again confirms our observation that there are three subgroups of genes among the known genes, and it also further explains that these three gene subgroups are mainly different in their functions for other metabolisms and transferase activities. This result illustrates sensitivity of the proposed model in identification of pathway-modulating genes, and it also shows that enrichment patterns of GO terms for genes (Eti) can be an effective tool to facilitate functional interpretation for novel pathway-modulating genes.    4.2. De novo identification of pathway-modulating genes\r\n  As we confirmed the power of the proposed model in identification of pathway-modulating genes and interpretation of their functions, we now apply the proposed hierarchical model to more arbitrary chosen set of genes, in order to evaluate its de novo knowledge discovery performance. Specifically, we considered 116 genes with the largest number of observed p-values and chose 100 GO terms that were observed most often with these 116 genes. Note that unlikely the previous section, no biological expert knowledge was used to select these 116 genes. Based on the median number of GO term clusters (∑  v = 6; Figure v S10c), we post-processed GO term clustering results by assuming six clusters, and the clusters 1-6 have 12, 19, 14, 15, 38, and 2 GO terms, respectively (Figure S11c). When we removed GO terms that were assigned to a cluster less than 90% of MCMC iterations (maxv P(Lt = v|⋅) ≤ 0.9) from each cluster, the GO term clusters 1-6 have 0, 18, 13, 9, 28, and 0 GO terms, respectively (Table S2). We found that GO term clusters 2-5 correspond to condition-dependent catabolic processes and positive regulation of biological processes (GO term cluster 2), glycolysis and catabolic processes (GO term cluster 3), metabolic processes (GO term cluster 4), and cell wall organization, homeostasis and localization (GO term cluster 5), respectively. Note that all the GO term clusters related to metabolic and catabolic processes (GO term clusters 2-4) are not significantly affected by the filtering step, which implies high confidence clusters.  We next evaluated gene clustering results for the 116 genes under consideration. Based on the median number of gene clusters (∑  v = 6; Figure S10d), we post-processed gene clustering results by assumv ing 10 clusters, but only nine clusters remained after the post-processing of cluster indices. The gene clusters 1-9 have 24, 9, 1, 3, 19, 25, 4, 8, and 23 genes, respectively (Figure S11d; Table II). When we removed genes that were assigned to a cluster less than 90% (maxk P(Mi = k|⋅) ≤ 0.9) from each cluster, the gene clusters 1, 3, 4, 7, and 8 were totally eliminated, and the gene clusters 2, 5, 6, and 9 have 4, 11, 12, and 18 genes, respectively (Table II). We further investigated these gene-clustering results using the KEGG database [ 13 ] as a ground truth because the pathway annotation provided by the KEGG database is human curated and considered as high quality. Here, we first focus on the \u201cGlycolysis/Gluconeogenesis\u201d (\u201csce00010\u201d) pathway in the KEGG database because this pathway overlapped most with the 116 genes we considered. The gene clusters 9 (18 genes) and 6 (12 genes) together shared 15 genes with the 21 genes that were previously reported to be associated with this pathway in the KEGG database (Table II). Specifically, these 15 genes include GLK1, FBP, PGI1, TDH1, TDH3, FBA1, PCK1, ADH1, ADH2, ALD6, ENO2 (gene cluster 9), PDC5, ACS1, ACS2, and CDC19 (gene cluster 6). In addition, the proposed model also identified 15 novel genes that might be potentially related to the glycolysis pathway, including IDP2, ILV5, GPD1, GRR1, BIO5, HXT4, STB5 (gene cluster 9), SNF3, RGT2, ACH1, MIG1, SUC2, SIP3, CIT3, and YAP3 (gene cluster 6). We found that both the gene clusters 9 and 6 are strongly associated with the GO term clusters 3 (Figure 4C,D) related to glycolysis and catabolic processes (Table S2), which explains why these two gene clusters showed a good overlap with the KEGG pathway \u201cGlycolysis/Gluconeogenesis.\u201d On the other hand, the gene cluster 9 is also associated with the GO term cluster 4 related to metabolic processes, which might explain differences between these two gene clusters in the sense of biological processes. Finally, the proposed model identified the gene cluster 5, which does not show significant overlap with any of the known KEGG pathways. This gene cluster consists of 11 The first block of table shows the gene clustering results compared with the KEGG pathway database, based on how many genes in the KEGG pathway \u201cGlycolysis/Gluconeogenesis\u201d (\u201csce00010\u201d) overlap each gene cluster. The second block of table shows the results after the filtering step (removing genes that were assigned to a cluster less than 90% of MCMC iterations (maxk P(Mi = k|⋅) ≤ 0.9) from each cluster). genes including TIS11, GIS1, HPS26, RTS3, INO1, INO4, PIR3, FIT2, YAP1, SIN4, and ERG28. While this gene cluster is also associated with the GO term cluster 3 (glycolysis and catabolic processes), it is specifically associated with the GO term cluster 5 related to diverse biological processes such as cell wall organization, homeostasis, and localization. Further investigation of this novel gene cluster (i.e., pathway) should be of great interest. These results show the power of the proposed model in identification of novel pathways, pathway-modulating genes, and gene subgroups within each pathway.     5. Discussion\r\n  In this paper, we proposed the unified statistical framework for biomedical literature mining, based on the idea of ontology fingerprint and a Bayesian hierarchical model, with the following desirable properties. First, its pathway-modulating gene identification is not limited to a single aspect of biology because it is based on the biomedical literature mining approach. Second, the proposed method can overcome the limitations of traditional approaches based on co-occurrence of genes in an abstract by identifying indirect relationship among genes using gene ontology information. Third, the proposed model gracefully addresses redundancy in GO terms and correlation among them by modeling clusters of GO terms instead of assuming independence among them. Fourth, the estimated association pattern of GO terms for genes facilitates easy interpretation of novel pathway-modulating genes. Fifth, our Bayesian hierarchical model provides the integrated automatic model selection procedures for gene and GO term clusters using the poor man's RJMCMC approach. Finally, our Metropolis-Hasting within Gibbs algorithm allows computationally efficient posterior inference. We believe that this proposed framework could provide a rigorous and interpretable statistical framework to improve biomedical literature mining. In addition, its software (R package 'bayesGO' that is publicly available at https://dongjunchung.github.io/bayesGO/) will further allow wider application of the proposed model.  There are a number of future directions for our work. First, the proposed model currently assumes that a gene can belong to only a single gene cluster. However, a gene may have multiple functions and thus may belong to multiple groups/pathways at the same time. Hence, the proposed model can be improved further by allowing overlapping gene membership. Second, the proposed Bayesian hierarchical model can be extended by incorporating other related information. For example, prior knowledge for reported pathways and hierarchical structure of GO terms can be incorporated by modifying prior distributions for gene (Mi) and GO term clusters (Lt). Similarly, the reported assignment of GO terms to genes [such as ones reported in the Gene Ontology Annotation (GOA) database (http://www.ebi.ac.uk/GOA)] can be incorporated as prior distributions for gene-GO term associations (Eti). Third, the proposed model is designed more for well-annotated and nearly completely sequenced genomes because it utilizes GO terms to identify gene clusters. We believe that this would be practically a reasonable assumption as a large number of genomes (including human, mouse, rat, and yeast, among others) has been well sequenced and annotated. In addition, before we calculated hypergeometric test p-values (Yti), we also extended the dictionary of GO terms significantly by considering its ancestors in the GO tree and using the string match. Specifically, while the original GOA file provides only 16,910 annotation pairs between a GO term and an abstract, our preprocessing (considering ancestors in the GO tree and using the string match) allows to identify 1,074,255 annotation pairs between a GO term and an abstract. As a result, our preprocessing step allows us to overcome this issue to some degree. However, it would still be interesting to modify the proposed model so that it can also investigate under-studied genes, especially those that are still not annotated with GO terms even after this preprocessing step. Fourth, as usually the case for various literature mining and the meta-analysis approaches, the findings generated by the proposed model can also be affected by experimental and publication biases. We believe that the proposed model should be relatively weakly affected by these biases because it is based on the p-values generated from hypergeometric tests applied to abstracts rather than the abstracts themselves. However, the data preprocessing steps to generate p-values can still be further improved by taking into account this issue when we apply hypergeometric tests to the abstracts. Fifth, in this paper, we used Metropolis-Hasting within Gibbs sampler for posterior inference. However, from a practical point of view, improved computational efficiency would be beneficial to scale up our proposed method for larger scale biological applications. For example, we can consider to use approximation approaches such as maximum a posteriori (MAP) estimation, for example, via simulated annealing, along with parallel and GPU computing. Finally, in this paper, we considered yeast data in the real-data analysis because yeast is one of the most well-studied model organisms and it is also easy to experimentally validate predicted novel candidate genes using yeast. As the proposed method showed superior performance in the analysis of yeast data, it would be of great interest to utilize the proposed method to analyze more complicated organisms such as human and mouse.    Acknowledgements\r\n  We thank Sean Courtney and Gary Hardiman for the valuable discussion on biological interpretation of Yeast PubMed data analysis results. This work was supported by the National Institutes of Health (grant numbers R56 LM010680, R01 GM122078, and R21 CA209848). Additional supporting information may be found online in the supporting information tab for this article.    ",
    "sourceCodeLink": "https://github.com/dongjunchung/bayesGO",
    "publicationDate": "0",
    "authors": [
      "Dongjun Chung",
      "Andrew Lawson",
      "W. Jim Zheng"
    ],
    "status": "Success",
    "toolName": "bayesGO",
    "homepage": ""
  },
  "76.pdf": {
    "forks": 214,
    "URLs": [
      "broadinstitute.github.io/picard",
      "github.com/NYU-BFX/lnc",
      "github.com/NYU-BFX/lncRNA-screen/releases/tag/v.02.Operating",
      "cole-trapnell-lab.github.io",
      "github.com/NYU-BFX/lncRNA-screen.Detailed",
      "github.com/NYU-BFX/lncRNA-screen.Archived",
      "github.com/NYU-BFX/RNA-Seq_Standard",
      "egg2.wustl.edu/roadmap/web_portal"
    ],
    "contactInfo": [
      "ioannis.aifantis@nyumc.org",
      "aristotelis.tsirigos@nyumc.org"
    ],
    "subscribers": 129,
    "programmingLanguage": "Java",
    "shortDescription": "A set of command line tools (in Java) for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF.",
    "publicationTitle": "lncRNA-screen: an interactive platform for computationally screening long non-coding RNAs in large genomics datasets",
    "title": "lncRNA-screen: an interactive platform for computationally screening long non-coding RNAs in large genomics datasets",
    "publicationDOI": "10.1186/s12864-017-3817-0",
    "codeSize": 113450,
    "publicationAbstract": "Background: Long non-coding RNAs (lncRNAs) have emerged as a class of factors that are important for regulating development and cancer. Computational prediction of lncRNAs from ultra-deep RNA sequencing has been successful in identifying candidate lncRNAs. However, the complexity of handling and integrating different types of genomics data poses significant challenges to experimental laboratories that lack extensive genomics expertise. Result: To address this issue, we have developed lncRNA-screen, a comprehensive pipeline for computationally screening putative lncRNA transcripts over large multimodal datasets. The main objective of this work is to facilitate the computational discovery of lncRNA candidates to be further examined by functional experiments. lncRNAscreen provides a fully automated easy-to-run pipeline which performs data download, RNA-seq alignment, assembly, quality assessment, transcript filtration, novel lncRNA identification, coding potential estimation, expression level quantification, histone mark enrichment profile integration, differential expression analysis, annotation with other type of segmented data (CNVs, SNPs, Hi-C, etc.) and visualization. Importantly, lncRNA-screen generates an interactive report summarizing all interesting lncRNA features including genome browser snapshots and lncRNA-mRNA interactions based on Hi-C data. Conclusion: lncRNA-screen provides a comprehensive solution for lncRNA discovery and an intuitive interactive report for identifying promising lncRNA candidates. lncRNA-screen is available as open-source software on GitHub.",
    "dateUpdated": "2017-10-16T15:26:56Z",
    "institutions": [
      "New York University School of Medicine",
      "University of Miami",
      "Inc."
    ],
    "license": "MIT License",
    "dateCreated": "2014-03-28T20:43:35Z",
    "numIssues": 102,
    "downloads": 0,
    "fulltext": "     Gong et al. BMC Genomics     10.1186/s12864-017-3817-0   lncRNA-screen: an interactive platform for computationally screening long non-coding RNAs in large genomics datasets     Yixiao Gong  2  4    Hsuan-Ting Huang  1    Yu Liang  3    Thomas Trimarchi  5    Iannis Aifantis  ioannis.aifantis@nyumc.org  2  4    Aristotelis Tsirigos  aristotelis.tsirigos@nyumc.org  0  2  4    0  Applied Bioinformatics Laboratories, New York University School of Medicine ,  New York, NY 10016 ,  USA    1  Department of Human Genetics, Miller School of Medicine, University of Miami ,  Coral Gables, FL 33136 ,  USA    2  Department of Pathology and Laura and Isaac Perlmutter Cancer Center, New York University School of Medicine ,  New York, NY 10016 ,  USA    3  Department of Population Health, New York University School of Medicine ,  New York, NY 10016 ,  USA    4  NYU Cancer Institute and Helen L. and Martin S. Kimmel Center for Stem Cell Biology, New York University School of Medicine ,  New York, NY 10016 ,  USA    5  Regeneron Pharmaceuticals, Inc. ,  Tarrytown, NY 10591 ,  USA     2017   18  2  19    24  5  2017    23  1  2017     Background: Long non-coding RNAs (lncRNAs) have emerged as a class of factors that are important for regulating development and cancer. Computational prediction of lncRNAs from ultra-deep RNA sequencing has been successful in identifying candidate lncRNAs. However, the complexity of handling and integrating different types of genomics data poses significant challenges to experimental laboratories that lack extensive genomics expertise. Result: To address this issue, we have developed lncRNA-screen, a comprehensive pipeline for computationally screening putative lncRNA transcripts over large multimodal datasets. The main objective of this work is to facilitate the computational discovery of lncRNA candidates to be further examined by functional experiments. lncRNAscreen provides a fully automated easy-to-run pipeline which performs data download, RNA-seq alignment, assembly, quality assessment, transcript filtration, novel lncRNA identification, coding potential estimation, expression level quantification, histone mark enrichment profile integration, differential expression analysis, annotation with other type of segmented data (CNVs, SNPs, Hi-C, etc.) and visualization. Importantly, lncRNA-screen generates an interactive report summarizing all interesting lncRNA features including genome browser snapshots and lncRNA-mRNA interactions based on Hi-C data. Conclusion: lncRNA-screen provides a comprehensive solution for lncRNA discovery and an intuitive interactive report for identifying promising lncRNA candidates. lncRNA-screen is available as open-source software on GitHub.    lncRNA  Comprehensive pipeline  Data integration  Fully automated  Interactive report       Background\r\n  The landscape of transcription in organisms is now known to be complex and pervasive, producing a wide range of small and long RNA species with a variety of biological functions discovered so far [ 1-3 ]. One of the least characterized yet largest class of RNA species are the long noncoding RNAs (lncRNAs). The most basic definition of a lncRNA is a long RNA, at least 200 base pairs in length, that does not encode protein. They can be further classified by features such as their genomic location, structure, and expression [ 4 ]. Of the small number of lncRNAs that have been characterized, they have been shown to be functionally important for chromatin and other cellular processes that affect organismal development and cancer [ 5, 6 ]. However, there are 28,031 lncRNA transcripts annotated to date in GENCODEv19 and 91,000 in MiTranscriptome, and this number will continue to grow as deeper and more sensitive RNA sequencing data are generated. Recently, large-scale lncRNA analyses of published data (e.g. TCGA) have been conducted [ 7, 8 ], however the authors have not made their pipelines available. A number of databases and bioinformatics tools have been developed to annotate and catalog lncRNAs that are known or novel [ 9 ]. LncRNA2Function [ 10 ], LNCipedia [ 11 ], lncRNAdb [ 12 ] and lncRNAtor [ 13 ] provide comprehensive databases for known, annotated lncRNAs. iSeeRNA [ 14 ], CPC [ 15 ] and CPAT [ 16 ] introduced machine learning-based approaches only focusing on assessment of the coding probability of potential lncRNAs. lncRScan [ 17 ] and its new version lncRScan-SVM [ 18 ] are pipelines which provide novel multi-exonic lncRNA only discovery from RNA sequencing (RNA-seq), lacking the ability to integrate other data types to further filter for interesting lncRNA candidates. It is known that lncRNAs, like coding genes, are enriched for histones that mark transcriptionally active sites such as histone H3K4me3 and H3K27ac [ 19, 20 ]. This approach has led to the successful identification of LUNAR1 and its function in regulating the IGF1R locus to sustain T-cell leukemia growth [ 21 ]. Thus integrating other genomic features allows for identification of lncRNAs with important biological functions. One of the fundamental issues in the field is to identify the function of the discovered lncRNAs. In this context, the ability to integrate a variety of genomic datasets to increase the probability of identifying lncRNAs that are functionally relevant will be of great value. Thus having an extensive bioinformatics pipeline that can quickly annotate and classify lncRNAs from RNA sequencing (RNA-seq) data will be valuable for identifying strong candidates for biological validation. To this end, we have developed an extensive computational pipeline to integrate different types of experimental data to annotate lncRNAs, which can be filtered by the user to identify specific lncRNAs of interest. The pipeline first aligns and assembles RNA-seq data to build a comprehensive transcriptome assembly for all the samples. Then, using a series of filtering criteria based on gene annotations, sequence length, expression level, coding potential and other features, a list of putative lncRNA candidates is defined containing basic information that includes transcript size, genomic location, and differential gene expression. Afterwards, depending on the raw data that the user may provide, our pipeline can process and annotate the putative lncRNAs with other processed information such as ChIPseq data, copy number variation, Hi-C interaction etc. Gene tracks for UCSC genome browser and lncRNA local genomic snapshots are generated simultaneously in order to quickly visualize and assess each lncRNA by its features. The output of the pipeline is a comprehensive table and an interactive HTML report containing all the putative lncRNAs with their corresponding genomic features that the user has added into the pipeline. This report can then be filtered interactively by the user for specific lncRNAs of interest based on any combination of the genomic features included in the analysis.    Implementation\r\n  The lncRNA-screen workflow lncRNA-screen is an extensive analysis pipeline providing various useful functions for lncRNA annotation and candidate selection. It encompasses multiple automated processes designed for lncRNA discovery and computational selection, including public data download, locally sequenced data integration (RNA-seq and ChIP-seq datasets), comprehensive transcriptome assembly, coding potential estimation, expression level quantification, differential expression comparisons and analysis, and lncRNA classification. Our pipeline enables fully customizable lncRNA discovery with insightful visualization embedded in each step of data processing. Most importantly, lncRNAscreen automatically generates an interactive lncRNA feature report that allows the user to conveniently search, filter, and rank by important features (e.g. expression level, presence of histone marks, etc.) extracted from the different input data types. Additionally, it provides a genome snapshot of each lncRNA locus to help user visually assess the relevance and quality of each candidate lncRNA. The main functionality of the lncRNA-screen pipeline compared to other published lncRNA analysis tools is summarized in Table 1. According to the table, lncRNA-screen provides the most extensive computational lncRNA discovery pipeline to date. The lncRNA-screen workflow can be divided into two main phases (Fig. 1). In Phase I, the pipeline first conducts RNA-seq alignment and assembly individually for each sample and then merges them in order to construct a comprehensive transcriptome assembly. Next, a series of filtration steps are applied in order to detect lncRNAs based on reference annotations and genomic location. A putative lncRNA candidate list is generated in Phase I for further classification in Phase II using different data types. In Phase II, expression levels of the putative lncRNAs are quantified in all the samples and summarized at multiple user-defined group levels. Moreover, by integrating with ChIP-seq data, we identify typical histone mark profiles (H3K4me3, H3K27ac) around the TSS region of all lncRNAs. Pairwise differential expression analysis can also be performed between any two groups. At the same time, lncRNA-screen annotates all putative lncRNAs using the information obtained from the analysis and supplemented by additional types of \u201csegmented\u201d data (CNVs, SNPs, Hi-C, etc.) in order to generate a comprehensive interactive lncRNA feature report. The user can easily adjust parameters (see README.md file on GitHub repository) to find the most confident and functionally relevant lncRNAs candidates for further experimental validation. Furthermore, lncRNAscreen automatically produces different summary plots and a flowchart providing intuitive guidance to the user for adjusting the parameters. Finally, for every lncRNA, lncRNA-screen generates a genome snapshot which shows the gene structure, expression pattern and histone mark profile for the neighboring area. Additionally, if processed Hi-C data is provided, a local Hi-C interaction snapshot revealing potential looping events between each lncRNA and its neighboring genes is also generated.  Obtain the software and environment setup lncRNA-screen is open-source software, free for academic use and available for download from GitHub repository: https://github.com/NYU-BFX/lncRNA-screen.Detailed instructions can be found in Additional file 1: Supplementary Material 2. It depends on a pipeline that performs standard RNA-Seq analysis, also available on GitHub: https://github.com/NYU-BFX/RNA-Seq_Standard (instructions are provided in Additional file 2: Supplementary Material 3). Most of the dependencies used in this pipeline are integrated into our software packages and do not need to be re-installed or re-complied in Linux systems. All the R packages used will be downloaded automatically as the pipeline runs. A detailed description of all the dependencies and environment setup can be found in Additional file 3: Supplementary Material 4. Users can easily follow the instructions and we are continuously providing support of any questions regarding the lncRNA-screen pipeline installation. Input data, sample sheet and group information setup lncRNA-screen provides fully automated and parallel download of raw RNA-seq FASTQ files from different public data repositories including the Sequence Read Archive (SRA) from the National Center for Biotechnology Information (NCBI) and The Cancer Genome Atlas (TCGA). The user only needs to provide a list of SRA accession numbers or TCGA UUIDs matched with userdefined sample names to be used throughout the analysis in a sample sheet file. The pipeline automatically downloads the relevant files, while the various tools within our pipeline automatically identify them as the appropriate inputs for downstream analyses. Processed ChIP-seq histone mark data (H3K4me3, H3K27ac and H3K4me1) is required in BED4 format where the fourth column corresponds to the ChIP enrichment score, calculated by MACS2 or any other peak calling tool. Any type of data that can be represented as segmented data in BED4 format (e.g. CNV or SNP data) is also supported as input to lncRNA-screen. For example, copynumber variation segments can be integrated by lncRNA-screen to identify lncRNAs located in recurrently amplified or deleted regions in cancer samples. A group information sheet is also required, where rows correspond to sample names and different grouping strategies can be designated by adding an arbitrary number of columns. These groups might be different cell types, experimental conditions etc. Matched histone mark data for each group are also assigned using this file. lncRNA-screen will automatically perform the analysis for all user-defined grouping strategies.  Read quality assessment for RNA-seq data FastQC (http://www.bioinformatics.babraham.ac.uk/projects/fastqc) is a commonly used package for assessing the Next-generation sequencing read quality. lncRNAscreen utilizes FastQC as an automatic quality control (QC) procedure for each sample. It reports the distribution of average per-base and per-sequence quality, perbase and per-sequence GC content, sequence length distribution, sequence duplication level, etc. This information allows the user to quickly diagnose irregularities in their input samples and take appropriate action. Based on the results, the user can decide whether to perform pre-processing of the FASTQ files such as trimming during the next step.   Read alignment\r\n  Next, all sequences are aligned using the Spliced Transcripts Alignment to a Reference (STAR) software [ 22 ]. A pre-built STAR genome index is required in advance. For each sample, the pipeline identifies all the raw reads belonging to each sample, automatically determines whether they are single-end or paired-end and properly groups read pairs as well as different sequence files originating from different sequencing lanes. Then, STAR will automatically determine the strand specificity and read length. This is a significant advantage compared to TopHat2 [ 23 ] and other aligners, given that it is oftentimes a challenging task to retrieve this information from large public datasets such as TCGA or SRA. Determining this automatically using STAR is also useful in subsequent steps, for example during transcriptome assembly performed by Cufflinks. Trimming, soft clipping and other type of necessary pre-preprocessing of the reads can be done by STAR during the alignment process. Only uniquely mapped reads are retained in the final alignment. Raw read counts are generated at the same time by the STAR aligner for all the annotated features in the GENCODEv19 GTF as default annotation file (or other user supplied annotation files) provided when aligning the samples.  Post-alignment assessment and processing After alignment, Picard-Tools (http://broadinstitute.github.io/picard) are used to assess the duplication rate and remove duplicate reads if necessary. We then generate read coverage signal track files in BIGWIG format with adjustable resolution compatible with IGV and UCSC genome browsers. The pipeline also provides a function which can merge and generate combined track files at the group level. An interactive HTML RNA-seq analysis report is generated automatically which incorporates an alignment quality report (example shown in Fig. 2; see Results for details) allowing the user to quickly inspect the alignment rates and the number of usable reads. A sample distance plot (example shown in Fig. 3; see Results for details) represents an unbiased clustering of the samples based on genes annotated by GENCODEv19. The pipeline also performs differential expression analysis for each pair of groups and reports all the GENCODEv19 annotated genes that pass a userdefined significance threshold if the number of groups provided by the user is sufficiently small (up to 10). The Aa. Alignment Category Percentage Bb. Alignment Category Reads 0 25 50 75 100 0 100 200 300 400   Percentage of Reads\r\n    Number of Reads in\r\n    Millions\r\n  to merge all the assemblies into a single comprehensive automatically for specific comparison groups if number assembly from all assembled transcripts with GENCOof groups exceeded 10. This interactive RNA-seq report DEv19 annotation as a guided assembly. is designed for providing an overview of the sample differences and similarities between and within groups and  Comprehensive identification of putative lncRNAs for verifying that the user-defined list of genes follow the To distinguish known transcripts from novel transcripts, expected expression pattern across different groups.     Transcriptome assembly construction\r\n  Aligned sequences are assembled individually by flinks 2.2.1 [ 24 ] using GENCODEv19 annotation  Cufas a we use the  Cuffcompare result generated during Cuffmerge process, which compares the comprehensive assembly from all assembled samples and GENCODEv19 reference annotation. Based on the Cuffcompare classification result, all of the transcripts are categorized into guide transcriptome with default parameters. All riboso12 different classes (see http://cole-trapnell-lab.github.io mal RNAs and snRNAs are masked. The strand specifi/cufflinks/cuffcompare/#transfrag-class-codes). The user city is automatically determined by the pipeline based can define which categories to keep. By default, we keep on the result of STAR aligner. Finally, we use Cuffmerge the three following categories: a transfrag falling entirely within a reference intron (class code \u201ci\u201d); unknown, intergenic transcript (class code \u201cu\u201d); an intron of the transfrag overlaps a reference intron on the opposite strand; exonic overlap with reference on the opposite strand (class code \u201cx\u201d). Annotated lncRNAs by GENCODEv19 will be added back to the filtered transcripts, forming a comprehensive annotated and novel lncRNA assembly. Then this lncRNA assembly is merged at gene level based on the unique gene ID given by Cuffmerge. Alternatively, spliced transcripts for each gene can be retrieved in the future if necessary by searching the entire transcript annotation by the Gene ID. Then, for each protein-coding gene in GENCODEv19, we extract its transcription start site (TSS) and extend it by 1.5 KB upstream and downstream (default value, modifiable by the user) to include potential alternative transcription start sites of protein-coding genes. Any putative lncRNA overlapping with these regions on the same strand will be excluded from the putative lncRNA list considering that alternative TSSs of the protein-coding genes may appear as novel transcripts. Annotated microRNAs, snRNAs, srpRNAs, tRNAs, scRNAs and antigen receptors datasets were obtained from UCSC and ENSEMBL databases. The last step of filtration is to exclude transcripts less than 200 nt (default value, modifiable by the use) in length, based on the lncRNA definition. This subset of putative lncRNA is considered to be the comprehensive putative lncRNA assembly and is used in all downstream analyses. Moreover, we annotate all the remaining putative lncRNAs into different categories based on their overlaps with RefSeq [ 25, 26 ], ENSEMBL and MiTranscriptome.    Estimation of lncRNAs abundances\r\n  We use featureCounts [ 27 ] to count the raw reads for all the genes included in the putative lncRNA assembly and GENCODEv19 annotations. Strand-specificity and counting single-end versus paired-end reads is determined by the pipeline automatically in the previous step. The read abundance calculation is performed at the gene level and all the reads included are uniquely mapped. FPKM values are calculated accordingly. A summary bar chart is automatically generated after featureCounts is performed (example in Additional file 4: Figure S1), showing the number and percentage of reads/fragments that have been utilized by the assembly. Problematic samples can be identified in this step and should be excluded from the study if the percentage of reads/fragments assigned to the assembly is too low.    Coding potential estimation\r\n  We use Coding Potential Assessment Tool (CPAT) to estimate the coding potential of the filtered putative lncRNA. Using the pre-trained human (hg19) logistic regression model, CPAT reports putative ORF size and coding probability for each transcript. The optimum cutoff for human gene annotation is 0.364 as determined in the CPAT manuscript. The distribution of ORF sizes and coding potential scores for protein-coding and noncoding transcripts is produced automatically (example shown in Fig. 4; see results for details).  Integration of histone mark ChIP-seq data ChIP-seq peaks for H3K4me3, H3K27ac and H3K4me1 can be used directly as input to lncRNA-screen. Alternatively, lncRNA-screen can perform its own ChIP-seq analysis starting from raw FASTQ files. Using Botwie2 [ 28 ] for alignment and MACS2 [ 29 ] for peak calling, histone mark peaks can be identified. Broad peak calling is used (q-value &lt;0.05) and fold enrichment compared to the input is calculated. A user-defined fold change cutoff can be applied. Then we extend each putative lncRNA transcription start site by 1.5 KB upstream and downstream and assign the histone marks that have a peak overlapping this extended TSS region. The fold enrichment value of each overlapping peak is reported for each lncRNA.    Defining group-enriched lncRNAs\r\n  In order to determine all expressed lncRNAs in a specific group, the pipeline allows the user to set a FPKM cutoff based on the distribution of the lncRNA expression level. Then, we compute the average FPKM values for all the putative lncRNAs among different sample groups. Samples which have FPKM value below the cutoff are excluded before computing average FPKM values within groups. The number of samples that have an FPKM value above the cutoff will also be reported. Next, combining the ChIP-seq histone mark overlaps with the FPKM cutoff, we are able to define group-enriched expressed lncRNAs based on their expression value and the histone mark enrichment in the extended TSS regions. The user can specify which histone mark (or combination of histone marks) must be present. By default, we define a lncRNA as being expressed in a specific group by requiring the mean FPKM in this group to be greater than a user-defined cutoff (default 0.5) while overlapping with H3K4me3 histone marks in its extended TSS region. Additionally, we define an enhancerRNA to be expressed in a specific group, if its mean FPKM in this group exceeds the cutoff value while overlapping with both H3K4me1 and H3K27ac but not with H3K4me3 histone marks in its extended TSS region. A \u201cpie matrix\u201d illustrating the number of lncRNAs in categories characterized by different histone marks, as well as the intersections between different sample groups is automatically generated (example shown in Fig. 5). The group information table, as described in a previous section, is essential for the pipeline to group the related samples or replicates under the same group name and to A 1.00 n o i tub0.75 iitr s eD0.50 v lit a u um0.25 C 0.00  B 1.00 n o i tub0.75 iitr s eD0.50 v ilt a u um0.25 C match the RNA-seq data and ChIP-seq data. Different levels of grouping can be achieved by adding an additional group column so that the user can explore the similarity and difference between samples in different ways.  Pairwise differential expression analysis between designated groups Our pipeline also integrates pairwise differential expression analysis using DESeq2 [ 30 ]. If a list of pairwise comparison groups of interest is provided, the pipeline performs all the differential expression analyses provided in the list. Otherwise, by default the program performs differential expression for up to 10 comparison groups.  P-values, FDR, and log2 fold changes are provided for each lncRNA. lncRNA selection and the comprehensive putative lncRNA feature report The default criteria of lncRNA selection are illustrated in the example flowchart of Fig. 6. The flowchart is generated automatically and shows in detail the filtering criteria applied in each step as well as the number of putative lncRNA selected (and excluded). This allows the user to inspect the breakdown of the impact of each filtration step. The user can then modify the parameter file and re-run all the steps after the \u201cexpression estimation\u201d step, thus obtaining an updated lncRNA list and the corresponding flowchart using their custom criteria. Finally, we collect all the results generated by our pipeline into a comprehensive putative lncRNA feature report which includes the columns shown in Table 3. The comprehensive putative lncRNA feature report is provided to the user in both HTML (example snapshot shown in Fig. 7) and Excel formats. Both versions allow Fig. 7 A screenshot of the interactive HTML report generated by lncRNA-screen, which enables sorting, filtering, searching based on different lncRNA features and a preview of the lncRNA genome snapshot and local heatmap the user to conveniently filter and search based on userdefined criteria. lncRNA heatmaps Two types of heatmaps are generated for selected lncRNAs which pass all the filtration criteria. First, we generated a supervised clustering heatmap (example shown in Fig. 8), designed to show the group-enriched lncRNAs. Because a lncRNA may be expressed in different groups at the same time, in this type of heatmap, the lncRNAs may appear multiple times. The order of the sample groups is the same as the order of the lncRNAs discovered in each group, which ensures that the group-enriched lncRNAs are always located near the diagonal of the heatmap. Second, we provide an unsupervised hierarchical clustering heatmap (example shown in Fig. 9) for samples (columns) as well as lncRNAs (rows). This heatmap lets the user inspect the similarity and specificity between sample groups in the filtered lncRNA level, and also allows the user to search for lncRNAs co-expressed or co-differentially expressed between groups. lncRNA genome snapshots The pipeline generates a genome snapshot (example shown in Fig. 10) for each lncRNA, centered around the lncRNA locus and zooming out 10 times from the lncRNA length. In this snapshot, the user can choose to display ENSEMBL, RefSeq, GENCODEv19, or other user-defined GTF or BED format annotations. The comprehensive merged assembly is also shown in this snapshot, containing all the transcripts assembled without any filtration criteria steps. Moreover, the user can choose to plot bigwig RNA-seq signal tracks either at the merged group level or at the sample level. The HTML report (Fig. 7) also includes a preview of the lncRNA genome snapshot for all lncRNAs.     Results\r\n  Quick installation, setup, execution and interactive browsing of the results lncRNA-screen can be downloaded as a single zip file from GitHub through the link below. The reference files setup script step will automatically download the necessary references and dependencies. Following instructions in the \u201chow to run\u201d link below, the user can easily set up the preferred parameters and run the entire pipeline using a single command. After the run is completed, the user can interactively browse the results using the automatically generated HTML report (see example below) or import the automatically generated table into Excel to do more complex filtering.  Setup and excute: https://github.com/NYU-BFX/lnc RNA-screen/blob/master/README.md  Browse the results: http://www.hpc.med.nyu.edu/~gon gy05/lncRNA-screen/H1_Cells/lncRNA_report.html lncRNAs in Roadmap Epigenomics To demonstrate the usage and performance of lncRNAscreen in big datasets, we used data from the Roadmap Epigenomics project [ 31 ], which contains RNA-seq and Single Column lncRNA ID Locus Number of Exons Gene Body Size ORF Size Coding Probability Gencode Annotation RefSeq Annotation Ensembl Annotation MiTranscriptome Annotation SNPs annotation Copy number gain/loss value  One column per group Mean FPKM above user-defined threshold Percentage of samples above FPKM threshold H3K27ac peak enrichment score H3K4me3 peak enrichment score H3K4me1 peak enrichment score Histone marks enrichment and FPKM cutoff combination Differential expression analysis between groups Hi-C interaction between lncRNA and neighboring coding genes Group Adipose Adrenal Gland Bladder CD14 CD19 CD34 CD3 CD4 CD56 Esophagus Table 2 List of features included in the final lncRNA feature report Table 3 Number of putative lncRNAs identified in each group Number of Group lncRNAs 8709 3950 695 3982 6188 3661 6113 5232 9596 3443 1568 2081 4429 4930 10,477 2472 4578 4621 8113 2448 10,330 Fetal Adrenal Gland 12,008 Fetal Fibroblasts Fetal Heart Fetal Kidney Fetal Lung Fetal Muscle Fetal Oary Fetal Large Intestine 5824 Fetal Renal Cortex Fetal Renal Pelvis Fetal Small Intestine 4508 Fetal Spinal Cord Fetal Spleen Fetal Stomach  Fetal Thymus Gastric H1 BMP4 derived mesendoderm H1 BMP4 derived trophoblast H1 derived mesenchymal 443 stem cells H1 derived neuronal progenitor H1 Heart Aorta Heart Central hESC-derived CD184 P endoderm hESC-derived CD56 P ectoderm hESC-derived CD56 P mesoderm HUES64 IMR90 Liver Lung Ovary Pancreas Placenta Psoas Muscle Sigmoid Colon Small Intestine spleen Testes Pool  Number of lncRNAs 5763 3978 1711 844 1067 8271 17,092 6126 1367 2086 900 1257 2152 785 2359 1056 5204 9030 2223 18,827 4760 2953 1266 ChIP-seq data representing a collection of human stem cells and tissue type. The raw FastQ reads of total 198 RNA-seq samples from Roadmap Epigenomics project were downloaded from SRA using SRA-toolkit and aligned to the GENCODEv19 reference genome by STAR (version 2.4.2a) with default parameters (see methods). After quality control (Fig. 2 and Additional file 4: Figure S1), 187 samples (see Additional file 4: Table S1) were successfully processed and classified into 40 groups based on cell type. Accepted reads for each sample were assembled individually using Cufflinks (version 2.2.1) providing the guide reference (RefSeq Flat Table GTF file). Cuffmerge was employed to merge all the assemblies into a comprehensive transcriptome assembly, yielding 491,218 transcripts, forming 229,442 genes in total. By comparing the merged comprehensive transcriptome assembly with GENCODEv19 annotated genes using Cuffcompare, transcripts are classified into different categories based on their structure compatibility of the GENCODEv19 reference annotation. All filtering steps in Phase I of lncRNA-screen, including the number of lncRNAs selected and discarded by various filtering criteria are shown in Fig. 6. The total number of putative lncRNAs identified are 178,473. Both our coding genes, annotated and novel lncRNA candidates were tested by CPAT and the coding potential distribution comparison are in Fig. 4. Novel lncRNA and annotated lncRNA discovered in this pipeline showed similar distribution in ORF and coding potential, and significantly different than coding genes. We use the recommended coding potential cutoff 0.364 for human genome, excluding 11,725 genes from our putative lncRNA list, which is 6% of our total candidates. We also included the ChIP-seq histone marks broad peak calling result from MACS2 (H3K4me3, H3K27ac and H3K4me1) from Roadmap Epigenomics project for all of the 40 groups and matched them with corresponding RNA-seq sample groups. The lncRNA feature report in both HTML (see the link in GitHub) and Excel format (see the link in GitHub) included all 178,473 putative lncRNAs and their features shown in Table 2. The lncRNA feature report (Fig. 9) not only enables sorting, filtering, searching for all lncRNA features extracted from Table 4 Number of transcripts by category for each multi-lineage differentiated embryonic stem cells Cell Type hESC Mesench Mesendo Neupro Trophect enhancers 6323 16,736 2371 1487 different data resources, but also includes a preview of the lncRNA genome snapshot. Examples of high confident lncRNA local snapshot is shown in Fig. 10. The report also includes a pre-set UCSC Genome browser session link to provide advanced genome browse function. With default filtration parameters, pie-matrix (Fig. 5) is showing all the lncRNAs expressed (mean FPKM &gt;0.5) in at least one group by categories defined by 3 histone mark overlaps in the extended TSS regions (TSS flanked by +/− 1.5 KB). It also shows pairwise overlap of lncRNAs between different groups in different categories. A total of 8207 unique lncRNAs were identified as expressed in at least one group and have all H3K4me3, H3K4me1 and H3K27ac histone marks enrichment in its matched cell type group. And a total of 26,144 unique enhancer-RNAs were identified as expressed in at least one group and having both H3K4me1 and H3K27ac but do not have H3K4me3 histone marks enrichment in the matched group. A breakdown list of number of lncRNAs identified in each group is in Table 3. For these lncRNAs identified in each group, we generated a supervised heatmap (Fig. 7) ensuring the order of the groups and the order of lncRNAs of each groups to be identical for rows and columns. Therefore, the diagonal position of the heatmap shows relatively higher FPKM values across all the samples which proved that lncRNAs identified in a specific group have the relatively higher expression values comparing to other groups. The unsupervised automatic hierarchical clustered heatmap (Fig. 8) for all the 8207 unique group-enriched lncRNAs revealed some clusters of lncRNAs which are differentially expressed in TALL cell lines comparing with other cell types.   Integration with Hi-C data\r\n  To demonstrate the flexibility of our pipeline we integrated RNA-seq/ChIP-seq data from Roadmap Epigenomics with matched Hi-C data from a previous study [ 32, 33 ]. First, we rerun the lncRNA-screen pipeline focusing only on the samples that have matched HiC data (the report is included in GitHub). For each cell type, we defined expressed mRNAs, lncRNAs including annotated and novel, enhancer-RNAs and enhancer regions using expression profile and H3K4me3, H3K27ac and H3K4me1 histone mark occupancy. The numbers of elements in each category for each cell type are included in Table 4. Hi-C analysis was performed using our HiCbench pipeline [ 34 ]. HiC-bench automatically produces various plots to help the user assess the quality of the data as well as compare different samples. Paired-end reads were mapped to the reference genome (hg19 or mm10) using Bowtie2 [ 28 ]. Local alignments of input read pairs were performed as they consist of chimeric reads between two (non-consecutive) interacting fragments. This approach yielded a high percentage of mappable reads (&gt;25%) for all datasets. Mapped read pairs were subsequently filtered for known artifacts of the HiC protocol such as self-ligation, mapping too far from the enzyme's known cutting sites etc. (Fig. 11). Samples clustered as expected by Principal Component Analysis A  IMR90−HindIII−rep2  IMR90−HindIII−rep1 H1_hESC−HindIII−rep2  H1_hESC−HindIII−rep1 H1_Trophect−HindIII−rep2 leH1_Trophect−HindIII−rep1 p m aS H1_Neupro−HindIII−rep2  H1_Neupro−HindIII−rep1 H1_Mesendo−HindIII−rep2 H1_Mesendo−HindIII−rep1 H1_Mesench−HindIII−rep2 H1_Mesench−HindIII−rep1 ds−accepted−intra ds−accepted−inter ds−duplicate−intra ds−duplicate−inter ds−same−fragment unmapped multihit ds−too−close single−sided ds−too−far B 0 (Additional file 4: Figure S2A), and the average Hi-C count showed the characteristic dependency on the distance between the interacting fragments as demonstrated in previous studies (Additional file 4: Figure S2B). Additional file 4: Figure S3A, B shows the sizes and the number of detected topologically-associated domains (TADs) detected in each cell type and each replicate, and Additional file 4: Figure S3C shows the pairwise overlaps of boundaries between all pairs of samples and replicates. HiC-bench also generates a table of all the interacting loci annotated with genes, ChIP-seq peaks and any other region file that the user provides. Using this feature, we compiled a comprehensive report of all interactions that involve the lncRNAs discovered by our lncRNA-screen pipeline in the matched RNAseq/ChIP-seq datasets. Overall, we found that 268 lncRNAs in hESC, 239 lncRNAs in mesendoderm cells, 9 lncRNAs in neural progenitor cells and 254 lncRNAs in mesenchymal cells interacting with at least one mRNA in cis within the context of topological domains. Most importantly, mRNA expression appears to be sensitive to changes in looping with their lncRNA interacting partners. We used HiCPlotter [ 35 ] to generate the Hi-C maps mRNA-lncRNA interaction plots. As an example we show the putative lncRNA CTD-2006C1.2 on chromosome 19 in Fig. 12. This lncRNA interacts with multiple protein-coding genes, mainly zinc-finger proteins within a single topological domain. When we compared both the expression and the Hi-C interaction intensity of the lncRNA to its neighboring genes, we observed an interesting contrasting pattern. Upstream, the lncRNA interacts with two protein-coding genes, ZNF441 and ZNF491: both genes are up-regulated in Mesenchymal cells compared to hESCs and show a concomitant increase in Hi-C looping. In contrast, downstream, the lncRNA interacts with ZNF878 and ZNF625, both down-regulated in Mesenchymal cells compared to hESCs with concomitant decrease in Hi-C looping.     Conclusions\r\n  We developed lncRNA-screen, an easy-to-use integrative lncRNA discovery platform for comprehensive mapping and characterization of lncRNAs using a variety of genomics datasets. The main objective of this work was to facilitate the computational discovery of lncRNA candidates to be further examined by experimental screening as well as functional experiments. More specifically, our goal was to enable experimental laboratories with limited genomics expertise to quickly and comprehensively characterize lncRNAs in their particular field of study (e.g. cancer, stem cells, development). Our pipeline can be installed using one self-contained installation package available on GitHub, and, importantly, is designed to enable execution of an entire analysis using a single command. Initializing a new analysis is also simplified into setting up a trivial sample sheet to describe the datasets involved in the study. Additionally, lncRNA-screen generates an interactive lncRNA report which allows the user to explore the results of the analysis, define their own custom criteria for selecting lncRNAs, and interactively visualize the filtered results using the UCSC Genome Browser and pre-built genome snapshots. The pipeline is compatible with both stand-alone server environments and high-performance computing clusters. In summary, our pipeline provides a comprehensive solution for lncRNA discovery and an intuitive interactive report for identifying promising lncRNA candidates. lncRNA-screen is available as free open-source software on GitHub and our bioinformatics team offers installation and usage support.    Additional files\r\n  Additional file 1: Supplementary material 2. (DOCX 128 kb) Additional file 2: Supplementary material 3. (DOCX 158 kb) Additional file 3: Supplementary material 4. (DOCX 100 kb) Additional file 4: Supplementary material 1. Table S1. Datasets included in this study. Figure S1. featureCounts assigned reads plot: (A) percentages, (B) counts. Figure S2. (A) Principal Component Analysis of Hi-C contact matrices. (B) Average Hi-C count as a function of distance between interacting loci. Figure S3. (A) Distribution of TAD sizes across all the samples. (B) Number of domains across all samples. (C) Pairwise overlaps of TAD boundaries across samples. (DOCX 1349 kb) Abbreviations CNVs: Copy number variations; FDR: False discovery rate; FPKM: Fragments per kilobase of transcript per million mapped reads; lncRNAs: Long Non-Coding RNAs; ORF: Open reading frame; SNPs: Single nucleotide polymorphisms; TSS: Transcription start site Acknowledgements We thank Igor Dolgalev and Stephen Kelly from the NYU Applied Bioinformatics Laboratories and all the Aifantis Lab members for inspiring discussions. We also thank Charalampos Lazaris for advice on how to use HiCPlotter. This work has used computing resources at the NYU HighPerformance Computing Facility (HPCF), so we thank Eric Peskin, Ali SiavoshHaghighi and Loren Koenig for their technical support.  Funding The study was supported by a Research Scholar Grant, RSG-15-189-01 - RMC from the American Cancer Society to Aristotelis Tsirigos (AT) and the reseach program R01CA194923 and R01CA169784 from National Institute of Health (National Cancer Institute) to Iannis Aifantis (IA). Aristotelis Tsirigos was supported by a Research Scholar Grant, RSG-15-189-01 - RMC from the American Cancer Society. Availability of data and materials Published RNA-seq and ChIP-seq data are from Roadmap Epigenomic Project website (http://egg2.wustl.edu/roadmap/web_portal) [ 31 ]. Published Hi-C data were downloaded from Gene Expression Omnibus, using the accession number GSE52457 [ 32, 33 ].  Project name: lncRNA-screen.  Project home page: https://github.com/NYU-BFX/lncRNA-screen.Archived version: https://github.com/NYU-BFX/lncRNA-screen/releases/tag/v.02.Operating system: Redhat Linux GNU (64 bit).  Programming language: R, C++, Python, Unix shell scripts, Perl. Other requirements: Python 2.7+, R 3.2.0+, Perl 5.0 + .  License: MIT.  Any restrictions to use by non-academics: None.  Authors' contributions YG designed and implemented the pipeline, performed computational analyses, generated figures and wrote the user manual. HH and TT offered expertise on lncRNA biology. TT designed an early prototype of the pipeline. AT analyzed the Hi-C data. YG and AT wrote the manuscript. AT and IA designed and supervised this research project. All authors read and approved the final manuscript.  Competing interests The authors declare that they have no competing interests.  Consent for publication Not applicable.  Ethics approval and consent to participate Not applicable.    Publisher\u2019s note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and Institutional affiliations.  Submit your next manuscript to BioMed Central and we will help you at every step:  Our selector tool helps you to find the most relevant journal    ",
    "sourceCodeLink": "https://github.com/broadinstitute/picard",
    "publicationDate": "0",
    "authors": [
      "Yixiao Gong",
      "Hsuan-Ting Huang",
      "Yu Liang",
      "Thomas Trimarchi",
      "Iannis Aifantis",
      "Aristotelis Tsirigos"
    ],
    "status": "Success",
    "toolName": "picard",
    "homepage": "http://broadinstitute.github.io/picard/"
  },
  "17.pdf": {
    "forks": 8,
    "URLs": [
      "github.com/Magdoll/cDNA_Cupcake/wiki/Cupcake-ToFU%3Asupporting-scripts-for-Iso-Seq-after-clustering-step",
      "github.com/NKI-TGO/SPLICIFY",
      "github.com/NKITGO/SPLICIFY"
    ],
    "contactInfo": ["R.Fijneman@nki.nl"],
    "subscribers": 10,
    "programmingLanguage": "Python",
    "shortDescription": "Miscellaneous collection of Python and R scripts for processing sequencing data",
    "publicationTitle": "Identification of differentially expressed splice variants by the proteogenomic pipeline Splicify",
    "title": "Identification of differentially expressed splice variants by the proteogenomic pipeline Splicify",
    "publicationDOI": "None",
    "codeSize": 117984,
    "publicationAbstract": "",
    "dateUpdated": "2017-08-19T09:45:46Z",
    "institutions": [
      "VU University Medical Center",
      "Pacific Biosciences",
      "Institute for Genomics and Multiscale Biology",
      "Netherlands Cancer Institute"
    ],
    "license": "No License",
    "dateCreated": "2016-05-09T20:08:59Z",
    "numIssues": 6,
    "downloads": 0,
    "fulltext": "     MCP Papers in Press. Published on July      Identification of differentially expressed splice variants by the proteogenomic pipeline Splicify     Malgorzata A Komor  0  3    Thang V Pham  0    Annemieke C Hiemstra  3    Sander R Piersma  0    Anne S Bolijn  3    Tim Schelfhorst  0    Pien M Delis-van Diemen  3    Marianne Tijssen  3    Robert P Sebra  2    Meredith Ashby  1    Gerrit A Meijer  3    Connie R Jimenez  0    Remond JA Fijneman  R.Fijneman@nki.nl  3    Running Title    0  Oncoproteomics Laboratory, Department of Medical Oncology, VU University Medical Center ,  Amsterdam ,  the Netherlands    1  Pacific Biosciences ,  Menlo Park, CA , USA    2  School of Medicine at Mount Sinai, Institute for Genomics and Multiscale Biology ,  New York, NY , USA    3  Translational Gastrointestinal Oncology, Department of Pathology, Netherlands Cancer Institute ,  Amsterdam ,  the Netherlands     2017   27  2017   Proteogenomic identification of splice variants by Splicify       Abbreviations\r\n  A3SS, alternative 3' splice site A5SS, alternative 5' splice site CCS, circular consensus sequencing CRC, colorectal cancer MXE, mutually exclusive exons RI, retained intron RNA-seq, RNA sequencing RT-qPCR, quantitative reverse transcription PCR SE, skipped exon siNT, siNon-Targeting siSF3B1, siRNA mediated down-modulation of SF3B1 siSRSF1, siRNA mediated down-modulation of SRSF1 SMART, Switching Mechanism at 5' End of RNA Template SMRT, single molecule real time    Summary\r\n  Proteogenomics, i.e. comprehensive integration of genomics and proteomics data, is a powerful approach identifying novel protein biomarkers. This is especially the case for proteins that differ structurally between disease and control conditions. As tumor development is associated with aberrant splicing, we focus on this rich source of cancer specific biomarkers. To this end, we developed a proteogenomic pipeline, Splicify, which is able to detect differentially expressed protein isoforms. Splicify is based on integrating RNA massive parallel sequencing data and tandem mass spectrometry proteomics data to identify protein isoforms resulting from differential splicing between two conditions. Proof of concept was obtained by applying Splicify to RNA sequencing and mass spectrometry data obtained from colorectal cancer cell line SW480, before and after siRNA-mediated down-modulation of the splicing factors SF3B1 and SRSF1. These analyses revealed 2172 and 149 differentially expressed isoforms, respectively, with peptide confirmation upon knock-down of SF3B1 and SRSF1 compared to their controls. Splice variants identified included RAC1, OSBPL3, MKI67 and SYK. One additional sample was analyzed by PacBio Iso-Seq full-length transcript sequencing after SF3B1 down-modulation. This analysis verified the alternative splicing identified by Splicify and in addition identified novel splicing events that were not represented in the human reference genome annotation. Therefore, Splicify offers a validated proteogenomic data analysis pipeline for identification of disease specific protein biomarkers resulting from mRNA alternative splicing. Splicify is publicly available on GitHub (https://github.com/NKI-TGO/SPLICIFY) and suitable to address basic research questions using preclinical model systems as well as translational research questions using patient-derived samples, e.g. allowing to identify clinically relevant biomarkers.    Introduction\r\n  Approximately 95% of multi-exon transcripts undergo alternative splicing, making the human transcriptome far more complex than the protein-coding genome ( 1 ). As a consequence of alternative splicing, a single gene can be transcribed into a variety of isoforms which, when translated into proteins, will differ in structure, location, and function. Abnormally spliced RNA can cause or contribute to disease. In particular aberrant splicing is associated with tumor progression and metastasis, and has been shown to affect each of the biological processes commonly referred to as the hallmarks of cancer ( 2 ). Therefore, studying aberrant splicing may reveal additional insights into tumor biology and phenotype. For instance, usage of an alternative 5' splice site of BCL2L1 causes a switch from a pro- to an anti-apoptotic isoform in cancer and contributes to resisting cell death ( 3 ). Usage of an alternative 3' splice site of VEGFA leads to a shift from an anti- to a pro-angiogenic isoform in cancer and induces angiogenesis ( 4 ). As aberrant splicing accompanies tumor progression, splice variants provide a promising source of clinically relevant biomarkers.  Splicing factors play a direct role in splicing regulation and isoform expression. Splicing factors can develop oncogenic activity, e.g. due to aberrant expression or somatic mutations, and through aberrant splicing lead to carcinogenesis ( 2 ). SF3B1 is a splicing factor required for the early spliceosome assembly and is also one of the most commonly mutated splicing factors in cancer ( 5 ). Recurrent mutations affecting this gene were found in leukemia, melanoma and in pancreatic, breast and bladder cancer. Even though the specific effects of these alterations on splicing are still to be explored, their features often suggest proto-oncogenic activity ( 6 ). In chronic lymphocytic leukemia, mutations in this splicing factor contribute to tumor progression, poor patient survival and poor chemotherapy response ( 7, 8 ). Overexpression of another splicing factor, SRSF1, was observed in different tumor types including breast (9), colon, thyroid, small intestine, kidney, lung, liver and pancreas (10) and was proven to lead to oncogenic activity ( 2, 11-13 ). Transcription of SRSF1 is directly regulated by MYC, a well-known oncogenic transcription factor. Through activation of SRSF1, MYC can affect alternative splicing of a subset of SRSF1 target genes and contribute to tumor development (14). For instance, in breast cancer upregulation of SRSF1 promotes transformation of mammary cells through abnormal splicing of BCL2L11 and BIN1 (15). In colorectal cancer (CRC), SRSF1 causes inclusion of exon 4 in RAC1, generating a Rac1b isoform that contributes to cell survival (16, 17).  RNA-seq allows studying the complexity of transcriptomes. While there is a lot of evidence for alternative splicing on the RNA level, for many of the isoforms it is still not known whether they are translated into proteins. This knowledge is crucial to understanding the biological consequences of alternative splicing, and toward identifying protein biomarkers that result from the translation of splice variants. Protein isoforms have significant potential as biomarkers to increase the accuracy of diagnosis, prognosis or therapy prediction of the disease (18). Identification of disease-specific protein isoforms enables the discovery of biomarkers with better sensitivity and/or specificity.  Protein isoforms can be studied on the proteome level with the use of in-depth tandem mass spectrometry. Proteogenomics is a dynamic field integrating genomic and proteomic data (19). One of the main focus areas in the field is to increase the knowledge of the human proteome and identify novel variant proteins resulting from single nucleotide variants or aberrant splicing (20, 21). The number of bioinformatics tools for performing proteogenomic analysis is rapidly increasing, including tools for proteogenomic database construction (22-27) or visualization of the peptides on the genome (28, 29). However, a number of these tools lack an automated, user-friendly down-stream analysis after MS/MS identification to extract interesting outcomes. Moreover, the tools are often designed for single sample or single cohort analysis without the flexibility to perform a differential comparison between case and control groups on both RNA and protein level. To identify disease specific biomarkers resulting from aberrant splicing there is a need for a tool that will perform a differential group analysis. Here we present a method to identify tumor-specific protein isoforms based on RNA-seq and mass spectrometry (LC-MS/MS) data. In this approach, RNA-seq analysis is used to perform quantitative isoform analysis and identify differential splice variants, and LC-MS/MS confirms translation of these variants into proteins. The method was applied to the CRC cell line SW480 upon down-modulation of the splicing machinery factors SF3B1 and SRSF1. In this way, a controlled setting was created that allowed to monitor changes in alternative splicing and consequently, to design a pipeline for proteogenomic analysis of spliced isoforms. The methodological novelty of this approach lies in differential analysis of alternative splicing between two groups in two molecular domains and could be applied in any comparative setting such as gene knock-down versus control or cancer versus healthy control.    Experimental procedures\r\n   Cell culture, gene knock-down and cell viability assay\r\n  SW480 cells cultured in Dulbecco's modified Eagle's medium (DMEM; Invitrogen, Bleiswijk, The Netherlands) containing 10% fetal bovine serum (FBS; Perbio Science, Etten-Leur, The Netherlands) were maintained in a humidified 5% CO2 atmosphere at 37 °C. 24 hours after seeding, cells were transfected in duplo with small interfering RNA (siRNA) pools against SF3B1 (siGENOME SF3B1 SMARTpool, M-020061-02; Thermo Fisher Scientific, Waltham, USA) and SRSF1 (siGENOME SRSF1 SMARTpool, M-018672-01), according to manufacturer's recommendations. A final siRNA concentration of 30 nM was obtained using DharmaFECT3 reagent (1:1000 dilution; T-2003-02, Thermo Fisher Scientific). A non-targeting siRNA pool (siGENOME Non-Targeting pool #2, D-001206-14) was used as negative control. Cell viability was determined after transfection using the MTT (3-(4,5dimethylthiazolyl-2)-2,5-diphenyltetrazolium bromide; ICN Biomedicals, Solon, Ohio, USA) assay, as described previously (30).    RNA isolation and quantitative reverse transcription PCR\r\n  Total RNA was isolated from viable cells, 48 hours after siRNA transfection with siSF3B1 and the siNonTargeting (siNT) control, and 72 hours after transfection with siSRSF1 and its siNT control using Trizol reagent (15596; Invitrogen, Breda, The Netherlands) and the miRNeasy Mini Kit (217004; Qiagen, Venlo, the Netherlands), following the manufacturer's protocol. Concentrations and purities were measured on a Nanodrop ND-1000 spectrophotometer (Isogen, Ijsselstein, The Netherlands). cDNA was synthesized using the Iscript cDNA synthesis kit (170-8891; Bio-Rad Laboratories, Hercules, USA). Quantitative reverse transcription PCR (RT-qPCR) was performed using SYBR Green (4309155, Thermo Scientific, Waltham, USA), to monitor SF3B1 and SRSF1 knock-down efficiencies and to evaluate efficiency of alternative splicing for ADD3, CTNND1, RAC1, SYK, MKI67 and OSBPL3. Beta-2-Microglobulin (B2M) was used as a housekeeping reference gene. In brief, gene expression was measured using Ϯʅl of 10 ng/ ʅ l cDNA in a 25 ʅ l SYBR Green reaction (see Supplementary Table 1 for primers and conditions), as described previously (30). cDNA library preparation and Illumina RNA sequencing cDNA libraries were prepared with the TruSeq Stranded mRNA LT sample Prep kit (RS-122-2101, Illumina, San Diego, USA) according to the TruSeq Stranded mRNA sample preparation guide (Part# 15031047, Revision E, October 2013). cDNA library quality control was performed using the Agilent 2100 Bioanalyzer (Agilent Technologies, Santa Clara, USA). Sample libraries were diluted and pooled to obtain a final concentration of 10 nM. Sequencing was performed on an Illumina HiSeq V4 2500, using a 125 bases paired end run with an input of 16 pM cDNA. Quality assessment of RNA-seq data was performed with FastQC version 0.11.4 (31) with default settings and visualized with MultiQC version 0.9 (32) with default parameters.    Protein isolation and separation\r\n  Proteins were isolated at the same time points as RNA extraction. After thorough washing with PBS, cells were lysed in reducing sample buffer (NuPAGE LDS sample buffer, NP0008, ThermoScientific; 65% Milli-Q, 25% 4*LDS, 10% 1M DTT) to obtain an approximate protein concentration of 1 ͬlʅŐ. Cells were scraped and transferred to eppendorf tubes. After heating for 5-10 minutes at 99°C and centrifugation for 1 minute at 14000 rpm aliquots of the samples were stored at -80°C until further use. Approximately 35 μg protein from the supernatant was loaded on a NuPAGE Novex 4-12% Bis-Tris Protein Gel, 1.5mm, 10-well (NP0335BOX; Thermo Fisher Scientific). Proteins were resolved at 150V for 1 hour in 200 ml NuPAGE MES SDS Running buffer (NP0002; Thermo Fisher Scientific) supplemented with 0.5 ml NuPAGE antioxidant (NP0005; Thermo Fisher Scientific). The gel was placed in a container with fixing solution (50% ethanol, 46.5% Milli-Q and 3.5% phosphoric acid) for 15 minutes and stained with colloïdal Coomassie (48.4% Milli-Q, 34% methanol, 15% ammonium sulfate, 2.5% phosphoric acid, 0.1% Coomassie Brilliant Blue G-250 (20279; Thermo Fisher Scientific)) overnight and destained with multiple changes of Milli-Q water. Each gel lane was sliced in 10 slices.    Whole gel in-gel digestion\r\n  The in-gel digestion procedure was done as described previously (33) with the following changes: gel pieces were dried in a centrifugal evaporator (SpeedVac) for approximately 30 minutes and peptides were extracted with 100 ʅ l 1% formic acid and two times 150 ʅ l 5% formic acid/50% acetonitrile. Concentrated extracts were transferred to Millipore filters (Millex-s,ǀĞŐǇŶ^ŝƌĚƚůĨ͕ϰƵϱϬ͘ŵʅ SLHVR04NL, Millipore), placed on autosampler vials and centrifuged for 5 minutes at room temperature in the centrifugal evaporator without vacuum. LC-MS/MS Peptides were separated by an Ultimate 3000 nanoLC-MS/MS system (Dionex LC-Packings, Amsterdam, ͿĞŚdǁƉŝƐƵĚƋŶĂůƌϰƚEϬŵпĐϱʅϳ/Ĩ ŽŵĂŬĞƚƵĚůŶĐŚƉƐϭǁϵŝ͘ϬʅϮ Å ReproSil Pur C18 aqua (Dr Maisch GMBH, Ammerbuch-Entringen, Germany). After injection, peptides ǁĞƌĚƚƉĂ ϬϭŵŶŝͬʅůŽĂϭϬŵпϬϭŵʅ/ƚƉĂƌŵŶĐŽƵůĞĚĂŬƉƚŚǁŝϱŵʅϮϬϭ Žůŝ^ƌZĞƉWƵ C18 aqua at 2% buffer B (buffer A: 0.5% acetic acid in MQ; buffer B: 80% ACN + 0.5% acetic acid in MQ) and separated at 300 nl/min in a 10-40% buffer B gradient in 60 min (90 min inject-to-inject). The nanoLC column was maintained at 50°C using a column heater (Phoenix S&amp;T, Chester, PA). Eluting peptides were ionized at a potential of +2 kVa into a Q Exactive mass spectrometer (Thermo Fisher, Bremen, Germany). Intact masses were measured at resolution 70.000 (at m/z 200) in the orbitrap using an AGC target value of 3 × 106 charges. The top 10 peptide signals (charge-states 2+ and higher) were submitted to MS/MS in the HCD (higher-energy collision) cell (1.6 m/z isolation width, 25% normalized collision energy). MS/MS spectra were acquired at resolution 17.500 (at m/z 200) in the orbitrap using an AGC target value of 1 × 106 charges and an underfill ratio of 0.5%. Dynamic exclusion was applied with a repeat count of 1 and an exclusion time of 30 s.    Full Length Isoform Sequencing \u2013 Iso-Seq\r\n  RNA isolated from siSF3B1- and siNT-treated SW480 cells was subjected to full-length RNA single molecule real time (SMRT) sequencing called Iso-Seq(34). Briefly, RNA (RIN score of ~9.0 assessed by Agilent Bioanalysis) was amplified using the ClonTech Switching Mechanism at 5' end of RNA Template (SMART) technology which incorporates known sequence at both ends of the cDNA product in the first strand synthesis process without the need for conventional adapter ligation strategies. 408 ng of siSF3B1 and 352ng siNT cDNA were used as input to the SMART cDNA amplification process to capture full-length, intact isoforms to be reverse transcribed and amplified into full-length cDNA representing the full transcriptome where the known sequences are used to complete SMRTbell library preparation using the cDNA products.  Once ample double stranded cDNA was synthesized, cDNA Iso-Seq sequencing libraries were prepared using the SMRTbell library preparation procedure resulting in a library containing molecular inserts that represent a single isoform per library molecule. These libraries were then size-selected to enrich for isoforms of interest by targeting a population of full-length transcripts to enhance coverage by loading individual size fractions on single SMRTcells. More specifically, the SageELF electrophoretic lateral fractionator instrument was used to separate independent fractions of library where isoforms that are 0 - 1kbp, 1kbp - 2kbp, 2kbp - 3kbp and 3kbp - 50kbp were split into independent SMRTbell libraries for sequencing so that larger isoforms weren't detrimentally dominated by smaller isoform library molecules during the sequencing process.  Finally, samples were sequenced using 6-hr movie collection on the PacBio RSII sequencer with two SMRTcells per cDNA size fraction. The RSII data yielded 523k to 750k subreads for each size fraction of the siNT sample, resulting in 66.8k to 98.3k CCS reads with up to 43k full length cDNA reads per size fraction. For siSF3B1, the RSII yield was 321k to 981k subreads for each size fraction, resulting in 47.5k to 97.3k CCS reads with up to 51.7k full-length cDNA reads per size fraction, using default Iso-Seq pipeline settings. Raw sequencing data was processed using Iso-Seq on PacBio SMRTportal (smrtanalysis v2.3.0) and ICE software (35) to predict low and high quality isoforms and generate high resolution transcriptome references.    RNA-seq and LC-MS/MS data analysis within the proteogenomic pipeline Splicify\r\n  The schematic overview of the proteogenomic pipeline, Splicify, is presented in Figure 1A. Low quality reads and adapter sequences were trimmed by Trimmomatic (36) version 3 to average quality score for a 4-base wide sliding window of 20, both at the beginning and at the end of the sequences (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10, LEADING:20, TRAILING:20, AVGQUAL:20, SLIDINGWINDOW:4:20). Due to the requirements of the further analysis (rMATS (37)) reads were processed to match length of 120bp, shorter reads were discarded and longer reads were trimmed (CROP:120, MINLEN:120). Mapping was performed with the use of STAR aligner (38) version 2.4.2a to the human genome (USCS RefSeq hg19 annotation, as STAR option genomeDir) with the following parameters; outSAMtype: BAM SortedByCoordinate, readFilesCommand: zcat, runThreadN: 28, outSAMattributes: All. Differential splice variants were identified with rMATS version 3.2.5 using UCSC RefSeq hg19 GTF file as annotation in the unpaired analysis type (parameters; len: 120, t: paired, analysis: U). Significant events were extracted (FDRч 0.05). Both inclusion- and exclusion-isoforms of spliced genomic fragments were taken into account for further analysis. Nucleotide acid sequences of splicing regions (upstream and downstream exons with and without spliced fragment) were obtained and translated in forward frame to amino acid sequences. In this way a database was obtained with protein sequences of potential splice variants that were all added to the human reference proteome database (Uniprot, release January 2014, no fragments, canonical &amp; isoform, 42104 entries (39)) forming an enriched human protein database. Peptide identification was performed by MaxQuant 1.5.3.8 (40) with the use of the enriched human protein database. Enzyme specificity was set to trypsin and up to two missed cleavages were allowed. Cysteine carboxamidomethylation was treated as fixed modification and methionine oxidation and Nterminal acetylation as variable modifications. Peptide precursor ions were searched with a maximum mass deviation of 4.5 ppm and fragment ions with a maximum mass deviation of 20 ppm. Peptide and protein identifications were filtered at an FDR of 1% using the decoy database strategy. Common contaminants were included in the MS/MS search. Evidence and peptides files were taken along for further analysis. Peptides specific for splice variants were extracted. Additionally, human database of canonical proteins (Swissprot, canonical, 20197 entries) was used to detect which of the splice variants represented non-canonical isoforms. Peptide intensities were normalized to the average of the samples' medians and log 10 transformed. Imputation was performed on the normalized and transformed matrix, where missing values were imputed from the normal distribution of mean equal to minimal intensity observed and standard deviation equal to mean of standard deviations calculated for each peptide. Differential peptide expression analysis was performed with a Bioconductor package limma (41) and log2 fold changes and p-values were obtained. Splicify is available at (https://github.com/NKITGO/SPLICIFY).    Isoform identification with the use of full length transcripts\r\n  Redundant transcripts were removed by first aligning them to the human genome (hg19) with GMAP (42) and collapsing highly similar transcripts predicted across FASTA files from various size fractions with the software cupcake ToFU (v1.3). In these steps both BAM and GTF files were produced for each sample. Samples were chained, to standardize transcript IDs and merge the transcripts from both experiments. Details of the workflow can be found here (43). The merged file was used as input to rMATS instead of human reference annotation GTF file. In this way the program can use the exon-exon and exon-intron junctions introduced by Iso-seq. Splice variants identified by rMATS were annotated by changing Iso-Seq transcript IDs into gene names based on genomic location, with the use of biomaRt Bioconductor package version 2.26.1(44). In case the Iso-Seq transcript was on the opposite strand than the gene, \u201cotherstrand\u201d was added to the gene symbol. In case there was no gene matching the coordinates of the transcript, \u201cintergenic\u201d was used as a gene symbol. The annotated output of rMATS was further processed as described in the RNA-seq and LC-MS/MS data analysis within the proteogenomic pipeline section, with the exclusion of the quantification step.     Results\r\n   Experimental model system to test the proteogenomic pipeline design\r\n  The schematic overview of Splicify, the proteogenomic data analysis pipeline for identification of differential splice variants, is presented in Figure 1A. In order to test the design of the proteogenomic pipeline, a model system needed to be established in which modulation of isoform changes could be controlled experimentally. For this purpose, the splicing factors SF3B1 and SRSF1, which play a key role in the splicing machinery, were down-modulated in the CRC cell line SW480, followed by RNA-seq-based transcriptomics and mass spectrometry-based proteomics analyses. A general overview of the experimental design is presented in Figure 2.  The efficiency of siRNA-mediated down-modulation of SF3B1 and SRSF1 in SW480 CRC cells was determined by RT-qPCR, and reached on average up to a 50% and 40% reduction of mRNA expression for SF3B1 and SRSF1, respectively (Supplementary Figure 1). Cell viability was reduced by 10-30% by down-modulation of SF3B1 at 48 hours after transfection, while no changes in cell viability were observed after the knockdown of SRSF1 at 72 hours after transfection (data not shown). To assure that down-modulation of SF3B1 and SRSF1 resulted functionally in changes in expression of certain isoforms, monitoring of positive controls was included in the experiment. Skipped exons in ADD3 and CTNND1 were identified by literature search as positive controls for alternative splicing in colorectal cancer tissue compared to normal colon tissue [35]. Indeed, RT-qPCR analysis for ADD3 exon 14 and CTNND1 exon 20 indicated that exclusion of these exons served as functional splicing controls for knock-down of SF3B1 and SRSF1, respectively (Supplementary Figure 2). These data demonstrate that a model system was established in which isoform switches can be modulated in a CRC cell line, suited to test the design of the proteogenomic pipeline.    Identification of differentially expressed RNA and protein isoforms by applying the proteogenomic pipeline\r\n  To investigate alternative splicing in both the RNA and protein molecular domains, the transcriptome and the proteome of each sample were analyzed with RNA-seq and tandem mass spectrometry. Quality assessment of RNA-seq and LC-MS/MS data is available in Supplementary Figures 3-5. Within the RNAseq data analysis, isoforms were identified with the use of reads spanning exon-exon and exon-intron junctions. These splice-variant specific reads, together with reads mapping to the spliced fragment, were further quantified to distinguish differential events between two conditions. In the proteomics data analysis, exon-exon and exon-intron junction-spanning peptides and peptides mapping on the spliced fragment were used to confirm translation of the isoforms detected on the RNA level into proteins (Figure 1B). The intensities of these peptides were used for quantification to identify differentially expressed protein isoforms. For details, see Figure 1A.    Differential mRNA isoforms induced by down-modulation of SF3B1 and SRSF1\r\n  Transcriptome analysis revealed a number of significantly differentially spliced events for siSF3B1 and siSRSF1 in comparison to their controls (Figure 3A; see Supplementary Tables 3-12 for details of all the events), proving that manipulation of the splicing machinery resulted in differential splicing. Alternative splicing was more affected upon manipulation of SF3B1 compared to SRSF1, as the number of alternatively spliced events was larger for this splicing factor, in particular for the events like skipped exon and mutually exclusive exons (Figure 3A). This might be due to the different roles that these splicing factors play in the spliceosome complex. The significantly skipped exon events included the positive controls of alternative splicing, higher exclusion levels of ADD3 exon 14 upon down-modulation of SF3B1 and higher exclusion levels of CTNND1 exon 20 upon down-modulation of SRSF1 (Supplementary Figure 6). These data show that the intermediate mRNA results of the proteogenomic pipeline reproduced the expected outcome, and yielded information about hundreds (for SRSF1) to thousands (for SF3B1) of additional alternative splicing events. To further validate our approach, four skipped exon splicing events were selected for confirmation by RT-qPCR, comprising SYK exon 7, RAC1 exon 4, OSBPL3 exon 9, and MKI67 exon 7 (Figure 4, Supplementary Table 2). These isoforms are also known as SYK(S) and SYK(L), Rac1b and MKI67 long and short isoforms. According to the RNA-seq analysis, all of the events were differentially spliced upon down-modulation of SRSF1 whereas OSBPL3 and MKI67 were affected by down-modulation of SF3B1. The differences in the expression of inclusion and exclusion variants between down-modulation and controls were validated with RT-qPCR (Supplementary Figure 7-9).    Differential protein isoforms induced by down-modulation of SF3B1 and SRSF1\r\n  All significant events identified on RNA level, comprising both exclusion and inclusion variants, were taken along for database construction for mass spectra identification. To prove that these splicing events are translated into proteins we searched for the peptides specific for the splice isoforms (Figure 1B). Over 5070 and 370 isoform-specific peptides were identified for differential isoforms upon downmodulation of SF3B1 and SRSF1, respectively (Table 1, see Supplementary Figure 10 for quality control of isoform-specific peptides). The differences in these numbers correspond to the sizes of the splice variant databases of the two experiments. Overall around 60% of the isoform-specific peptides turned out to map on target, peptides spanning exon-exon junction comprised around 40% and exon-intron junctions were identified far less frequently (Table 2).  Based on all the isoform-specific peptides, 2172 and 149 isoforms on protein level were identified for siSF3B1 and siSRSF1, respectively (Table 3). On average for approximately 15% of the splicing events peptide confirmation was observed for both inclusion and exclusion variants of the same event. Most of these isoforms are considered canonical proteins based on the Swissprot canonical sequence database. Approximately 5% and 25% of the identified isoforms were classified as non-canonical for siSF3B1 and siSRSF1, respectively. A subset of peptides mapped to two or more isoforms, usually due to the overlapping exons between the different isoforms. More confirmation for inclusion variants was obtained than for exclusion variants, due to the longer sequences of the inclusion variants. Among the identified isoforms all categories of alternatively spliced events were represented, with the majority of peptides supporting the skipped exon splicing category due to the predominance of this class already at the RNA level. Relatively, looking at the ratios of number of splicing events on RNA and protein level, mutually exclusive exons are more frequently detected (Figure 3B). This is mainly due to the fact that mutually exclusive exons do not have an exclusion variant as both isoforms include an additional exon, thereby increasing the overall fragment length and consequently the probability of peptide identification within the spliced region. Even though for the splicing controls ADD3 and CTNND1 no variant-specific peptides were detected, other events such as alternatively skipped exon in SYK, RAC1, OSBPL3 and MKI67 were confirmed on peptide level (Supplementary Tables 13-14).  Differential peptide expression analysis was performed for all of the splice-specific peptides and revealed that a subset of these peptides did significantly differ between splice factor knock-downs and controls, indicating concordant events between mRNA genomic and proteomic results (Table 4, Supplementary Tables 13-14). For both experiments around 65% of the significantly differentially expressed isoform-specific peptides showed concordant expression differences as observed on the RNA level. For instance, upon down-modulation of SF3B1 three split peptides spanning inclusion of exon 9 in OSBPL3 and one split peptide supporting the exclusion of this exon were identified. Two of the inclusion specific peptides show significantly lower expression upon down-modulation of SF3B1 while the exclusion specific peptide indicates higher expression in comparison to the control (Figure 5; Supplementary Table 13). Another example is lower expression of the Rac1b isoform, resulting from the inclusion of exon 4 in RAC1 gene, upon down-modulation of SRSF1, which is in line with the current knowledge of the SRSF1 effect on alternative splicing of RAC1 in colorectal cancer (16). This result was detected in the proteogenomic pipeline at RNA level, both by RNA-seq and by RT-qPCR (Figure 4; 16 Supplementary Figure 9B). On protein level only inclusion specific peptides were identified. Even though the differences in peptide intensities between siSRSF1 down-modulation and the control were not significant, log2 fold changes suggest a similar effect as on RNA level (Supplementary Figure 11; Supplementary Table 14).    Full length transcripts validation\r\n  In order to examine if sequencing of full length transcripts can validate the isoforms identified within Splicify and enrich these results with novel transcripts, Iso-Seq was performed in SW480 cells upon down-modulation of SF3B1 and its siNT control (see Figure 2). As Iso-Seq provides qualitative information, transcripts detected by this technique were used as the source of transcriptome variation instead of the human reference annotation, which could be further quantified upon mapping back the shorter, but higher density Illumina reads. On RNA level, within each alternative splicing category, the number of significantly differential isoforms identified with the use of Iso-Seq data exceeded the results compared to the approach making use of the reference annotation (Figure 6A; see Supplementary Tables 15-24 for details). There was a large overlap between detection of alternatively spliced events by Illumina-sequencing using the human reference annotation and the analysis that used Illumina reads with the Iso-Seq full length transcripts, thereby validating detection of alternatively spliced events by Splicify (Figure 6B). Additionally, full length isoform sequencing revealed a number of novel events that were not detected with the standard Splicify approach before due to absence of these events in the reference genome annotation. The largest effect is noticed for detection of retained intron events, where rMATS uses a database of annotated retained introns instead of all the introns in the genome. On the protein level, the majority of the isoform-specific peptides were identified with both approaches (Figure 6C). However, the protein database composed of the Iso-seq based findings increased the number of identified isoform-specific peptides compared to the use of the human reference annotation (Supplementary Tables 25-26). For example, three peptides supporting intron retention in FXR1 were 17 identified by sequencing of full-length transcripts that included this intron, which therefore was included in the annotation file. Illumina short reads supported this event and provided quantitative proof that it is higher expressed upon down-modulation of SF3B1 compared to its control (Figure 6D). These data indicate that in order to unravel differential splicing events more comprehensively, one should provide annotation files enriched with novel transcripts from e.g. transcriptome assembly tools or full-length transcript sequencing.     Discussion\r\n  Splicify was designed to identify differentially expressed splice variants on RNA and protein level. Splicify was applied on CRC cell line SW480 upon down-modulation of splicing machinery and non-targeting controls. We showed that this method can successfully identify condition-specific aberrant splicing events on protein level, by performing comparative splice variant analysis on both RNA and protein level. A subset of the RNA-seq based results of Splicify was validated by RT-qPCR. This proved that the pipeline yielded real splice variants on RNA level. Additionally, applying Splicify using PacBio Iso-Seq fulllength transcript sequencing confirmed the existence of the identified isoforms and increased the transcriptomic space to detect novel events. These were especially prevalent in the retained intron and alternative 3' and 5' splice site splicing events, where the overlap between Splicify with reference annotation and Splicify with Iso-Seq full length transcripts was smaller than for skipped exon and mutually exclusive exons splicing events. This shows that the reference annotation is still lacking a number of alternatively spliced isoforms which include whole or a part of the intronic sequence. A number of the novel events were also detected on protein level. This indicates that Splicify, next to the standard approach with the use of the human reference annotation, can also be applied using an alternative transcriptome annotation file that extends isoform identification with novel splicing events. On protein level, we identified a number of non-canonical isoforms, which is a valuable finding as it indicates their translation into proteins that may play a different functional role in comparison to their canonical counterparts. This is known for the Rac1b isoform, which has been shown to have a different functional role than the canonical RAC1 protein enhancing cell survival (45). Splicing of RAC1 is known to be dependent on SRSF1 activity, which was confirmed with the Splicify pipeline applied to the SW480 CRC cell line upon down-modulation of SRSF1. These data indicate that the results on protein level are in line with current literature. Other interesting findings include the detection of differential splice variants of OSBPL3. These isoforms have been shown to be differentially expressed on RNA level in various tissues, indicating that the OSBPL3 splice variants might have different functionality (46). Translation of these splice-variants into proteins and differential protein isoform expression was now shown by Splicify. These results demonstrate that Splicify successfully identifies differentially expressed mRNA and protein isoforms.  Our findings include identification of several other biologically interesting isoforms that might be linked to SF3B1 and SRSF1 activity. For instance, SYK splice variants, SYK(S) and SYK(L), have been shown to play a role in breast, liver and colorectal cancers (47). In particular alternative splicing of SYK has been demonstrated to regulate colorectal cancer progression and sensitivity of CRC cells to chemotherapy (48). Here, identification of differential splicing of SYK upon downregulation of SRSF1 might indicate possible impact of SRSF1 on alternative splicing of SYK and subsequently on colorectal cancer progression and chemotherapy resistance. Another interesting finding is identification of differential expression of MKI67 long and short isoforms upon modulation of SF3B1 as well as SRSF1 expression. It is speculated that MKI67 long isoform plays a role in cell differentiation by causing the cell to exit the cell cycle, while the short isoform leads to permanent cell cycle (49). Based on our findings, one could hypothesize that SF3B1 and SRSF1 might regulate cell proliferation through alternative splicing of MKI67. However, further studies are needed to support these statements. In addition to these examples, Splicify provided a large number of other differentially spliced isoforms. Studies that aim to 19 investigate gene function or biomarker utility could focus on splice events with peptide evidence, as these events confirm RNA translation that implies functional consequences. Moreover, different filtering approaches can be applied, e.g. based on fold change in RNA and protein expression, false discovery rates or the number of split peptides required.  The small number of protein isoforms that were detected compared to the results obtained based on analyses of RNA-seq data demonstrated the current struggles in the field of proteogenomics. There might be various reasons why a number of mRNA splice variants were not identified on protein level, including biological and technical ones. First of all, not all of the aberrant isoforms are translated into proteins. For instance, if there is a stop codon on the fragment that is alternatively spliced in, it will lead to degradation of the shorter transcript via nonsense-mediated decay. There are also splicing events called detained introns that may not exit the nucleus and therefore do not undergo translation (50). Another reason might be the kinetics of transcription and translation, in particular concerning the siRNA mediated down-modulation. It is possible that while transcripts are already present on the RNA level, they might not be translated into proteins yet at the time of RNA and protein isolation. Also, low protein isoform count can be a result of post-translational modifications of the spliced regions, for instance phosphorylation, which requires alternative sample processing preceding mass spectrometry to obtain high resolution of phosphopeptide identifications. There are also technical issues that limit the identification of splice-specific peptides, especially for the exclusion variants. If one would exclude the peptides with missed cleavages, there can only be one split peptide spanning an exclusion variant. This peptide needs to have a suitable distribution of lysine and arginine so that it spans the junction, while also having the required length and physicochemical features to be identified by a mass spectrometer. Inclusion isoforms are identified more frequently due to their longer sequence and therefore higher probability to contain a suitable tryptic peptide within the fragment of interest. All these issues explain the current advantage of RNA-seq over mass spectrometry in terms of performing quantitative analyses of splice fragments. The aberrant isoforms are often lower expressed than canonical proteins, which further complicates differential isoform expression analysis on protein level ( 5, 51 ). The 65% consistency of splice variant expression differences on RNA and protein level was expected in the context of multiple studies reporting modest correlation between RNA and protein expression (21, 52, 53). However, the qualitative information provided by mass spectrometry is highly valuable and crucial to determine what isoforms are translated into proteins. Detection of protein isoforms gives more confidence in the functional relevance of splice variants identified on RNA level, and enables to prioritize candidate biomarkers for further studies when identified in both molecular domains. In terms of biomarkers studies, Splicify can be applied in a clinically relevant setting, e.g. to compare a large series of cancer samples to healthy control tissues, and reveal differentially expressed isoforms. As the proteogenomic approach within Splicify is an unbiased first discovery step, these candidate biomarkers should be further quantified by e.g. multiple reaction monitoring or data independent acquisition, preferably both in human tissues and in relevant human body fluids for which a biomarker test is being developed (54-57). Ultimately, a highly robust approach of detecting these isoforms is necessary that could be applied in a clinical setting. For instance, antibodies targeting the spliced region could be incorporated into an immunoassay for testing large cohorts of human samples (56, 57). In conclusion, the output of proteogenomic analysis within Splicify provides answers to basic research and translational research questions, allowing identifying biologically and clinically relevant isoform-specific biomarkers.   Acknowledgements\r\n  This work was supported by KWF Kankerbestrijding, project number 2013-6025 and PacBio AACR SMRT Grant Award. We thank Bo Han, Sarah Kingan and Elizabeth Tseng from PacBio for the support with data analysis and David Cozijnsen for his contribution. The transcriptomics data was obtained at Leiden Genome Technology Center, VUmc Clinical Genetics, and Genome Core Facility at the NKI. This work was carried out on the Dutch national e-infrastructure with the support of SURF Foundation. We also thank HPC facility at the NKI for making the computational analysis possible.    Data availability\r\n  The mass spectrometry proteomics data have been deposited to the ProteomeXchange Consortium via the PRIDE (58) partner repository with the dataset identifier PXD006486. 8. Oscier, D. G., Rose-Zerilli, M. J., Winkelmann, N., Gonzalez de Castro, D., Gomez, B., Forster, J., Parker, H., Parker, A., Gardiner, A., Collins, A., Else, M., Cross, N. C., Catovsky, D., and Strefford, J. C. (2013) The clinical significance of NOTCH1 and SF3B1 mutations in the UK LRF CLL4 trial. Blood 121, 468475 9. Anczuków, O., Akerman, M., Cléry, A., Wu, J., Shen, C., Shirole, N. H., Raimer, A., Sun, S., Jensen, M. A., Hua, Y., Allain, F. H. T., and Krainer, A. R. (2015) SRSF1-Regulated Alternative Splicing in Breast Cancer. Molecular cell 60, 105-117 10. Karni, R., de Stanchina, E., Lowe, S. W., Sinha, R., Mu, D., and Krainer, A. R. (2007) The gene encoding the splicing factor SF2/ASF is a proto-oncogene. Nature structural &amp; molecular biology 14, 185193 11. David, C. J., and Manley, J. L. (2010) Alternative pre-mRNA splicing regulation in cancer: pathways and programs unhinged. Genes &amp; development 24, 2343-2364 12. Ladomery, M. (2013) Aberrant alternative splicing is another hallmark of cancer. International journal of cell biology 2013, 463786 13. Moore, M. J., Wang, Q., Kennedy, C. J., and Silver, P. A. (2010) An Alternative Splicing Network Links Cell Cycle Control to Apoptosis. Cell 142, 625-636 14. Das, S., Anczukow, O., Akerman, M., and Krainer, A. R. (2012) Oncogenic splicing factor SRSF1 is a critical transcriptional target of MYC. Cell reports 1, 110-117 15. Anczukow, O., Rosenberg, A. Z., Akerman, M., Das, S., Zhan, L., Karni, R., Muthuswamy, S. K., and Krainer, A. R. (2012) The splicing factor SRSF1 regulates apoptosis and proliferation to promote mammary epithelial cell transformation. Nature structural &amp; molecular biology 19, 220-228 16. Goncalves, V., Henriques, A. F., Pereira, J. F., Neves Costa, A., Moyer, M. P., Moita, L. F., GamaCarvalho, M., Matos, P., and Jordan, P. (2014) Phosphorylation of SRSF1 by SRPK1 regulates alternative splicing of tumor-related Rac1b in colorectal cells. RNA (New York, N.Y.) 20, 474-482 17. Matos, P., and Jordan, P. (2008) Increased Rac1b expression sustains colorectal tumor cell survival. Molecular cancer research : MCR 6, 1178-1184 18. Mischak, H., Apweiler, R., Banks, R. E., Conaway, M., Coon, J., Dominiczak, A., Ehrich, J. H., Fliser, D., Girolami, M., Hermjakob, H., Hochstrasser, D., Jankowski, J., Julian, B. A., Kolch, W., Massy, Z. A., Neusuess, C., Novak, J., Peter, K., Rossing, K., Schanstra, J., Semmes, O. J., Theodorescu, D., Thongboonkerd, V., Weissinger, E. M., Van Eyk, J. E., and Yamamoto, T. (2007) Clinical proteomics: A need to define the field and to begin to set adequate standards. Proteomics. Clinical applications 1, 148156 19. Ruggles, K. V., Krug, K., Wang, X., Clauser, K. R., Wang, J., Payne, S. H., Fenyo, D., Zhang, B., and Mani, D. R. (2017) Methods, tools and current perspectives in proteogenomics. Molecular &amp; cellular proteomics : MCP 20. Liu, S., Im, H., Bairoch, A., Cristofanilli, M., Chen, R., Deutsch, E. W., Dalton, S., Fenyo, D., Fanayan, S., Gates, C., Gaudet, P., Hincapie, M., Hanash, S., Kim, H., Jeong, S. K., Lundberg, E., Mias, G., Menon, R., Mu, Z., Nice, E., Paik, Y. K., Uhlen, M., Wells, L., Wu, S. L., Yan, F., Zhang, F., Zhang, Y., Snyder, M., Omenn, G. S., Beavis, R. C., and Hancock, W. S. (2013) A chromosome-centric human proteome project (C-HPP) to characterize the sets of proteins encoded in chromosome 17. Journal of proteome research 12, 45-57 21. Zhang, B., Wang, J., Wang, X., Zhu, J., Liu, Q., Shi, Z., Chambers, M. C., Zimmerman, L. J., Shaddox, K. F., Kim, S., Davies, S. R., Wang, S., Wang, P., Kinsinger, C. R., Rivers, R. C., Rodriguez, H., Townsend, R. R., Ellis, M. J., Carr, S. A., Tabb, D. L., Coffey, R. J., Slebos, R. J., and Liebler, D. C. (2014) Proteogenomic characterization of human colon and rectal cancer. Nature 513, 382-387 22. Wang, X., and Zhang, B. (2013) customProDB: an R package to generate customized protein databases from RNA-Seq data for proteomics search. Bioinformatics (Oxford, England) 29, 3235-3237 23. Li, Y., Wang, X., Cho, J. H., Shaw, T., Wu, Z., Bai, B., Wang, H., Zhou, S., Beach, T. G., Wu, G., Zhang, J., and Peng, J. (2016) JUMPg: an Integrative Proteogenomics Pipeline Identifying Unannotated Proteins in Human Brain and Cancer Cells. Journal of proteome research 15, 2309-2320 24. Wen, B., Xu, S., Sheynkman, G. M., Feng, Q., Lin, L., Wang, Q., Xu, X., Wang, J., and Liu, S. (2014) sapFinder: an R/Bioconductor package for detection of variant peptides in shotgun proteomics experiments. Bioinformatics (Oxford, England) 30, 3136-3138 25. Ruggles, K. V., Tang, Z., Wang, X., Grover, H., Askenazi, M., Teubl, J., Cao, S., McLellan, M. D., Clauser, K. R., Tabb, D. L., Mertins, P., Slebos, R., Erdmann-Gilmore, P., Li, S., Gunawardena, H. P., Xie, L., Liu, T., Zhou, J. Y., Sun, S., Hoadley, K. A., Perou, C. M., Chen, X., Davies, S. R., Maher, C. A., Kinsinger, C. R., Rodland, K. D., Zhang, H., Zhang, Z., Ding, L., Townsend, R. R., Rodriguez, H., Chan, D., Smith, R. D., Liebler, D. C., Carr, S. A., Payne, S., Ellis, M. J., and Fenyo, D. (2016) An Analysis of the Sensitivity of Proteogenomic Mapping of Somatic Mutations and Novel Splicing Events in Cancer. Molecular &amp; cellular proteomics : MCP 15, 1060-1071 26. Woo, S., Cha, S. W., Merrihew, G., He, Y., Castellana, N., Guest, C., MacCoss, M., and Bafna, V. (2014) Proteogenomic database construction driven from large scale RNA-seq data. Journal of proteome research 13, 21-28 27. Ghali, F., Krishna, R., Perkins, S., Collins, A., Xia, D., Wastling, J., and Jones, A. R. (2014) ProteoAnnotator--open source proteogenomics annotation software supporting PSI standards. Proteomics 14, 2731-2741 28. Wang, X., Slebos, R. J., Chambers, M. C., Tabb, D. L., Liebler, D. C., and Zhang, B. (2016) proBAMsuite, a Bioinformatics Framework for Genome-Based Representation and Analysis of Proteomics Data. Molecular &amp; cellular proteomics : MCP 15, 1164-1175 29. Askenazi, M., Ruggles, K. V., and Fenyo, D. (2016) PGx: Putting Peptides to BED. Journal of proteome research 15, 795-799 30. Sillars-Hardebol, A. H., Carvalho, B., Tijssen, M., Belien, J. A., de Wit, M., Delis-van Diemen, P. M., Ponten, F., van de Wiel, M. A., Fijneman, R. J., and Meijer, G. A. (2012) TPX2 and AURKA promote 20q amplicon-driven colorectal adenoma to carcinoma progression. Gut 61, 1568-1575 31. Andrews, S. (2015) FastQC A Quality Control tool for High Throughput Sequence Data. 32. Ewels, P., Magnusson, M., Lundin, S., and Kaller, M. (2016) MultiQC: summarize analysis results for multiple tools and samples in a single report. Bioinformatics (Oxford, England) 32, 3047-3048 33. Piersma, S. R., Warmoes, M. O., de Wit, M., de Reus, I., Knol, J. C., and Jimenez, C. R. (2013) Whole gel processing procedure for GeLC-MS/MS based proteomics. Proteome science 11, 17 34. Eid, J., Fehr, A., Gray, J., Luong, K., Lyle, J., Otto, G., Peluso, P., Rank, D., Baybayan, P., Bettman, B., Bibillo, A., Bjornson, K., Chaudhuri, B., Christians, F., Cicero, R., Clark, S., Dalal, R., Dewinter, A., Dixon, J., Foquet, M., Gaertner, A., Hardenbol, P., Heiner, C., Hester, K., Holden, D., Kearns, G., Kong, X., Kuse, R., Lacroix, Y., Lin, S., Lundquist, P., Ma, C., Marks, P., Maxham, M., Murphy, D., Park, I., Pham, T., Phillips, M., Roy, J., Sebra, R., Shen, G., Sorenson, J., Tomaney, A., Travers, K., Trulson, M., Vieceli, J., Wegener, J., Wu, D., Yang, A., Zaccarin, D., Zhao, P., Zhong, F., Korlach, J., and Turner, S. (2009) Realtime DNA sequencing from single polymerase molecules. Science (New York, N.Y.) 323, 133-138 35. Gordon, S. P., Tseng, E., Salamov, A., Zhang, J., Meng, X., Zhao, Z., Kang, D., Underwood, J., Grigoriev, I. V., Figueroa, M., Schilling, J. S., Chen, F., and Wang, Z. (2015) Widespread Polycistronic Transcripts in Fungi Revealed by Single-Molecule mRNA Sequencing. PloS one 10, e0132628 36. Bolger, A. M., Lohse, M., and Usadel, B. (2014) Trimmomatic: a flexible trimmer for Illumina sequence data. Bioinformatics (Oxford, England) 30, 2114-2120 37. Shen, S., Park, J. W., and Lu, Z. X. (2014) rMATS: robust and flexible detection of differential alternative splicing from replicate RNA-Seq data. Proceedings of the National Academy of Sciences of the United States of America 111, E5593-5601 38. Dobin, A., Davis, C. A., Schlesinger, F., Drenkow, J., Zaleski, C., Jha, S., Batut, P., Chaisson, M., and Gingeras, T. R. (2013) STAR: ultrafast universal RNA-seq aligner. Bioinformatics (Oxford, England) 29, 1521 39. The Uniprot Consortium (2017) UniProt: the universal protein knowledgebase. Nucleic acids research 45, D158-D169 40. Cox, J., and Mann, M. (2008) MaxQuant enables high peptide identification rates, individualized p.p.b.-range mass accuracies and proteome-wide protein quantification. Nature biotechnology 26, 13671372 41. Ritchie, M. E., Phipson, B., Wu, D., Hu, Y., Law, C. W., Shi, W., and Smyth, G. K. (2015) limma powers differential expression analyses for RNA-sequencing and microarray studies. Nucleic acids research 43, e47 42. Wu, T. D., and Watanabe, C. K. (2005) GMAP: a genomic mapping and alignment program for mRNA and EST sequences. Bioinformatics (Oxford, England) 21, 1859-1875 43. Magdoll (03/14/2017) https://github.com/Magdoll/cDNA_Cupcake/wiki/Cupcake-ToFU%3Asupporting-scripts-for-Iso-Seq-after-clustering-step. GitHub 44. Durinck, S., Spellman, P. T., Birney, E., and Huber, W. (2009) Mapping identifiers for the integration of genomic datasets with the R/Bioconductor package biomaRt. Nature protocols 4, 11841191 45. Singh, A., Karnoub, A. E., Palmby, T. R., Lengyel, E., Sondek, J., and Der, C. J. (2004) Rac1b, a tumor associated, constitutively active Rac1 splice variant, promotes cellular transformation. Oncogene 23, 9369-9380 46. Collier, F. M., Gregorio-King, C. C., Apostolopoulos, J., Walder, K., and Kirkland, M. A. (2003) ORP3 splice variants and their expression in human tissues and hematopoietic cells. DNA and cell biology 22, 1-9 47. Krisenko, M. O., and Geahlen, R. L. (2015) Calling in SYK: SYK's dual role as a tumor promoter and tumor suppressor in cancer. Biochimica et biophysica acta 1853, 254-263 48. Ni, B., Hu, J., Chen, D., Li, L., Chen, D., Wang, J., and Wang, L. (2016) Alternative splicing of spleen tyrosine kinase differentially regulates colorectal cancer progression. Oncology Letters 12, 17371744 49. Schmidt, M. H., Broll, R., Bruch, H. P., Finniss, S., Bogler, O., and Duchrow, M. (2004) Proliferation marker pKi-67 occurs in different isoforms with various cellular effects. Journal of cellular biochemistry 91, 1280-1292 50. Boutz, P. L., Bhutkar, A., and Sharp, P. A. (2015) Detained introns are a novel, widespread class of post-transcriptionally spliced introns. Genes &amp; development 29, 63-80 51. Gonzàlez-Porta, M., Frankish, A., Rung, J., Harrow, J., and Brazma, A. (2013) Transcriptome analysis of human tissues and cell lines reveals one dominant transcript per gene. Genome biology 14, R70 52. Gry, M., Rimini, R., Strömberg, S., Asplund, A., Pontén, F., Uhlén, M., and Nilsson, P. (2009) Correlations between RNA and protein expression profiles in 23 human cell lines. BMC Genomics 10, 365 53. Kosti, I., Jain, N., Aran, D., Butte, A. J., and Sirota, M. (2016) Cross-tissue Analysis of Gene and Protein Expression in Normal and Cancer Tissues. Scientific Reports 6 54. Gillet, L. C., Navarro, P., Tate, S., Röst, H., Selevsek, N., Reiter, L., Bonner, R., and Aebersold, R. (2012) Targeted Data Extraction of the MS/MS Spectra Generated by Data-independent Acquisition: A New Concept for Consistent and Accurate Proteome Analysis. Molecular &amp; cellular proteomics : MCP 11 55. Anderson, L., and Hunter, C. L. (2006) Quantitative mass spectrometric multiple reaction monitoring assays for major plasma proteins. Molecular &amp; cellular proteomics : MCP 5, 573-588 56. Carr, S. A., and Anderson, L. (2008) Protein Quantitation Through Targeted Mass Spectrometry: the Way Out of Biomarker Purgatory? Clinical chemistry 54, 1749-1752 57. Rifai, N., Gillette, M. A., and Carr, S. A. (2006) Protein biomarker discovery and validation: the long and uncertain path to clinical utility. Nat Biotech 24, 971-983 58. Vizcaino, J. A., Csordas, A., del-Toro, N., Dianes, J. A., Griss, J., Lavidas, I., Mayer, G., PerezRiverol, Y., Reisinger, F., Ternent, T., Xu, Q. W., Wang, R., and Hermjakob, H. (2016) 2016 update of the PRIDE database and its related tools. Nucleic acids research 44, D447-456 A The schematic overview of the Splicify data analysis. Within Splicify RNA-seq data analysis is performed by combining exemplar open-source RNA-seq analysis software, including quality and adapter trimming with Trimmomatic (36), reads mapping with STAR (38), differential splicing analysis with rMATS (37), where differential splice variants on RNA level are identified. These splice variants undergo 3-frame translation into potential protein isoform sequence database (FASTA). This database together with the human protein database from Uniprot (39) can be further used with MaxQuant (40), a search engine to identify MS/MS spectra originating from the same samples as RNA-seq reads. Downstream analysis of MaxQuant output is performed with the use of the results from RNA-seq analysis. Isoform-specific peptides are extracted and quantified and based on these peptides differential protein isoforms are identified. Splicify produces a final table with both RNA and protein isoform information. B Example of peptides supporting translation of splicing events for skipped exon and retained intron. Split peptides map to both sides of an exon-exon junction, spanning peptides span exon-intron junctions (specific for inclusion variants for retained intron, alternative 3' and 5' splice sites) and peptides on target map to a spliced fragment. splice variants were identified and several candidates were validated with RT-qPCR. Isoform specific peptides were identified and differential expression of these peptides was performed. Downmodulation of SF3B1 was repeated in a separate experiment, including PacBio Iso-Seq sequencing of full length transcripts while excluding isoform-specific peptide quantitative analysis due to the lack of replicates.  Figure 3 The number of splicing events identified on RNA and protein level upon knock-down of SF3B1 and SRSF1.  A Number of significant alternatively spliced events on RNA level upon down-modulation of SF3B1 and SRSF1 versus their controls. B The number of alternative splicing events for which at least one variant (inclusion/exclusion) was confirmed by identification of isoform-specific peptides. SE - skipped exon; MXE - mutually exclusive exons; A5SS - alternative 5' splice site; A3SS - alternative 3' splice site; RI - retained intron.  Figure 4 RT-qPCR validation of differential splicing events identified by RNA-seq data analysis with the proteogenomic pipeline, Splicify.  The exclusion isoforms of OSBPL3 exon 9 and MKI67 exon 7 are higher expressed upon downmodulation of SF3B1 and SRSF1. The inclusion isoform of SYK exon 7 and the exclusion isoform of RAC1 exon 4 are higher expressed upon down-modulation of SRSF1. Exclusion levels were calculated by dividing exclusion spanning reads by the sum of inclusion and exclusion spanning reads. Figure 5 Splicing isoforms of OSBPL3 presented in two molecular domains A. Screenshot from IGV of the spliced region of OSBPL3 exon 9; in blue -RefSeq genes, in black inclusion and exclusion variants identified with RNA-seq, in pink - inclusion and exclusion specific peptides identified in mass spectrometry. B. Peptide intensities upon down-modulation of SF3B1 and its control for two inclusion specific peptides and one exclusion specific peptide for exon 9 in OSBPL3. Peptide number on the x-axis corresponds to the peptide sequence in the table. Intensities of the overlapping peptides TYSAPAINAIQGGCFESPK and TYSAPAINAIQGGCFESPKK were manually summed and annotated as TYSAPAINAIQGGCFESPK[K] in the table and as peptide number 2 on the figure. Differential peptide expression analysis was performed with limma with no imputation for all the isoform-specific peptides including the merged peptide TYSAPAINAIQGGCFESPK[K] instead of the two. Even though not all of the peptides are significantly up or down-regulated, the signal is concordant with RT-qPCR and RNA-seq results, with higher exclusion and lower inclusion of the exon upon down-modulation of SF3B1. Figure 6 Comparison of the standard Splicify approach with the reference annotation and Splicify analysis with Iso-seq full length transcripts used as annotation.  A Number of significant alternatively spliced events on RNA level for down-modulation of SF3B1 versus the non-targeting controls with the use of Iso-Seq full-length transcripts or Reference Annotation, SE skipped exon, MXE - mutually exclusive exons, A5SS - alternative 5' splice site, A3SS - alternative 3' splice site, RI - retained intron. Illumina reads were quantified on alternatively spliced events originating from reference annotation or Iso-Seq full-length transcripts. B Overlap analysis between alternatively spliced events upon down-modulation of SF3B1 and its control, identified with reference annotation or Iso-Seq used as annotation. Overlap was defined by chromosome number and coordinates of the spliced fragment. In case of skipped exon, retained intron and alternatively spliced sites it was one 29 fragment, in case of mutually exclusive exons, coordinates of both exons were taken into the overlap. C Overlap analysis of splice-specific peptides identified with the databases based on the approach including reference annotation or Iso-Seq data. Differential splicing events were translated in 3-frame into potential proteins. These databases were used for mass spectra identification with MaxQuant. Splice-specific peptides were extracted from the MaxQuant output. Overlap analysis was performed based on unique peptide sequences. D IGV screenshot of retained intron in FXR1 gene. Blue and red coverage plots represent Illumina reads for samples siSF3B1-4 and siNT-4, respectively. Below in dark blue - reference annotation, in green - Iso-Seq transcripts obtained from the same samples, in black retained intron event identified with Iso-Seq and quantified by Illumina reads, in pink - 3 peptides spanning the exon-intron junction and supporting intron retention on protein level.  Experiment siSF3B1 vs siNT siSRSF1 vs siNT  Experiment siSF3B1 vs siNT siSRSF1 vs siNT  Experiment siSF3B1 vs siNT siSRSF1 vs siNT     ",
    "sourceCodeLink": "https://github.com/Magdoll/cDNA_Cupcake",
    "publicationDate": "0",
    "authors": [
      "Malgorzata A Komor",
      "Thang V Pham",
      "Annemieke C Hiemstra",
      "Sander R Piersma",
      "Anne S Bolijn",
      "Tim Schelfhorst",
      "Pien M Delis-van Diemen",
      "Marianne Tijssen",
      "Robert P Sebra",
      "Meredith Ashby",
      "Gerrit A Meijer",
      "Connie R Jimenez",
      "Remond JA Fijneman",
      "Running Title"
    ],
    "status": "Success",
    "toolName": "cDNA_Cupcake",
    "homepage": ""
  },
  "25.pdf": {
    "institutions": ["Paźmańy Pet́er Catholic University"],
    "URLs": ["github.com/PPKE-Bioinf/"],
    "contactInfo": ["gaspari.zoltan@itk.ppke.hu."],
    "fulltext": "     J. Chem. Inf. Model.     10.1021/acs.jcim.7b00066   CoNSEnsX+ Webserver for the Analysis of Protein Structural Ensembles Reflecting Experimentally Determined Internal Dynamics     Da ńiel Dudola  0    Bertalan Kova ćs  0    Zolta ń Ga śpa ŕi  0    0  Faculty of Information Technology and Bionics, Paźmańy Pet́er Catholic University ,  1083 Budapest, Prat́er u. 50/A ,  Hungary     2017   57  1728  1734    3  2  2017     Ensemble-based models of protein structure and dynamics reflecting experimental parameters are increasingly used to obtain deeper understanding of the role of dynamics in protein function. Such ensembles differ substantially from those routinely deposited in the PDB and, consequently, require specialized validation and analysis methodology. Here we describe our completely rewritten online validation tool, CoNSEnsX+, that offers a standardized way to assess the correspondence of such ensembles to experimental NMR parameters. The server provides a novel selection feature allowing a user-selectable set and weights of different parameters to be considered. This also offers an approximation of potential overfitting, namely, whether the number of conformers necessary to reflect experimental parameters can be reduced in the ensemble provided. The CoNSEnsX+ webserver is available at consensx.itk.ppke.hu. The corresponding Python source code is freely available on GitHub (github.com/PPKE-Bioinf/ consensx.itk.ppke.hu).       INTRODUCTION\r\n  Besides their structure, the internal dynamics of proteins is regarded as a key determinant of their biological function. This dynamics ranges from the high conformational diversity of intrinsically disordered proteins and segments1,2 to well-defined structural rearrangements in globular proteins.2,3 While NMR spectroscopy provides unique possibilities to obtain information about conformational heterogeneity and internal motions at a diverse range of time scales, detailed structural interpretation of these parameters requires ensemble representations of protein structures that correspond to the experimental parameters.4,5 Such ensembles, referred to as \u201cdynamic structural ensembles\u201d below, differ substantially from those routinely deposited in the PDB. The reason for this is that most conventional structure calculation methods enforce the correspondence of the experimental parameters to each individual conformer in the ensemble and the diversity of the conformers stems from differences of the solutions to the same structure calculation problem. In contrast, ensemble-based methods inherently treat NMR-derived parameters as an ensemble property, and require the ensemble as a whole to correspond to these parameters. This treatment is expected to reflect reality better as with the exception of slow motions on the NMR time scale, the measured parameters represent an ensemble average and, in general, no individual conformation can be expected to fulfill all of them simultaneously.6 Thus, these ensembles are usually substantially more diverse than typical \u201cNMR ensembles\u201d and require ensemblebased validation in terms of correspondence to experiments.7  Structural ensembles reflecting experimental parameters can be obtained either by ensemble-restrained molecular dynamics simulations8,9 or by selecting a subensemble from a suitable pool of conformers.10,11 Examples of parameters used for ensemble restraining include, among others, chemical shifts,3 S2 order parameters,12,13 and residual dipolar couplings.14,15  Whereas in such an ensemble each conformer should be checked for correct geometry just like for any protein structural model, validation in relation to experimental data is only meaningful at the level of the ensemble7 and not on a per-model basis. As usual, an important aspect is cross-validation with data sets not used for the generation of the ensemble, requiring a general tool capable of handling a number of parameter types. Another important issue for dynamic structural ensembles is overfitting. Overfitting (or under-restraining) is the phenomenon where the number of replicas is larger than minimally required to conform to all measured parameters.8 Larger ensembles might satisfy the experimental restrains more easily while incorporating conformations that are not realistically populated by the motions of the protein at the time scale modeled. Several aspects of overfitting can be handled during ensemble generation, e.g. the MUMO (minimal under-restraining, minimal over-restraining) calculation scheme was optimized with this respect.8 Specifically, the overfitting stemming from the nonlinear averaging of nuclear Overhauser effect (NOE) based distances within the ensemble is suggested to be minimized by avoiding averaging the distances between more than two replicas.8,16  To our knowledge, currently available software tools for ensemble generation and analysis are related to various aspects of selection and are focusing on ensembles with high structural variability such as intrinsically disordered proteins or multidomain proteins with flexible linkers. The downloadable packages ENSEMBLE11 and ASTEROIDS10 provide robust selection-based solutions primarily for intrinsically disordered proteins. The MaxOcc service17 determines the maximum possible weight of a structural model within ans ensemble that can be compatible with the observed parameters.  There are a number of tools available for the validation of NMR structures, some of the capable of ensemble averaging, such as RPF18 or PROSESS.19 These servers are not specifically designed to handle dynamic structural ensembles, handle NMR chemical shift and NOE distance data only and do not offer a selection option.  Our aim was to develop a freely accessible web service that is capable of performing general ensemble-based validation step in a standardized and user-friendly way and also offering a simple and straightforward, multipurpose selection option. Therefore, updating our previous service CoNSEnsX,20 we introduce CoNSEnsX+ with enhanced data handling and visualization features and, most importantly, a subensemble selection option that can both be used to generate dynamic ensembles and to check the presence of possible overfitting. ■    RESULTS AND DISCUSSION\r\n   Features of the CoNSEnsX+ Server. The CoNSENsX+\r\n  server is a complete reimplementation of our previous CoNSEnsX service and is available at consensx.itk.ppke.hu. The server takes three files as input: a multimodel PDB file containing the ensemble an NMR-Star file with NOE-based distance restraints (accessible as \u201cV2 NMR Restraints\u201d at rcsb.org) a BMRB-style NMR-Star format file with NMR parameters (chemical shifts, scalar and residual dipolar couplings, order parameters; see in detail below) While submitting a PDB-format structural ensemble is mandatory, it is sufficient to provide only one of the BMRB/distance restraint files. The server is capable of evaluating backbone and side-chain S2 order parameters, amide N, H, Hα, Cα, and Cβ chemical shifts, 3JHNHα, 3JHNCα, 3JHNCβ, and 3JHNC scalar couplings as well as residual dipolar couplings. The server reports RMSD and Pearson correlation between all measured and ensembleaveraged back-calculated parameters, as well as the Q-factor for each RDC set. A plot is also presented showing the relationship between the correlation values obtained for the ensemble average and those calculated for the individual conformers.  For the back-calculation of RDCs, the server offers two options: By default, PALES is invoked for each individual conformer with the SVD flag, resulting the alignment that provides the best correspondence to experimental RDCs. While this approach probably artificially improves the ensemble-level correspondence to RDCs, it accounts for the fact that individual conformers might align differently21−23 in the medium and does not suffer from the limitations inherent in approximating the alignment based on the structures alone. Optionally, the steric alignment of each conformer can be estimated using PALES. This approach seems less biased than the previous one and might also account for the diversity of alignments between conformers but alignment estimations yield worse results than SVD for individual molecules and can not be expected to perform equally well in all of the highly diverse medium types available for RDC measurements.  The server does not provide a single \u201cmeasure of goodness\u201d because the availability and quality of different kinds of NMR-derived parameters may greatly vary from case to case and thus no single measure is expected to be directly comparable between different proteins and would be both misleading and of limited practical use.  Key novel features of the CoNSEnsX+ server relative to the original version are an option for structure superposition using a specified range of residues recognition and analysis of side-chain methyl S2 order parameters three Karplus equation parameter sets24,24,25 can be selected for the calculation of 3J-couplings from the ϕ torsion recognition of RDC data obtained in different experiments and invoking PALES separately for each RDC set measured under different conditions improved graphical representation of the parameter comparisons  The most important novel feature of the server is the incorporation of a greedy selection approach for selecting a subensemble best corresponding to a user-defined set of parameters. The selection can be invoked after a round of initial evaluation and can optimize the RMSD or Pearson correlation between the experimental and back-calculated parameters (if only RDCs are included in the target function, the Q-value can also be chosen) uses parameter sets selected by the user to be included in the target function with a weight ranging from 0 to 10 is capable of overcoming the addition of conformers which cause temporary deterioration of the target value if later additions cause further improvements  Evaluation of the Ensemble Selection Feature. It should be noted here that in a dynamic structural ensemble it is not necessarily expected that any single member exactly corresponds to a conformer that is indeed occurring with a non-negligible probability in reality, but the conformational space represented by the ensemble as a whole should match that of the modeled real protein as closely as possible. Therefore, here we will focus on comparing the conformational space sampled by different ensembles to estimate the presence and extent of overfitting.  To evaluate and demonstrate the ensemble selection feature, we have selected three example cases including an SH3 domain, the peptidyl-prolyl cis−trans isomerase Pin1, and the disordered α-synuclein.  SH3 Domain with Synthetic Parameters. We have performed molecular dynamics calculations on the N-terminal SH3 domain of the Drk protein (PDB ID 2A36).26 A 100 ns simulation was run with a time step of 1 fs at 300 K in implicit solvent using the GBSA algorithm.27 The first model in PDB 2A36 was used as an input structure. The final 20 ns of the obtained trajectory was used as a reference ensemble for which various types of parameters (chemical shifts, five independent RDC sets and backbone S2 order parameters) were calculated. Using the chemical shifts and RDCs, a constricted ensemble was generated from the reference ensemble to test whether the observed parameters can be represented by a smaller set of conformers. As input for an independent selection, an initial pool covering a larger conformational space was generated with the accelerated molecular dynamics (AMD) scheme28 as implemented by our group previously in GROMACS.29 Four parallel 20 ns AMD simulations were carried out from the same starting structure but with different initial atomic velocities with an AMD boost potential of E = 2400 kJ/mol and an acceleration factor α = 0.5. The resulting set of conformers, denoted AMD pool below, is composed of the last 18.75 ns of the four AMD trajectories (3044 conformers). From this pool a number of subensembles were selected using various combinations of the \u201cexperimental\u201d parameters calculated for the reference ensemble (Table S1). The conformational space covered by the different ensembles was approximated using principal component analysis.  In the reference ensemble, the first motional mode corresponds to a bending motion of the loop connecting the β2−β3 strands, denoted as the N-src loop in the literature, which is one of the loops flanking the ligand binding pocket.30 The second mode describes the heterogeneity of the C-terminal 3 residues of the protein, which is not considered functionally relevant (Figure 1A−C, the position of a bound peptide is shown using the structure 4LNP31).  All selected ensembles reflect the first motional mode of the reference ensemble while achieving reasonable correspondence to all parameters considered (Table S1 and Figure S1). In the case of the constricted ensemble (Figure 1C), this reveals that the most important features of the reference ensemble can be well represented by a smaller set of conformers, providing a model case for assessing the extent of overfitting. Selections using the AMD pool reveal that similar ensembles can be selected from a pool that covers the conformational space of the reference ensemble (Figure 1E−H). Our deterministic greedy approach is not expected to be sensitive to the exact distribution of the conformers in the initial pool as long as it sufficiently covers the reference ensemble, as the probability of selecting a particular conformer to extend the ensemble depends only on its ability to optimize the correspondence to the parameters and not on the density of conformers in specific regions of the conformational space. Importantly, this kind of selection does not guarantee the selection of the best subensemble in terms of correspondence to the parameters and a stochastic selection might yield better results. Exact optimum could only be achieved by enumeration of all conformer combinations. Nevertheless, our approach is capable of selecting a subensemble corresponding reasonably well to the parameters and can be used to assess the extent of potential overfitting.  Pin1 Peptidyl-Prolyl Cis−Trans Isomerase. The Pin1 peptidyl-prolyl cis−trans isomerase was chosen as an example of a protein where a motional mode affecting the subdomains surrounding the substrate biding site has been suggested to play a role in catalysis and its regulation.32,33 We have generated three initial ensembles with different AMD parametrization from the starting structure 2RUC.34 We have then evaluated these ensembles using the experimental chemical shifts deposited for this structure (BMRB ID 11559) and performed selections based on the chemical shifts. Correspondence to chemical shifts was generally high in all ensembles investigated and selected. However, all ensembles generated with AMD performed better than the original 2RUC ensemble available from the PDB, compatible with the general notion that a diverse ensemble fulfills the parameters more easily. This correspondence is still slightly improved for the selected subensembles which typically have between 5 and 15 members depending on the types of chemical shifts and measure (correlation or RMSD) used in the selection (see Table S2 for data on some highlighted ensembles). Although the differences are subtle, it is apparent that the subensembles selected from the AMD4800 pool exhibit the best correspondence to the experimental chemical shifts (Table S3). Principal component analysis reveals that the AMD4800 ensemble, generated with the longest simulation time, is the most diverse, and the subensembles selected from it contain conformers from the conformational space covered only by this ensemble (Figure S2A and B). To assess the significance of this observation, we checked whether the selected ensembles reflect a biologically relevant motion. To this end, we used the motional modes identified in our previous work using the consensus of three different dynamic structural ensembles and other parvulintype PPIase (Rotamase) domains.33 We focused on a subset of 53 CA atoms identifed previously as common to different PPIase domains based on a structure alignment. Results of a principal component analysis of these atomic positions were compared to our previously published PCA results.33 It is clear the first motional mode of the ensembles selected from the AMD4800 pool shows a moderate but clearly detectable overlap with the motional mode believed to be functionally most relevant (Figure S2C and D), corresponding to the opening−closing motion around the ligand-binding site. In contrast, the full AMD5000 and AMD5200 pools and the subensembles selected from them show no such overlap. This analysis, besides highlighting the well-known importance of sufficient conformational sampling, reveals that even subtle improvements in the correspondence to experimental parameters can be relevant. Thus, at least in special cases, even this admittedly simple selection scheme is capable of producing a small ensemble with good overall correspondence to chemical shifts andgiven a suitable initial pool of conformersreflecting a biologically relevant dynamical feature.  α-Synuclein. An α-synuclein ensemble36 available in pE-DB5 (ID 9AAC) has been chosen to test the server on an intrinsically disordered protein. This ensemble was calculated based on paramagnetic relaxation enhancement (PRE) data,37 the analysis of which is currently not implemented in our server. Therefore, we performed the calculations and selection using chemical shift data only, using BMRB entry 16300.38 Thus, in this case the ensemble has been calculated based on experimental data different from those used for our selection. The selected ensemble consists of 37 members. PCA analysis reveals that the selected ensemble still covers the conformational variabilty described in the first four PCA modes of the original ensemble (Figure S3). Analysis of the secondary structure of the original and the selected ensembles shows that the positions of the helical segments are well reproduced in the selected ensemble, although the propensities are usually higher. It is important to stress here that the chemical shift data set does not exactly correspond to the ■ ■ experiments used for the calculation of the original ensemble, thus the slightly different secondary structure propensities might reflect relevant dependence on he exact conditions. In summary, we can conclude that the most relevant features of the conformational diversity of the disordered α-synuclein are still reasonably well captured with the selected ensemble, although the accuracy of the ensemble can not be expected to be comparable to that calculated based on PRE retraints.     CONCLUSIONS\r\n  Protein structural ensembles reflecting internal dynamics require specialized validation and analysis tools. The CoNSEnsX+ web server described here provides a convenient and reproducible way to assess the correspondence of ensembles to experimental NMR parameters. The selection feature allows both the generation of ensembles consistent with the data and to assess the extent of possible overfitting. Our selection approach does not guarantee to produce the smallest possible ensemble with a given level of correspondence to experimental data, but still sets an upper limit on the size of such well-performing ensembles. As our deterministic greedy approach starts with the single conformer best reflecting the selected data, the structural diversity of the selected ensemble is expected to be required to account for the experimental results and is not expected to be a result of arbitrary choice of the ensemble members. This consideration is in line with our results on various sample cases of selection described above.  Potential further uses of the CoNSEnsX+ tool could be in the field of protein structure prediction, because ensemble-based evaluation of protein models is an important issue in assessing the accuracy of the predicted conformations.39 In addition, the selection feature can aid the choice of suitable low-dimensional projections to direct effective conformational sampling for proteins.40    METHODS\r\n   Implementation of the CoNSEnsX+ Server. The\r\n  CoNSEnsx+ method was implemented in Python, and the webserver interface has been built using the Django framework. The web application can be used with any webserver which implements the WSGI (webserver gateway interface) specification, like Nginx or Apache.  The following external libraries and programs have been incorporated:  NMRPystar library for parsing BMRB parameter files ProDy protein structure analysis library for handling atomic coordinates including structure superposition41 SHIFTX program for the calculation of NMR chemical shifts from the structures,42 chosen over the more accurate SHIFTX243 for its higher speed PALES program to provide a standard way of calculating residual dipolar couplings from the structures44 PRIDE-NMR program to assess the general correspondence of the conformers to NOE-based distance restraints45 Uploaded data are stored in a SQL database on the server, as well as the pages with the results of the calculations. Since no registration is required to use the service, the result pages of the calculations ran by the user can be accessed using the permalink shown on the corresponding page. All calculated data are also stored in the SQL database to allow easy reuse. Loading the permalink gets the source of the generated result page in a single SQL query.  Ensemble-averaged values of all back-calculated parameters except NOE distances are calculated by simply taking the arithmetic mean of the values obtained for the individual conformers. NOE distances are averaged using the r−6 scheme for intramolecular distances in the case of restraints belonging to more than a single atom pair. For the full ensemble, r−6 or r−3 averaging can be chosen by the user. All back-calculated data except S2 order parameters are stored for each conformer for easy reuse in the selection algorithm.  The implemented selection algorithm is a version of a deterministic greedy approach. The process starts from a single conformer (for most experimental data types) or an ensemble of two conformers (for S2 order parameters) best corresponding to the included parameters according to the chosen measure. The approach iteratively adds further conformers resulting in the best possible correspondence for the enlarged ensemble at each step. It is generally required that the addition of the new conformers enhances the correspondence, whenever possible. To allow the generation of well-performing ensembles not necessarily achievable with the requirement of monotonic increase of the correspondence with each addition step, a so-called overdrive parameter can be set to allow decrease of the correspondence at several consecutive steps. Further extending the ensemble with additional conformers might yield an overall better correspondence even in such cases. The minimum and maximum size of the final subensemble to be returned can be specified as well as the measure by which the subensembles will be evaluated (Pearson correlation, Q-factor, or RMSD). If the calculation contained RDC sets and single value decomposition (SVD) was enabled, all the RDC parameters in a given set must have the same weight, if included in the target function. This policy is enforced by the weighting of the target function and can not be overridden. The output is the correspondence of the parameters to the full ensemble and the selected subensemble as well as the subensemble as a downloadable PDB file, in which the models are superimposed and their number in the original ensemble is marked.   Molecular Dynamics Calculations and Analysis of the\r\n  Test Cases. For the generation of SH3 and Pin1 ensembles, molecular dynamics simulations were run with GROMACS 4.5.5,46 when stated, using the accelerated molecular dynamics (AMD) scheme28 as implemented previously by our group in this package.29 PCA analysis in all cases was performed with ProDy.41  For the SH3 test case, a 100 ns simulation was run with a time step of 1 fs at 300 K in implicit solvent using the GBSA algorithm.27 The first model in the PDB ensemble 2A3626 was used as an input structure. The final 20 ns of the obtained trajectory was used as a reference ensemble for which various types of parameters (chemical shifts, five independent RDC sets, and backbone S2 order parameters) were calculated. As input for an independent selection, an initial pool covering a larger conformational space was generated with the AMD scheme. Four parallel 20 ns AMD simulations were carried out from the same starting structure but with different initial atomic velocities with an AMD boost potential of E = 2400 kJ/mol and an acceleration factor α = 0.5. The resulting set of conformers, denoted the AMD pool, is composed of the last 18.75 ns of the four AMD trajectories (3044 conformers). From this pool a number of subensembles were selected using various combinations of the \u201cexperimental\u201d parameters calculated for the reference ensemble.  For the Pin1 test case, the first conformer of the PDB ensemble 2RUC34 as a starting structure of three different AMD simulations (Table 1). Other parameters were the same as described for the SH3 test case.  Visual inspection of the ensembles was performed with MOLMOL.35  Secondary structure content evaluation of the α-synuclein ensmebles was performed by running DSSPcont47 on each ensemble member and averaging the results. ■      ASSOCIATED CONTENT\r\n  *S Supporting Information The Supporting Information is available free of charge on the ACS Publications website at DOI: 10.1021/acs.jcim.7b00066.  Figure S1: PCA and displacement plots of selected SH3 ensembles obtained by selection from the AMD pool. Figure S2: PCA analysis of different SH3 ensembles selected from the AMD pool. Figure S3: PCA plots of the full α-synuclein ensemble and the selected one, helical propensities of the ensembles (PDF) Table S1: Correspondence of parameters in selected SH3 ensembles to those derived from the reference ensemble. Table S2: Correspondence of back-calculated and experimental chemical shifts in selected Pin1 ensembles. Table S3: Correspondence of back-calculated and experimental shifts in the selected α-synuclein ensembles (XLSX) ■    AUTHOR INFORMATION\r\n   Corresponding Author\r\n  *E-mail: gaspari.zoltan@itk.ppke.hu. Phone: +36 (1)886 4780. Fax: +36 (1)886 4724.    ORCID\r\n  Dańiel Dudola: 0000-0002-5086-3213 Zoltań Gaśpaŕi: 0000-0002-8692-740X    Notes\r\n  The authors declare no competing financial interest. ■     ACKNOWLEDGMENTS\r\n  The authors acknowledge the support of the National Research, Development and Innovation Office (NKFIH) through grant no. NF104198. The authors thank Prof. Gyula Batta from the University of Debrecen for insightful discussions regarding the manuscript. ■    REFERENCES\r\n  (1) Kosol, S.; Contreras-Martos, S.; Cedeno, C.; Tompa, P. Structural Characterization of Intrinsically Disordered Proteins by NMR Spectroscopy. Molecules 2013, 18, 10802−10828. (2) Jensen, M. R.; Ruigrok, R. W.; Blackledge, M. Describing Intrinsically Disordered Proteins at Atomic Resolution by NMR. Curr. Opin. Struct. Biol. 2013, 23, 426−435. (3) Kukic, P.; Alvin Leung, H. T.; Bemporad, F.; Aprile, F. A.; Kumita, J. R.; De Simone, A.; Camilloni, C.; Vendruscolo, M. Structure and    ",
    "publicationTitle": "CoNSEnsX+ Webserver for the Analysis of Protein Structural Ensembles Reflecting Experimentally Determined Internal Dynamics",
    "title": "CoNSEnsX+ Webserver for the Analysis of Protein Structural Ensembles Reflecting Experimentally Determined Internal Dynamics",
    "publicationDOI": "10.1021/acs.jcim.7b00066",
    "publicationDate": "0",
    "publicationAbstract": "Ensemble-based models of protein structure and dynamics reflecting experimental parameters are increasingly used to obtain deeper understanding of the role of dynamics in protein function. Such ensembles differ substantially from those routinely deposited in the PDB and, consequently, require specialized validation and analysis methodology. Here we describe our completely rewritten online validation tool, CoNSEnsX+, that offers a standardized way to assess the correspondence of such ensembles to experimental NMR parameters. The server provides a novel selection feature allowing a user-selectable set and weights of different parameters to be considered. This also offers an approximation of potential overfitting, namely, whether the number of conformers necessary to reflect experimental parameters can be reduced in the ensemble provided. The CoNSEnsX+ webserver is available at consensx.itk.ppke.hu. The corresponding Python source code is freely available on GitHub (github.com/PPKE-Bioinf/ consensx.itk.ppke.hu).",
    "authors": [
      "Da ńiel Dudola",
      "Bertalan Kova ćs",
      "Zolta ń Ga śpa ŕi"
    ],
    "status": "Success",
    "toolName": "PPKE-Bioinf"
  },
  "68.pdf": {
    "forks": 1,
    "URLs": ["github.com/appliedbioinformatics/runBNG"],
    "contactInfo": ["dave.edwards@uwa.edu.au"],
    "subscribers": 2,
    "programmingLanguage": "Shell",
    "shortDescription": "An easy way to run BioNano genomic analysis",
    "publicationTitle": "runBNG: a software package for BioNano genomic analysis on the command line",
    "title": "runBNG: a software package for BioNano genomic analysis on the command line",
    "publicationDOI": "10.1093/bioinformatics/btx366",
    "codeSize": 3392,
    "publicationAbstract": "Summary: We developed runBNG, an open-source software package which wraps BioNano genomic analysis tools into a single script that can be run on the command line. runBNG can complete analyses, including quality control of single molecule maps, optical map de novo assembly, comparisons between different optical maps, super-scaffolding and structural variation detection. Compared to existing software BioNano IrysView and the KSU scripts, the major advantages of runBNG are that the whole pipeline runs on one single platform and it has a high customizability. Availability and implementation: runBNG is written in bash, with the requirement of BioNano IrysSolve packages, GCC, Perl and Python software. It is freely available at https://github.com/appliedbioinformatics/runBNG. Contact: dave.edwards@uwa.edu.au Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-09-14T13:54:47Z",
    "institutions": [
      "University of Queensland",
      "University of Western Australia"
    ],
    "license": "MIT License",
    "dateCreated": "2016-11-24T09:16:34Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx366   runBNG: a software package for BioNano genomic analysis on the command line     Yuxuan Yuan  1    Philipp E. Bayer  1    Huey-Tyng Lee  0  1    David Edwards  1    0  School of Agriculture and Food Sciences, University of Queensland ,  Brisbane, QLD ,  Australia    1  School of Biological Sciences, University of Western Australia ,  Perth, WA ,  Australia     2017   1  1  3   Summary: We developed runBNG, an open-source software package which wraps BioNano genomic analysis tools into a single script that can be run on the command line. runBNG can complete analyses, including quality control of single molecule maps, optical map de novo assembly, comparisons between different optical maps, super-scaffolding and structural variation detection. Compared to existing software BioNano IrysView and the KSU scripts, the major advantages of runBNG are that the whole pipeline runs on one single platform and it has a high customizability. Availability and implementation: runBNG is written in bash, with the requirement of BioNano IrysSolve packages, GCC, Perl and Python software. It is freely available at https://github.com/appliedbioinformatics/runBNG. Contact: dave.edwards@uwa.edu.au Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: John Hancock    1 Introduction\r\n  Next generation sequencing (NGS) technologies have advanced our understanding of biology. Most high-throughput sequencing technologies produce reads shorter than 300 bp  (Goodwin et al., 2016) , yet these are not sufficient in resolving large and complex repetitive regions resulting in collapsed and fragmented genome assemblies. Long read sequencing technologies, such as those produced by Pacific Biosciences and Oxford Nanopore have much longer read lengths, however, they suffer from high error rates, relatively low throughput and high costs. Optical mapping produces contiguous information spanning long and complex repeat regions, facilitating genome assembly and structural variation detection  (Tang et al., 2015) .  Optical mapping uses restriction cut sites to physically map genomic regions. High error rates and low throughput initially hindered the uptake of optical mapping. However, with recent improvements in technology and computational algorithms, particularly those implemented by BioNano Genomics, optical mapping has become increasingly applied in genomic analysis. Currently, BioNano optical mapping has been applied in humans  (Lam et al., 2012; Mostovoy et al., 2016; Seo et al., 2016; Shi et al., 2016) , plants  (Hastie et al., 2013; Stankova et al., 2016; Yang et al., 2016) , vertebrates  (Howe and Wood, 2015) , insects  (Rosenfeld et al., 2016)  and fish  (Xiao et al., 2015) .  IrysView is the software bundled with the BioNano Irys system and as such has dominated BioNano data analysis. However, some features of this software have limited its use for large-scale projects. IrysView can only run on Windows platforms. For large-scale analysis such as de novo assembly and hybrid assembly, an additional remote server is required. The establishment of a remote server from IrysView is not a trivial task. Without installing sun grid engine (SGE), it is not easy to submit a computational job from IrysView. Installing SGE needs a superuser or root permission and it is impossible to install SGE by normal users. If users do not use SGE, IrysView will directly connect to a login node if there are no extra settings, in where heavy workload jobs are prohibited. Furthermore, users cannot run IrysView in the background and close the window if a local job is running. It does not allow multiple jobs at the same time. These limitations restrict the use of BioNano optical mapping for different projects through IrysView.  KSU Lab has released its pipeline and scripts to standardize the length of each captured BioNano single molecule map and assist scaffolding  (Shelton et al., 2015) . In this pipeline, they applied different parameter settings and statistical methods to improve scaffolding quality. However, the limited customizability, unique server requirements, specific folder structure requirements and hard coding for different variables limit the use of the KSU pipeline.  To resolve these problems and enhance the flexibility of BioNano data analysis, we developed runBNG, a free software package to perform BioNano data analysis on Linux servers. runBNG was developed based on the functions implemented in IrysView. However, it does not suffer from the limitations of the Windows based IrysView system. runBNG supports commonly used cluster jobs, such as PBS, PBSPro, PBSTorque, Slurm, and SEG jobs. It has a high customizability, which allows users to customize each analysis depending on the aims of their research and the availability of their computational resource. Multiple jobs can be run simultaneously. Jobs can be run in the background.    2 Materials and Methods\r\n  runBNG is written in bash and implemented in a Linux environment. The minimum Linux system requirement is Ubuntu LTS, or CentOS6.4 or equivalent. Minimum dependencies are python v2.7.5, perl v5.10.x, v5.14.x or v5.16.x, gcc 4.4.7, and glibc 2.15. The BioNano IrysSolve tools including BioNano RefAligner, BioNano Assembler also have to be present. If users want to visualize the analysed results, a local browser tool JBrowse  (Skinner et al., 2009)  is recommended, but it is also possible to import the results into IrysView (Fig. 1).    3 Results\r\n  runBNG provides the same functions that are implemented in BioNano IrysView. A comparison between IrysView and runBNG is given in Table 1. A test using runBNG and IrysView is presented in the Supplementary Material. In Table 1, runBNG demonstrates advantages in data handling, customizability and running feasibility. Compared to KSU scripts, although users can modify KSU scripts to make them work on their own server, it is not a trivial task for normal users. Firstly, users need knowledge of Perl. Secondly, users need to find the key variables in the KSU scripts and modify them. Thirdly, most of the variables in KSU scripts are hard coded, which means it is not easy to modify them. Furthermore, users cannot use their customized folder structure if they do not change the variable settings in KSU scripts, such as the path to BioNano scripts and tools. Users need to find the corresponding script in the KSU scripts directory and run each analysis step. The function integration in KSU scripts is relatively low. By contrast, runBNG is highly customizable, can be easily modified, integrates different functions in one script and can be run on diverse Linux machines.    4 Conclusion\r\n  runBNG is useful for Linux-based BioNano genomic analysis since it avoids the need to switch between different platforms. It is easy to customize parameter settings, and the platform requirement to run runBNG is flexible. Users can easily run each IrysView function implemented in runBNG without the limitation of the windows interface or the requirement for a dedicated custom server.    Acknowledgements\r\n  Y.Y. would like to thank the China Scholarship Council (CSC) for supporting his PhD studies at the University of Western Australia. We thank Susan Brown for permission to use her demo data. Support is also acknowledged from the Australian Genome Research Facility (AGRF) and the Queensland Cyber Infrastructure Foundation (QCIF), the Pawsey Supercomputing Centre with funding from the Australian Government and the Government of Western Australia and resources from the National Computational Infrastructure (NCI), which is supported by the Australian Government.    Funding\r\n  This work was supported by the Australia Research Council [Projects LP140100537 and LP130100925].  Conflict of Interest: none declared. Goodwin,S. et al. (2016) Coming of age: ten years of next-generation sequencing technologies. Nat. Rev. Genet., 17, 333-351.  Hastie,A.R. et al. (2013) Rapid genome mapping in nanochannel arrays for highly complete and accurate de novo sequence assembly of the complex Aegilops tauschii genome. PLoS One, 8, e55864.    ",
    "sourceCodeLink": "https://github.com/appliedbioinformatics/runBNG",
    "publicationDate": "0",
    "authors": [
      "Yuxuan Yuan",
      "Philipp E. Bayer",
      "Huey-Tyng Lee",
      "David Edwards"
    ],
    "status": "Success",
    "toolName": "runBNG",
    "homepage": ""
  },
  "82.pdf": {
    "forks": 2,
    "URLs": ["github.com/citiususc/pastaspark"],
    "contactInfo": ["josemanuel.abuin@usc.es"],
    "subscribers": 6,
    "programmingLanguage": "Python",
    "shortDescription": "PASTASpark is an extension to PASTA (Practical Alignments using SATé and TrAnsitivity) that allows to execute it on a distributed memory cluster making use of Apache Spark.",
    "publicationTitle": "PASTASpark: multiple sequence alignment meets Big Data",
    "title": "PASTASpark: multiple sequence alignment meets Big Data",
    "publicationDOI": "10.1093/bioinformatics/btx354",
    "codeSize": 33810,
    "publicationAbstract": "Motivation: One basic step in many bioinformatics analyses is the multiple sequence alignment. One of the state-of-the-art tools to perform multiple sequence alignment is PASTA (Practical Alignments using SATe´ and TrAnsitivity). PASTA supports multithreading but it is limited to process datasets on shared memory systems. In this work we introduce PASTASpark, a tool that uses the Big Data engine Apache Spark to boost the performance of the alignment phase of PASTA, which is the most expensive task in terms of time consumption. Results: Speedups up to 10 with respect to single-threaded PASTA were observed, which allows to process an ultra-large dataset of 200 000 sequences within the 24-h limit. Availability and implementation: PASTASpark is an Open Source tool available at https://github.com/citiususc/pastaspark Contact: josemanuel.abuin@usc.es Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-10-09T17:42:37Z",
    "institutions": ["Universidade de Santiago de Compostela"],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2017-02-15T13:15:10Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx354   PASTASpark: multiple sequence alignment meets Big Data     Jos e´ M. Abuın  0    Tomas F. Pena  0    Juan C. Pichel  0    0  CiTIUS, Universidade de Santiago de Compostela ,  15782 Santiago de Compostela ,  Spain     2017   0  0  1  3   Motivation: One basic step in many bioinformatics analyses is the multiple sequence alignment. One of the state-of-the-art tools to perform multiple sequence alignment is PASTA (Practical Alignments using SATe´ and TrAnsitivity). PASTA supports multithreading but it is limited to process datasets on shared memory systems. In this work we introduce PASTASpark, a tool that uses the Big Data engine Apache Spark to boost the performance of the alignment phase of PASTA, which is the most expensive task in terms of time consumption. Results: Speedups up to 10 with respect to single-threaded PASTA were observed, which allows to process an ultra-large dataset of 200 000 sequences within the 24-h limit. Availability and implementation: PASTASpark is an Open Source tool available at https://github.com/citiususc/pastaspark Contact: josemanuel.abuin@usc.es Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Alfonso Valencia    1 Introduction\r\n  Multiple sequence alignment (MSA) is essential in order to predict the structure and function of proteins and RNAs, estimate phylogeny, and other common tasks in sequence analysis. PASTA  (Mirarab et al., 2015)  is a tool, based on SATe´  (Liu et al., 2012) , which produces highly accurate alignments, improving the accuracy and scalability of other state-of-the-art methods, including SATe´. PASTA is based on a workflow composed of several steps. During each phase, an external tool is called to perform different operations such as estimating an initial alignment and tree, computing MSAs on subsets of the original sequence set, or estimating the maximumlikelihood tree on a previously obtained MSA. Note that computing the MSAs is the most time-consuming phase, implying in some cases over 70% of the total execution time.  PASTA is a multithreaded application that only supports shared memory computers. In this way, PASTA is limited to process smallor medium-sized input datasets, because the memory and time requirements of large datasets exceed the computing power of any shared memory system. In this work we introduce PASTASpark, an extension to PASTA that allows to execute it on a distributed memory cluster making use of Apache Spark  (Zaharia et al., 2010) . Apache Spark is a cluster computing framework that supports both in-memory and on-disk computations in a fault tolerant manner, using distributed memory abstractions known as Resilient Distributed Datasets (RDDs). PASTASpark reduces noticeably the execution time of PASTA, running the most costly part of the original code as a distributed Spark application. In this way, PASTASpark guarantees scalability and fault tolerance, and allows to obtain MSAs from very large datasets in reasonable time.    2 Approach\r\n  PASTA was written in Python and Apache Spark includes APIs for Java, Python, Scala and R. For this reason, authors use the Spark Python API (known as PySpark) to implement PASTASpark. The design of PASTASpark minimizes the changes in the original PASTA code. In fact, the same software can be used to run the unmodified PASTA on a multicore machine or PASTASpark on a cluster: If Python is used, the original PASTA is launched, while if the job is submitted through Spark, PASTA is automatically executed in parallel using the Spark worker nodes.  The PASTA iterative workflow consists of four steps or phases. In the first phase (P1), a default starting tree is computed from the input sequence set S. Afterwards, using the tree and the centroid decomposition technique in SATe´-II, S is divided into disjoint sets S1; . . . ; Sm and a spanning tree T on the subsets is obtained. In the second phase (P2), the MSAs on each Si are obtained. By default, MAFFT  (Katoh et al., 2005)  is used in this step, but other aligners could be employed. The resulting alignments are referred as type 1 subalignments. Now, in the third phase (P3), OPAL  (Wheeler and Kececioglu, 2007)  is used to align the type 1 subalignment for every edge (v, w) in T , producing the so-called type 2 subalignment, from which the final MSA is obtained through a sequence of pairwise mergers using transitivity. Finally, in the fourth phase (P4), if an additional iteration is desired, FastTree-2  (Price et al., 2010)  is executed to estimate a maximumlikelihood tree on the MSA produced on the previous step, and the process is repeated using this tree as input.  As it was stated in  Mirarab et al. (2015) , the most time-consuming phase in PASTA is the computation of MSAs using MAFFT (P2). Due to this, PASTASpark focuses on parallelizing this step. In the original PASTA, P2 is parallelized using Python multiprocessing. As MAFFT requires a file as input, the computed subsets Si have to be stored in files from which each PASTA process calls the aligner using the Python class Popen. By default, PASTA creates so many processes as cores are available in the machine. This procedure has several important limitations: it only works on shared memory machines, it implies storing to disk a large amount of data, which could be a bottleneck, and, finally, it prevents MAFFT to run in parallel taking advantage of its OpenMP implementation.  To overcome these limitations, PASTASpark creates an inmemory RDD of key-value pairs, in case it detects that the Spark engine is running. In particular, the key is an object that includes information about the aligner and the required parameters to run it, while the value is an object containing a subset Si. This RDD is automatically distributed by Spark to the worker nodes in the cluster, receiving each worker a slice of the RDD. Then, a map transformation is applied to the RDD. As a consequence, the input data are stored to each local disk of the worker nodes, and the aligner is invoked to process each local file using the Popen class. Notice that the amount of data stored to any of the local disks is divided by the number of workers. We must also highlight that the storing process is done in parallel, which reduces significantly the I/O cost. Besides, if the worker nodes are multicore machines, the aligner software could run in parallel using several threads, which is also an important advantage of PASTASpark over the original PASTA application. The output of the map transformation is a new RDD that contains the type 1 subalignments. The resulting RDD is collected by the Spark Driver in order to continue the PASTA workflow. In this way, P2 is performed by the workers, while P1, P3 and P4 are executed by the Spark Driver. More details about PASTA and PASTASpark can be found in the Supplementary Material.    3 Results and discussion\r\n  Input datasets from the original PASTA publication are used to test PASTASpark performance. A summary of their main characteristics is shown in Table 1. Note that a starting tree is available for all the considered datasets, so its calculation in P1 is avoided. The experimental evaluation was carried out considering two different clusters, CESGA and AWS (see description in the Supplementary Material).  Table 2 shows the execution times of PASTA and PASTASpark when running on the CESGA cluster (PASTA can only run on a single 8-core node). Important improvements were observed when using PASTASpark with different number of workers (cores). Note that the table displays between brackets the percentage of time spent in the alignment phase (P2), which was the one parallelized by PASTASpark. Those values are essential to understand the corresponding speedups achieved by PASTASpark with respect to original PASTA (see Fig. 1a and b). In these figures, we have used the Amdahl's law  (Amdahl, 1967)  to estimate the theoretical maximum speedup achievable by PASTASpark. This law states that if a fraction s of the work for a given problem cannot be parallelized, with 0 &lt; s 1, while the remaining portion, 1 s, is p-fold parallel, then the maximum achievable speedup is 1=½s þ ð1 sÞ=p . In our particular case, P1, P3 and P4 phases in PASTA are multithreaded, so they could not be, in theory, considered sequential code. However, the execution time of P1 and P3 is really small with respect to P2 and P4, so without losing precision we can consider their execution time as a constant. On the other hand, the scalability of P4 (FastTree-2) is poor and it does not scale beyond 1.5-1.7 using three or more threads. Therefore, as a valid approximation for the current implementation of PASTA we consider P1, P3 and P4 as sequential processes. Red lines in Figure 1a and b show the Amdahl's law theoretical maximum speedup applied to D1 and D2 datasets. It can be observed that PASTASpark is close to the upper limit, obtaining speedups up to 10 when using 64 workers (cores).  Finally, Figure 1c displays the performance results of PASTASpark running on the AWS cluster using a different number of computing nodes. Each node in this system consists of 16 cores. It is worth to mention that PASTASpark is able to process an ultralarge dataset of 200 000 sequences (D3) within the 24-h limit using only eight AWS nodes.  16 PASTASpark: multiple sequence alignment meets Big Data    Funding\r\n  This work was supported by MINECO [TIN2016-76373-P, TIN201454565-JIN]; Xunta de Galicia [ED431G/08]; European Regional Development Fund; and Amazon Web Services Cloud Credits for Research program.  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/citiususc/pastaspark",
    "publicationDate": "0",
    "authors": [
      "Jos e´ M. Abuın",
      "Tomas F. Pena",
      "Juan C. Pichel"
    ],
    "status": "Success",
    "toolName": "pastaspark",
    "homepage": ""
  },
  "51.pdf": {
    "forks": 0,
    "URLs": [
      "github.com/sjackman/gfalint",
      "github.com/thegenemyers/DFA-SPEC",
      "github.com/GFA-spec/GFA-spec",
      "git.io/vyXr6",
      "github.com/pmelsted/pyGFA",
      "github.com/lh3/miniasm",
      "github.com/GFA-spec/GFA-spec/blob/master/GFA2.md",
      "github.com/vgteam/vg",
      "github.com/ggon"
    ],
    "contactInfo": ["gonnella@zbh.uni-hamburg.de"],
    "subscribers": 1,
    "programmingLanguage": "Yacc",
    "shortDescription": "Check a GFA file for syntax errors",
    "publicationTitle": "GfaPy: a flexible and extensible software library for handling sequence graphs in Python",
    "title": "GfaPy: a flexible and extensible software library for handling sequence graphs in Python",
    "publicationDOI": "10.1093/bioinformatics/btx398",
    "codeSize": 36,
    "publicationAbstract": "Summary: GFA 1 and GFA 2 are recently defined formats for representing sequence graphs, such as assembly, variation or splicing graphs. The formats are adopted by several software tools. Here, we present GfaPy, a software package for creating, parsing and editing GFA graphs using the programming language Python. GfaPy supports GFA 1 and GFA 2, using the same interface and allows for interconversion between both formats. The software package provides a simple interface for custom record types, which is an important new feature of GFA 2 (compared to GFA 1). This enables new applications of the format. Availability and implementation: GfaPy is available open source at https://github.com/ggon nella/gfapy and installable via pip. Contact: gonnella@zbh.uni-hamburg.de Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-06-19T18:10:17Z",
    "institutions": ["Universita\u20act Hamburg"],
    "license": "MIT License",
    "dateCreated": "2016-09-28T23:23:00Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx398   GfaPy: a flexible and extensible software library for handling sequence graphs in Python     Giorgio Gonnella  0    Stefan Kurtz  0    0  ZBH - Center for Bioinformatics, MIN-Fakulta\u20act, Universita\u20act Hamburg ,  20146 Hamburg ,  Germany     2017   1  1  2   Summary: GFA 1 and GFA 2 are recently defined formats for representing sequence graphs, such as assembly, variation or splicing graphs. The formats are adopted by several software tools. Here, we present GfaPy, a software package for creating, parsing and editing GFA graphs using the programming language Python. GfaPy supports GFA 1 and GFA 2, using the same interface and allows for interconversion between both formats. The software package provides a simple interface for custom record types, which is an important new feature of GFA 2 (compared to GFA 1). This enables new applications of the format. Availability and implementation: GfaPy is available open source at https://github.com/ggon nella/gfapy and installable via pip. Contact: gonnella@zbh.uni-hamburg.de Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Bonnie Berger    1 Introduction\r\n  GFA (Graphical Fragment Assembly, https://github.com/GFA-spec/GFA-spec) are text-based tab-separated file formats allowing for the description of generic sequence graphs, such as different kinds of assembly graphs (de Bruijn, overlap and string graphs), variation graphs and gene splicing graphs. They are based on similar conventions as the SAM format, the de facto standard format for read mapping results. GFA is increasingly supported by several sequence analysis software tools including assemblers, e.g. ABySS ( S (Simpson et al., 200 9), Readjoiner ( r (Gonnella and Kurtz, 201 2), Canu ( u (Koren et al., 201 7) and Miniasm (https://github.com/lh3/miniasm), variation graph tools (vg, https://github.com/vgteam/vg) and visualization tools (Bandage, Wi  Wick et al. (2015 )). All current GFA implementations write and read GFA 1, which was first defined by Heng Li in 2014 (https://git.io/vyXr6) and developed further by the community. Recently, initiated by Jason Chin, Richard Durbin and Gene Myers (https://github.com/thegenemyers/DFA-SPEC), a new version of the format (GFA 2) was collaboratively developed and released (https://github.com/GFA-spec/GFA-spec/blob/master/GFA2.md).  GFA 2 introduces several new features, which will extend the range of applications of the format. In particular, gap records allow representing scaffolding graphs. Fragment records represent the alignment of reads to contigs in an assembly. The support for generic alignments and a compact alignment representation (traces), makes GFA 2 useful for assembly of long noisy reads. GFA 2 simplifies the syntax for the definition of paths and allows specifying arbitrary subgraphs. Custom record types provide considerable flexibility for format extensions.  The combination of different programs and data sources in pipelines often requires editing and manipulating intermediate data. For example, when merging GFA files from different sources, renaming of records is necessary to avoid name clashes. For such an application, text substitution without regard to the underlying data representation is error prone and may lead to corrupted data. Furthermore, for sequence assembly tasks, additional information not available to the assembler can be systematically incorporated into the assembly graph to support its manual finishing.  In such contexts, the ability to programmatically access and traverse GFA graphs, ideally in commonly used scripting languages, is important. Here we present GfaPy, a software library implementing an application programming interface for creating, parsing, editing and writing GFA 1 and GFA 2 files. It is the first library which allows for comprehensive handling of GFA files using Python and the first publicly available implementation in any language fully supporting the GFA 2 specification.    2 Implementation and features\r\n  2.1 Python API for parsing, editing and writing GFA The Python scripting language is popular in the bioinformatics community. Until now, the support of the GFA format in Python was limited. The only publicly available Python implementation of the GFA format is PyGFA (https://github.com/pmelsted/pyGFA). However, it only offers a limited functionality for editing GFA data and traversing a graph in GFA 1 format. Moreover, it cannot handle the GFA 2 format.  The authors previously presented RGFA  (Gonnella and Kurtz, 2016) , a Ruby library to handle GFA 1. It does not support GFA 2. Since Ruby is not as widely used as Python in the Bioinformatics community, we decided to continue our development concerning GFA in Python.  GfaPy is the first Python library offering a complete interface to parse files in the GFA formats. It provides a clear API for accessing the data in a GFA file and simple traversal of a GFA graph, as described in the user manual (http://gfapy.readthedocs.io). Figure 1 shows a simple example applying the GfaPy -API. In the Supplementary Material, we demonstrate that GfaPy is a significant advance over RGFA.  The core of the GfaPy implementation is independent of the concrete record types and the datatype of specific fields. It can therefore easily be modified to incorporate future changes in the GFA specification. To ensure stability and quality of the implementation, GfaPy is complemented with a comprehensive suite of over 350 unit tests. 2.2 Syntactic and semantic validation of GFA gfalint (https://github.com/sjackman/gfalint) is a syntactic validator for both GFA 1 and GFA 2 files. However, a conforming syntax does not guarantee semantically valid GFA data. For example, an edge could point to a non-existing segment or use a position beyond the end of the segment sequence. To identify such issues, GfaPy provides both syntactic and semantic validation.   2.3 Support for GFA 2\r\n  GfaPy is the first software library with a complete interface to handle GFA 2 files. It includes operations to access the records data and traverse the graph, independent of the GFA version.  Current tools using the GFA format (except for GfaPy and gfalint) are based on GFA 1. However, tools employed in assembly pipelines for PacBio reads, are likely to adopt GFA 2. Combining these tools requires conversion between the two GFA formats. For such an application we provide a GfaPy -based command line script gfapy-convert.    2.4 Custom extensions to GFA\r\n  An important feature of GFA 2 is its support for custom extensions of the format. The first field in the GFA records contains a string defining the record type. According to the GFA 2 specification, a core parser may ignore all lines starting with a non-standard string, and these are reserved to be used for custom record types.  In GfaPy, these custom records can be accessed with a generic interface, which can be improved by the user via the implementation of an extension module. This specifies further requirements for the custom records. For example, specifying for each field a name and datatype (which can be user-defined) enables named access to the fields and validation of the data. If the custom records contain references to other lines, these can be resolved by GfaPy, thus fully integrating existing and custom line types.  In the Supplementary Material, we provide a description of how to write extensions, including an example applying GfaPy in a metagenomics context. In particular, we show how to integrate and use custom record types for representing taxa and assignments of contigs to taxa.     Acknowledgement\r\n  Conflict of Interest: none declared.  We thank Tim Weber for implementing parts of an earlier version of GfaPy.    ",
    "sourceCodeLink": "https://github.com/sjackman/gfalint",
    "publicationDate": "0",
    "authors": [
      "Giorgio Gonnella",
      "Stefan Kurtz"
    ],
    "status": "Success",
    "toolName": "gfalint",
    "homepage": ""
  },
  "86.pdf": {
    "forks": 4,
    "URLs": ["github.com/dongfanghong/deepboost"],
    "contactInfo": ["zengjy@gmail.com"],
    "subscribers": 0,
    "programmingLanguage": "Python",
    "shortDescription": "",
    "publicationTitle": "A deep boosting based approach for capturing the sequence binding preferences of RNA-binding proteins from high-throughput CLIP-seq data",
    "title": "A deep boosting based approach for capturing the sequence binding preferences of RNA-binding proteins from high-throughput CLIP-seq data",
    "publicationDOI": "10.1093/nar/gkx492",
    "codeSize": 802,
    "publicationAbstract": "Characterizing the binding behaviors of RNA-binding proteins (RBPs) is important for understanding their functional roles in gene expression regulation. However, current high-throughput experimental methods for identifying RBP targets, such as CLIP-seq and RNAcompete, usually suffer from the false negative issue. Here, we develop a deep boosting based machine learning approach, called DeBooster, to accurately model the binding sequence preferences and identify the corresponding binding targets of RBPs from CLIP-seq data. Comprehensive validation tests have shown that DeBooster can outperform other state-of-the-art approaches in RBP target prediction. In addition, we have demonstrated that DeBooster may provide new insights into understanding the regulatory functions of RBPs, including the binding effects of the RNA helicase MOV10 on mRNA degradation, the potentially different ADAR1 binding behaviors related to its editing activity, as well as the antagonizing effect of RBP binding on miRNA repression. Moreover, DeBooster may provide an effective index to investigate the effect of pathogenic mutations in RBP binding sites, especially those related to splicing events. We expect that DeBooster will be widely applied to analyze large-scale CLIPseq experimental data and can provide a practically useful tool for novel biological discoveries in understanding the regulatory mechanisms of RBPs. The source code of DeBooster can be downloaded from http://github.com/dongfanghong/deepboost.",
    "dateUpdated": "2017-10-08T07:03:32Z",
    "institutions": [
      "University of California",
      "Tsinghua University",
      "Carnegie Mellon University"
    ],
    "license": "No License",
    "dateCreated": "2016-10-19T09:37:05Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Nucleic Acids Research     10.1093/nar/gkx492   A deep boosting based approach for capturing the sequence binding preferences of RNA-binding proteins from high-throughput CLIP-seq data     Shuya Li  4    Fanghong Dong  1    Yuexin Wu  1  3    Sai Zhang  1    Chen Zhang  1    Xiao Liu  4    Tao Jiang  0  2    Jianyang Zeng  zengjy@gmail.com  1    0  Department of Computer Science and Engineering, University of California ,  Riverside, CA 92521 ,  USA    1  Institute for Interdisciplinary Information Sciences, Tsinghua University ,  Beijing 100084 ,  China    2  MOE Key Lab of Bioinformatics and Bioinformatics Division, TNLIST/Department of Computer Science and Technology, Tsinghua University ,  Beijing 100084 ,  China    3  Present address: Language Technologies Institute, Carnegie Mellon University ,  Pittsburgh, PA 15232 ,  USA    4  School of Life Sciences, Tsinghua University ,  Beijing 100084 ,  China     2017   45  14  3  17    23  5  2017    19  12  2016    12  5  2017     Characterizing the binding behaviors of RNA-binding proteins (RBPs) is important for understanding their functional roles in gene expression regulation. However, current high-throughput experimental methods for identifying RBP targets, such as CLIP-seq and RNAcompete, usually suffer from the false negative issue. Here, we develop a deep boosting based machine learning approach, called DeBooster, to accurately model the binding sequence preferences and identify the corresponding binding targets of RBPs from CLIP-seq data. Comprehensive validation tests have shown that DeBooster can outperform other state-of-the-art approaches in RBP target prediction. In addition, we have demonstrated that DeBooster may provide new insights into understanding the regulatory functions of RBPs, including the binding effects of the RNA helicase MOV10 on mRNA degradation, the potentially different ADAR1 binding behaviors related to its editing activity, as well as the antagonizing effect of RBP binding on miRNA repression. Moreover, DeBooster may provide an effective index to investigate the effect of pathogenic mutations in RBP binding sites, especially those related to splicing events. We expect that DeBooster will be widely applied to analyze large-scale CLIPseq experimental data and can provide a practically useful tool for novel biological discoveries in understanding the regulatory mechanisms of RBPs. The source code of DeBooster can be downloaded from http://github.com/dongfanghong/deepboost.       -\r\n  RNA binding proteins (RBPs) play important roles in multiple aspects of gene expression regulation, such as alternative splicing, RNA modification, mRNA export and localization ( 1 ). Not only does the dysregulation of RBPs induce abnormality, but also the mutations in their binding targets have the potential to cause diseases ( 2 ). So, capturing the intrinsic binding preferences of RBPs and identifying their binding targets in a precise and large-scale manner are essential to understand the regulatory roles of RBPs and reveal their connections to the pathogenesis of complex diseases.  Before the development of high-throughput techniques for characterizing RNA-protein interactions, only a few RBPs were well studied based on the small-scale experiments, such as in vitro EMSA ( 3 ) and in vivo fluorescence methods ( 4 ). Recently, several high-throughput sequencingbased approaches, e.g. CLIP-seq ( 5-7 ), SELEX ( 8,9 ) and RNAcompete ( 10,11 ), have been proposed to measure RBP binding sites and binding affinities in a transcriptome-wide manner. However, despite the huge amount of data generated by these techniques, they still suffer from the false negative issue mainly due to experimental noise and bias ( 12 ). To overcome these drawbacks, various computational models ( 13-20 ) have been developed to learn RBP binding preferences and detect putative RBP targets based on abundant experimental data.  As many RBPs have been validated to recognize structured regions ( 21 ), there is a tendency in recent studies to incorporate the structural features of target RNAs into prediction models, such as MEMERIS ( 15 ), GraphProt ( 17 ) and our recent deep learning based model ( 19 ), where the integration of RNA structural information has been shown to largely boost the prediction performance. Nevertheless, the current transcriptome-wide experimental techniques for measuring RNA structures are far from maturity. On the other hand, predicting RNA structures using computational models usually requires a substantial amount of additional effort and time, and a predicted RNA structure is generally less accurate compared to that derived from experimental approaches. In addition, systematic integration of both sequence and structural information generally requires a more complex prediction model. So far, it remains largely unknown whether we can derive a sequence based prediction model that only takes RNA sequence as input, while still achieving prediction performance comparable to that of the state-of-the-art prediction methods that require both sequence and structural profiles. To fill this gap between modeling accuracy and computational complexity, we develop a deep boosting based model, called DeBooster, that requires only sequence information and can capture RBP binding preferences and predict binding sites from high-throughput CLIP-seq data with high accuracy and efifciency.  Through testing on 24 CLIP-seq datasets, we have shown that even without using RNA structural information, DeBooster can outperform the state-of-the-art methods that take both sequence and structural information as input, including both GraphProt ( 17 ) and our previous deep learning based model ( 19 ). In addition, we have performed comprehensive tests to validate the superiority of DeBooster: (i) DeBooster can accurately capture RBP binding preferences and generate RBP binding motifs that are consistent with previous studies in the literature; (ii) The predictions of DeBooster can be successfully validated through crossplatform datasets.  In addition to the above extensive validation tests, we have further demonstrated several new possible applications of DeBooster in studying the regulatory roles of RBPs. With an integrative analysis based on other types of data and our prediction results, we not only derive literatureconsistent results concerning RBP regulation, but also hope to gain novel insights into the biological rationale of the regulatory roles of RBPs. In particular, we have confirmed that the binding targets of the RNA helicase MOV10 predicted by DeBooster are highly associated with the fold changes of mRNA half-lives, providing another evidence on the regulatory functions of RNA helicases on mRNA half-lives. In addition, it has been confirmed that a fraction of ADAR2 binding events are 'non-productive', i.e. these bindings may not trigger any RNA editing ( 22 ). We have also observed two potentially different types of ADAR1 binding sites which also show the similar 'productive' and 'non-productive' patterns, respectively. Moreover, we applied DeBooster to study the antagonizing effect of RBP binding on miRNA repression. In particular, it has been known that in the 3 UTR of the oncogene ERBB2, the RBP ELAVL1 (also called HUR) antagonizes the repression effect of the miRNA miR-331-3p by binding to a U-rich element (URE) near the miRNA target region called miR331b ( 23 ). With a mutant URE, we have observed that the new ELAVL1 binding sites predicted by DeBooster shift to a position more distant from the miR-331b region, which is largely consistent with the previous experimental studies. At last, we have used DeBooster to predict the effects of the single nucleotide variant (SNV) mutations on the RBP binding sites related to splicing events, which may provide useful hints for identifying pathogenic mutations and investigating their connections to the pathogenesis of complex diseases. Based on these test results, we expect that DeBooster will have great application potentials and be widely used by the community to analyze more CLIP-seq experimental data and discover more biologically relevant findings on the functional roles of RBPs in post-transcriptional gene regulation.    MATERIALS AND METHODS\r\n   The DeBooster framework\r\n  We have developed a deep boosting based approach, called DeBooster, to predict the sequence specificities of RNAbinding proteins (RBPs) from high-throughput CLIP-seq data (Figure 1). As RNA primary sequence can be viewed as a string over the alphabet {A, U, C, G}, we mainly use the basic bag-of-words model ( 24 ) as in the natural language processing field to encode the features of a given RNA sequence (Figure 1A). In particular, for each word of fixed length k, we count how many times it appears in the RNA sequence and store its frequency information in a vector of length 4k. We extract the word frequency information for both an RBP target region and its upstream and downstream aflnking regions of 150 nucleotides each. We consider words (i.e. the substrings) of lengths 1, 2, 3, which results in 2 × (4 + 42 + 43) = 168 features in total.  Note that the bag-of-words model mainly focuses on the occurrences of words and reflects little about the order of the letters in a sequence. In other words, if we swap the first half and the second half of an RNA sequence, the features provided by the bag-of-words model would roughly remain the same. To better incorporate the order of letters into the model, we further use the following scheme to extract the 'second-order' word count information. For a fixed stride m and a given RNA sequence a1a2···at, we count the words a1am + 1, a2am + 2, ..., at − mat and use a vector to record the corresponding count information. As before, we also consider both an RBP target region and the flanking regions of 150 nucleotides both upstream and downstream. We consider the stride lengths 4, 5 and 6, which generates 2 × 3 × 42 = 96 more features in total. Moreover, we consider vfie additional features, such as the length of the target region, whether the target length is a multiple of 3, whether the target region contains the stop codons UAG, UAA and UGA. Thus, overall we extract 168 + 96 + 5 = 269 features for a given RNA sequence.  We then apply a deep boosting based method, to learn a classification model from the above encoded features (Figure 1B). The deep boosting method ( 25 ), similar to other boosting methods like Adaboost ( 26 ), learns an ensemble of base classifiers. Here, a base classifier is a 'weak' classiifer that classifies the samples in a moderate accuracy, but the combination of multiple base classifiers into a stronger one can achieve higher accuracy. In particular, the classifier employed in our deep boosting method is in the following form: n t=1 f (x) = αtht(x), ( 1 ) where f(x) is the final classifier to output, each ht(x) is a base classifier (in our case a decision tree) and t is the weight of the corresponding base classifier.  For the ith training example (xi, yi), the label yi is ±1 and we want the output of f(xi), namely tn=1 αtht(xi ), to be as close to yi as possible, which is equivalent to requiring yi ( tn=1 αtht(xi )) as close to 1 as possible. Therefore, we have the following objective function during the training process:  1 m E = m i=1 (1 − yi n t=1 n t=1 αtht(xi )) + (λrt + β)αt, ( 2 ) where (xi, yi) stands for the ith training example, m stands for the total number of training examples, stands for the loss function (e.g. the exponential ( 26 ) or logistic function ( 27 )), rt is a regularization term for the tth decision tree classifier, and and are two hyper-parameters to be determined. As the deeper decision trees can always fit the training examples better, but may lose the generalization for the unseen samples, the second term in Equation ( 2 ) serves as a regularization term to overcome this overfitting problem. Specifically, if ht(x) is a decision tree of depth d, then rt is the Rademacher complexity ( 28 ) of the set of all decision trees of depth d.  The above objective function can be optimized using the same techniques as in other boosting methods ( 26,29 ). After the training process, the learned model can be used to predict the binding specificities of RBPs and also generate the corresponding binding motifs.  DeBooster is implemented by a combination of C++ and Python. All our computational experiments were performed on a 64-bit version of CentOS server.    Training datasets\r\n  We used 24 CLIP-seq datasets to train and validate our prediction model. These datasets were preprocessed in ( 17 ) to construct both positive and negative samples. In particular, the CLIP-seq binding site was extended with 150 nt both upstream and downstream (i.e. resulting in sequences that were 300 nt longer than the binding sites) to generate positive samples. For negative samples, the unbound sites were selected by shuffling the coordinates of CLIP-seq binding sites among the genes with at least one CLIP-seq binding site. Then, these selected unbound sites were also extended with 150 nt both upstream and downstream to generate the negative samples. The list of all RBP names in these datasets can also be found in Figure 2A. Among these datasets, AGO1-4 contained the binding sites for four RBPs of the argonaute family, and IGF2BP1-3 contained the binding targets of three insulin-like growth factor 2 mRNA-binding proteins. ELAVL1 HITS-CLIP, ELAVL1 PAR-CLIP(A), ELAVL1 PAR-CLIP(B) and ELAVL1 PAR-CLIP(C) included the binding sites of the RBP ELAVL1 measured from different experimental platforms.    Determination of hyperparameters\r\n  We use an independent validation dataset of RBP C22ORF28 to determine the optimal setting of the hyperparameters of DeBooster, including the type of the loss function (denoted by ), the number of the base decision tree classifiers (denoted by n), the maximum depth of these decision trees (denoted by k), and parameters , controlling the relative importance of the complexity penalty. This process yields the following optimal setting of the hyperparameters: the exponential function as the loss function , n = 200, k = 5, = 0.3 and = 0.    Motif generation\r\n  We use the following procedure to generate representative motifs of the RBP binding sites predicted by DeBooster. First, we use the set of the weighted decision trees resulting from the deep boosting algorithm to evaluate the relative importance of each encoded feature. In particular, for each decision tree with weight in the model, we identify the feature and the corresponding threshold used to split the root node for this feature. Suppose that at the root node a fraction p1 of all examples in the training set are positive, and at the right child of the root node (in which the value of feature is larger than ), a proportion p2 of all examples in the training set are positive. We then use (p1 − p2) to represent the importance of feature . By doing so, we score each feature based on its contribution to RBP binding. A higher absolute value of a positive score means higher contribution to RBP binding, while a higher absolute value of a negative score means less contribution to RBP binding. We use a vector s to store the importance scores of all encoded features. Next, we go through all 8-mers and extract the feature vector vi for each of them. We then rank these 8-mers according to the inner product of vi and s, and we select the top 500 8-mers with the highest ranking scores. As the top 8-mers may come from shifts around the best one, we align all 8-mers with respect to the top one such that the largest number of base matchings is achieved. After that, we generate the binding motif based on this alignment step and visualize it using the WebLogo site ( 30 ).    Predicting the MOV10 targets along the 3 UTRs\r\n  To predict the MOV10 binding sites along the 3 UTRs of the genes whose fold-changes of mRNA half-lives were measured after MOV10 knock-down ( 31 ), we trained DeBooster using the same MOV10 dataset as in the training data for the validation test, and then performed the analysis on the same set of 7000 genes as in ( 31 ). The UTRs of these genes were obtained based on UCSC genome annotation ( 32 ). For a gene with two or more annotated 3 UTRs, we chose the longest one within 3000 nucleotides. We then scanned the whole UTR using a sliding window whose length was equal to the average length of MOV10 binding sites in the training data (34 nt). The step size was about one-fourth of the average length of MOV10 binding sites.    Predicting ELAVL1 binding scores along the 3 UTR of gene\r\n  ERBB2 Both wild-type and mutant 3 UTR sequences of gene ERBB2 were obtained from ( 23 ) (Supplementary Notes). The lengths of these sequences are all 119 nt. For each sequence, we took a window of length 41 nt (the average length of the ELAVL1 target regions over training samples) and slided this window along the 3 UTR of mRNA ERBB2 with a stride length of 1 nt. For each sliding window, we assigned the resulting prediction score to the central nucleotide of this window. Overall, we obtained the prediction scores along positions 21-99 for each sequence (Figure 7), and the first and last 20 nucleotides were not included in our analysis.    Studying the effects of mutations in RBP binding targets\r\n  The mutation data related to splicing events were derived from COSMIC ( 33 ). Sequences with mutation sites in the middle and lengths equal to those of the corresponding RBP binding targets were prepared as input samples to DeBooster. For both pathogenic or neutral mutations near 5 or 3 splice sites, we selected those single-nucleotide variant (SNV) mutations within 10 nt from splice sites. The lengths of RBP binding targets are usually larger than 20 nt, so generally splice sites were covered by samples centered at mutation positions. In total, we collected 7000 neutral mutations in both regions near 5 and 3 splice sites, and 4000 and 20 000 mutations in regions near 5 and 3 splice sites, respectively. In Figure 8, the change of the prediction score resulting from a mutation was calculated as '(prediction score for the mutant sequence)−(prediction score for the wild-type sequence)'.  In Figure 9 and Supplementary Figure S3, the prediction scores for regions around the mutation sites along both wild-type and mutant sequences were shown. For each selected mutation, we showed the prediction scores for 41 positions, including the mutation site and the flanking regions of 20 nucleotides both upstream and downstream. For each site, its prediction score was calculated using the window centered at this position and of length equal to the average length of the corresponding RBP targets in the training data.     RESULTS\r\n   DeBooster captures the sequence preferences of RBP binding\r\n  We used 24 sets of CLIP-seq based data about RBP binding sites to train and validate our prediction model. Details about the datasets can be found in the Materials and Methods section. We first ran a 10-fold cross-validation procedure for each of 24 CLIP-seq datasets to evaluate the overall prediction performance of DeBooster. The hyperparameters in the deep boosting framework were determined using an independent dataset (Methods). We also compared the performance of DeBooster with the state-of-the-art approaches for predicting RBP target sites, including GraphProt ( 17 ) and the deep belief net (DBN) method ( 19 ). The comparison results (Figure 2A-C) showed that DeBooster can significantly outperform both GraphProt and the DBN method, with the increase of the area under receiver operator characteristic curve (AUROC) by up to 10.1%. Note that GraphProt and the DBN method integrate both RNA sequence and structural information (i.e. RNA secondary structural information ( 17 ) or both RNA secondary and tertiary structural profiles (  19 )) into the prediction framework, while DeBooster requires only RNA sequence information. We also performed additional tests to demonstrate that the performance improvement in DeBooster was attributed to both our new feature encoding scheme (see Figure 1A and Materials and Methods) and the better predictive power of the underlying deep boosting model (Supplementary Notes). We further showed that adding extra structural features did not improve the performance of DeBooster (Supplementary Notes).  Through a transcriptome-wide analysis on RBP binding targets, we also found that the difference in the predicted binding scores of DeBooster over different characterized genomic regions mostly reflected the known functions of individual RBPs (Supplementary Notes). In addition, we examined the sequence motifs of the RBP binding sites generated from training data (Methods). Our results indicated that the sequence motifs resulting from DeBooster agreed well with those reported in the literature (Figure 2D). For example, the binding sequence motif of AGO2 computed by DeBooster was enriched with A, U and C but depleted of G, which was consistent with the previous study ( 34 ). PTB, as indicated by its name (polypyrimidine tract-binding protein), mainly binds to the U/C-rich regions ( 35 ), which was also reflected in the sequence motif derived from DeBooster. EWSR1, FUS and TAF15 belong to the FET family. Although several works showed that they bind to the GU-rich motif ( 36,37 ), recent studies found that the FET protein family prefers binding to the AU-rich stem loops, and the AU-rich sequences achieve higher binding affinities than those enriched with G and U ( 38 ). Such an AUrich pattern was also observed in the sequence motif generated by DeBooster. It has been found that the binding targets of QKI usually contain a core sequence NACUAAY (where Y stands for a pyrimidine) and a half-site UAAY ( 39 ). The binding motif of QKI identified by DeBooster also agreed well with such a pattern. DeBooster yielded a U-rich sequence motif for the binding sites of HNRNPC, which can also be supported by a known fact that HNRNPC generally binds to the poly-U tracts ( 40 ). According to the DeBooster prediction results, SFRS1 prefers binding to a GA-rich motif, which aligned well with the previous result ( 41 ). As shown in the previous study ( 7 ), PUM2 binds to a consensus motif UGUANAUA, which shared high similarity with the corresponding binding motif predicted by DeBooster. The majority of the TDP43 binding sites predicted by DeBooster contained the (UG)n motif and was relatively less enriched with A and C. Such an observation agreed well with the previous known result ( 42 ). Motifs for all the 24 training datasets and the comparisons between the motifs generated by DeBooster and GraphProt ( 17 ) are also provided in Supplementary Notes. Taken together, most of the sequence motifs of RBP binding sites captured by DeBooster were consistent with the previous known results in the literature.    The predictions of DeBooster can be validated through crossplatform datasets\r\n  It is well-known that different CLIP-seq experiments can yeild a large fraction of non-overlapping results and individual experiments may miss a vast number of true RBP binding sites ( 43,44 ). Here, we showed that the prediction results of DeBooster can be validated through cross-platform CLIP-seq datasets (Figure 3). In particular, we tested DeBooster on different cross-platform ELAVL1 datasets, which displayed a large degree of discrepancy between the original RBP binding targets measured from CLIP-seq experiments (Figure 3A). Such a large variation indicated that in general a single CLIP-seq experiment cannot cover all RBP binding sites and individual datasets may have high false negative rates in current experimental measurement. The tests on the cross-platform ELAVL1 datasets showed that the predictions of DeBooster from one dataset can be well validated by another one collected from a different platform, achieving both high AUROC scores and similar sequence motifs (Figure 3B). We also evaluated the crossdataset AUROC scores based on the GraphProt ( 17 ) predictions, and showed that GraphProt performed less well than DeBooster on this task (Supplementary Notes). In addition, most of the sequence features encoded in DeBooster displayed highly correlated weights except the outliers G and UNNNNU (Figure 3C and D), which was probably due to experimental bias introduced from the original CLIP-seq data. These results implied that the predictions of DeBooster can be well validated through cross-platform CLIP-seq datasets.  We also investigated the agreement of the DeBooster prediction results between different RBPs from the same family. In particular, we examined the consistency between the DeBooster prediction scores of 8-mers for TAF15, FUS and EWSR1, all belonging to the FET family. Consistent with the previous results that these three RBPs have a large overlap in binding sites ( 38 ), our tests showed that the 8mers from different RBPs exhibited highly correlated prediction scores (Figure 3E and F). Such observations further supported the above argument that the prediction results of DeBooster can be verified from cross-platform CLIPseq datasets, even for different RBPs from the same family. These results suggested that DeBooster was not prone to overfitting, and may provide a practically useful tool to analyze high-throughput CLIP-seq data.    The binding scores predicted by DeBooster match the experimentally measured binding affinity data\r\n  To investigate whether the prediction results of DeBooster can truly reflect the RBP binding preferences, we further checked the agreement between the binding scores predicted by DeBooster and the experimentally determined binding affinity data. In particular, we checked the agreement between the prediction scores of DeBooster, which was trained using the in vivo CLIP-seq data, and the experimentally determined Kd values for two RBPs, including SFRS1 and TDP43 (Figure 4A and B). Our comparison showed that for the 8-mers as the potential RNA targets of SFRS1, the prediction scores of DeBooster closely matched the in vivo measured Kd values ( 45 ) (Figure 4A). In addition, for the RNA nucleotides as the potential binding targets of TDP43, the prediction scores of DeBooster aligned well with the Kd values experimentally measured from the electrophoretic mobility shift assay (EMSA) ( 46 ) (Figure 4B).    The predicted targets of RNA helicases may provide useful hints for understanding the regulation of mRNA degradation\r\n  RNA helicases, such as MOV10, regulate the life cycle of mRNAs and thus gene expression by remodeling RNA secondary structures and RNA-protein interactions ( 47 ). Here, we showed that the RNA targets of MOV10 predicted by DeBooster can be connected to the regulation of mRNA half-lives and thus may provide useful hints for understanding the functional roles of MOV10 in controlling gene expression. Our analysis was performed on a set of 7000 mRNAs, in which the fold changes of their half-lives had been measured after MOV10 knockdown ( 31 ). These mRNAs were basically divided into four groups according to the fold changes of their half-lives, i.e. top 25%, 25-50%, 50-75% and bottom 25%, which corresponded to Group 1, Group 2, Group 3 and Group 4, respectively. Only the bottom group (i.e. Group 4) contained those genes whose expression levels were unchanged or up-regulated after MOV10 knockdown.  Compared to the results derived directly from the original CLIP-seq data (Figure 5A), the fraction of UTRs with MOV10 binding resulting from DeBooster prediction displayed a more evident decreasing trend (Figure 5B). In addition, the sum of all positive prediction scores per UTR, which basically considered both binding strength and the number of hits for the MOV10 binding targets on individual genes, also exhibited the same decreasing order for four groups of genes that were divided and ranked according to the fold changes of mRNA half-lives (Figure 5C). Moreover, when we grouped all transcripts according to the DeBooster prediction scores, the resulting fold changes of mRNA half-lives also presented a similar decreasing trend (Figure 5D). Similar analysis was also conducted using GraphProt ( 17 ) for comparison (Supplementary Notes). Although both DeBooster and GraphProt performed comparably well on this task, there is still an advantage to use DeBooster, as it can run much faster than GraphProt, which requires the prediction of secondary structure for each input RNA sequence. Furthermore, the DeBooster prediction scores for seven genes also showed good agreement with the fold changes of mRNA half-lives experimentally measured by qRT-PCR (Figure 5E). Taken together, the above results demonstrated that the binding targets of the RNA helicase MOV10 predicted by DeBooster were associated with the changes of mRNA half-lives. Thus, the prediction results from DeBooster may provide useful clues for further understanding the regulatory mechanisms of RNA helicases on the life cycle of mRNAs.    DeBooster may distinguish two potentially different types of\r\n    ADAR1 binding patterns\r\n  ADARs are a family of homologous enzymes catalyzing adenosine-to-inosine (A-to-I) editing in the RNA, and have similar double-stranded RNA binding domains (dsRBDs) and a common deaminase domain ( 48 ). Despite their major  UCAGAGGA UCACUGGA  UUAGAGGGA  SFRS1 UCAGAUGA  UCGGUUGA UCAGAGUA  B role as RNA-editing enzymes, a fraction of ADAR2 binding events have been confirmed to be 'non-productive', that is, these bindings might not trigger any RNA editing ( 22 ). On the contrary, those ADAR2 binding events that indeed produce RNA editing were considered 'productive'. Recent studies showed that many ADAR1 binding sites are distant from the editing sites ( 49 ). It was found that ADAR1 actually has diverse functions rather than simply catalyzing RNA editing, and some of these functions are independent of its editing activity ( 49 ). To investigate whether ADAR1 also has potentially different binding patterns, such as 'productive' and 'non-productive' binding modes like ADAR2, we compared the prediction results from three DeBooster models, which were trained using all, productive and nonproductive ADAR1 binding sites, respectively.  We first introduced the concept of the binding-editing distance, which was defined as the genomic distance between a known or predicted ADAR1 binding position and its closest editing site. The known RNA editing sites were obtained from the RADAR database ( 50 ). Our first model, also called the all-binding model, was trained using all ADAR1 binding sites measured from CLIP-seq experiments ( 49 ) as the positive samples. The negative samples were defined as those unbound regions that were adjacent to the positive samples in transcripts and had the lengths equal to those of the corresponding positive samples. In our second model, also called the productive binding model, the CLIP-seq sites (i.e. the ADAR1 binding sites measured from CLIP-seq experiments) with small binding-editing distances (0−100 nt) were used as the positive samples, while the CLIP-seq sites with large binding-editing distances (&gt;1000 nt) together with the adjacent unbound regions were used as the negative samples. In our third model, also called the nonproductive binding model, the CLIP-seq sites with large binding-editing distances (&gt;1000 nt) were used as the positive samples, while the CLIP-seq sites with small bindingediting distances (0−100 nt) together with the adjacent unbounded regions were used as the negative samples. We then used the three trained models to search novel ADAR1 binding sites in the human transcriptome. The distances from the three groups of the new predicted binding sites to editing sites were calculated and shown in Figure 6A. The median of the binding-editing distances resulting from the allbinding model was 814 nt (Figure 6A), which was roughly on the same scale as from the original CLIP-seq data (606 nt). The median of the binding-editing distances from the productive binding model was zero (i.e. the ADAR1 binding region contained at least one editing site), which was significantly different from that of the non-productive binding model (4665 nt, Figure 6A). The above results showed that DeBooster may be able to learn the difference between different groups of ADAR1 binding sites, and such difference may be possibly related to the editing activity of the enzyme.  We also examined the sequence motifs of the ADAR1 binding sites identified by three different DeBooster models (Figure 6B). Although all three sequence motifs showed high GC content, the motif generated by the productivebinding model had relatively higher frequencies of As and Us than those from the other two models. This observation indicated that those ADAR1 binding sites with relatively lower GC content might be more prone to being edited. This result was also in agreement with the known evidence that the published motif of the ADAR1 binding sites ( 49 ) contained relatively higher GC content than that of the genomic regions near the editing sites ( 51 ). We also trained GraphProt ( 17 ) on the same three different datasets, and compared the analysis result to that of DeBooster. More details about the comparison can be found in Supplementary Notes.  Although the results shown in Figure 6 may be caused by the definition of productive and non-productive binding sites in our problem setting, we argued that this is very unlikely the case because of the following reasons. First, although we labeled productive and non-productive binding sites mainly based on the binding-editing distances, such a distance feature was not fed into our model as input data. Once the training data had been selected, DeBooster only used the sequence features of these training data. On the other hand, we showed that our trained model can well capture the intrinsic difference between these two binding types, including the binding-editing distances and the binding motifs (Figure 6). Second, the ADAR binding sites measured from CLIP-seq experiments are generally relatively long (average 190 nt in our training data) compare to those of other RBPs. The difference between the percentages of As in productive and non-productive binding sites were almost negligible (23.4% versus 22.9%). In addition, among all As inside the ADAR1 binding sites, only a small fraction of them were edited (about 2.88% in our training data). Thus, it unlikely that the motif difference shown in Figure 6B was introduced by the bias from our original definition of productive and non-productive binding sites. Therefore, most likely our model can capture and distinguish the intrinsic sequence features of productive and non-productive binding sites in our setting.  To summarize, we can seperate the ADAR1 binding sites into two groups. One contained the binding sites close to the editing sites, while the other covered the binding sites that are thousands of nucleotides away from the editing sites. The different patterns between these two types of binding sites can be learned by DeBooster and reproduced in the DeBooster predictions, which indicated that there might be different regulatory mechanisms underlying these two different groups of ADAR1 binding sites. However, it will still need more comprehensive investigation to study whether these two different groups of binding sites actually truly relfect the different binding behaviors of the RNA editing enzyme, and whether this difference embodies the diverse regulatory roles of ADAR1.    The shift of the predicted RBP binding scores from mutations may predict the antagonizing effect of RBP binding on miRNA repression\r\n  RBPs and miRNAs are two classes of essential regulators controlling mRNA degradation and expression, and they often interplay with each other to display co-regulatory effects ( 52 ). For example, in the 3 UTR of an oncogene ERBB2, the RBP ELAVL1 (also called HUR) antagonizes the repression effect of the miRNA miR-331-3p by binding to a U-rich element (URE) near the miRNA target region called miR-331b ( 23 ). With a mutant URE, the repres  A sion effect of ELAVL1 binding on miR-331-3p is weakened, since the new ELAVL1 binding sites shift to a position that is more distant from the miR-331b region (Figure 7A), and also reduces the binding affinity of ELAVL1 (the magnitude of the experimentally measured Kd values change from 10−8 M to 10−7 M) ( 23 ). Here, we showed that DeBooster can successfully identify this mutational effect that was consistent with the previous experimental observation.  We used the CLIP-seq dataset of ELAVL1 measured from the Hela cells ( 53 ) as training data (those overlapping records about the measured binding sites in the 3 UTR of gene ERBB2 were removed) and performed a comparative study on the predicted binding scores of four cases, i.e. WT-URE/WT-331b, MT-URE/WT-331b, WT-URE/MT-331b and MT-URE/MT-331b, which represented the wild-type sequence, a URE mutant with the wild-type miR-331b region, the wild-type URE with a miR331b mutant, and a sequence with mutations in both URE and miR-331b regions, respectively (Figure 7B). All the binding scores predicted by DeBooster showed obvious peaks near the URE, indicating the high-affinity binding of ELAVL1 in this region. More importantly, the prediction results of DeBooster displayed a clear position-shifted and affinity-decreased pattern of ELAVL1 binding on a URE mutant (Figure 7B). The curves of the predicted binding scores for WT-URE/WT-331b (i.e. wild-type) and WTURE/MT-331b (i.e. only mutations in the miR-331b region) had similar shapes, which was consistent with the pre0.25 0.20 vious experimental result that the mutations in the miR331b region rarely affect ELAVL1 binding ( 23 ). In addition, the peaks of these two curves were approximately located in positions 50-90 along the 3 UTR of ERBB2, while the peaks of the other two curves with mutations in the URE region (i.e. MT-URE/WT-331b and MT-URE/MT-331b) were located around positions 45-60. Such a position shift of the ELAVL1 binding sites identified by DeBooster in fact agreed with the previous experimental RNA footprinting results (see Figure 7B in ( 23 )). Moreover, the decrease of the binding scores predicted by DeBooster was also consistent with the loss of the experimentally-determined Kd values with respect to the same mutations ( 23 ). Taken together, these results indicated that DeBooster can successfully identify the changes of the RBP binding scores caused by the mutations in binding targets which may be used to predict the antagonizing effect of RBP binding on miRNA repression.    The prediction scores of DeBooster may provide a useful index to study pathogenic mutations affecting RNA splicing\r\n  Recent studies revealed that abnormal splicing play a vital role in development of many human diseases, such as cancer and neurological disorders ( 55-57 ). The mutations near splice sites or on splicing regulatory elements, such as exonic splicing enhancers (ESEs) and exonic splicing silencers (ESSs), may influence RNA splicing and cause human diseases by disrupting RBP binding ( 2 ). Here, we were particularly interested in whether DeBooster can provide a useful tool to study the mutational effects of sequence variants related to splicing events. We first examined the overall changes of the predicted binding scores of individual RBPs with respect to the sequence variants of their binding targets near 5 and 3 splice sites (Methods) and checked whether the DeBooster prediction results can relfect the difference between pathogenic mutations and neutral sequence variants. Our comparisons showed that the changes of the binding scores predicted by DeBooster for a majority of pathogenic sequence variants in regions near 5 and 3 splice sites were significantly different from those of neutral mutations (Figure 8). We also confirmed that if the mutations occurred outside the RBP binding sites, the predicted RBP binding scores were only affected to a much smaller extent (Supplementary Notes). In addition, almost all of these pathogenic mutations displayed relatively larger changes in the predicted binding scores than neutral variants. On the other hand, most of the neutral mutations near 5 and 3 splice sites displayed similar effects (with only 4 among 20 RBPs showing significant difference  exon ←A intron WT-CDH1 ...ACCACUGGGCUGGACCGAGAGGUCAGGGGUCAGGAGGAUCC...  -20 0 20 intron ←C exon WT-TRRAP ...AAUUUCUCUUCCCGUUAGGUUUUCGUUCUCAAAUUCCAC...  -20 0 20 C WT-TRRAP MT-TRRAP  WT-ATM MT-ATM WT-ATM MT-ATM A with P &lt; 0.001 in the Student's t test). Furthermore, the pathogenic mutations near splice sites generally showed a greater extent of difference in the predicted binding scores than those pathogenic mutations randomly chosen from the COSMIC records ( 33 ) (Supplementary Figure S2). For instance, among 20 RBPs, 15 and 18 proteins exhibited significantly different mutational effects on the pathogenic variants near 5 and 3 splice sites, respectively, compared to only seven RBPs in those pathogenic mutations randomly selected from COSMIC (Figure 8 and Supplementary Figure S1). Such an observation implied that the sequence disruptions of the RBP binding targets around splice sites may generally play a more important role in the pathogenesis of a disease. Overall, our studies indicated that the binding scores derived from DeBooster may provide an effective indicator for distinguishing pathogenic mutations from neutral variants in RBP binding targets near splice sites.  Next, we further analyzed the mutational effects predicted by DeBooster for a number of known pathogenic single-nucleotide variants (SNVs) obtained from COSMIC ( 33 ). Below we describe several examples (Figure 9). First, a synonymous substitution of the last base in Exon 7 (G to A) of gene CDH1 (which encodes the E-cadherin protein) led to an increase in the SFRS1 binding scores predicted by DeBooster near a 5 splice site (Figure 9A), which may be related to the dysregulation of CDH1 that causes tumor metastasis ( 58 ). Such an observation may also be supported by a previous experimental validation study that this mutation can actually alter splicing by causing intron retention to various extents ( 59 ).  As a second example, a mutation from G to A in a TCF7L2 exon ( 60 ) disrupted the ESE motifs (which are 6 nt motifs located in exons and bound by SR proteins to promote exon splicing ( 61 )) and suppressed SFRS1 binding (Figure 9B), while a mutation from U to A in a THRAP3 exon ( 60 ) enriched the ESE motifs and thus enhanced SFRS1 binding (Figure 9C). Such disruptions in those disease-relevant genes may influence the binding behaviors of the important splicing regulator SFRS1, and thus may be related to the tumorigenesis associated with aberrant splicing ( 45 ).  In our third example, the mutation from U to C near a 3 splice site of gene TRRAP ( 60 ) weakened TIA1 binding (Figure 9D). TRRAP interacts with oncoproteins MYC and E2A ( 62 ), and its mis-regulation can be heavily related to various types of cancers ( 63 ). On the other hand, another mutation from C to U near a 3 splice site of gene KTN1 ( 60 ) strengthened TIA1 binding (Figure 9E). KTN1 encodes kinectin 1, and has been shown to display different splicing patterns in cancers ( 64 ). Thus, these two sequence variants in the binding sites of TIA1 may be associated with cancer pathogenesis by changing the alternative splicing modes of its target genes.  Another interesting example is the intronic mutation near a 5 splice site of gene ATM ( 60 ), which increased the binding scores of both FUS and QKI (Figure 9F). Such a mutation may influence the splicing result of this tumor suppressor (i.e. ATM) ( 65 ) by creating new potential binding sites for both splicing regulators (i.e. FUS and QKI).  In addition to the above cases, there were other examples to demonstrate that the prediction scores of DeBooster may reflect the pathogenic effects of sequence disruptions in RBP binding. For instance, a substitution from C to U near a 5 splice site of gene NF1 ( 60 ) enhanced HNRNPC binding (Supplementary Figure S3A), which may be associated with the known related neurologic disorders ( 66 ). On the other hand, a mutation from U to C near a splice site of the proto-oncogene BRAF ( 60 ) decreased the HNRNPC binding score (Supplementary Figure S3B). In addition, a mutation from A to G ( 60 ) near a splice site of gene TET2 may help form a novel GU-repeat region for strong TDP43 binding (Supplementary Figure S3C), and thus influence the splicing process. Moreover, the SMAD4 splicing site may be disrupted by the mutation from G to U ( 60 ) that may increase the PTB binding score (Supplementary Figure S3D) and thus alter the corresponding splicing result. Both TET2 and the SMAD4 genes act as tumor suppressors ( 67,68 ), so the inhibition of their normal splicing may thus facilitate cancer formation.  Taken together, the above examples illustrated that the RBP binding scores predicted by DeBooster may offer a useful index to investigate the pathogenic effects of sequence disruptions related to RNA splicing.     CONCLUSION\r\n  We developed DeBooster, a deep boosting based framework to model the sequence binding specificities of RNAbinding proteins (RBPs) from high-throughput CLIP-seq data. Compared to the state-of-the-art methods which usually require both sequence and structure profiles, DeBooster uses only sequence information as input. Tests on 24 CLIPseq datasets demonstrated that DeBooster can achieve better prediction performance than previous methods. In addition, the binding sites predicted by DeBooster can be validated through cross-platform datasets. Moreover, the prediction scores of DeBooster agreed with the experimentallydetermined binding affinity scores, such as in vivo measured Kd values.  We further showed the great application potentials of DeBooster by applying it to study the regulatory roles of several important RBPs. In particular, we demonstrated that the predicted targets of the RNA helicase MOV10 can better explain its binding effects on the regulation of mRNA degradation than the original CLIP-seq data. In addition, the predicted RBP binding sites may help understand the potentially different binding patterns of the RNA-editing enzymes ADARs. We also showed that a shift of the predicted ELAVL1 binding scores from wild-type to mutant in a U-rich element (URE) region of gene ERBB2 can effectively predict the antagonizing effect of RBP binding on miRNA regulation. Moreover, DeBooster may be used as an effective index to identify pathogenic mutations from normal sequence variants and study the effects of potential disease-causing mutations in RBP binding sites related to splicing. Based on these test results and analyses, we expect that DeBooster will provide a promising tool to analyze more large-scale CLIP-seq data and gain more biological insights related to RBP regulation.  The training datasets used in our study were originally prepared in GraphProt ( 17 ), in which the negative data was randomly selected from the unbound regions of target genes. Through this manner, the constructed negative data can include comprehensive information of the background. However, to better control the local sequence bias in the prediction results, using the upstream/downstream regions near target sites as negative samples might be also worthy of consideration. Further work will be needed to improve the quality of training and test datasets to achieve better predictions.    SUPPLEMENTARY DATA\r\n  Supplementary Data are available at NAR Online.    ACKNOWLEDGEMENTS\r\n  The authors are grateful to Dr. Qiangfeng Zhang and Mr. Hailin Hu, Mr. Bin Zhou and Mr. Xuan He for their helpful discussions about this work. They thank the anonymous reviewers for their helpful comments and suggestions.    FUNDING\r\n  National Natural Science Foundation of China [61472205]; US National Science Foundation [DBI-1262107, IIS1646333]; China's Youth 1000-Talent Program; Beijing Advanced Innovation Center for Structural Biology. Funding for open access charge: China's Youth 1000-Talent Program. Conflict of interest statement. None declared.    ",
    "sourceCodeLink": "https://github.com/dongfanghong/deepboost",
    "publicationDate": "0",
    "authors": [
      "Shuya Li",
      "Fanghong Dong",
      "Yuexin Wu",
      "Sai Zhang",
      "Chen Zhang",
      "Xiao Liu",
      "Tao Jiang",
      "Jianyang Zeng"
    ],
    "status": "Success",
    "toolName": "deepboost",
    "homepage": ""
  },
  "43.pdf": {
    "forks": 0,
    "URLs": [
      "github.com/casecpb/KSEA/",
      "casecpb.shinyapps.io/ksea/",
      "github.com/casecpb/KSEAapp/"
    ],
    "contactInfo": ["mark.chance@case.edu"],
    "subscribers": 0,
    "programmingLanguage": "R",
    "shortDescription": "Kinase-Substrate Enrichment Analysis (KSEA) App",
    "publicationTitle": "The KSEA App: a web-based tool for kinase activity inference from quantitative phosphoproteomics",
    "title": "The KSEA App: a web-based tool for kinase activity inference from quantitative phosphoproteomics",
    "publicationDOI": "10.1093/bioinformatics/btx415",
    "codeSize": 3657,
    "publicationAbstract": "Summary: Computational characterization of differential kinase activity from phosphoproteomics datasets is critical for correctly inferring cellular circuitry and how signaling cascades are altered in drug treatment and/or disease. Kinase-Substrate Enrichment Analysis (KSEA) offers a powerful approach to estimating changes in a kinase's activity based on the collective phosphorylation changes of its identified substrates. However, KSEA has been limited to programmers who are able to implement the algorithms. Thus, to make it accessible to the larger scientific community, we present a web-based application of this method: the KSEA App. Overall, we expect that this tool will offer a quick and user-friendly way of generating kinase activity estimates from highthroughput phosphoproteomics datasets. Availability and implementation: the KSEA App is a free online tool: casecpb.shinyapps.io/ksea/. The source code is on GitHub: github.com/casecpb/KSEA/. The application is also available as the R package 'KSEAapp' on CRAN: CRAN.R-project.org/package¼KSEAapp/. Contact: mark.chance@case.edu Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-05-02T20:46:24Z",
    "institutions": [
      "Case Western Reserve University",
      "Center for Proteomics and Bioinformatics"
    ],
    "license": "MIT License",
    "dateCreated": "2017-02-11T20:08:43Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx415   The KSEA App: a web-based tool for kinase activity inference from quantitative phosphoproteomics     Danica D. Wiredja  1    Mehmet Koyutu¨ rk  0    Mark R. Chance  1    0  Department of Electrical Engineering and Computer Science, Case Western Reserve University ,  Cleveland, OH 44106 ,  USA    1  Department of Nutrition, Center for Proteomics and Bioinformatics     2017   1  1  3   Summary: Computational characterization of differential kinase activity from phosphoproteomics datasets is critical for correctly inferring cellular circuitry and how signaling cascades are altered in drug treatment and/or disease. Kinase-Substrate Enrichment Analysis (KSEA) offers a powerful approach to estimating changes in a kinase's activity based on the collective phosphorylation changes of its identified substrates. However, KSEA has been limited to programmers who are able to implement the algorithms. Thus, to make it accessible to the larger scientific community, we present a web-based application of this method: the KSEA App. Overall, we expect that this tool will offer a quick and user-friendly way of generating kinase activity estimates from highthroughput phosphoproteomics datasets. Availability and implementation: the KSEA App is a free online tool: casecpb.shinyapps.io/ksea/. The source code is on GitHub: github.com/casecpb/KSEA/. The application is also available as the R package 'KSEAapp' on CRAN: CRAN.R-project.org/package¼KSEAapp/. Contact: mark.chance@case.edu Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Alfonso Valencia    1 Introduction\r\n  An understanding of kinase regulation is crucial for understanding many different biological signaling processes, the molecular pathogenesis of many diseases, and their potential reversals by kinasealtering therapies. Mass spectrometry-based phosphoproteomics has emerged as the leading high-throughput platform for measuring the identities and intensities of thousands of phosphopeptides simultaneously  (Olsen and Mann, 2013) . Consequently, there is a growing interest in generating bioinformatic tools to distill these highly complex datasets into biologically meaningful inferences of kinase activity changes. Currently available applications that offer such analyses include IKAP  (Mischnik et al., 2016) , KinasePA  (Yang et al., 2016) , CLUE  (Yang et al., 2015)  and KEA  (Lachmann and Ma'ayan, 2009) , now updated as KEA2. However, the present implementation of IKAP is platform-specific, KinasePA and CLUE are limited to multi-condition studies, and KEA is focused on substrate overrepresentation rather than kinase scoring.  As an alternative, Kinase-Substrate Enrichment Analysis (KSEA) scores each kinase based on the relative hyper-phosphorylation or dephosphorylation of the majority of its substrates, as identified from phosphosite-specific Kinase-Substrate (K-S) databases. The negative or positive value of the score, in turn, implies a decrease or increase in the kinase's overall activity relative to the control. Unfortunately, while KSEA offers a concise and easily interpretable scoring system, its accessibility remains restricted to programming experts, as the original source code was never released. Thus, this method has not been widely implemented due to the lack of a userfriendly tool. To make KSEA available to the greater scientific community, we present a web-based implementation: the KSEA App. This online tool is designed for users with wide-ranging backgrounds who wish to identify and visualize kinase-level annotations from their quantitative phosphoproteomics datasets. We hope that this application would allow KSEA to become a routine, if not standard, analysis approach for phosphoproteomics.    2 Materials and methods\r\n   2.1 KSEA algorithm overview\r\n  The KSEA formula was previously described  (Casado et al., 2013) . Assume that we are given a phosphoproteomics dataset with test and control samples, in which the fold change (FC) between test and control is computed for each phosphosite. As defined previously, the kinase's normalized score is calculated as follows: ffiffiffiffi score ¼ ðs pdÞpm  Here, s denotes the mean log2(FC) of known phosphosite substrates of the given kinase, p represents the mean log2(FC) of all phosphosites in the dataset, m denotes the total number of phosphosite substrates identified from the experiment that annotate to the specified kinase, and d denotes the standard deviation of the log2(FC) across all phosphosites in the dataset. This formula is based on a z-score transformation, and we assume that the resulting scores (denoted as 'z-score' in the KSEA App outputs) are normally distributed. Subsequently, the P-value is determined by assessing the one-tailed probability of having a more extreme score than the one measured, followed by a Benjamini-Hochberg FDR correction for multiple hypothesis testing.  Interpretation of Results: The score of a kinase is based exclusively on the collective phosphorylation status of its substrates. Sites on the kinase itself are disregarded. For a FC ¼ test/control, a kinase with a negative score has substrates that are generally dephosphorylated with the test group. This kinase, in turn, has decreased activity output in the test condition and is deemed downregulated. The inverse is true for positive scores.    2.2 Kinase\u2013Substrate (K\u2013S) dataset\r\n  To identify the substrates for each kinase, the KSEA App sources K-S annotations from PhosphoSitePlus (PSP)  (Hornbeck et al., 2012)  and from NetworKIN  (Linding et al., 2007) . PSP annotations are curated, and our current implementation uses annotations that are restricted to human proteins from the July 2016 release. In contrast, NetworKIN offers predicted relationships, which we downloaded as pre-computed data (calculated against human ENSEMBL version 59) from the KinomeXplorer-DB website  (Horn et al., 2014) . By default, the KSEA App utilizes the PSP resource alone. However, users have the option to include NetworKIN annotations, and they can adjust the threshold on the NetworKIN confidence score to change the inclusion stringency. Additional explanations on the NetworKIN contributions and the KSEA formula are found in the Supplementary file 'Details on Methods.pdf'. Since the KSEA App utilizes downloaded K-S sources, we will annually release newer versions as the databases get updated.    2.3 Implementation\r\n  This KSEA App version 1.0 is hosted on the shinyapps.io server as a free online tool: https://casecpb.shinyapps.io/ksea/. The source code and local access details are found in https://github.com/casecpb/KSEA/. The User Manual, found within both sites, offers comprehensive instructions. Alternatively, this tool is available as the R package 'KSEAapp' in  CRAN: https://CRAN.R-project.org/package¼KSEAapp/, and the source code details are found in https://github.com/casecpb/KSEAapp/.     3 Results\r\n   3.1 Performance evaluation: KSEA without NetworKIN\r\n  KSEA was applied to a published quantitative phosphoproteomics dataset that studied MEK inhibition in lung adenocarcinoma  (Kim et al., 2016) . We restricted the analysis to the A549 cell line differentially treated with selumetinib (AZD-6244), a highly selective MEK1/2 noncompetitive inhibitor, vs. DMSO control. As a start, K-S relationships were based solely on annotations from the PSP database. Fold changes were calculated by the ratio of selumetinib/ DMSO so that a negative kinase z-score represents relative collective dephosphorylation of substrates with selumetinib.  Following KSEA analysis, MEK1 (gene name: MAP2K1) exhibited statistically significant downregulation, along with some decrease in ERK1 (MAPK3), ERK2 (MAPK1) and RSK1 (RPS6KA1) signaling, as reflected in the negative z-scores (Supplementary Fig. S1, Table S1A). These results are consistent with (i) published enzymatic assays that demonstrated decreased MEK activity with selumetinib  (Yeh et al., 2007)  and (ii) predicted blunting of effector kinases downstream of MEK, based on the canonical MAPK signaling pathway  (Anjum and Blenis, 2008; Roberts and Der, 2007) . These findings suggest that the KSEA App correctly identifies key kinase perturbations from this phosphoproteomics dataset.    3.2 Performance evaluation: KSEA 1 NetworKIN\r\n  Since only a small fraction of experimentally identified phosphosites have documented K-S annotations in PSP, the majority of the phosphoproteomics dataset could not be used in the previous KSEA calculations. Thus, to maximize the number of the phosphosites from the input that can be used in the calculations, we expanded the K-S database to include relationships predicted by NetworKIN. We then reran KSEA against this supplemented database with a NetworKIN score minimum of 5. This adjustment nearly doubled the total usable phosphosite count from 676 to 1327, which resulted in an improved 18% coverage. Furthermore, the number of scored kinases increased from 175 to 235. Permutation tests on the NetworKIN annotations are highlighted in Supplementary Figure S3.  Even with a large influx of new K-S predictions, many of the KSEA þ NetworKIN results remained consistent with the previous findings. The MAPK signaling nodes that were downregulated in the earlier analysis retained the same directionality (Supplementary Fig. S2, Table S1C). More interestingly, however, EGFR showed statistically significant increase in activity output with the NetworKIN addition (Supplementary Fig. S2, Table S1C), whereas it did not meet the P-value cutoff before (Table S1A). This is due to the recruitment of 4 predicted EGFR substrates that exhibited strong hyperphosphorylation with drug (Table S1D). This finding, along with upregulation of PDPK1 protein, is consistent with previous observations that noted enhanced phosphorylation through the EGFRPDPK1-AKT axis  (Kim et al., 2016) . Overall, based on our case study, the NetworKIN predictions improve phosphosite coverage and may boost the scores of kinases with few curated substrates in PSP.     Acknowledgements\r\n  The authors thank Dr. Jean-Eudes Dazard (for help in the R package development) and the Goutham Narla research group, especially Caitlin O'Connor and Sarah Taylor (for testing the KSEA App).    Funding\r\n  This work has been supported by the National Institutes of Health [1R01GM117208-01AI, P30-CA-043703, UL1TR000439 and TL1 TR000441].  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/casecpb/KSEA",
    "publicationDate": "0",
    "authors": [
      "Danica D. Wiredja",
      "Mehmet Koyutu¨ rk",
      "Mark R. Chance"
    ],
    "status": "Success",
    "toolName": "KSEA",
    "homepage": ""
  },
  "13.pdf": {
    "forks": 1,
    "URLs": ["github.com/VTcbil/FASP"],
    "contactInfo": ["lintian@ucdavis.edu"],
    "subscribers": 2,
    "programmingLanguage": "Java",
    "shortDescription": "Functional AStrocyte Phenotyping (FASP) is a Fiji plugin that automatically analyzes and characterizes the spatiotemporal functional status of astrocytes from time-lapse Ca2+-fluorescence microscopy imaging data. ",
    "publicationTitle": "Automated Functional Analysis of Astrocytes from Chronic Time-Lapse Calcium Imaging Data",
    "title": "Automated Functional Analysis of Astrocytes from Chronic Time-Lapse Calcium Imaging Data",
    "publicationDOI": "10.3389/fninf.2017.00048",
    "codeSize": 101951,
    "publicationAbstract": "Recent discoveries that astrocytes exert proactive regulatory effects on neural information processing and that they are deeply involved in normal brain development and disease pathology have stimulated broad interest in understanding astrocyte functional roles in brain circuit. Measuring astrocyte functional status is now technically feasible, due to recent advances in modern microscopy and ultrasensitive cell-type specific genetically encoded Ca2+ indicators for chronic imaging. However, there is a big gap between the capability of generating large dataset via calcium imaging and the availability of sophisticated analytical tools for decoding the astrocyte function. Current practice is essentially manual, which not only limits analysis throughput but also risks introducing bias and missing important information latent in complex, dynamic big data. Here, we report a suite of computational tools, called Functional AStrocyte Phenotyping (FASP), for automatically quantifying the functional status of astrocytes. Considering the complex nature of Ca2+ signaling in astrocytes and low signal to noise ratio, FASP is designed with data-driven and probabilistic principles, to flexibly account for various patterns and to perform robustly with noisy data. In particular, FASP explicitly models signal propagation, which rules out the applicability of tools designed for other types of data. We demonstrate the effectiveness of FASP using extensive synthetic and real data sets. The findings by FASP were verified by manual inspection. FASP also detected signals that were missed by purely manual analysis but could be confirmed by more careful manual examination under the guidance of automatic analysis. All algorithms and the analysis pipeline are packaged into a plugin for Fiji (ImageJ), with the source code freely available online at https://github.com/VTcbil/FASP.",
    "dateUpdated": "2017-04-24T19:58:10Z",
    "institutions": [
      "Virginia Polytechnic Institute and State University",
      "Co-first author",
      "University of California Davis School of Medicine",
      "Pennsylvania State University",
      "National Institute of Standards and Technology"
    ],
    "license": "No License",
    "dateCreated": "2016-05-25T14:12:40Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     10.3389/fninf.2017.00048   Automated Functional Analysis of Astrocytes from Chronic Time-Lapse Calcium Imaging Data     Yinxue Wang  0  1  4    Guilai Shi  1  2  4    David J. Miller  3  4    Yizhi Wang  0  4    Congchao Wang  0  4    Gerard Broussard  2  4    Yue Wang  0  4    Lin Tian  lintian@ucdavis.edu  2  4    Guoqiang Yu  0  4    0  Bradley Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University ,  Arlington, VA ,  United States    1  Co-first author    2  Department of Biochemistry and Molecular Medicine, University of California Davis School of Medicine ,  Davis, CA ,  United States    3  Department of Electrical Engineering, School of Electrical Engineering and Computer Science, Pennsylvania State University ,  University Park, PA ,  United States    4  Edited by: Hanchuan Peng, Allen Institute for Brain Science, United States Reviewed by: Bing Ye, University of Michigan, United States Chung-Chuan Lo, National Tsing Hua University, Taiwan Peter Bajcsy, National Institute of Standards and Technology ,  United States Lin Tian Guoqiang Yu     2017   11   Recent discoveries that astrocytes exert proactive regulatory effects on neural information processing and that they are deeply involved in normal brain development and disease pathology have stimulated broad interest in understanding astrocyte functional roles in brain circuit. Measuring astrocyte functional status is now technically feasible, due to recent advances in modern microscopy and ultrasensitive cell-type specific genetically encoded Ca2+ indicators for chronic imaging. However, there is a big gap between the capability of generating large dataset via calcium imaging and the availability of sophisticated analytical tools for decoding the astrocyte function. Current practice is essentially manual, which not only limits analysis throughput but also risks introducing bias and missing important information latent in complex, dynamic big data. Here, we report a suite of computational tools, called Functional AStrocyte Phenotyping (FASP), for automatically quantifying the functional status of astrocytes. Considering the complex nature of Ca2+ signaling in astrocytes and low signal to noise ratio, FASP is designed with data-driven and probabilistic principles, to flexibly account for various patterns and to perform robustly with noisy data. In particular, FASP explicitly models signal propagation, which rules out the applicability of tools designed for other types of data. We demonstrate the effectiveness of FASP using extensive synthetic and real data sets. The findings by FASP were verified by manual inspection. FASP also detected signals that were missed by purely manual analysis but could be confirmed by more careful manual examination under the guidance of automatic analysis. All algorithms and the analysis pipeline are packaged into a plugin for Fiji (ImageJ), with the source code freely available online at https://github.com/VTcbil/FASP.    astrocyte  astrocyte activity  functional phenotype  calcium dynamics  time-lapse calcium image   signal propagation       INTRODUCTION\r\n  Astrocytes, which constitute nearly half the volume of the adult human brain, have long been considered to play only passive roles in the central nervous system, such as supplying trophic factors, maintaining ion homeostasis, and serving as an inert scaffold. In recent years, active roles in regulating various aspects of neuronal function have been identified  (Agulhon et al., 2008; Khakh and Sofroniew, 2015; Bazargani and Attwell, 2016) . Neuron-astrocyte communication at synapses regulates synaptic transmission and plasticity, breathing, memory formation, motor function, and sleep, and is implicated in many neuropsychiatric disorders  (Haydon, 2001; Volterra and Meldolesi, 2005; Halassa and Haydon, 2010; Clarke and Barres, 2013) . Astrocytes interact with synapses through release of soluble factors driven by intracellular Ca2+ elevations  (Agulhon et al., 2008; Haustein et al., 2014)  and, as a result, the Ca2+ dynamics are the best established correlates of the excitatory state and functional readout of astrocytes. With the convergence of recent advances in both modern microscopy and ultrasensitive cell-type specific genetic encoded calcium indicators  (GECI; Knöpfel and Boyden, 2012; Broussard et al., 2014) , it is now possible to conduct chronic optical imaging to record activities of a large number of astrocytes with high spatial and temporal resolution  (Srinivasan et al., 2015) , resulting in overwhelmingly large data sets and making manual analysis prohibitive. Note that the term \u201cchronic imaging\u201d used in this paper is essentially equivalent to \u201clong-term imaging\u201d which is also widely used in literature. However, effective automatic computational tools for analyzing functional astrocyte data have lagged far behind the capability of generating large-volume Ca2+ imaging data.  Quantifying the functional status of astrocytes here relies on three core analytical tasks: (1) identifying astrocytic functionally independent units (FIUs), (2) estimating the characteristic curves of Ca2+ dynamics, and (3) extracting functional features of astrocytes or astrocytic FIUs. The detailed definition of FIU and characteristic curve is referred to Section Problem Statement and Formulation.  Analyzing functional astrocyte data is challenging due to the complex nature of astrocyte Ca2+ signaling. First, one single astrocyte may contain multiple FIUs due to calcium compartmentalization, varying from large areas in soma to small and local microdomains in processes, which have different Ca2+ activity patterns. Thus, it is infeasible to use cellular information such as nucleus to identify FIUs. Second, Ca2+ elevations do not occur simultaneously in an FIU, but propagate as a Ca2+ wave within an astrocyte or between astrocytes with complex speed and direction patterns  (Fiacco and McCarthy, 2006) . Examples are shown in Figures 1B,C. The time delay between two parts in the same FIU can be larger than the duration of an event  (Figure 1C; Fiacco and McCarthy, 2006) , so it is impossible to eliminate time lags within an FIU by downsampling. As an intrinsic property of astrocyte Ca2+ signaling, signal propagation is quite prevalent  (Fiacco and McCarthy, 2004, 2006; Matyash and Kettenmann, 2010) . In our real data of in vitro human astrocyte induced pluripotent stem cells (hiPSCs), we found &gt;90% astrocytic FIUs having discernable propagation. Thus, any modeling effort must explicitly take into account the propagation.  As a similar but relatively better studied type of cellular excitation indicator, neuronal Ca2+ spikes also propagate, but the propagation is generally so fast that the signal can be regarded as synchronized, that is to say, any time lag between Ca2+ elevations in two parts of a cell is shorter than the temporal resolution of imaging. Consequently, the neuronal Ca2+ signal pattern can be considered as homogeneous in a cell from an image analysis point of view  (Mukamel et al., 2009) . Third, the morphological features of FIUs, such as size and shape, are irregular and heterogeneous, so morphology cannot be used for detection. Last but not least, heterogeneous expression of protein indicators and other factors give rise to a wide range of signal to noise ratios (SNRs) in the same field of view  (Knöpfel and Boyden, 2012) ; and in a considerable proportion of FIUs, the Ca2+ signals have low SNR, due to the fundamental limitations of living Ca2+ imaging such as the existence of auto-fluorescence and imperfect selectivity of the calcium indicator  (Myers, 2012) . The low SNR demands sophisticated modeling to maximize information extraction and a rigorous statistical framework to accurately control the false positive rate. These challenges significantly increase the difficulty in repurposing existing methods designed for other dynamic imaging data, such as neuronal Ca2+ data, and necessitate development of new approaches.  Since complete astrocyte Ca2+ data is new, there are very few analytical tools specifically designed for astrocytes. The only two scripts to our knowledge are GECIquant  (Srinivasan et al., 2015)  and CaSCaDe  (Agarwal et al., 2017) . However, both scripts are \u201csemi-automated,\u201d involving significant manual effort which is prone to operator bias and variation. GECIquant requires manually setting multiple parameters, adjusting thresholds, and drawing polygons, while CaSCaDe needs manually labeling thousands of ROIs for training. In contrast, the large-scale data we are modeling requires a fully automated algorithm.  In addition, to identify FIUs, GECIquant projects a raw image stack into a single map of pixel-wise temporally maximum intensity and then binarizes it using a user-given threshold, losing the rich information of temporal patterns and confounding active units with silent cells that have high baseline intensity.  Similarly, CaSCaDe projects the image stack into a map by summing up all the intensities and uses a threshold to binarize the map. Most critically, even though both were designed for astrocyte data, signal propagation were neither modeled nor quantified.  More broadly, from a technical point of view, our problem falls under the general category of time-lapse Ca2+ image analysis. To date, a handful of algorithms have been developed for analyzing neuronal Ca2+ imaging data  (Reidl et al., 2007; Mukamel et al., 2009; Smith and Häusser, 2010; Valmianski et al., 2010; Andilla and Hamprecht, 2013; Diego et al., 2013; Pachitariu et al., 2013; Kaifosh et al., 2014; Maruyama et al., 2014; Soelter et al., 2014; Pnevmatikakis et al., 2016) . However, none of them can be applied to astrocytic Ca2+ data due to the specific challenges mentioned above. For more detailed discussion, see Section Necessity of Specific Tools for Analyzing Time-Lapse Astrocyte Ca2+ Imaging Data.  Here, we formulate a statistical model for astrocyte Ca2+ dynamics imaging data, and propose an integrated suite of algorithms, Functional AStrocyte Phenotyping (FASP), to simultaneously identify astrocytic FIUs, extract their functional features, and further characterize the functional status. The flowchart of FASP is shown in Figure 2. FASP possesses several unique features. First, recognizing that the core tasks-the identification of FIUs, the extraction of the corresponding characteristic curves and the estimation of propagation patterns-are mutually dependent, FASP addresses them in a unified way by building an integrated probabilistic model.  Second, FASP is data-driven, learning model parameters using machine learning techniques without constraints on the form of the characteristic signals, the morphological patterns of FIUs, the spatial distribution/sparsity of FIUs, or the total number of FIUs. Thus, it can flexibly account for various Ca2+ events including waves and microdomain fluctuations, heterogeneous morphology, a large range of unit sizes and spatial intensities of FIUs, and, critically, it does not require a pre-assumed total number of FIUs. Third, FASP models Ca2+ propagation explicitly, dealing with cases with or without intracellular propagation phenomena. This explicit modeling not only contributes to faithfully identify FIU, but also enables extracting characteristic features related to propagation patterns. Fourth, FASP takes full advantage of spatial structural information to facilitate learning and enhance performance. Lastly but very importantly, it is deeply probabilistically principled. By judicious application of various statistical theories, FASP accurately distinguishes signals from noises and controls false positive rate, which is essential for analyzing noisy data. FASP also confers tuning parameters with probabilistic meaning, greatly facilitating usability of parameter setting.  We evaluated FASP on both synthetic data simulating Ca2+ signaling and real data of in vitro human astrocyte induced pluripotent stem cells (hiPSCs). The quantitative evaluation demonstrated FASP's effectiveness, flexibility, and robustness. Compared to a manual ROI drawing, FASP generated more accurate contour and detected quite a few otherwise missed activities. The experimental comparison between FASP and a representative of neuron-targeted methods that assume in-FIU synchronization  (Mukamel et al., 2009)  validated the necessity of developing methods specifically modeling intracellular propagation, and also showed the superior performance of FASP for this task. To further validate our method, we applied FASP to study agonist-induced Ca2+ activities in in vitro rat astrocytes. FASP successfully detected the induced FIUs responding to three known agonists for astrotytic Ca2+ signaling: ATP, glutamate and 3,5-dihydroxyphenylglycine (3,5-DHPG). Many quantitative features and patterns of the astrocytic activities revealed by FASP are consistent with what have been reported in the literature.    METHODS\r\n    Problem Statement and Formulation\r\n  For convenience of discussion, we consider time-lapse 2D imaging data, which can be easily extended to the 3D imaging case. We formulate the astrocyte Ca2+ data as a 3-dimensional array Y[i, j, t], where i ∈ {1, 2, . . . , Dh} is the horizontal spatial index, j ∈ {1, 2, . . . , Dv} is the vertical spatial index, and t ∈ {1, 2, . . . , Dt} the temporal index. Define an astrocytic FIU as a group of spatially connected pixels sharing a characteristic Ca2+ temporal pattern with possibly different temporal phases and noises. Define the characteristic curve of an astrocytic FIU as the core time series of fluorescence intensity F shared by pixels in the FIU, from which the Ca2+ dynamics quantified as1F/F0 = (F − F0) /F0 can be obtained, where F0 is the baseline fluorescence. We model the fluorescence microscopy imaging data of astrocyte Ca2+ dynamics Y[i, j, t] consisting of M FIUs as  Y i, j, t = β0 i, j +  XM  m=1 βm i, j Xm t − τij + ε[i, j, t], (1) kXmk2 = 1, E [Xm] = 0, for m = 1, 2, . . . , M s. t.  PmM=1 I βm i, j 6= 0 ≤ 1, for i = 1, 2, . . . , Dh, j = 1, 2, . . . , Dv    c (G (βm)) ≤ 1, for m = 1, 2, . . . , M where β0 i, j is a constant associated with each pixel i, j ; βm i, j is the coefficient for the mth FIU; Xm is the characteristic curve of the mth FIU; τij is the time lag of i, j 's time-intensity curve with respect to the characteristic curve, as a result of calcium signal propagation; and ε i, j, t is an independent Gaussian noise which, given any pixel i, j , has zero mean and a common variance σ02 i, j across different time points t. Note that image noise is often non-Gaussian with intensitydependent variance, but we can apply variance stabilization technique to the data to satisfy the assumption of Gaussian noise with common variance  (Starck et al., 1998; Foi et al., 2008) . The constraint kXmk2 = 1 removes the model nonidentifiability arising from multiplying Xm by a factor and dividing βm by the same factor. I (·) is an indicator function and the constraint PmM=1 I βm i, j 6= 0 ≤ 1 ensures that each pixel will be associated with at most one FIU. c (G (βm)) is the number of connected components in the graph induced by the non-zero βm i, j with the edges derived from the pixel neighborhood structure. The constraint c (G (βm)) ≤ 1 ensures all pixels associated with the same FIU are connected. Model (1) assumes no cell migration or deformation, or that the data is preprocessed by migrating cell tracking and registration techniques.  We primarily concentrate upon three core tasks in analyzing astrocyte Ca2+ data, especially the first two: (1) the identification of FIUs and (2) the estimation of the corresponding characteristic curves; and (3) the extraction of biologically interesting quantitative features of astrocyte functional status. Based on model (1), the first two tasks can be addressed by learning the parameters βm i, j and Xm [t] , for m = 1, 2 . . . , M. The mth FIU is the collection of all pixels with non-zero βm; the corresponding characteristic curve can be obtained directly from Xm. Hence, in this report we focus on the model learning. Note that automatically determining the number of FIUs, M, is an important part of learning, and that accurate estimation of τi\u2032js is necessary for the correct estimation of Xm. Put simply, the problem is to learn all parameters, β0 i, j , βm i, j , Xm[t], τij and M from the observed data Y i, j, t . This is a mathematically challenging problem, because the number of parameters is huge and all parameters interact in a highly non-linear way to give rise to the observed data. In the following, we will discuss how a bunch of advanced probabilistic and machine learning techniques are integrated as FASP to solve the problem.    Overview and Design Principles of FASP\r\n  Our method, FASP, addresses the problem using three major modules (Figure 2). First, it distinguishes \u201cactive regions\u201d from inactive background. In this paper, we call a pixel an \u201cactive pixel\u201d if it has a non-zero βm[i, j] for some m in model (1), and call a region an \u201cactive region\u201d if it contains only active pixels and all its neighboring pixels are \u201cinactive,\u201d or, not active pixels. Second, given each active region, FASP sequentially identifies distinct FIUs within the active region one by one and estimates the corresponding characteristic curves. It also automatically determines the number of FIUs. Finally, it performs quality control by automatic post processing and proofreading, and computes various quantitative features to characterize the functional status of astrocytes.  Learning model (1), or jointly estimating βm, Xm, and τij with multiple indices m, on the entire field of view simultaneously will be very time-consuming and may be prone to severe overfitting. As a critical part to make the whole strategy feasible, we designed two schemes to combat this problem. Firstly, we designed a special map of neighborhood correlation so that FASP can, in its first module, detect active regions, without needing to explicitly specify the unknown m for any pixel. As outputs of the first module, we obtain a set of active regions. Secondly, in a given active region, different pixels may have non-zero βm i, j for different m: multiple FIUs may be spatially connected and hence pixels of them may be in the same active region, which constantly occurs in astrocyte Ca2+ data. So we resort to a sequential approach to identify the FIUs in the same region one by one. In this multistep framework, by constraining the model learning problem to active regions only and using sequential identification, the parameter search space becomes much smaller than the original space. In this way, FASP successfully mitigates the computational burden and relieves the potential overfitting problem.  In addition, there are other two critical principles for modeling astrocytic FIUs. First, fully utilizing spatial neighborhood information could effectively assist the model learning. According to the definition of FIUs, the spatial distribution of an FIU's pixels is localized, that is, pixels belonging to the same FIU must be spatially connected. Besides, neighboring pixels in an FIU should be similar in terms of signal pattern. We expect that incorporating neighborhood information will lead to higher statistical power to detect active regions and FIUs, compared with inspecting each pixel individually. Second, due to the nature of Ca2+ imaging, large noises (small signal-to-noise ratio) are typically observed and challenging the performance of analysis. In such cases, probabilistic principles are needed to enable information extraction under great uncertainty. We embrace the probabilistic principles in all steps of algorithm development. A set of statistical tools are exploited-maximum likelihood estimation for characteristic curve learning, Bayesian decision theory for distinguishing adjacent but different FIUs, Fisher's transformation and order statistics theory for hypotheses testing and error control. This principle is also crucial for practical use, as it allows end users to determine the number of FIUs and to anticipate the false positive rate. The statistical framework also helps to endow parameters with either physical or probabilistic meaning, which makes parameter tuning more objective.    Module 1: Detecting Active Regions\r\n  In order to constrain the parameter search space and thus facilitate the model learning, we first detect active regions. From a different point of view, in this module, FASP identifies all background or inactive pixels, and estimates their βms to be zero for all m, with βms of active pixels and Xms kept unknown. This is feasible owing to the fact that any two active pixels sharing the same characteristic curve Xm are expected to have higher correlation than those which do not, so the correlation between neighboring pixels alone is sufficient for inferring active regions. Therefore, in this module, we first build a map of neighborhood correlation in which the value at each pixel is basically the correlation between this pixel's intensity time series with its neighboring pixels'. The formulation of neighborhood correlation score is given in Section Building a Map of Correlation between Neighboring Pixels. Then the problem of detecting active regions is translated into a problem of binarizing this neighborhood correlation map, distinguishing true correlation between signals from random correlation due to noises.  In applications with low-SNR, probabilistic models and statistical approaches often play positive roles in suppressing the impact of large noises. Besides, significance assessment is often critically important for biomedical applications, as we care much about false positives, but no existing image segmentation method to our knowledge gives significance assessment. Thus, we aim to build a probabilistically principled strategy for detecting active regions, or, segmenting the neighborhood correlation map. To this end, we construct a z-score map from the correlation map (see Section Constructing a Z-Score Map of Neighborhood Correlation), endowing the correlation scores explicit probabilistic meaning. Any background pixel should have a correlation z-score following a standard Gaussian distribution (the null distribution), while active pixels are expected to have statistically significantly large positive z-score. This transformation also stabilizes the variance, which means to make the variance of score independent from its magnitude, assuring that all pixels will be treated equally in the following analysis.  Having got the correlation z-score map, rather than testing each pixel in isolation (e.g., do thresholding to the z-score), FASP evaluates a set of connected pixels as a whole so that the cumulative information in spatial neighborhoods is fully utilized to enhance confidence in decisions. We develop a region growing algorithm, starting from a singleton containing only one pixel, known as \u201cseed,\u201d and iteratively growing the region, or pixel set, until it reaches the greatest statistical significance. These procedures are repeatedly conducted till all qualified seed pixel singletons are labeled as searched. The main difficulty and a key technique here lies in how to assess the statistical significance of a region, since the operations that FASP does to enlarge the region in each iteration introduce some dependency structure among the pixels. Realizing that such a structure can be modeled using order statistics, we design a region-level hypothesis test based on the theory of order statistics, which is one of the major innovations of our work. We will introduce this region growing method in detail in Section Active Region Detection on Z-Score Map by Order-Statistics Guided Hypothesis Testing, and compare it with three other popular segmentation methods in Section The Binarization Method Based on Order-Statistics Theory is Effective.   Building a Map of Correlation between Neighboring\r\n    Pixels\r\n  We define the neighborhood correlation at pixel [i, j] as rsc i, j = cor Y[i, j, :], Y¯ nb[i, j, :] (2) where cor (, ) is the Pearson correlation coefficient between two curves; Ynb[i, j, :] is the average curve of the eight directneighbor pixels of [i, j]. Inspection of real data confirmed that the neighborhood correlation at pixel [i, j] indicates how active this pixel is. For example, comparing Figures 3A,B, we can see that pixels in FIUs have much higher neighborhood correlation than the pixels out of FIUs.  One may notice that we intentionally ignore the time shifts between neighboring pixels. Although, for two arbitrary pixels in the same FIU the time shifts can be quite different due to propagation, we expect that the time shifts between neighboring pixels are minor and can be ignored in computing the average curve and the correlation coefficient. For cases where SNR is high while propagations are very slow (time shifts between neighboring pixels are large), we also designed an alternative neighborhood correlation score to explicitly model time shifts and thus better deal with the Ca2+ propagation phenomena, with a price of losing some power to suppress noises. More discussion and comparison of the two definitions are given in Section Alternative Design of Neighborhood Correlation Score.    Constructing a Z-Score Map of Neighborhood\r\n    Correlation\r\n  We make a monotonic transformation to each pixel in the neighborhood correlation map: zpx i, j = F rsc i, j = √N − 3 2 ln 1 + rsc i, j ! 1 − rsc i, j (3) where F (·) denotes the normalized Fisher transformation  (Fisher, 1915, 1921) , and N is the total number of time points.  According to the properties of the Fisher transformation, zpx i, j asymptotically follows a standard Gaussian distribution under the null hypothesis that the pixel [i, j] has zero βm for all m, that is, i, j is not associated with any FIU. If the pixel i, j belongs to one FIU, the time curve at the pixel i, j will be correlated with the ones at its neighboring pixels because of the spatial localization property of FIU. As a consequence, rsc i, j is expected to be positive. Due to the monotonicity of the normalized Fisher transformation, zpx i, j is thus expected to be larger than 0. Note that the normalized Fisher transformed score always follows Gaussian distributions with constant variance of 1, regardless of the expected value of the score, or in other words, no matter in the background or in FIUs. The chosen transformation facilitates subsequent probabilistic analysis owing to its effect of variance stabilization and the good properties of Gaussian distributions.    Active Region Detection on Z-Score Map by\r\n    Order-Statistics Guided Hypothesis Testing\r\n  Having obtained a score map zpx computed as in Equation (3), FASP binarizes the map into foreground (active regions) and background (inactive regions) using a region growing strategy. Each round of region growing process starts from a singleton{a0}, where the seed pixel a0 has the largest zpx among the remaining pixels that have not been labeled as \u201csearched.\u201d Then the region (pixel set) iteratively grows. In each iteration, from the set of directly neighboring pixels of the current region we select a subset and attach it to this region, such that the resulting new region's statistical significance (given by a hypothesis test defined in the following paragraphs) is increased from the current region as much as possible. The growing stops if the termination criterion is reached, that is, the region's statistical significance no longer increases as this region continues to grow. It can be expected that, being expanded from a seed pixel, the most statistically significant region does not contain silent background pixels, because inclusion of silent pixels moves the test statistics toward the null. Once a round of region growing process is finished, another round is started as long as any pixel remained not labeled as \u201csearched.\u201d Finally the union of all found regions forms the map of active regions.  Both the selection of neighboring pixels and the termination criterion rely on the assessment of statistical significance of regions. Given a set of pixels, the null hypothesis here states that all pixels in this set are inactive pixels, in other words, their zpxs all approximately follow a standard Gaussian distribution. Thus, the alternative hypothesis is that some pixels in this region are active pixels and their zpxs tend to be positive. Note that a pixel set containing both active and inactive pixels is expected to have lower significance than its subsets containing active pixels only. As a consequence, the procedure of always finding the maximum significance rules out such cases.  Under the null hypothesis, the pixel-level z-scores zpx of all pixels in a region can be considered as independently standardGaussian distributed. However, the average zpx in each candidate region in the region growing process does not follow the Gaussian distribution suggested by the central limit theorem, because these pixels are conditioned on the operation of region growing and thus not independent anymore. The conditioning causes biases in both mean and variance of the average zpx. Indeed, if these pixels are still approximately treated as independent standard Gaussian noises under the null hypothesis, the significance assessment can be quite straightforward using the central limit theorem. For comparison, we also experimented with the same region growing procedure but assessed significance using the central limit theorem, but it resulted in an endlessly expanding web of thin strings in simulation studies when no active regions were embedded.  Hence, a more sophisticated approach is needed to assess the statistical significance for each candidate active region. One critical innovation in the proposed framework is the realization that the biases and dependency structure come from the ranking and selection operation in the region growing procedure and the significance can thus be modeled by order-statistics theory  (David and Nagaraja, 2003) . Denote the current region A = a1, a2, . . . , anA and the surrounding set of pixels B = b1, . . . , bnB . A candidate region C has the form C = A ∪ B\u2032 , where B\u2032 is a subset of B. We have a score for the region C = c1, c2, . . . , ck, . . . , cnC as,  1 s (C) = √nC  Xkn=C1 zpx [ck] where zpx [ck] is the z-score of neighborhood correlation of pixel ck. Let us sort increasingly all pixels from A ∪ B by their zpx scores and record the ascending orders of elements of C as J. Thus, J k denotes the ascending order of pixel ck in A ∪ B regarding zpx. Based on the asymptotic distribution of linear combination of order statistics, known in the orderstatistics theory  (David and Nagaraja, 2003) , the score s (C) can be approximated by a normal distribution with the following mean E [S (C)] and variance Var (S (C)) ,  E [S (C)] = Var (S (C)) =  Φ−1 (vk), 1 √nC  XnC  k=1 2 nC (nA + nB)  vk2 1 − vk1 φ Φ−1 vk1 φ Φ−1 vk2  , XnC k1=1  XnC k2=k1 (4) (5) (6)     Module 2: Identifying FIUs and Extracting\r\n    Characteristic Curves\r\n  As shown in Figure 3 and Supplementary Video 1, it was repeatedly observed that spatially-connected FIUs can have different signal patterns, indicating that one active region can contain multiple FIUs. Identifying FIUs from an active region requires estimating all Xms that have non-zero βms in this active region and assigning each pixel to the most likely Xm. Learning all FIUs' parameters in parallel without knowing the number of FIUs in this active region is very time consuming, algorithmically complex, and prone to overfitting. A feasible alternative way is to resort to a sequential approach: in each round we seek for one Xm with non-zero βm at some pixels in the active region, and then determine the spatial area of the mth FIU by finding pixels whose curves are well-explained by Xm. Once an FIU is detected, we inspect its statistical significance: if its p-value is smaller than a given threshold, all pixels in this FIU are removed from the active region, and in the remaining region FASP continues to search for more FIUs until no significant FIU can be found. It is worth noting that, through this sequential strategy, the number of FIUs is determined by setting threshold for the statistical significance, which is easy and intuitive. We leave the significance threshold as a parameter for users to specify. Consequently, users can control the number of FIUs by setting different significance thresholds for claiming a significant FIU.  Thus, the three main components in this module respectively aim to: (1) learn an Xm and its corresponding parameters βm, τ , and σ02 at each pixel in the remaining part of the active region (Section Learning Model Parameters βm, Xm, τ , and σ 2 Associated with the mth FIU); (2) create a zscore map assessing how each pixel is likely to belong to the mth FIU (Section Constructing a Pixel-level Z-Score Map Associated with the mth FIU); (3) identify the mth FIU's pixel set on the z-score map. The third task can be accomplished by re-applying the order statistics guided region growing strategy as discussed previously in Section Active Region Detection on Z-Score Map by Order-Statistics Guided Hypothesis Testing, so here we focus on the former two in this module.   Learning Model Parameters βm, Xm, τ , and σ 2\r\n    Associated with the mth FIU\r\n  FASP employs a sequential strategy to alleviate the burden and drawbacks of simultaneously estimating all FIUs' parameters. However, even within one round of sequential model learning, where we only deal with one FIU, model learning is still a nontrivial problem because the parameters, Xm, βm, τ , and σ02, are all unknown and we have to estimate them at the same time. To tackle this problem, we developed an iterative, alternating approach reminiscent of the expectation-maximization (EM) algorithm  (Dempster et al., 1977; Little and Rubin, 2014)  to learn the model parameters. An Xm is initialized as the time-intensity curve of the pixel with the largest zpx in the region. Denote this initial pixel as i0, j0 . τ i0, j0 is initialized as 0. In each iteration, given a current estimate of Xm, for each pixel [i, j] we update the where vk = J k + 0.5 /(nA + nB), φ (·) is the standard Gaussian PDF and Φ−1 (·) is the standard inverse Gaussian CDF. The essential idea behind (5) and (6) is that a linear combination of ordered variables is asymptotically normally distributed as the number of variables increases. other parameters as follows: τ ∗ i, j = τˆ inb, jnb + argmax1t∈[Uτ ,Uτ ] ˆ  ncor Y i, j, : + τˆ inb, jnb + t , Xˆ m o βˆm∗ i, j = Xˆ mT Y hi, j, : + τˆ∗ i, j i  1 σ 2∗ i, j = T ˆ0  XT t=1  Y hi, j, t + τˆ ∗ i, j i − βˆm∗ i, j Xˆ m 2 (7) (8) (9) where Y i, j, : + τ i, j means the curve Y i, j, : shifted by τ i, j ; τˆ inb, jnb is the estimated time shift of pixel i, j 's neighbor inb, jnb . In Equation (7), FASP basically searches for the best τ i, j that gives the highest correlation between Y i, j, : + τ i, j and Xˆ m. Thanks to the spatial continuity of Ca2+ propagation, the candidate range of τˆ [i, j] can be narrowed to a small window of size 2Uτ around the time shift of the neighbor inb, jnb , thus we actually search for the best 1t ∈ [−Uτ , Uτ ] that leads to the highest correlation between Y i, j, : + τˆ inb, jnb + 1t and Xˆ m. After scanning all pixels in the region and having their βˆm, τˆ and σˆ02 all updated to βˆm∗, τˆ∗ and σˆ02∗, we update Xm as a weighted summary of all n pixels within the region:  \u2032 Xˆ m =  Xn k=1 σˆ02∗ ik, jk βˆm∗[ik, jk]  Y hik, jk, : + τˆ ∗ ik, jk i  ∗ Xˆ m =  Xˆ m\u2032 Xˆ m\u2032 2 .  (10) (11) The parameter updating is iterated until convergence is reached. Here we decide the convergence when the relative change of Xˆ m between two iterations, defined as sd Xˆ ∗m − Xˆ m /sd Xˆ ∗m , is less than a given threshold (sd (·) means the standard deviation over time points, Xˆ ∗m is the estimate in current iteration, and Xˆ m is the estimate in last iteration). The learning algorithm proposed here is experimentally observed to converge robustly. Indeed, in our experiments, we have found that the updating often converges in a dozen or fewer iterations.   Constructing a Pixel-Level Z-Score Map Associated\r\n  with themth FIU Given the estimated parameters associated with the mth FIU, we construct a z-score map within the active region under inspection, where the z-score value at any active pixel indicates how likely this pixel belongs to the mth FIU rather than other FIUs. The goal is to design a feature (score) differentially distributed at pixels in FIU m and at pixels of other FIUs. We expect that pixels in FIU m should have activities that are highly correlated with Xm. Hence, this correlation, or equivalently the value of βm, can be used to determine which pixels belong to FIU m. However, the characteristic curves from different FIUs are often not mathematically orthogonal to each other, suggested by observations from real data (Figure 3C). Consequently, the curve of a pixel belonging to another currently unknown FIU m\u2032 m\u2032 6= m , which is characterized by Xm\u2032 , can also be correlated with Xm due to moderate but non-negligible correlation between Xm and Xm\u2032 . In such a case, by simply assessing goodness of fit to Xm, it is difficult to well-distinguish pixels in FIU m from pixels in FIU m\u2032. In Section Neighborhood Correlation in Residual Signals Enables a Sequential FIU Identification, we have more detailed discussion on this problem. To solve it, here we propose a method to evaluate the confounding effects of other (latent) FIUs without knowing their characteristic curves, which is another important innovation in our work. By combining the measure of goodness of fit to Xm and this measure of confounding effects of other FIUs, FASP can well tell apart FIU m and others.  From the model learning procedure proposed in Section Learning Model Parameters βm, Xm, τ , and σ 2 Associated with the mth FIU, we have obtained Xˆ m and corresponding βˆm, τˆ at each pixel. We realized that, if a pixel is actually in FIU m\u2032 m 6= m\u2032 , the residual Y [t] − βˆ0 − βˆmXˆ m t − τˆ contains not only noise ε but also some true signal that cannot be explained by Xm but can be explained by Xm\u2032 . We further realized that residuals of any two pixels in the same FIU m\u2032 m\u2032 6= m should be correlated as a result of their common correlation to Xm\u2032 , while residuals of pixels in FIU m are independent. This key observation directly leads to the conclusion that the neighborhood correlation in residuals can be used as a measure of confounding effects of other FIUs. More discussion can be found in Section Neighborhood Correlation in Residual Signals Enables a Sequential FIU Identification. Accordingly we designed a special pixel-level score, inspired by Bayesian decision theory and indicating the competition between FIU m and other FIUs, as follows: (fit) zpx  1 i, j = √2 1 √2 F rres i, j Φ−1 Φ2Uτ F rfit i, j − (12) where rfit i, j is the correlation coefficient between pixel i, j 's observed curve and the characteristic curve Xm; rres i, j is the correlation coefficient between pixel i, j 's residual signal and the average residual signal of its eight direct neighboring pixels; Φ (·) and Φ−1 (·) respectively represent the standard Gaussian CDF and its inverse; F (·) denotes the normalized Fisher transformation [the same as in Equation (3);  (Fisher, 1915, 1921) ]; and Uτ &gt; 0 is the half window size of the candidate pool of τ at each pixel. Because rfit i, j is obtained by searching for the optimal τ i, j from the candidate pool, based on the properties of multiple tests, we introduce the transformation in the first term to ensure that zp(xfit) i, j follows a standard Gaussian distribution.  We then apply the region growing as in Section Active Region Detection on Z-Score Map by OrderStatistics Guided Hypothesis Testing to identify pixels associated with the FIU and assess its statistical significance simultaneously.    Refining Characteristic Curves\r\n  We need to refine Xm∗ to remove the imposed constraint of unit L2 norm and to reflect that only pixels in the FIU should contribute to the calculation of the characteristic curve. Hence, denoting the region mask for FIU m as Km we have the final characteristic curve:  Cm = X  βm∗[ik, jk] k∈Km σ 2∗ ik, jk 0  Y hik, jk, : + τ ∗ ik, jk i.  (13)      Module 3: Quality Control and Quantitative\r\n    Characterization of Astrocyte Functional\r\n    Status\r\n  The accompanying software package provides users opportunity to proof-read and edit the results. A number of informative summary statistics are computed and presented to users for further analysis.   Quality Control: Post-processing and Proofreading\r\n  In principle, the proposed algorithm has the power to detect any significant Ca2+ activity, regardless of what pattern it has. However, not all signals are biologically interesting. Prior knowledge guided post-processing helps users control the quality of results, by expert proofreading or computationbased screening of detected FIUs. The automatic screening filters out a candidate FIU if it does not meet some predefined requirements regarding its features obtained from the quantitative analysis illustrated in Quantitative Characterization of Astrocyte Functional Status, such as smoothness of the timeintensity curve, number of peaks or the area of the region.    Quantitative Characterization of Astrocyte Functional\r\n    Status\r\n  On the basis of the learned model of a time-lapse Ca2+ imaging data sample, various quantitative analyses can be conducted to extract features of astrocytic FIUs, which we expect would provide direct and interesting information for biological researches. FASP, by itself, summarizes several basic features as listed below.  Ca2+ signal curves The raw time-intensity curves F (t) = Cm (t) (Equation 13) are firstly transformed into the description of Ca2+ signals: signalto-baseline ratio of fluorescence (1F/F0 = (F − F0) /F0), where the baseline fluorescence F0 is estimated as the 10th percentile of the fluorescence levels (intensities) at all the time points during measurement.  Amplitudes of Ca2+ events The amplitude of a Ca2+ event is calculated as the maximum 1F/F0 during the transient. Ca2+ events are recognized by detecting peaks in the smoothed 1F/F0 curves. Frequency of Ca2+ fluctuations We define the frequency of Ca2+ fluctuations as the inverse of the average duration between two contiguous events. Compared with counting events in a given time period, this definition of frequency ensures more reliable estimation especially in cases where only one or very few events occur during observation, just like what we often see in time-lapse Ca2+ imaging data of astrocytes.  The half time (T0.5) T0.5 of a Ca2+ event is calculated using linear interpolation as the time from peak to half amplitude of an event.  Area of FIUs Area of each FIU is represented in both pixels and μm2. Average spatial propagation velocity of Ca2+ transients in an FIU Based on the estimated time shift of each pixel's observed curve from the characteristic curve, we can locate all the wavefronts of Ca2+ transients in an FIU. Then the intracellular propagation velocity of Ca2+ transients is obtained by estimating the average distance between wavefronts.     EXPERIMENTS AND RESULTS\r\n    Real Astrocyte Calcium Imaging Data\r\n  To validate FASP's effectiveness in analyzing real data, we derived astrocytes in vitro from dissociated primary rat culture and human astrocyte induced pluripotent stem cells (hiPSCs). Cells were infected with lentivirus expressing EF1a-GCaMP6, and imaged under a confocal microscope 3 days after infection. Before imaging, the culture medium was removed, and cells were washed with HBSS (Life Tech, with 2 mM Ca2+/Mg2+) three times. The cells were incubated with HBSS, and imaged using the time series mode (968 ms per image, 2 s interval, for 200 images). Several agonists known to trigger astrocytic Ca2+ signaling  (Kim et al., 1994; Pasti et al., 1997; Bowser and Khakh, 2004; Fiacco and McCarthy, 2004; Jourdain et al., 2007; Hamilton et al., 2008)  were then respectively added to the cells, including ATP, glutamate and metabotropic glutamate receptors (mGluRs). Agonists were added during the first several frames, and incubated for the remaining imaging process.    Synthetic Data\r\n  For the purpose of comprehensively and reliably assessing the performance of FASP, we also generated synthetic data sets that simulate Ca2+ dynamics in astrocytes but allow for large flexibility of parameter setting (see Figure 4 and Supplementary Video 2 for examples). A synthetic sample was constructed by simulating a spatial distribution of FIUs and Ca2+ inactive cells/units, a map of baseline brightness level, characteristic temporal dynamics, intracellular Ca2+ propagation patterns of different FIUs, a map of noise level (variance), and a map of non-zero coefficients βm in each FIU.  The centroids of FIUs and inactive units were all uniformly distributed in the field of view. We set the proportion of FIUs among the overall units as 1/4 to simulate the spontaneously active cell distribution in in vitro Down's syndrome astrocyte iPS cells (Figure 1B). Given the irregular morphology of astrocyte Ca2+ FIUs, as well as the complex baseline brightness patterns and weak edges of astrocytes, basic geometric shapes, and artificial baseline distributions do not reflect reality. Thus, we extracted more than four thousand FIU shape and baseline intensity templates from real-world astrocyte imaging data, and, when generating specific simulated datasets, randomly selected from these templates and resized them randomly. Taking diverse potential applications into consideration, we did not impose a prior assumption on the number and density of FIUs in the field of view, but rather tested different combinations in our simulation experiments.  To generate astrocytic FIU characteristic signal curves, we first randomly chose a series of onset times of Ca2+ transient events, denoted as ti(on), i = 1, 2 . . . nevt. Then the temporal dynamics were modeled by  Mukamel et al. (2009) : t − ti(on) e− t−ti(on) /η · It&gt;ti(on) (14) where It&gt;ti(on) is an indicator function with value 1 for the set nt| t &gt; ti(on)o; η regulates the duration of transients and varies from FIU to FIU. Then XM [t] was rescaled such that the maximum fluorescence signal max 1F0F = [max (XM (t)) − min (XM (t))] / min (XM (t)) is in the range from 0.5 to 4, which is commonly seen in real astrocyte Ca2+ imaging data.  Propagation patterns were modeled by setting a distribution on the time delay τ . In each FIU, assuming the Ca2+ elevation wave starts from a random pixel and expands to the surrounding pixels at a constant velocity, the time delay of the source pixel was set to be 0 while the other pixels' delays were set proportional to their distances from the source pixel. The map of the noise level was set according to the characteristic curve and the average SNR of each FIU. Following the traditional definition in imaging, signal-to-noise ratio (SNR) is here defined as the ratio of the signal magnitude (maximum intensity change) to the standard deviation of the noise, and then measured in decibels (dB) using the industry standard 20 log rule. The average SNR can either be different from FIU to FIU or kept the same across all FIUs in an imaging sample, depending on different experimental objectives. And the map of non-zero βm is designed to reflect the gradually fading strength of activities at the boundaries of FIUs, a critical impact factor for the binarization of z-score maps, which makes the decision of FIU boundaries challenging.  In our simulation experiments, the overall imaging duration was 200 s, with a between-frame time interval of 2 s. Gaussian noises were subsequently added in, in accordance with the given map of noise level.    Performance Metrics\r\n  In our experiments, the following metrics were employed to assess the performance of FASP:   Recall\r\n  Recall is defined as the fraction of ground-truth FIUs that are detected. We say a ground-truth FIU is detected, if there exists one output FIU reported by an algorithm covering more than 50% area of the ground-truth FIU.    Precision\r\n  Precision is defined as the fraction of reported FIUs that are true FIUs. A reported FIU is thought of as true if it covers more than 50% area of one ground-truth FIU and does not cover more than 10% area of any other ground-truth FIUs. If any ground-truth FIU is falsely divided into two or more parts (reported FIUs), this definition ensures that only one reported FIU will be treated as a correct detection.    Fidelity\r\n  As a measure of signal estimation quality, the fidelity of a learned characteristic curve is defined as the Pearson's correlation coefficient between the learned curve and the ground-truth characteristic curve.    Area Accuracy\r\n  Any reported FIU considered as true uniquely indicates one ground-truth FIU. The detection accuracy of the reported FIU is further evaluated by area accuracy, defined as the fraction of the corresponding ground-truth FIU that is covered by this reported FIU.     FASP Can Handle Datasets of\r\n    Heterogeneous and Low SNR Values\r\n  The performance of FASP was evaluated as a function of SNR using simulated data (Figure 5A). For each SNR value, 25 videos were generated with 40 FIUs per video. Ca2+ dynamics were randomly simulated using Equation (14), and the propagation speed was also uniformly sampled from the interval [1, 30] pixels per frame. Since FIUs were independently created, we evaluated detection performance averaging over all FIUs which were considered as individual detection events.  As one would expect, performance improves with increasing SNR. But it is notable that the algorithm works quite well even when the SNR is 5 dB, getting an overall recall (in terms of all FIUs) of 0.941, an overall precision of 0.982, and a mean fidelity of 0.928 with more than 90% of learned characteristic curves having fidelities &gt;0.9. When the SNR is higher than 5 dB, all the three performance scores tend asymptotically to 1. A side note is that on the real data, the typical SNR is between 3 and 10 dB.    FASP Is Robust to Various FIU Spatial\r\n    Patterns and Temporal Activity Patterns\r\n  Astrocyte Ca2+ signaling has very complex spatial and temporal patterns. Firstly, the morphological heterogeneity of astrocytes are commonly observed in studies  (Oberheim et al., 2012) . And since an FIU is usually only a part of an astrocyte, astrocytic FIUs could be even more irregular in terms of shape or size (Figure 1 and Supplementary Video 1). Secondly, though some typical astrocytic Ca2+ transient models have been built, the characteristic curves still heavily differ from each other in overall pattern, especially in different experiments.  FASP makes no assumption about FIUs' morphology, propagation speed, or any parametric model for the characteristic time courses. The morphology of FIUs and curves' patterns are learned from the data. These properties endow FASP with excellent flexibility and stability with these complex impact factors. As the synthetic data were generated using shape templates from real data, good overall performance shown in Section FASP Can Handle Datasets of Heterogeneous and Low SNR Values already demonstrates that FASP can work with various morphology patterns of FIUs. In addition, with image size, SNR, and number of FIUs fixed, and with all other impact factors randomly sampled from given distributions, we investigated FASP's performance as response to size of FIU and frequency of Ca2+ transients. Figure 5C suggest the impact of FIU size is not substantial as long as an FIU covers more than ∼ 10 pixels. And Figure 5D shows that the frequency has almost no impact on the algorithm's performance.    FASP Is Robust to Various FIU Density and\r\n    Total Number in the Field of View\r\n  Due to different experimental conditions and different requirements from applications, fluorescence imaging data of astrocyte Ca2+ signaling could exhibit considerable variation in FIU population density and the total number of FIUs in the field of view. Generally speaking, the more FIUs one has, the more difficult the analysis is. However, it is a fundamental requirement for automatic analysis of large-volume datasets to simultaneously and accurately detect hundreds or even more FIUs without prior knowledge of the number of FIUs. Quantitative assessment on simulation datasets shows that FASP has flexibility with number and density of FIUs in the field of view and can thus successfully meet this demand.  In simulations with fixed image size, fixed SNR, random FIU size (≥10 pixels) and random propagation speed, samples consisting of different numbers of FIUs were tested. The impact factor of interest here is the population density of FIUs. As shown in Figure 5B, our algorithm's recall mildly drops as the population density of FIUs increases, but still retains good performance (mean fidelity of 0.891, recall of 0.861, and precision of 0.994) even with more than 200 FIUs in a 128∗128-pixel video and SNR of 5 dB. Most errors were caused by adjacent FIUs whose ground truth characteristic signals are intrinsically similar to each other and hence difficult for algorithms or even humans to distinguish. The denser the FIUs are in the field of view, the more likely FIUs connect spatially to each other, and hence the more difficult it is to distinguish them.  The second set of simulation experiments targeted the influence of the total number of FIUs given the FIU population density. We did simulations with fixed FIU intensity in the field of view, fixed SNR, random FIU size (≥10 pixels) and random propagation speed. Samples of different image sizes were generated such that they contained 25, 100, 400, and 1,600 FIUs. The blue lines in Figure 6A show the very promising performance of FASP on data with a large number of FIUs. No matter the SNR is 5 or 10 dB, good performance is preserved as the total amount of FIUs dramatically increases.  Well Modeling the Propagation of Ca2+    Transients Endows FASP with Good\r\n    Performance\r\n  One of the key challenges in astrocyte Ca2+ activity analysis is the pervasive phenomena of slow signal propagations of intracellular Ca2+ transients. Figures 1B,C give two real-world examples of this phenomenon. Figure 6B shows time lags between pixels in a real astrocyte functional unit, as a result of the slow propagation.  All these examples demonstrate the special nature of astrocyte Ca2+ time-lapse imaging data, which, we anticipate, rules out algorithms without specific design for the complex phenomena.  On the contrary, by explicitly modeling the propagation, we expect that FASP gains the power to effectively eliminate the ill effects of it and to correctly capture the actual astrocyte Ca2+ dynamic signals.  To the best of our knowledge, designed for astrocyte or any other cell types, no existing method of time-lapse Ca2+ image analysis has so far made any effort to tackle the problems caused by the propagation phenomena. To validate the necessity of explicitly modeling the propagation phenomena, we compared FASP with \u201cCellSort\u201d  (Mukamel et al., 2009) , one of the most popular Ca2+ dynamic detection algorithms that work quite well on neuronal Ca2+ imaging data. Ignoring the effect of propagation, CellSort makes a basic assumption of within-FIU synchronization, which approximately holds true for neuronal Ca2+ data. The violation of this basic assumption in astrocyte time-lapse Ca2+ image data constitutes fundamental problems for CellSort in detecting astrocytic FIUs. On the one hand, as the possible time lags between observed signals in different parts of an FIU are neglected, only pixels with almost the same phase of signal are going to be clustered together. Put differently, pixels in one ground-truth FIU (Figure 6C1) could be falsely detected as several FIUs, as shown in Figure 6C3.  In contrast, FASP successfully detected the FIU as a whole (Figure 6C2). On the other hand, under the assumption of within-FIU synchronization, the learned characteristic curve is essentially a weighted average over all pixels' signal curves without considering the time lags between pixels, leading to distortion of the characteristic curve (Figure 6B), which makes the subsequent analysis misleading. The characteristic curve estimated by FASP, however, well matches the ground truth (Figure 6B). For comparison between FASP and CellSort regarding average performance, we tested them on both real data and synthetic data. Figure 6D and Supplementary Videos 1, 3 together give a comparison on real data, supporting our conclusion that CellSort would often falsely split one FIU into several, due to the impact of pervasive propagation phenomena.  Because of the lack of ground-truth for real data, we decide FIGURE 6 | Well modeling the propagation of intracellular Ca2+ transients endows FASP with good performance. (A) Quantitative performance comparison between FASP and CellSort  (Mukamel et al., 2009) , a popular algorithm that works well on data with no propagation phenomena. FASP outperforms CellSort significantly, suggesting the importance of modeling propagation phenomena in the analysis of astrocytes. (B) FASP resolves the negative impact of propagation phenomena on the estimation of Ca2+ dynamic curves. The first three curves are the observed time courses at pixels in the same FIU, showing time lags due to propagation. The red curve is the average of observed curves at all pixels in this region without considering the time lags, and hence the curve is distorted from the observed ones, with much longer rise time and decay time of Ca2+ transients. The blue curve is the fitted signal curve given by FASP. (C1) Selected frames from a time-lapse data sample show apparent propagation of Ca2+ elevation. (C2) The results produced by FASP, including the spatial map of one FIU and the corresponding time curve. (C3) The sorting results produced by CellSort, related to the FIU highlighted in (C2). The connected domain was falsely split and identified as three differently functioning ROIs. Their fitted curves are actually just the same one, if time lags are taken into account. (D) Comparison between FASP and CellSort on a real data sample of astrocytes derived from human induced pluripotent stem cells (hiPSCs). (D1) FIUs identified by FASP highlighted by carmine boundary lines on the first frame of the original (Continued) one FIU should not be separated into parts if we see the same temporal Ca2+ fluctuation pattern in all parts of this FIU, without propagation or only with spatially continuous and smooth propagation (see Supplementary Videos 1, 3). Differently from CellSort, FASP correctly detected the FIUs. Quantitative comparison was done by testing CellSort on the same synthetic dataset generated in Section FASP is Robust to Various FIU Density and Total Number in the Field of View, with a common and fixed FIU density in the field of view but different image sizes. Figure 6A clearly shows that our algorithm is superior to CellSort. CellSort reported a lot of redundant units because it allows for totally or partially overlapping FIUs and it falsely separates real FIUs into sub-regions. The false separations also give rise to a smaller area accuracy. Besides, the fidelity of curves estimated by CellSort is lower. In cases with SNR of 5 dB, CellSort missed a lot of FIUs, while our method still found around 88% ground-truth FIUs, suggesting a much better robustness to large noises. All these results emphasize the importance of wellmodeling the propagation phenomena in astrocyte Ca2+ image analysis.  Then we did quantitative analysis to assess the robustness of FASP to the severity of propagation phenomena. FASP's performance was evaluated as a function of Ca2+ propagation velocity. With other parameters randomly sampled from a given distribution, we generated synthetic data using different propagation velocities. The performance is shown in Figure 6E.  When the intracellular calcium transients propagate at a velocity of 1 pixel per frame or faster, the velocity generally has little impact on the performance. A natural corollary of this observation is that our algorithm is compatible with fast-propagation applications such as neuronal Ca2+ dynamic detection. When the propagation velocity is slower than 1 pixel per frame, the precision drops a little, perhaps because the differences between adjacent pixels are larger and hence the pixelwise neighborhood correlations are smaller, although we did not observe such slow propagation in real data.    Case Study: Analyzing Agonist-Induced\r\n  Ca2+ Signaling in Rat Astrocytes We applied FASP to analyse the calcium dynamics of rat hippocampal astrocytes in response to agonist treatment (Figure 7). When ground truth is absent for validation, data of stimuli-triggered activities present an alternative way to test data analysis methods. Astrocytic Ca2+ signaling can be triggered by the excitatory neurotransmitter including ATP and glutamate through P2Y1 receptors and metabotropic glutamate receptors (mGluRs) respectively  (Kim et al., 1994; Pasti et al., 1997; Bowser and Khakh, 2004; Fiacco and McCarthy, 2004; Jourdain et al., 2007; Hamilton et al., 2008) . 3,5-dihydroxyphenylglycine (3,5-DHPG), a potent agonist of group I mGluRs  (Wisniewski and Car, 2002) , including mGluR1 and mGluR5, can evoke astrocyte Ca2+ elevations in the astrocyte soma and fine processes  (Zur Nieden and Deitmer, 2006) . We thus monitored the calcium dynamics in response to ATP, glutamate, and DHPG on cultured rat astrocytes, respectively, followed by FASP analysis to detect the evoked Ca2+ activities. For each sample, the astrocytes were imaged twice, before and after the agonist was added in.  To assess the accuracy of automatic analysis, we first manually labeled FIUs in all the control/case samples, and compared them with the outputs of FASP. It turned out that the FIUs automatically detected by FASP were very consistent to those manually labeled, but had more complex and accurate contours. FASP was able to successfully detect some FIUs that have low basal fluorescence and therefore were neglected by manual labeling. We confirmed those neglected FIUs by double-checking those areas by purposefully applying transformations which facilitate human vision. The detected FIUs included whole somas, long processes, and smaller sub-regions within cells (probably microdomains; Figures 7A-C,E). Connected FIUs were successfully detected as separate units.  The control samples for all the three agonists showed almost no spontaneous activity before the application of the agonists, but a large number of FIUs were activated with the agonists applied separately (Figures 7A-C). One hundred and twentynine induced FIUs were detected after glutamate was applied, covering 28.45% of the area of all astrocytes (active/silent) in the sample, while ATP alone induced 88 FIUs that covered 57.27% of the area. These observations suggest strong effects of these agonists. DHPG showed a relatively milder effect, activating 33 FIUs, 18.50% of the area of overall astrocytes. Agonistevoked Ca2+ was well-captured by the algorithm. The agonists were added in during the first several frames of imaging, and an obvious burst of Ca2+ elevations can be seen in all the three samples with agonists. Accordingly, characteristic curves reported by FASP show a common Ca2+ increase shared by the majority of FIUs at about the 6th frame (Figures 7A-C). The estimated half-rise time ranges 1-6 s and half-decay time ranges 4-10 s, which is consistent with what was previously reported about Ca2+ dynamics in response to ATP stimuli  (De Pitta et al., 2012) . The response to ATP is asynchronous. From the curves of induced Ca2+ signals (Figure 7D), we can recognize FIUs that responded only once right after the agonist was added, FIUs with evoked oscillations of generally constant frequency and constant amplitude, and FIUs with evoked oscillations of decreasing frequency or decreasing amplitude. Similar patterns were also observed in other studies, suggesting that ATP is a modulator for both frequency and amplitude  (Cornell-Bell et al., 1990) .  We next test the accuracy of FASP's outputs in detecting dose response to agonists. When ATP of different doses is added, the number of FIUs responding to the agonist stimuli is supposed to increase as the dose increases. Figure 7E and Supplementary Videos 4-7 gives a comparison among the detected responses to no ATP (control), 1, 10, and 100 uM ATP. The control sample only contained one FIU with spontaneous activity; 1 uM ATP induced 9 FIUs; 10 uM stimulated 136 FIUs; while the field of view was full of responding FIUs (182 FIUs) after 100 uM ATP was applied. The peak response to different doses is also reflected in the histogram of estimated Ca2+ activity amplitude (maximum 1F/F0; Figure 7F). The higher the dose is, the higher the histogram is (because more FIUs responded), and the more large-amplitude activities are observed. These results suggest that FASP successfully captured the association between the strength of stimulated activities and agonist dose.    The Binarization Method Based on\r\n    Order-Statistics Theory Is Effective\r\n  The z-score maps generated from astrocyte Ca2+ timelapse images usually possess the following characteristics which make the binarization task difficult: (1) large noises and small foreground/background contrast due to low-SNR in input image stack data; (2) weak and blurred edges of foreground areas; and (3) uneven intensity distributions across different foreground regions. Taking advantage of the statistical nature of z-score maps, we based our method for binarizing z-score maps on order-statistics guided hypothesis testing (Section Active Region Detection on Z-Score Map by Order-Statistics Guided Hypothesis Testing). It was demonstrated by experiments that our method has strong power to binarize z-score maps and outperforms many other binarization techniques especially in low-SNR cases (Table 1 and Figure 8A).  Literature shows several methods that are most often applied to binarize grayscale microscopy images of cells  (Meijering, 2012) : thresholding, watershed transformation, active contour methods, and graph cuts based algorithms. Unfortunately, due to the special characteristics and difficulties of our data, some key assumptions explicitly or implicitly required by these methods are not satisfied, such as assumptions of separable foreground/background intensity distributions, intensity homogeneity within foreground/background regions, and sharp intensity change at the boundaries between foreground and background. Experimental results on synthetic data confirmed the concern that the violation of these strong assumptions makes existing methods intrinsically problematic to solve our problem. Watershed approach typically relies on predefined markers of foreground and background areas which are unavailable in our problem. For the other three categories of techniques, we selected one representative automatic algorithm from each of them and quantitatively evaluate their performance: Kittler-Illingworth thresholding  (Kittler and Illingworth, 1986) , the best among 40 thresholding methods according to a survey given by Sezgin  (Sezgin, 2004) ; Distance Regularized Level Set Evolution (DRLSE)  (Li et al., 2010) , which has been applied to cell segmentation problems  (Dzyubachyk et al., 2010) ; and Al-Kofahi's method  (Al-Kofahi et al., 2010) , a well-accepted graph cuts based algorithm with fully automatic initialization. Parameters were tuned by experiments, with the best performance achievable reported here. According to the experimental results (Table 1 and Figure 8A), our hypothesis testing based region growing method (\u201cFASP-bin\u201d) has the best overall binarization performance among the four, in terms of both misclassification rate and F-measure. Though the precision of our method is slightly lower than that of Kittler-Illingworth thresholding and Al-Kofahi's, the difference is quite small (&lt;0.015) and their recall scores are much lower than ours (&gt;0.160). Figure 8A further reveals that our method shows strength and superiority mainly in low-SNR cases. The missing pixel rates of the other three methods increase tremendously as SNR decreases from 5 to 0 dB.  One key design in our method that makes a critical contribution to this promising performance is the utilization of neighborhood pixels' relationships in hypothesis testing. Specifically, instead of testing each pixel in isolation, we designed a region-level z-score (Equation 4) and tested one candidate region as a whole. In this way the statistical power of test is much enhanced, since information from connected pixels can confirm each other and thus jointly provide us with higher confidence in decisions. On both real data (Figure 8B) and synthetic data (Figure 8C), we compared two hypothesis testing based methods for binarization: (1) a right-tailed test on each pixel using the pixel-level z-score; (2) a right-tailed test on any possible connected domain using the region-wide z-score (accelerated by our heuristic region search process without loss of information). In both methods, pixels (groups of pixels) of interest were claimed positives using a significance level of 0.05. Commonly seen in bioimaging data, the z-score is not homogeneous even within a single FIU. Pixel-level tests discard low scoring pixels, resulting in a coarse and incomplete map, while region-level tests retain these pixels because of their neighbors. More importantly, because of low expression level of fluorescence protein, it is common that some FIUs' signals are too weak that in-FIU pixels can hardly be distinguished from random noise pixels in terms of intensity. But weak signals in a group of connected pixels together reveal the non-randomness, which allows region-wide tests to detect them.    Neighborhood Correlation in Residual\r\n    Signals Enables a Sequential FIU\r\n    Identification\r\n  In the sequential process of identifying FIUs in an active region, as discussed in Section Module 2: Identifying FIUs and Extracting Characteristic Curves, at each step only one FIU m (with characteristic curve Xm) is modeled and all the remaining FIUs are \u201chidden.\u201d We designed a score zp(xfit) (Equation 12), aiming to distinguish pixels in FIU m from pixels in other FIUs.  One important innovation in our framework is the realization FASP-bin Kittler-Illingworth thresholding Al-Kofahi's (Graph Cuts based) DRLSE (Level Sets based)  Misclassification rate Each pixel is classified/labeled as either a foreground pixel or background pixel, and the four metrics are based on the proportion of correctly classified pixels or misclassified pixels in an image. The evaluation scores in the table are presented in the format \u201caverage ± SD,\u201d where SD is the standard deviation calculated over multiple runs of simulation. Bold fonts indicate the best result in terms of each performance measure. FASP-bin is the method developed in this study. The synthetic data here were generated with all parameters randomly sampled from given distributions that reflect typical parameter values in real data. All methods were tested on the same datasets. that correlation between neighboring pixels' residual signals can be used as a measure of signals that cannot be explained by Xm and thus an indicator of the existence of other FIUs.  Neglecting this information may lead to inaccurate boundaries between FIUs, or even to total failure in separating different FIUs.  We verify the important role that neighborhood correlation of residual signals plays by inspecting the difference between Equation (12) and the following fitness definition: (fit)∗ i, j = Φ−1 Φ2Uτ F rfit i, j zpx , (15)  (fit) which is linearly related to the first term of zpx and does not include the information about residual signals. Figure 9 provides an indicative example comparing the two scores. Given two simulated FIUs whose characteristic curves are different but correlated (correlation coefficient = 0.4), the ground truth characteristic curve of one FIU is used to calculate the scores at all  (fit) pixels. The results support our expectation that zpx yields better  (fit)∗. After applying our z-score map binarization contrast than zpx method to both maps, the FIU of interest was correctly identified  (fit) from zpx map but falsely merged with the confounding FIU  (fit)∗ map. Generally, the lower the correlation between using zpx performs better. The specific design of zp(xfit) (Equation 12) is one of the keys to the good performance of FASP.    Alternative Design of Neighborhood\r\n    Correlation Score\r\n  When calculating the neighborhood correlation score using Equation (2), we focused more on controlling the damaging effects of noises in low-SNR cases and ignored the potential time lags between adjacent pixels. To examine the impact of this ignorance and investigate FASP's applicability to various situations, we have also considered an alternative neighborhood correlation score, taking propagation phenomena into account. Based on the fact that the signals are always synchronized along the surface orthogonal to the propagation direction, we calculate the neighborhood correlation at pixel [i, j] as rsc i, j = maxd=1,2,3,4 cor where cor (, ) is the Pearson correlation coefficient, d1 and d2 are the first and second pixels along the direction d. For each pixel, there are four directions to search as shown in Figure 10A. Though we do not know the propagation direction a priori, the maximum value among the four approximately represents the correlation between the pixel and its synchronized neighbors. Accordingly we adjust the definition of the z score of the neighborhood correlation. Considering that the correlation map was obtained by taking the maximum value among four directions, we make a transformation zpx i, j = 8−1 84 F rsc i, j , (17) where 8 (·) denotes the standard Gaussian CDF and F (·) is the normalized Fisher transformation defined in Equation (3). Based on the properties of the Fisher transformation and of extreme order statistics, it can be shown that zpx i, j defined in Equation (17) also follows a standard Gaussian under the null hypothesis that the pixel [i, j] has zero βm for all m. This design is expected to be better to cope with propagation but less powerful to suppress noises, as a result of not taking full use of the eight neighboring pixels but only utilizing two of them in calculating the correlation.  Simulation experiments were conducted to compare Equations (2) and (16) under different ground-truth SNR settings (Figure 10B). Propagation velocities were randomly sampled from a wide range, mimicking real data. The results show that, when SNR is high, Equation (16) leads to better contrast between active and silent pixels than Equations (2) and (3) (p &lt; 2.2 × 10−16 by Wilcoxon signed rank test). This suggests that the special design in Equation (16) successfully works to suppress the effect of Ca2+ propagation. On the other hand, when the SNR is low, the opposite is observed: Equations (2) and (3) assign larger zpx to active pixels than Equations (16) and (17) do (p &lt; 2.2 × 10−16 by Wilcoxon signed rank test). This supports the notion that, in low-SNR cases, though Equation (16) still models the propagation better, the benefit of restraining noises dominates. Since astrocyte Ca2+ imaging data typically has low SNR, we decided to adopt Equation (2). In applications where SNR is basically high while the propagations are extremely slow, Equation (16) can be applied instead.    DISCUSSION\r\n    Necessity of Specific Tools for Analyzing\r\n  Time-Lapse Astrocyte Ca2+ Imaging Data To the best of our knowledge, no tool has been developed for fully automatic analysis of astrocytic signaling from timelapse Ca2+ images. However, many analytical tools have been proposed to analyse similar imaging data of other types of cells, especially neurons. Is it necessary to design new analytical tools specifically for astrocytes? In addition to the discussion about CellSort in Section Well Modeling the Propagation of Ca2+ Transients Endows FASP with Good Performance, our survey also shows that neuron-targeted algorithms can hardly be directly generalized to work on astrocyte Ca2+ imaging data, demonstrating the needs for new designs.  The earliest and most elementary types of neuron-targeted methods are manual  (Dombeck et al., 2007; Göbel et al., 2007)  and semi-automated approaches  (Junek et al., 2009; Peters et al., 2014)  that cannot be applied to large-scale data sets and which are also biased by subjective judgments. Another group of early stage methods is based on detecting a region of interest (ROI) on temporally averaged intensity images without considering the time course information  (Pachitariu et al., 2013; Kaifosh et al., 2014) , and thus can neither distinguish functionally active cells from silent ones nor separate spatially connected but functionally different units. Some methods rely on priors about morphological properties such as size and shape patterns  (Valmianski et al., 2010) , and thus are very likely to fail for the astrocyte problem due to the irregular and heterogeneous morphologies of FIUs. The most well-accepted set of sophisticated tools is grounded on matrix decomposition techniques such as independent component analysis  (Reidl et al., 2007; Mukamel et al., 2009) , sparse dictionary learning  (Diego et al., 2013) , multilevel sparse matrix factorization  (Andilla and Hamprecht, 2013) , nonnegative matrix factorization  (Maruyama et al., 2014; Soelter et al., 2014) , and constrained non-negative matrix factorization  (Pnevmatikakis et al., 2016) . However, just like CellSort, all these neuron-targeted algorithms make either an explicit or implicit assumption that pixels in an FIU are synchronized under the given imaging resolution. The violation of this assumption in astrocyte data leads both to false splits of a single FIU and to inaccurate estimates of characteristic curves of FIUs. Being blind source separation strategies, they also have difficulties in pre-determining the number of sources and in interpreting resultant overlapped components. Additionally, many of these methods make assumptions of spatial sparsity, temporal sparsity or mutual independence of Ca2+ signals, which are often not satisfied by astrocyte data. Last but not the least, matrix decomposition methods basically treat pixels as independent, leaving out the information embedded in spatial relationships among neighboring pixels.    General Applicability and Limitations\r\n  Though designed for time-lapse imaging data of astrocytic Ca2+ dynamics, FASP is also potentially applicable to many other types of imaging data that have similar properties, which can be captured by the proposed model (1). Many of FASP's advantages still hold in such cases, including flexibility, outstanding performance on large-scale data, and probabilistically controlled parameter tuning.  Particularly, it is interesting to see how FASP can be applied to model neuronal Ca2+ dynamics  (Mukamel et al., 2009) . Recent advances in high-throughput time-lapse Ca2+ imaging of large populations of neurons  (Ahrens et al., 2013; Prevedel et al., 2014)  have generated a tremendous amount of data and hold the potential to understand the neuronal ensemble activities and coding. There are two possible routes to apply FASP to neuronal data. (1) When the Ca2+ imaging is focused on soma and hence the analysis is of cellular level, the somatic intracellular propagation of Ca2+ transients is so fast that the signals can be regarded as synchronized among pixels in a single neuron. So, we do not need to explicitly model the signal propagation within cell. However, the simplified version of FASP with an intracellular synchronization assumption can still help us to automate the analysis, ease the parameter setting and reduce false positive discoveries, due to the machine-learning and probabilistic nature of the FASP framework. (2) When the Ca2+ imaging is focused on dendrites, the neuronal Ca2+ signals look more like the ones in astrocytes than the somatic signals, because of the existence of calcium compartmentalization and signal propagation in dendrites  (Yuste et al., 2000; Higley and Sabatini, 2008) . In this case, we expect the application of FASP will be especially useful to extract information that is otherwise missed, because of its data-driven and unbiased nature. However, caution needs to be exercised. Dendritic calcium signaling may be very different from astrocytic calcium signaling, such as the frequency of events, the size and the density of regions of interest, and the signal-to-noise ratio.  More broadly, although FASP was motivated by a specific biological problem, some of the technical innovations that we developed during the process are quite generic and can be applied to other problems. For example, the order-statisticsbased binarization is one of the key innovations in this paper. Due to the prevalence of segmentation problem in imaging analysis, we expect this new segmentation method may find broad applications, especially when the pixel values can be assigned statistical meaning.  One possible limitation of FASP arises from its assumption of fixed spatial positions of FIUs. Cells sometimes migrate in the field of view, and their shapes sometimes change as well. This problem can be entirely or partially solved by either pre-processing, which registers cells, or post-processing, which eliminates abnormal output FIUs.  Another concern may be about the assumption that a single pixel is associated with at most one FIU. Due to limited resolution along the z-axis or other reasons, the signals at some pixels are actually mixtures of multiple biological signals. However, without ground truth, results of blind decomposition are often difficult to explain, or turn out to be wrong in simulation experiments. Instead, FASP does not make an attempt to decompose the mixed underlying biological signals but sticks to the objective presentation of observed signal patterns. On the basis of FASP's outputs, downstream analysis can still be performed to unmix the true sources.    CONCLUSIONS\r\n  We developed a data-driven and probabilistically principled algorithm to automatically quantify the functional status of astrocytes from astrocyte time-lapse Ca2+ imaging data. Integrated with a series of statistical machine learning techniques, FASP was demonstrated to be able to successfully decode complex spatiotemporal patterns of calcium signaling and control the false positive rate. We expect that broad applications of this tool would greatly facilitate analyzing astrocyte function to uncover its complicated roles in neuronal circuits.    ETHICS STATEMENT\r\n  This study was carried out in allegiance with active biological use authorization and animal protocols. The protocols were approved by institutional animal care and use committee at University of California, Davis.    AUTHOR CONTRIBUTIONS\r\n  GY and LT conceived, designed, and coordinated the study. YXW and GY designed the mathematical modeling and executed the programming. YXW performed all the computational experiments. GS performed all the wet-lab experiments. YZW and CW helped with the algorithm design and computational experiments. YXW and GY wrote the manuscript with critical inputs from DM, GB, YW and LT.    ACKNOWLEDGMENTS\r\n  Research reported in this publication was partly supported by the Hartwell foundation (LT), NIH DP2 MH107059 (LT), NIH R21NS095325 (LT) and NIH R01MH110504 (GY). An early abbreviated version of this work has been published in 2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI 2016)  (Wang et al., 2016) .    SUPPLEMENTARY MATERIAL\r\n  The Supplementary Material for this article can be found online at: http://journal.frontiersin.org/article/10.3389/fninf.2017.00048/full#supplementary-material Conflict of Interest Statement: The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.  Copyright © 2017 Wang, Shi, Miller, Wang, Wang, Broussard, Wang, Tian and Yu. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.    ",
    "sourceCodeLink": "https://github.com/VTcbil/FASP",
    "publicationDate": "0",
    "authors": [
      "Yinxue Wang",
      "Guilai Shi",
      "David J. Miller",
      "Yizhi Wang",
      "Congchao Wang",
      "Gerard Broussard",
      "Yue Wang",
      "Lin Tian",
      "Guoqiang Yu"
    ],
    "status": "Success",
    "toolName": "FASP",
    "homepage": ""
  },
  "56.pdf": {
    "forks": 2,
    "URLs": ["github.com/itsmeludo/Phyloligo/"],
    "contactInfo": ["ludovic.mallet@inra.fr"],
    "subscribers": 4,
    "programmingLanguage": "Python",
    "shortDescription": "Bioinformatics / Explore oligonucleotide composition similarity between assembly contigs or scaffolds to detect contaminant DNA.",
    "publicationTitle": "PhylOligo: a package to identify contaminant or untargeted organism sequences in genome assemblies",
    "title": "PhylOligo: a package to identify contaminant or untargeted organism sequences in genome assemblies",
    "publicationDOI": "10.1093/bioinformatics/btx396",
    "codeSize": 59925,
    "publicationAbstract": "Motivation: Genome sequencing projects sometimes uncover more organisms than expected, especially for complex and/or non-model organisms. It is therefore useful to develop software to identify mix of organisms from genome sequence assemblies. Results: Here we present PhylOligo, a new package including tools to explore, identify and extract organism-specific sequences in a genome assembly using the analysis of their DNA compositional characteristics. Availability and implementation: The tools are written in Python3 and R under the GPLv3 Licence and can be found at https://github.com/itsmeludo/Phyloligo/.Contact: ludovic.mallet@inra.fr Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2016-10-27T09:40:02Z",
    "institutions": [
      "IRD - IUC",
      "Unite ́ Mathe ́ matiques et Informatique Applique ́ es de Toulouse (MIAT)"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-06-17T08:36:22Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx396   PhylOligo: a package to identify contaminant or untargeted organism sequences in genome assemblies     Ludovic Mallet  1    Tristan Bitard-Feildel  0    Franck Cerutti  1    He´ l e`ne Chiapello  1    0  CNRS UMR7590, Sorbonne Universite ́ s, Universite ́ Pierre et Marie Curie - Paris 6, MNHN, IRD - IUC ,  Paris ,  France    1  INRA UR875, Unite ́ Mathe ́ matiques et Informatique Applique ́ es de Toulouse (MIAT) ,  Auzeville, 31326 Castanet- Tolosan ,  France     2017   1  1  3   Motivation: Genome sequencing projects sometimes uncover more organisms than expected, especially for complex and/or non-model organisms. It is therefore useful to develop software to identify mix of organisms from genome sequence assemblies. Results: Here we present PhylOligo, a new package including tools to explore, identify and extract organism-specific sequences in a genome assembly using the analysis of their DNA compositional characteristics. Availability and implementation: The tools are written in Python3 and R under the GPLv3 Licence and can be found at https://github.com/itsmeludo/Phyloligo/.Contact: ludovic.mallet@inra.fr Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  The development of sequencing technologies has enabled to target the genome of complex non-model organisms and communities of organisms. Some of these non-model organisms can be challenging to isolate from their environment or cannot be cloned in vitro. They might alternatively be compulsorily associated with cognate commensal or parasitic organisms, or even embedded within a host. Consequently, genome assembly datasets sometimes include DNA from unexpected sources like mixture of untargeted species, but may also contain organelles or even laboratory contaminants. The presence of additional untargeted species was indeed reported in several recent genome assemblies, for instance in the draft assembly of domestic cow  (Merchant et al., 2014) , in several isolates of the phytopathogenic fungi Magnaporthe oryzae  (Chiapello et al., 2015)  or recently in the tardigrade genome  (Delmont and Eren, 2016) . Such mixed assemblies may produce several biases and problems in downstream bioinformatics analyses and raise the need for tools able to deal with mixed-organism DNA assemblies.  Several tools were recently designed or used to detect and filter untargeted organisms from sequence datasets. A first type of approach, used by khmer software  (Crusoe et al., 2015) , is to compute k-mer frequencies on short reads to pre-process and filter read datasets prior to de novo sequence assembly. Other packages like Blobtools  (Kumar et al., 2013)  and Anvi'o  (Eren et al., 2015)  combine sequence properties (GC content, oligonucleotide profile) and additional information such as depth of coverage, similarity to public databases and reference gene sets to identify untargeted species using both raw reads and assembled contigs of a genomic dataset. Finally, a last type of approach is to use software dedicated to metagenomic species read binning, such as CONCOCT  (Alneberg et al., 2014)  that use sequence composition and coverage across multiple samples to automatically cluster contigs into genomes or Kraken  (Wood and Salzberg, 2014)  that relies on exact alignment of k-mers to a k-mer reference database to assign taxonomic labels to metagenomic DNA.  Here we present PhylOligo, a toolset designed to explore, segment and subtract untargeted material from assembled sequences using an ab initio alignment-free approach relying only on the intrinsic oligonucleotide signature of an assembled genomic dataset. Compared to existing software, PhylOligo provides several features to explore assemblies, including: (i) a customizable oligonucleotide pattern, including continuous and spaced pattern k-mers  (Brinda et al., 2015; Leimeister et al., 2014; Noe´ and Martin, 2014) . (ii) handling bare contig-level assemblies (raw reads and coverage information are not required for detecting untargeted species) (iii) an interactive cladogram-based visualization of the contig signature similarity and cumulative size to explore the signature clusters to profile putative additional materials (iv) an effective sliding window-based partitioning scan of the assembly based on a supervised learning and a doublethreshold system asserting that regions are labelled as untargeted organism when meeting two criteria: (i) being distant enough from the host sequence oligonucleotide profile (first threshold) and (ii) being close enough from a cluster of untargeted sequences previously selected by supervised learning (second threshold).  Our strategy present several advantages. (i) Unlike approaches that process short read datasets prior to the de novo sequence assembly and use sequence homology information, PhylOligo allows the identification of potentially uncharacterised and distantly related sequences in already assembled genomic datasets, the handling of any type of genome assembly, shunning the dependency on the availability of raw sequencing reads data, additional data and patchiness of knowledge in databases; (ii) The double-threshold species-specific filtration prevents the removal of HGTs and the subsequent fragmentation of the assembly; (iii) Learning the compositional profile on longer and assembled sequences such as contigs compared to unassembled reads, allows for a refined oligonucleotide profile, unbiased from heterogeneous sequencing depth along the sequence. Moreover, the partitioning process of PhylOligo provides the possibility to detect and split chimeric sequences or mis-scaffolding;    2 Workflow strategy\r\n  Our strategy includes 3 main steps: (i) assembly exploration using an interactive tree visualization based on oligonucleotide profiles computed from all genomic contigs, (ii) oligonucleotide profile prototype learning based on contig subsets selected by the user at nodes of the tree and (iii) assembly partitioning to locate organism-specific regions and classify contigs or segments according to the learned prototypes.   2.1 Assembly exploration\r\n  PhylOligo allows for a visual exploration of the compositional similarity distribution and structure of the contigs in an assembly based on either continuous (k-mers) or spaced-pattern oligonucleotide frequencies. The oligonucleotide profile of each contig is computed and a pairwise distance matrix based on metrics including Euclidean or Jensen-Shannon is produced (Fig. 1A) to generate an interactive Neighbour-Joining tree. Branch width is drawn proportional to the cumulated length of the contigs in a clade, allowing the user to track where the main part of the assembly clusters (assumed to correspond to the targeted organisms) and what significant clades branch out as hint for separate organisms (see Fig. 1B). Thanks to the Ape package  (Paradis et al., 2004) , sequences from a clade are interactively selected on the tree and exported to fasta files to learn a prototype of their oligonucleotide profile. An alternative unsupervised clustering method relying on HDBSCAN  (Campello et al., 2013)  and t-SNE  (van der Maaten and Hinton, 2008)  for visualization is also implemented (Fig. 1C).    2.2 Prototype learning\r\n  ContaLocate then allows the learning of oligonucleotide profiles from the main and presumed additional organisms identified by user-selected subsets from the previous step. These subsets must cumulate at least 50 Kb in order to generate an accurate prototype sufficiently representative of an organism. Learning subsets can be generated or complemented from public sequences, specialised databases  (Me´nigaud et al., 2012)  or other tools  (Alneberg et al., 2014; Eren et al., 2015; Kumar et al., 2013) .    2.3 Assembly partitioning\r\n  The assembly is then scanned with sliding windows to locate organism-specific regions using oligonucleotide divergences computed against the targeted and the additional profiles. The distribution of the divergence against both is used to establish two thresholds best separating the different modes in the density functions (see Fig. 1D). The thresholds are visually validated by the user and can also be adjusted manually. Genomic regions with a divergence simultaneously crossing respective thresholds to the targeted and to the additional profiles are labelled as part of the additional organism and exported as a GFF file.     3 Results\r\n   3.1 Synthetic datasets\r\n  We evaluated the performances of PhylOligo by generating artificial contaminations on 32 contig datasets generated by GRINDER  (Angly et al., 2012)  from real Refseq genome data (see section 6.1 of Supplementary Material for detailed protocol). The species were chosen to cover the main domain of life (archea, bacteria, fungi, protozoa and vertebrate) and different degrees of genome complexity, content, length and composition. We benchmarked the automatic version of PhylOligo that uses the unsupervised HDBSCAN clustering and evaluated performances by using three indicators: (i) the cluster specificity i.e. the maximum fraction of contaminant in a cluster, (ii) the cluster sensitivity, i.e. the fraction of the whole contaminant A C  B  D Fig. 1. Visualization and interactive exploration of assemblies. (A) Pairwise compositional divergence of contigs produced by PhylOligo. Contigs are reordered by hierarchical clustering. (B) Contig tree produced by PhylOligo on the tardigrade genome. The clade in red is the current selection pointed by the user. (C) Contigs clustered by HDBSCAN on oligonucleotide frequencies, Data from Magnaporthe oryzae. Red and blue are predicted clusters, grey are unclassified. The hyperspace is reduced to 2 dimensions with t-SNE. (D) Determination of the untargeted threshold in ContaLocate based on the distribution of distances between the untargeted clade and the scanning windows over the whole assembly K-mer pattern aggregated in the cluster and (iii) an hybrid score, which indicated the best computed value of the product of cluster specificity and sensitivity. We used PhylOligo default parameters on all the combinations of the 32 simulated genomes assemblies. We also evaluated the impact of the k-mer parameter by panelling continuous and spaced-pattern kmers on a focus subset of ten pairs (see results in Table 1). Complete results are detailed in section 6.2 of Supplementary Material. Overall, the benchmark demonstrates a great ability to discriminate contaminant clusters with very high specificity and good sensitivity, suited with the requirements for supervised learning and partitioning. Concerning k parameter impact, we obtained best results according to our hybrid score for two spaced patterns: 11001 (mean:0.8133, median: 0.9499) and 110101 (mean:0.8344, median:0.9459). Continuous k-mers of length 4 and 5 also performed well but with slightly lower scores (median scores of 0.7909 and 0.9305 for k ¼ 4 and 5 respectively).    3.2 Real datasets\r\n  PhylOligo has been successfully applied to identify untargeted large bacterial regions in four out of nine fungal genomic datasets of Magnaporthe  (Chiapello et al., 2015) . GOHTAM  (Me´nigaud et al., 2012)  taxonomical assignment of these additional regions confirmed their homogeneity and origin from Burkholderiales. Targeted Blast comparisons indicated that some of these supplementary regions were almost identical to Burkholderia fungorum sequences (100% identity for 16S, recA and gyrB genes) suggesting an origin or relatedness to one or several bacterial isolate(s) of this species. PhylOligo was applied to filter genome assemblies, validated with BUSCO  (Sima~o et al., 2015)  and DOGMA  (Dohmen et al., 2016)  (see Supplementary Material) and allowed to continue further bioinformatics analyses without rebuilding the costly initial genome assembly and annotation processes.  PhylOligo was also used to explore the scaffolds of the tardigrade assembly  (Boothby et al., 2015) , for which a multiple contamination was previously proposed  (Delmont and Eren, 2016; Koutsovoulos et al., 2016) . We compared the the topology of the compositional cladograms established with PhylOligo on both the initial and the filtered assembly obtained with Anvi'o  (Eren et al., 2015) . Our results showed that the cladogram produced with PhylOligo exhibited a topology where the curated assembly was monophyletic, with a sequence subset and topology highly concordant with the results of Anvi'o (see Supplementary Material Section 5.2).    3.3 PhylOligo performances\r\n  PhylOligo handles assembly contigs up to a count of several dozen thousand on a modern workstation within minutes and up to few hundred thousand on a high-memory server. Input sequences can be quality- filtered or sub-sampled with a preprocessing tool to allow for improved signal and quick tests. Several parallel computation optimizations and data compression methods including HDF5 are available to improve performance on larger datasets.     Acknowledgements\r\n  We thank the INRA bioinformatics platforms MIGALE and Genotoulbioinfo for resources and Thomas Schiex and Nathalie Villa-Vialaneix for their input.  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/itsmeludo/Phyloligo",
    "publicationDate": "0",
    "authors": [
      "Ludovic Mallet",
      "Tristan Bitard-Feildel",
      "Franck Cerutti",
      "He´ l e`ne Chiapello"
    ],
    "status": "Success",
    "toolName": "PhylOligo",
    "homepage": ""
  },
  "27.pdf": {
    "forks": 24,
    "URLs": [
      "CRAN.R-project.org/package=gProfileR",
      "www.bioconductor.org/packages/release/data/annotation/manuals/org.Hs.eg.db/man/",
      "github.com/Ensembl/ensembl-vep",
      "www.karger.com/doi/10.1159/000477561",
      "github.com/doritool",
      "bioconductor.org/packages/org.Hs.eg.db/",
      "doritool.github.io/",
      "bioconductor.org/packages/release/bioc/html/topGO.html",
      "choosealicense.com/licenses/agpl-3.0/#"
    ],
    "contactInfo": [],
    "subscribers": 13,
    "programmingLanguage": "Perl",
    "shortDescription": "The Variant Effect Predictor predicts the functional effects of genomic variants",
    "publicationTitle": "DoriTool: A Bioinformatics Integrative Tool for Post-Association Functional Annotation",
    "title": "DoriTool: A Bioinformatics Integrative Tool for Post-Association Functional Annotation",
    "publicationDOI": "10.1159/000477561",
    "codeSize": 40818,
    "publicationAbstract": "The emergence of high-throughput data in biology has increased the need for functional in silico analysis and prompted the development of integrative bioinformatics tools to facilitate the obtainment of biologically meaningful data. In this paper, we present DoriTool, a comprehensive, easy, and friendly pipeline integrating biological data from different functional tools. The tool was designed with the aim to maximize reproducibility and reduce the working time of the researchers, especially of those with limited bioinformatics skills, and to help them with the interpretation of the results. DoriTool is based upon an integrative strategy implemented following a modular design pattern. Using scripts written in Bash, Perl and R, it performs a functional in silico analysis annotation at mutation/variant level, gene level, pathway level and network level by combining up-to-date functional and genomic data and integrating also third-party bioinformatics tools in a pipeline. DoriTool uses GRCh37 human assembly and online mode. DoriTool provides nice visual reports M P 3 5 :: 3 1 8 7 y 01 ra /2 ilrLb /829 :ybd icade .-347 laoed ioBm .4102 nowD LAUC .1914",
    "dateUpdated": "2017-10-13T07:31:13Z",
    "institutions": [
      "Centro de Investigación Biomédica en red Cáncer",
      "Evangelina López de Maturana",
      "Spanish National Cancer Research Centre",
      "Universidad San Pablo CEU",
      "Isabel Martín-Antoniano and Lola Alonso Genetic and Molecular Epidemiology Group Spanish National Cancer Research Center (CNIO) C/Melchor Fernandez Almagro",
      "Núria Malats",
      "Spanish National Cancer Research Centre (CNIO)"
    ],
    "license": "No License",
    "dateCreated": "2016-04-05T12:08:24Z",
    "numIssues": 6,
    "downloads": 0,
    "fulltext": "     10.1159/000477561   DoriTool: A Bioinformatics Integrative Tool for Post-Association Functional Annotation     Isabel Martín-Antoniano  1  2  3  4  5    Lola Alonso  0  1  2  4  5    Miguel Madrid  1  4  5  6    0  Centro de Investigación Biomédica en red Cáncer ,  CIBERONC    1  Evangelina López de Maturana    2  Genetic and Molecular Epidemiology Group, Spanish National Cancer Research Centre ,  CNIO    3  Instituto de Medicina Molecular Aplicada (IMMA), Facultad de Medicina, Universidad San Pablo CEU    4  Isabel Martín-Antoniano and Lola Alonso Genetic and Molecular Epidemiology Group Spanish National Cancer Research Center (CNIO) C/Melchor Fernandez Almagro ,  3, ES-28029 Madrid (Spain) E-Mail iamartin    5  Núria Malats    6  Structural Computational Biology Group, Spanish National Cancer Research Centre (CNIO) ,  Madrid ,  Spain     2017   3  12    17  5  2017    15  5  2017     The emergence of high-throughput data in biology has increased the need for functional in silico analysis and prompted the development of integrative bioinformatics tools to facilitate the obtainment of biologically meaningful data. In this paper, we present DoriTool, a comprehensive, easy, and friendly pipeline integrating biological data from different functional tools. The tool was designed with the aim to maximize reproducibility and reduce the working time of the researchers, especially of those with limited bioinformatics skills, and to help them with the interpretation of the results. DoriTool is based upon an integrative strategy implemented following a modular design pattern. Using scripts written in Bash, Perl and R, it performs a functional in silico analysis annotation at mutation/variant level, gene level, pathway level and network level by combining up-to-date functional and genomic data and integrating also third-party bioinformatics tools in a pipeline. DoriTool uses GRCh37 human assembly and online mode. DoriTool provides nice visual reports M P 3 5 :: 3 1 8 7 y 01 ra /2 ilrLb /829 :ybd icade .-347 laoed ioBm .4102 nowD LAUC .1914    eol&gt;Association  Cancer  Functional annotation   Genome-wide association study  Genomics   Next-generation sequencing       -\r\n  including variant annotation, linkage disequilibrium proxies, gene annotation, gene ontology analysis, expression quantitative trait loci results from Genotype-Tissue Expression (GTEx) and coloured pathways. Here, we also show DoriTool functionalities by applying it to a dataset of 13 variants associated with prostate cancer. Project development, released code libraries, GitHub repository (https://github.com/doritool) and documentation are hosted at https://doritool.github.io/. DoriTool is, to our knowledge, the most complete bioinformatics tool offering functional in silico annotation of variants previously associated with a trait of interest, shedding light on the underlying biology and helping the researchers in the interpretation and discussion of the results. © 2017 S. Karger AG, Basel    Introduction\r\n   In the last decade, high-throughput genomics technologies, including genotyping arrays and next-generation sequencing, have revolutionized the characterization of\r\n  Isabel Martín-Antoniano and Lola Alonso contributed equally to this work. disease at molecular resolution. Large-scale sequencing projects, such as the 1000 Genomes Project (1000GP) [ 1 ], UK10K [ 2 ] and NHLBI GO Exome Sequencing Project (ESP) [ 3 ], are being followed by even larger projects, such as the 100,000 Genomes Project [ 4 ]. Although those datasets are of great interest to both researchers and clinicians, their ultimate value will depend not on the number of variants identified, but rather on their functional interpretation [ 5 ]. In fact, the interpretation of results from association analyses (genome-wide association studies [GWAS] and, more recently, next-generation sequencing [NGS]) remains challenging [ 6 ]. Too frequently, a comprehensive interpretation of association results based on functional in silico analysis characterizing the mechanism behind the association or identifying the real causal variant/gene [ 7 ] is missing. Therefore, once a set of variants has been found to be associated with a particular phenotype of interest, a post-association functional annotation should be a common strategy of every analysis in order to interpret mutations/variants lists. However, this task has several difficulties.  The first drawback concerns the selection of the bioinformatics tools to be used, which will largely depend on the analysis options we intend to cover. In the last decade, many alternatives have been developed to interpret the mutations/variants lists providing functional information. However, many popular pipelines have not been updated in years. Furthermore, while most tools focus on the convenient mapping of diverse gene identifiers, few provide functional enrichment analysis as part of a large platform, the majority being web services (i.e., Babelomics, g:Profiler, and DAVID) [ 8 ]. Functional enrichment analysis can also be performed using R packages (i.e., FG    Net and KEGGREST) [9], Perl scripts and Java applica\r\n  tions [ 10 ], but a more accessible and comprehensive tool would be desirable.  Another critical point is the selection of the databases that need to be consulted to find the information required to annotate the functional effect of synonymous or nonsynonymous single-nucleotide polymorphisms (SNPs) on the genes, or to obtain their predicted functional effect. In addition, it is important to know whether those databases are curated and updated, or which kind of predictor (e.g., SIFT, PolyPhen-2 or Condel) is the most suitable one [ 11-14 ].    The last but not least difficulty is related to the number\r\n  of mutations/variants to be explored. When the association analysis results in a short list of statistically significant variants, functional annotation can be performed and curated manually (extracting information from ge2 nome browsers, such as Ensembl or UCSC). However, this is a tedious work that becomes unfeasible with large numbers of mutations/variants. The research community increasingly requires automatic and programmatic access to bioinformatics tools, since few available modern webbased genomic resources offer sophisticated tools for further analysis of these data.  At present, there is a lack of a bioinformatics tool that provides complete functional information of the GWAS hits at a glance. Even Variant Effect Predictor (VEP), which is a software suite that performs annotation and analysis of most types of genomic variation in coding and non-coding regions of the genome, does not provide a complete functional annotation [ 15 ].  Here, we present DoriTool, a pipeline built to combine different bioinformatics algorithms and public databases into one comprehensive, easy, fast and friendly tool, integrating different functional instruments in order to perform a complete functional in silico assessment, taking as its starting point a list of GWAS- or NGS-derived variants. DoriTool helps maximizing the reproducibility and research timelines, reducing the working time of the researchers, especially of those with limited bioinformatics skills, and helping them in the interpretation of their results.     Materials and Methods\r\n   DoriTool is based upon an integrative strategy implemented\r\n  following a modular design pattern (Fig. 1). It allows performing a functional in silico analysis at (1) mutation/variant level, performing annotation of a set of mutations/variants, reporting expression quantitative trait loci (eQTLs) results from Genotype    Tissue Expression (GTEx) and providing their linkage disequilib\r\n  rium (LD) proxies; (2) gene level, performing annotation of the genes tagged by the set of input variants and reporting also their gene ontology (GO); (3) pathway level; and (4) network level.    Our tool requires as input data a mutation/variant call format\r\n  file (VCF) or an rs identifier SNP list (Table 1), which does not need to meet any requirement regarding size. DoriTool uses    GRCh37 human assembly and online mode. A more detailed description of DoriTool pipeline as well as the existing bioinformatics tools used follow below (Table 2).\r\n  Mutation/Variant Level  Variant Annotation. Input variants are functionally annotated using VEP [ [1 6], which is an open-source, free-to-use, actively maintained and developed toolset (https://github.com/Ensembl/ensembl-vep) for the analysis, annotation and prioritization of genomic variants in both coding and non-coding regions. VEP allows determining the effect of the variants (SNPs) on genes, transcripts and proteins, as well as regulatory regions, non-genic variants, and including transcription factor binding sites [1 17 ], using regularly updated data files that are distributed by Ensembl, and    Martín-Antoniano  et al.\r\n  M P 3 5 :: 3 1 8 7 y 01 ra /2 lirLb /829 :ybd icade .-347 leaod ioBm .1042 nowD LAUC .1914 .VCF .CSV  SNPs  annotaƟŽŶ 3 Summary_VEP.  html  Results  CompleteReport .tsv  VEP 1  Cytoband  DB  GTEx eQTLs  DB Filtered DataFiltered  Data Results 4  Other_ variant_ in_ LD_1000GP.  tsv Pairwise_  LD.tsv SummaryReport.tsv org.Hs.eg.  db  Gene annotaƟŽŶ͕  clusters and KEGG pathways Fig. 1. Schematic diagram of the DoriTool architecture. The workflow starts with an input file (VCF format, list of chromosome positions of the variants and their alleles or list of variant rs identifiers). (1) Variant Effect Predictor (VEP) annotates the input variants with the tagged gene, the impact and some annotations described in the main text, querying the Ensembl database. The next step involves locating the variants in their corresponding cytobands, and the user has the option to query the Genotype-Tissue    Expression (GTEx) database locally by providing a tissue-specific\r\n  file with the expression quantitative trait loci (eQTLs). (2) DoriTool uses topGO (FGNet), g:Profiler and DAVID (FGNet) R packages to produce gene ontology terms, functional networks, enriched pathways and other annotations. (3) The main output files are the CompleteReport that contains all the annotations at transcript level provided by VEP and the SummaryReport that includes only selected information from the former, collapsing the results by gene and effect in the transcript. (4) As an option, the user may ask for Linked variants in a window size of 500 kb, with a linkage disequilibrium (LD) cut-off specified in the main call. (5) In case the user has asked for the previous task, a new CompleteReport profiling the linked variants will be produced. SNP, single-nucleotide polymorphism; GP, Genome Project; DB, database. its output follows a standard form (VCF). Table  3 contains the plugins and default parameters of VEP in DoriTool. DoriTool also locates each variant in its corresponding cytoband, retrieving the information from the database downloaded from UCSC hg19 (http://hgdownload.cse.ucsc.edu/goldenpath/hg19/database/cytoBand.txt.gz).  Expression Quantitative Trait Loci. The user has the option to provide a database including tissue-specific significant SNP-gene pairs (with the same format as the ones GTEx provides) to obtain the effect size of the eQTLs as well as the eGene [ 18 ].    LD Proxies. DoriTool also explores proxy and putatively functional SNPs for a query SNP in a selected 1000GP population, using the Ensembl REST API in an integrated Perl script [19]. It computes and returns pairwise LD values (1) among the input variants\r\n    An Integrative Bioinformatics Tool for\r\n    Functional Annotation\r\n    Variant effect prediction tool eQTLs\r\n    Variants in LD with any overlapping existing variants from the Ensembl variation databases\r\n    Functional analysis\r\n    Annotation\r\n    Functional annotation\r\n    Annotation\r\n    Cluster\r\n    Colour pathways\r\n    Colour pathways\r\n    Cluster\r\n    Colour pathways\r\n    Script language\r\n    Perl\r\n    Bash\r\n    Perl R R R\r\n  R R R R  VEP, Variant Effect Predictor; GTEx, Genotype-Tissue Expression; LD, linkage disequilibrium; SNP, single-nucleotide polymorphism; VCF, variant call format; CSV, comma-separated values; ID, identifier; ENS, Ensembl Gene Stable IDs; eQTLs, expression quantitative trait loci.    Adds the gene symbol to the output\r\n    Changes the distance to call upstream and downstream consequence types\r\n    Finds the nearest gene to an intergenic variant\r\n    Single-nucleotide polymorphism effect prediction\r\n  and (2) between each input variant and all other variants in a surrounding window. The default parameters in DoriTool for window size and strength of LD (r2) are 500 kb and 0.90.  Gene Level    Gene Annotation. DoriTool uses 3 different tools to perform\r\n  the gene annotation, therefore allowing interpreting and identifying the biological processes for the gene list tagged by the input list of mutations/variants [ 20 ]: (1) org.Hs.eg.db (Genome wide annotation for Human, R package version 3.4.1) [ 21 ]; (2) FGNet, which uses TopGO R package [ 22 ]; and (3) g:Profiler R package [ 8 ]. The    Entrez Gene identifiers mapped to an Ensembl gene are obtained\r\n  using the genome-wide annotation for human org.Hs.eg.db [ [2 1]. It is an R package maintained by Bioconductor at bioconductor. org (http://bioconductor.org/packages/org.Hs.eg.db/). DoriTool retrieves the following gene information to annotate: symbol, gene name, and Online Mendelian Inheritance in Man (OMIM) identifier, and it focuses primarily on inherited or heritable genetic diseases, GO identifiers, protein families (PFAM) and pathway. topGO. topGO functions are included in FGNet R package (https://bioconductor.org/packages/release/bioc/html/topGO.html) to facilitate semi-automated enrichment analysis for GO terms [2 22 ], mapping the genes tagged by the input variants with the associated biological annotation terms (e.g., GO terms), and then statistically examine the enrichment of gene members for each of the annotation terms on the basis of gene counts. The default parameters in DoriTool are nodeSize = 1 (no prune) and pValThr = 0.01.  g:Profiler. g:Profiler is an open-source, free-to-use R package actively maintained on CRAN (https://CRAN.R-project.org/package=gProfileR) that performs functional enrichment analysis, including transcription factor binding site predictions, Mendelian disease annotations, information about protein expression and complexes, statistically significant GO terms, pathways and other gene function-related terms [8 8 ]. Multiple testing correction is performed by selecting the Benjamini-Hochberg false discovery rate. DoriTool manages the output given by g:Profiler as a visual report that complements and helps to interpret the results given by the other tools. 4    Martín-Antoniano  et al.\r\n  M P 3 5 : 3 1 : 8 7 y 01 ra /2 ilrLb /829 :ybd icade .-347 laeod ioBm .1042 nowD LAUC .1914 Pathway Level    Pathway Annotation. It is performed using 3 R packages: FG\r\n    Net, which uses DAVID, org.Hs.eg.db, which maps Entrez Gene\r\n  identifiers to KEGG pathways (https://www.bioconductor.org/packages/release/data/annotation/manuals/org.Hs.eg.db/man/ org.Hs.eg.db.pdf). Mappings were based on data provided by    KEGG GENOME (http://www.genome.jp/kegg/genome.html).\r\n    For the DoriTool pipeline, we included a code to obtain coloured\r\n    KEGG pathways (Table 4) considering the number of variants per\r\n  gene by using KEGGprofile [ 23 ]. Moreover, the offline mode of DAVID is considered, since it does not need specific considerations for the installation and guarantees always obtaining results even if the server is down. However, the offline mode does not allow to modify the default settings (default cut-off linkage: 0.5 and overlap = 4, initialseed = 4, finalseed = 4), resulting in a lower number of KEGG pathways and GO terms, which is compensated with the org.Hs.eg.db annotation.  Network Level  DoriTool also provides coloured functional networks of the list of genes tagged by the input variant list (Table 4). These functional connections between the different genes were based on annotations (GO) [ 24 ] and given by DAVID functions included in FGNet [ 25 ]. Building functional networks provides an overview of the biological functions of the genes/terms and permits links between genes, overlapping between clusters.  Installation    DoriTool modules are written in Perl, Bash, and R scripts and\r\n  run on any UNIX-like operating system. To install DoriTool, simply download and run the installer script, which automatically downloads the necessary libraries, packages and annotation files.    Alternatively, the user can install its dependencies manually or using the Docker container provided. The full source code and the container are freely available on the GitHub repository (https:// github.com/doritool).\r\n    The DoriTool website includes general information about the\r\n  purpose of the tool, instances and explanations about its uses, as well as the link to connect to the GitHub repository in order to download the tool directly (https://doritool.github.io/). The    DoriTool is available under a GNU AGPLv3 license (https:// choosealicense.com/licenses/agpl-3.0/#).\r\n   DoriTool is a pipeline designed to perform functional\r\n  annotation using a range of reliable proven tools and databases. Therefore, it was designed for a post-association analysis (e.g., GWAS and NGS). It combines up-to-date functional and genomic data and serves the community through a pipeline with nice visual reports.    Here, we demonstrate the abilities of DoriTool by ap\r\n  plying it to a dataset of 13 SNPs previously reported as associated with prostate cancer, which were selected ad hoc to show the scope and the full potential of DoriTool (see online suppl. File S1; for all online suppl. material, see www.karger.com/doi/10.1159/000477561).    Several output files were obtained: 4 text files (Sum\r\n  maryReport.tsv; CompleteReport.tsv; Pairwise_LD.tsv; and other_variants_in_LD_1000GP_EU.tsv), an image (gprofilerResults.png), 2 HTML files (DAVID.html and    Summary_VEP.html) and 4 folders (DAVID, KEGG,\r\n  topGO and VEP), containing the results obtained in each module of DoriTool. Online supplementary Files S2-S9 are the most important outputs obtained after running    DoriTool with the example dataset.\r\n    The output file SummaryReport.tsv is a tab-delimited\r\n  text file containing in each row, apart from the input information related to the mutation/variant, its consequence defined by the sequence ontology (http://www.sequenceontology.org/), its impact (high, moderate, low and modifier), transcript quality flags (cds_start_NF: CDS 5\u2032 incomplete, cds_end_NF: CDS 3\u2032 incomplete), the symbol of the gene tagged by that variant (considering a region of 10 kb upstream of the transcription start site and 5 kb downstream of the gene end), information regarding the transcription factor binding site related to the variant, the nearest gene in case of intergenic variants, the    Condel score (i.e., a consensus score considering SIFT\r\n  and PolyPhen-2, which ranges from 0 to 1, 0 being neutral and 1 deleterious), the cytoband, the KEGG pathway in which the tagged gene was annotated, the description of the gene, OMIM, PFAM, pathway, variants ID, Entrez    Gene, the GO term, the eGene and the eQTLs effect size.\r\n    Table 5 contains the first rows of our example, and online\r\n  supplementary File S3 shows the complete file.  The output file CompleteReport.tsv is also a tab-delimited text file containing in each row, apart from the information reported in the SummaryReport.tsv file, additional information, such as feature identifier and feature type (e.g., transcript and regulatory feature), position of the input variant in cDNA, coding sequence and protein position of the input variant, changed amino acid,     An Integrative Bioinformatics Tool for\r\n    Functional Annotation\r\n  5 na, not applicable; GO, gene ontology. codon change (the alternative codons with the variant base in upper case), existing variation (dbSNP or COS   MIC variations), distance to the transcription start site,\r\n  strand of the feature, source of the symbol (e.g., VEGA,    Ensembl or HGNC) and HGNC ID.\r\n    The LD proxies of the input variants (i.e., 1000GP vari\r\n  ants in high LD considering the individuals of European descent) are provided in the pairwise_LD.tsv and other_ variants_in_LD_1000GP_EU.tsv files. In addition to the IDs of the proxies, the D' (difference between the observed and the expected frequency of a given haplotype) and r2 (correlation) for each pair are reported, as well as the functional annotation of the proxies (SummaryReport_LDproxies.tsv), in order to facilitate more information regarding the putative functional variant (see online suppl. File S6).    The image gprofilerResults.png, resulting from the\r\n  implementation of g:Profiler package, is also reported as an output of DoriTool. This png file is a static image that 6 does not provide the possibility to interact, which can affect the interpretability of the results when many genes are tagged by the input SNPs (Fig. 2).    In g:Profiler, evidence codes distinguish different\r\n  types of associations that can occur between a gene and a property, for example GO term (stronger evidence is presented in red and weaker evidence in blue), KEGG pathway (strong evidence is presented in black) or TRANS    FAC motif. DAVID.html file was generated by the\r\n    DAVID version in FGNet (see online suppl. File S8). It\r\n  contains a comprehensive report including different views of the functional network where the genes are interconnected, the cluster/meta-group legend and some further statistics derived from the genes tagged by the variants from the input list.    The functional network is based on 2 networks/incidence matrices: common clusters and common geneterm sets. These are their transitivity values, that is, the probability that adjacent vertices of a vertex are connect\r\n     Martín-Antoniano  et al.\r\n  M P 3 5 :: 3 1 8 7 y 01 ra /2 ilrLb /829 :ybd icade .-347 laeod ioBm .1042 nowD LAUC .1914    An Integrative Bioinformatics Tool for\r\n    Functional Annotation\r\n  Public Health Genomics DOI: 10.1159/000477561 7 7 1 y 0 ra /2 r ib 29 L /8 l a PDK1  KLK3 cl 1: Metabolic process, ... cl 2: Binding, ...  Functional network  SLC22A3 KLK2  Functional network: Number of clusters: 2 Number of genes included in all clusters: 5 Number of genes included in non-filtered clusters: 5 Filter clusters (ClusterEnrichmentScore &lt;0): none Exports: Functional network: iGraph (.RData), adjacency matrices (.RData) Simplified cluster terms table (shown below): simplifiedTermsTable.txt SIGLEC6 b PDK1  Genes in several clusters  KLK3 cl1 cl2 Intersection network  KLK2 Fig. 3. DAVID results from our 13-variants set. Green circles represent genes affected by 1 variant, white circles are genes with 2 variants and red circles (not shown in the figure) would be genes affected by more than 2 variants (see Table 4). a Functional network: representation of the results as gene-term groups. Genes with a coloured background have a unique cluster (and their colour corresponds to the one shown in the legend), while genes with a white background are shared genes between clusters. b Intersection network: simplified functional network where all the genes that belong to only one meta-group are clustered into a single node. Clusters are represented by square boxes and genes by circles. Here, we show one of the clusters obtained. ed. Figure 3 shows the cluster results from our 13-variants set. Figure 3a shows 2 clusters into gene-term groups in green (only 1 variant) and white circles (2 variants). Genes with a coloured background have a unique cluster (and their colour corresponds to the one shown in the legend), while genes with a white background are shared genes between clusters. Figure 3b shows the intersection network, which is a simplified functional network where all the genes that belong to only 1 meta-group are clustered into a single node. Clusters are represented by square boxes and genes by circles. The outputs reported by   DAVID, KEGG, topGO and VEP are generated in 4 folders (Table 6).\r\n    Among them, the most relevant ones are Summary_\r\n  VEP.html (VEP folder) and the png files starting with the hsa prefix, which will be described in the following. The output file Summary_VEP.html in the VEP folder shows general statistics (variants processed, novel/existing variants, genes, transcripts and regulatory features overlapped) on the top. The variant classes and consequences (most severe) are shown using pie graphs in a user-friendly form (see online suppl. File S9). 8     Martín-Antoniano  et al.\r\n  M P 3 5 :: 3 1 8 7 y 01 ra /2 ilrLb /829 :ybd icade .-347 loaed ioBm .4102 nowD LAUC .1914 suppl. File S9). Figure 4 shows the online supplementary   File S10, which represents the pathway in cancer, one of\r\n  the KEGG pathways obtained after running DoriTool in the example dataset. Note that DoriTool reports the genes coloured according to the number of input variants tagging them.    Running DoriTool is not computationally demanding.\r\n    Less than 5 min were needed to obtain the results for the\r\n  in silico functional analyses of the 13 input variants on a    1,600-MHz laptop (running Linux).\r\n    DoriTool provides the nearest gene for the intergenic\r\n  variants as well as the consequence type. Most of the SNPs from our example were intron variants (38.5%) and upstream gene variants (38.5%) and were distributed differently by chromosome. Chromosome 6 and 19 harboured the largest number of variants for this instance (see online Conclusions    DoriTool is a new and automated integrative bioinformatics pipeline that performs a functional in silico analysis of variants previously associated with a trait of interest through a comprehensive annotation. At present, it is the\r\n     An Integrative Bioinformatics Tool for\r\n    Functional Annotation\r\n  9  M P 3 5 :: 3 1 8 7 y 01 ra /2 ilrLb /829 :ybd icade .-347 leaod ioBm .1042 nowD LAUC .1914 most complete bioinformatics tool that allows obtaining, processing and interpreting the results from genetic association analyses in a timely manner. DoriTool is mainly a gene-centric tool to find ontologies and pathways, shedding light on the underlying biology and helping the researchers in the interpretation and discussion of the results in a comprehensive and friendly manner. Acknowledgment    This work has been supported by Fondo de Investigaciones\r\n    Sanitarias (FIS), Instituto de Salud Carlos III, Spain (Grant #PI1501573).\r\n    The authors have no disclosures to declare.\r\n     ",
    "sourceCodeLink": "https://github.com/Ensembl/ensembl-vep",
    "publicationDate": "0",
    "authors": [
      "Isabel Martín-Antoniano",
      "Lola Alonso",
      "Miguel Madrid"
    ],
    "status": "Success",
    "toolName": "ensembl-vep",
    "homepage": "http://www.ensembl.org/vep"
  },
  "7.pdf": {
    "forks": 0,
    "URLs": [
      "github.com/mbeccuti/PGS",
      "doi.org/10.1214/009053606000001433",
      "doi.org/10.1155/2015/198363",
      "doi.org/10.1186/1471-2105-6-148",
      "doi.org/10.1109/TKDE.2008.239"
    ],
    "contactInfo": ["federica.martina@unito.it"],
    "subscribers": 0,
    "programmingLanguage": "R",
    "shortDescription": "",
    "publicationTitle": "Peculiar Genes Selection: A new features selection method to improve classification performances in imbalanced data sets",
    "title": "Peculiar Genes Selection: A new features selection method to improve classification performances in imbalanced data sets",
    "publicationDOI": "None",
    "codeSize": 22,
    "publicationAbstract": "High-Throughput technologies provide genomic and trascriptomic data that are suitable for biomarker detection for classification purposes. However, the high dimension of the output of such technologies and the characteristics of the data sets analysed represent an issue for the classification task. Here we present a new feature selection method based on three steps to detect class-specific biomarkers in case of high-dimensional data sets. The first step detects the differentially expressed genes according to the experimental conditions tested in the experimental design, the second step filters out the features with low discriminative power and the third step detects the class-specific features and defines the final biomarker as the union of the class-specific features. The proposed procedure is tested on two microarray datasets, one characterized by a strong imbalance between the size of classes and the other one where the size of classes is perfectly balanced. We show that, using the proposed feature selection procedure, the classification performances of a Support Vector Machine on the imbalanced data set reach a 82% whereas other methods do not exceed 73%. Furthermore, in case of perfectly balanced dataset, the classification performances are comparable with other methods. Finally, the Gene Ontology enrichments performed on the signatures selected with the proposed pipeline, confirm the biological relevance of our methodology. The download of the package with the implementation of Peculiar Genes Selection, `PGS', is available for R users at: http://github.com/mbeccuti/PGS.",
    "dateUpdated": "2017-05-22T09:27:05Z",
    "institutions": [
      "University of Turin",
      "Harbin Institute of Technology Shenzhen Graduate School"
    ],
    "license": "No License",
    "dateCreated": "2017-05-19T20:53:45Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     August      Peculiar Genes Selection: A new features selection method to improve classification performances in imbalanced data sets     Federica Martina  federica.martina@unito.it  0  1    Marco Beccuti  0  1    Gianfranco Balbo  0  1    Francesca Cordero  0  1    0  Computer Science Department, University of Turin ,  Turin, Italy, 2 GSK Vaccines, Siena ,  Italy    1  Editor: Bin Liu, Harbin Institute of Technology Shenzhen Graduate School ,  CHINA     14  8  2017   14  2017    27  4  2017    2  1  2017     High-Throughput technologies provide genomic and trascriptomic data that are suitable for biomarker detection for classification purposes. However, the high dimension of the output of such technologies and the characteristics of the data sets analysed represent an issue for the classification task. Here we present a new feature selection method based on three steps to detect class-specific biomarkers in case of high-dimensional data sets. The first step detects the differentially expressed genes according to the experimental conditions tested in the experimental design, the second step filters out the features with low discriminative power and the third step detects the class-specific features and defines the final biomarker as the union of the class-specific features. The proposed procedure is tested on two microarray datasets, one characterized by a strong imbalance between the size of classes and the other one where the size of classes is perfectly balanced. We show that, using the proposed feature selection procedure, the classification performances of a Support Vector Machine on the imbalanced data set reach a 82% whereas other methods do not exceed 73%. Furthermore, in case of perfectly balanced dataset, the classification performances are comparable with other methods. Finally, the Gene Ontology enrichments performed on the signatures selected with the proposed pipeline, confirm the biological relevance of our methodology. The download of the package with the implementation of Peculiar Genes Selection, `PGS', is available for R users at: http://github.com/mbeccuti/PGS.       -\r\n  Data Availability Statement: The download of the package with the implementation of Peculiar Genes Selection, `PGS', is available for R users at: https://github.com/mbeccuti/PGS.  Funding: This study was sponsored by Novartis Vaccines, now acquired by the GSK group of companies. FM has received PhD fellowship from GSK Vaccines srl. The funder provided support in the form of salaries for author FM, but did not have any additional role in the study design, data collection and analysis, decision to publish, or    Introduction\r\n  High Throughput (HT) experiments have become one of the major source of genomic and trascriptomic information, providing insights into the modulation of gene expression profiles of samples under different conditions. The high potential of HT technologies lies in the quantity of information obtained by one experiment which may increase the possibility of discovering unknown mechanisms underpinning the differences in the biological conditions of interest. For this reason, one of the goals of HT data analysis is the detection of biomarkers for classification purposes [1±3]. Despite this high potential, both the high-dimensional output of preparation of the manuscript. The specific roles of this author are articulated in the `author contributions' section. such technologies and the characteristics of the data set analyzed may represent an issue for classification.  The problem of high-dimensionality of the output is known as the large p small n problem, where p indicates the number of available predictorsÐi.e. the thousands of genes assessedÐ and n indicates the number of conditions tested in the experimentÐi.e. the number of samples. In the last years, several machine-learning approaches have been proposed to deal with the risk of overfitting deriving from the large p small n problem, leading to a vaste literature about classification methods and features selection/extraction [4±8].  Features selection and features extraction methods reduce the data dimensionality by removing noise and non-informative data and by storing the information needed for the classification purpose in a subset of features called signature.  The difference of the two approaches relies in how the signature is created from original data. Indeed, feature selection methods shrink the information related to sample classification in the (sub-)optimal subset by removing irrelevant or redundant variables without altering the original representation of the features. Feature extraction methods creates new predictors as combination of the features [9].  Once the signature is selected, a classification method is applied to test it. Even though both feature extraction and feature selection are good solutions for the large p small n problem, feature selection allows extraction of meaningful biological rules from the classifier without altering the original characteristics of the selected features and it is less computationally expensive to perform [9].  Another important aspect to consider in classification tasks is the possibility that the data set analyzed is characterized by imbalance among the size of classes. The issue of class imbalance and how to derive good biomarkers from such datasets has received a great deal of interest from the research community and has been the focus of several recent studies [10±13]. Recent works proposed an undersampling-based approach to handle the imbalance between classes size [ 2, 3 ]. Also new classifiers, specific for imblanced data sets, were developed and succesfully tested implemented [13, 14]. Classification algorithms are indeed affected in their accuracy performances by imbalance because it is harder to detect the discriminating characteristics of the underrepresented class. This fact leads to high levels of misclassification of samples belonging to the underrepresented class even if there is a good overall accuracy.  The imbalance problem frequently occurs in the new discipline of systems vaccinology [15, 16] where HT experiments are mainly used for the detection of gene signatures predictive of possible adverse reactions related to vaccination or suboptimal vaccine responsiveness. In this clinical context the imbalance in the data sets is due to the fact that a vaccine reaching the human testing phase is supposed to elicit an high response in the majority of the subjects, leading to a poorly populated non-responders response category.  HT studies are commonly used also in cancer-related researches where the capability to distinguish between cancerous and noncancerous tissues or to classify different type of cancer is indeed an inestimable help in medical and biological domain.  Microarrays are a commonly used HT technology which measure the expression levels of large numbers of genes simultaneously or to genotype multiple regions of a genome extracted from a relatively small human samples. The necessity of tools to extract significant information from microarray experiments leads to a vaste amount of softwares and packages available [17±19]. However, the absence of the possibility, in case of imbalanced data sets, to set the parameters of a classifier in order to correctly classify samples belonging to the underrepresented class, is the cause of one of the shortcoming of currently available tools for microarray data analysis. 2 / 18  In this study we present a new features selection method, called Peculiar Genes Selection (PGS), to identify predictive biomarkers that are robust to class imbalance and improve the Support Vector Machine (SVM) classification performances in case of imbalanced data sets. The PGS procedure is also applied to a balanced dataset to confirm that it perform well also when the size of the classes is the same. The biomarkers generated by this procedures are therefore a suitable compromise between maximum overall accuracy and correct classification within the underrepresented class.  The novelty of PGS relies on the simple and fast computation of a binary matrix, created by fitting a logistic regression model using single gene expression as predictor of the class label of samples. Grids of parameters for the SVM (i.e. different kernels, same kernels with different initial coefficient and degree, cost and class weights) were explored to maximize one of the three metrics related to classification tasks: overall accuracy, specificity and sensitivity. Since these three metrics are strongly inversely related, the parameters maximizing one do not maximize the other two.  We then compare the classification results obtained on two public microarray data sets with those obtained using one of the available packages for microarray data analysis: the Classification for MicroArrays package, CMA, available under the Bioconductor distribution for R software [18] and with those obtained using MRMD, Maximum Relevance Minimum Distance, a correlation-based feature selection procedure minimizing the feature redundancy and maximizing the correlation with the target class [20].    Materials and methods\r\n   Data sets\r\n  We applied our pipeline on two public sets of data available at NCBI GEO database: a vaccination-related, (GEO accession code: GSE48024) [21], and a cancer-related, (GEO accession code: GSE19804) [22].  Vaccination dataset description. This dataset is the result of two studies conducted on two different cohorts of patients. The first cohort, consisting of 119 adult male subjects vaccinated with the 2008±2009 inactivated trivalent influenza vaccine (A/ Brisbane/ 59/ 2007 [H1N1], A / Brisbane/10/ 2007 [H3N2], B/ Florida/ 4/ 2006, Sanofi-Pasteur, Lyon, France). A second cohort, included 128 adult females that received the 2009±2010 trivalent influenza vaccine (A/ Brisbane/ 59/ 2007 [H1N1], A/ Brisbane/ 10/ 2007 [H3N2], B/ Brisbane/ 60/ 2008 strains, Sanofi-Pasteur, Lyon, France).  Fig 1 shows the schedule of the visits along with the data that were used for this study. Peripheral blood samples were taken immediately before (day 1−) and after vaccination on days 1, 3, and 14 whereas the antibody titers measurements were available before vaccination and on days 14 and 28 after vaccination.  Fig 1. Schematic representation of the vaccination time scheduling experiment used in the present study. 3 / 18  Classification of subjects. Since the proposed pipeline is specific for binary classification problems, the selected subjects were classified into two classes: `High responders' and `Low responders'. The classification is usually based on the 4 fold-increase of the antibody titers following the vaccination. However, because of the well known baseline effect [23, 24], consisting in an inverse relation between fold-increase and baseline levels of the antibody titers, in influenza-related trials this rough criterion might lead to misclassification. To circumvent this issue, a threshold Ti, i 2 { H1N1, H3N2, FluB } was detected as the highest baseline level at which a subject was able to reach a 4 fold-increase in his antibody titers.  Cancer data set description. The cancer-related data set consists in 60 paired samples of tumor and adjacent normal lung tissue coming from a study conducted in Taiwan on nonsmokers females aged 50 to 70 years old. Three tumors are represented in the data set: Adenocarcinoma, Bronchioloalveolar and Squamous carcinoma (56, 3, 1 sample respectively).    Methodology\r\n  The proposed methodology is a new feature selection procedure called Peculiar Genes Selection (PGS), that detects genes characterizing the class of the samples they belong to. The classification accuracy performances of the gene signature derived from the application of PGS are evaluated using a SVM classifier exploring grids of parameters to maximize the desired metric: overall accuracy and specificity.  The Peculiar Genes Selection procedure. In the proposed pipeline the feature selection is performed in three steps: 1. Identification of Differentially Expressed Genes (DEGs) 2. Identification of good predictors 3. Selection of the peculiar good predictors for each class  Step 1 allows the detection of J differentially expressed genes under two conditions of interest leading to a significative data dimensionality reduction [25].  Step 2 is based on the computation of a regression model in which single variable levels are used to predict the class label of each subject [26].  Let N be the number of subjects with n + m = N, n being the number of subjects belonging to class 0 (`C0') and m the number of subjects belonging to class 1(`C1'). Let also J be the number of DEGs detected in step 1. We indicate with Xj = {xj1, . . ., xjN}, j 2 {1, . . ., J} the expression of the j-th DEG across all subjects and with y the ordered N dimensional vector of true classifin N cation labels, where fyigi\u00881 \u0088 0, and fyigi\u0088n\u00871 \u0088 1.  PGS computes J logistic regressions to predict the probability of each subject to be a success, in other words to belong to class `1', given the expression of the j-th independent variable Xj. Eq (1) shows the model of logistic regression used, where pi is the predicted probability of success for subject i, β0 the intercept of the model, βj the fitted parameter and Xji the expression of the j-th gene of subject i.  logit\u0085pi\u0086 \u0088 ln 1 pi pi \u0088 b0 \u0087 bjXji  The logistic-regression fit lead to J N-dimensional vectors p of predicted probabilities of `success', where each component is the pi calculated in Eq (1). Since the possible class labels are only 0 and 1, the classification vector y^, predicted using the j-th gene as independent variable, 4 / 18 \u00852\u0086 \u00853\u0086 \u00854\u0086  The peculiar genes are now easily detectable as the genes (rows of M) voting for the correct classification of the most misclassified subjects and they are denoted by G0 and G1 for `low responders'and `high responders'respectively.  It may occurs that either G0 \u0088 ; or G1 \u0088 ;. In this case, one can relax the thresholds that define the misclassified samples or introduce a `tolerance' in the features choice, for example including features misclassifying a pre-selected number of most misclassified samples. is obtained by applying the following criterion: y^i \u0088 ( 1 if 0 else pi t where τ stands for pre-selected threshold value that can varies in case of strong imbalance between classes size.  The comparison of y^ with y measures the ability of each predictor to correctly classify the subjects. This quantity is called predictive power (pp) and it is defined as follows: ppj \u0088  XN i\u00881 1fy^i\u0088yig ;  N  8j 2 f1; . . . ; Jg:  The J values of pp form a distribution of predictive power values, describing the ability of the DEGs of classifying the samples. The P, with P J, good predictors are chosen among the DEGs which pp &gt;= q, with q being a quantile of the predictive powers distribution, describing the minimal number of subjects a DEG needs to correctly classify to be considered a good predictor.  Step 3 consists in the analysis of the binary matrix MP N , where each row M\u0085p; \u0086 is represented by one of the P binary vectors y^ created in step 2 and each column M\u0085 ;i\u0086 contains the classification labels assigned to subject i by each predictor.  In an unrealistic situation where all the DEGs have pp = 1, meaning that y^i \u0088 yi, 8i 2 f1; . . . ; Ng, M would have the first n columns filled with 0 and the remaining m columns filled with 1.  To select the peculiar predictors for `C0', denoted by G0, the algorithm focuses on the first n column of M containing the n samples belonging to class 0. It detects the features that assign the correct class label to the majority of the samples and simultaneously detects the most mislclassified samples. In fact, the basic idea is that the peculiar genes are those capable to correctly classify a subject when the majority of genes misclassifies them.  The criterion used to identify the most misclassified subjects is based on the analysis of the n columns of M. Whenever the p-th gene misclassifies the i-th sample then M\u0085i;p\u0086 \u0088 1 since we are focusing on samples whose correct classification is 0. Hence, the sum overall the i-th column of M represents the number of features misclassifying the sample. The column indexes corresponding to highest sum values are the most misclassified samples. The same procedure applies to samples belonging to class 1, being careful to minimize the sum values instead of maximizing them, in order to find the most misclassified samples. To formalize this procedure, Kk, k 2 {0, 1} indicates the chosen quantile of the misclassification distribution to detect the most misclassified subjects in the two classes:  X n i\u00881 M\u0085 ;i\u0086 &gt; K0;  M\u0085 ;l\u0086 &lt; K1: X  N l\u0088n\u00871 The final selected signature will be the union of the two lists of features, denoted by S \u0088 G0 [ G1 \u00855\u0086  The classifier parameters setting. The PGS procedure provides a signature S that is used to build our classification model. Classification algorithms extract meaningful rules from available data to build a model capable of correctly classify new inputs with the right label. Learning procedures can be supervised or unsupervised. In the first case the problem is presented with example inputs and their desired outputs, given by a priori knowledge and the goal is to learn a general rule that maps inputs to outputs. In the second case no labels are given to the learning algorithm, leaving it on its own to find structures capable of classify its input [ 27, 28 ].  In this paper we present a supervised approach using the Support Vector Machine (SVM) as classifier [ 29 ]. SVMs are widely studied and used classifiers in many different domains as described in [ 30 ] and [ 31 ]. Recently, they also became a useful tool in the classification of samples coming from microarray experiments [ 32, 33 ].  SVMs separate a given set of binary labeled training data finding the equation of the hyperplane that maximizes the distance between the two classes. In case of noisy/sparse data the linear separation of the two classes is not always possible in the input space. In this case SVMs can perform a non-linear mapping of data in a so called feature space where the classes are linearly separable by using the `kernel' technique [ 34 ].  Let S be a sample of n labeled data points: S = {(x1, y1), . . ., (xn, yn)}, where xi 2 R , n yi 2 {0, 1} and let : I Rn ! F RN be a mapping from the input space to the feature space F. The kernel technique allow to define the inner product in the feature space without computing the mapping of inputs xi ! ϕ(xi) by the relation K(xi, xj) = ϕ(xi) ϕ(xj). Classical choices for kernel functions are kxi xjk Gaussian : Kij\u0085xi; xj\u0086 \u0088 e s2 Polynomial : Kij\u0085xi; xj\u0086 \u0088 \u0085hxi; xji \u0087 c0\u0086d Sigmoid : Kij\u0085xi; xj\u0086 \u0088 tanh\u0085axiTxj \u0087 r\u0086 where σ, d, a, r are kernel parameters to be tuned.  SVMs give the possibility to chose a constant c to account penalties for misclassification. This pipeline uses SVM as classifier and explores a grid of parameters in order to detect the setting which allows the best classification performances on the selected metric.  In order to assess the accuracy of the model, it is common practice to split the original data set into k training and validation sets [ 35, 36 ]. This procedure is called k-fold cross-validation and usually uses the 80% of the original data for training the model and the 20% of the remaining samples to check the accuracy of the model on new inputs [ 37 ]. All the performances of the model presented in this work are evaluated with a 10 fold cross-validation procedure.     Results\r\n  To evaluate the proposed approach, we applied the PGS on the two public microarray data sets described in Materials and Methods and we compared the classification performances to those obtained using MRMD software and the CMA package. 6 / 18 Fig 2. Negative association between baseline MN titers and titers fold-increase. Almost all subjects with a baseline MN titers higher than 256 did not reach the 4 fold-increase, fact that may lead to confounding effects in the classification procedure. This condition was met with all three antigens of the vaccine H1N1 (A), H3N2 (B), FluB(C).   Vaccination dataset\r\n  The proposed pipeline is specific for binary classification tasks. As described in Materials and Methods section, the binary classification of the samples was based on the 4-fold increase in the antibody titers levels: subjects reaching a 4-fold increase between day 1− and day 28 against at least one of the three antigens were labeled as `high responders'while the others were labeled as `low responders'.  Fig 2A, 2B and 2C show that subjects with antibody-titers T such that Ti &gt; 256, i 2 {H1N1, H3N2, FluB} at the baseline, never exceed a 4-fold increase. Therefore, to avoid misclassification due to pre-existing immunity, all subjects with a baseline titer Ti &gt; 256 were excluded from subsequent analysis.  According to the classification criterion, 49 of the remaining samples were labeled as low responders and 144 as high responders. With such a relatively small sample size, it appears evident the imbalance between the two class sizes: the number of high responders is around three times the number of low responders. This situation is not surprising: the vaccination is expected to elicit a good response in the majority of the people.  The features selection procedure. PGS, as described in Materials and Methods, consists of three steps. The identification of DEGs, (Step 1), was performed using limma package [17], available under Bioconductor distribution. We set as contrasts the difference in genes expression one day before and one day after the vaccination. This step allowed us to reduce the data dimensionality of one magnitude order: from 28450 genes present on the microarray, only 3605 were significantly differentially expressed (Adjusted p-value 0.01).  For the detection of the good predictors (Step 2), we applied the logistic regression model using fold-change of the gene expression between day 0 and day 1 as independent variable. To predict the class label of each subject we then applied the criterium presented in Eq (2), setting τ = 0.6. The fit of the model allowed us to compute, for each gene, the proportion of subjects correctly classified, which is referred to as predictive power (pp). We defined good predictors the genes whose pp belonged to the 5th percentile of the pp distribution of all the DEGs as showed in Fig 3.  The peculiar genes selection (Step 3), required the analysis of the binary matrix obtained in Step 2. Computational experiments showed that by setting K0 = 165 and K1 = 20, we identified 2 highly frequently misclassified `low responders' and 5 frequently misclassified `high responders'. The algorithm detected 11 peculiar genes belonging to G0 and 19 belonging to G1. 7 / 18 Fig 3. Histogram showing the predictive power distribution of all DEGs. The right 5th percentile of the distribution, in black, represents the number of genes selected as good predictors. The numbers on the columns of the histogram represent the number of genes with the same pp.  Tables 1 and 2 shows that the Gene Ontology (GO) on S, performed with Erichr [ 38, 39 ], found statistically significant enrichments in the biological processes involved in the immune response to virus. More specifically, the cytokine-mediated signaling pathway and in the type-I interferon signaling pathway are known to play an important role in host defense against virus. Table 3 shows that the antigen processing and presentation pathways are significantly enriched in the signature S, confirming the biological relevance of PGS. Adjusted p-value Adjusted p-value  To conclude the comparison of the feature selection procedures, we compared the lists of genes belonging to the different signatures. Fig 4 showed that MRMD, PGS and Random Forest selected signatures with the lowest overlap with all the others, whereas the other features selection procedures show a good overlap of genes. Fig 4. Number of genes shared by the signatures selected from the different feature selection methodologies. 9 / 18  To further investigate the properties of the genes shared among the different sigantures, we detected 43 genes selected by at least 5 different feature selection procedures. The GO perfermed on the list of the most selected genes showed results reported in Table 4, where no biological process is significantly enriched, suggesting that even considering the union of the most selected genes in the different signatures, the number of genes representing a particular biological process is too low to lead to a significant enriched pathway.  Classification results. We performed classification experiments using the feature selected as described in the previous paragraph to build a classification model with SVMs using a 10-fold cross-validation. Table 5 shows the results obtained applying our pipeline on the two most interesting cases for this set: the maximization of the accuracy on the entire validation set and the maximization of the specificity -i.e. when we want to detect all the samples belonging to the underrepresented class-.  The 10 fold-cross validation showed that the model reached its best performance maximizing the overall accuracy when the kernel of the SVM is a polynomial; -i.e. of the form Kij(xi, xj) = (hxi, xji + c0)d, where d = 5 is the degree and c0 the initial coefficient. The associated cost is 0.1, where the cost parameter for an SVM represents the tolerance of misclassification within each training example. When the parameter is small it means that the SVM looked for a larger-margin separating hyperplane allowing misclassification. To maximize the overall accuracy, the weight vector, the parameter accounting for the imbalance in the SVM has equal components: the same weight is given to both classes. Interestingly the situation is completely different when we want to detect all the subjects belonging to the underrepresented class: in this case, the imbalance in the classes size is reflected by the weight vector that shows the same strong imbalance between its two components.  Table 6 shows the best classification performances using different feature selection procedures in combination with the same classifier SVM where the tuning was possible only for fewer parameters. The list of genes selected by all the feature selection procedures tested are reported in S1 File.    Cancer data set\r\n  In Cancer data set description we pointed out that this data set is composed by paired-samples: each subject indeed, provided two samples of tissue, one normal and one tumor. This important remark has two main consequences on the subsequent analysis: Coefficient 1 -6  Cost 1. no risk of misclassification in labeling the samples with 0 or 1 2. perfect balance between the size of the two classes, with 60 samples in each.  The features selection procedure. To detect DEGs in this case study (Step 1) we used the limma setting as contrast the class labels of the samples. We detect 10901 DEGs starting from a chip containing 21655 genes. To detect the good predictors, as described in Step 2, we applied the logistic regression model using the gene expression of each sample to predict its class. For this dataset the τ used to predict the class labels of the samples was τ = 0.5. We obtained the pp distribution and, we defined good predictors the genes whose pp belonged to the 99th percentile of the pp distribution of all the DEGs as showed in Fig 5.  For this data set, the peculiar gene selection (Step 3 of our procedure) required the analysis of the binary matrix obtained in Step 2. Computational experiments showed that by setting K0 = 10 and K1 = 80 this analysis led to the detection of 2 highly frequently misclassified `low responders' and 6 highly frequently misclassified `high responders'. The algorithm, found 4 peculiar genes belonging to G0 and 20 peculiar genes belonging to G1. The GO performed on the signature S did not show any biological process nor molecular function significantly enriched whereas the analysis using the oncogenic signature database showed an enrichment on epidermal growth factor receptors as reported in Table 7.  Also for this data set, we compared the lists of genes selected by the different feature selection procedures. As in the vaccination data set, Fig 6 shows that the overlap among the genes selected by PGS and MRMD with those selected by the other feature selection procedures is low. On the contrary, despite the small number of genes per signature, the other feature selection methods seem to have a better agreement on the choice of the predictors. The list of genes selected by all the feature selection procedures tested are reported in S2 File.  Accordingly to what we did for the vaccination data set, we selected the genes that appeared in at least 5 different signatures as the `most selected genes'and we performed a gene ontology study. The GO results showed that no biological process nor molecular functions were significantly enriched, results confirmed also for the `most selected genes'. However, using the oncogenic signature database, we found again a significant enrichment for the epidermal growth 11 / 18 Fig 5. Histogram showing the predictive power distribution of all DEGs. The 99th percentile of the distribution, in black, represents the number of genes selected as good predictors. factor, p-value &lt;0.05 but not for KRAS. Significant enrichments were detected for genes upregulated during early stages of differentiation of embryoid bodies from embryonic stem cells or embryonic fibroblasts (ESC V6.5 UP EARLY.V1 UP, NFE2L2.V2, PRC2 SUZ12 UP.V1 UP, ESC V6.5 UP LATE.V1 UP). The results of GO performed on the oncogenic database are showed in Table 8.  Classification results. We applied PGS method to the cancer data set and compared the results with those obtained by using the CMA package with all the feature selection methods available. For this data set no parameter tuning was necessary: the optimal results were obtained by using the default settings of the SVM classifier, choosing a polynomial kernel: the default degree is 3, the initial coefficient is 1. The optimal cost here is, again, 0.1. The perfect balance between the two classes size lead to the logical choice of a weight vector whose components are equal (i.e. w0 = w1 = 1). Adjusted p-value 0.007 0.01 12 / 18 Fig 6. Number of genes shared by the signatures selected from the different feature selection methodologies. 13 / 18     Discussion\r\n  The advent of HT technologies are fostering the implementation of different computational approaches for classification tasks. However, intrinsic characteristics of the data set, such the imbalance between the size of the classes, still represent an issue for the classification purpose.  In this paper we present a new feature selection method in 3 steps, called Peculiar Genes Selection, for the analysis of high dimensional data sets. The proposed pipeline detects the features that characterize the two classes and use them as a biomarker for predicting the class label of new inputs.  We applied PGS on two different data sets and then compared the classification performances with those obtained using other features selection methods already implemented in the CMA package and MRMD software. Following the recent literature about classification tasks with biological data, we decided to use as classifier an SVM. However, the presented pipeline can be used in combination with any other classifier that better suits the researcher purposes.  Two case study are considered, both concerning microarray experiments: one from a vaccination trial, the other from a cancer study. Despite all data come from microarray experiments, the two data sets have different characteristics that impact on the classification task. In the vaccination case, the blood samples come from healthy subjects who probably already encountered influenza virus before being enrolled for the trial; this situation is clearly reflected The `NA'for the MRMD method means that the machine run out of memory for this data set. 14 / 18 by the baseline effect underlined in Results section. Additionally, the vaccination data set required a first step of analysis to assign the class label to samples, step that was unnecessary for the cancer data set. In such a scenario we have to consider also the noise present in the data, explained by the across-subjects variability and by the fact that the expression of transcripts in the whole blood is a surrogate tissue to measure the immune response.  Second, the strong imbalance in the two classes size prevent the correct classification of both classes simultaneously: either the SVM privileges the overall accuracy penalizing the underrepresented class or it can detect all the underrepresented class but does not correctly classify the other one.  For this case study, the classification results reported in Tables 5 and 6 show that it is not possible to reach an overall accuracy higher than 82% using PGS as feature selection method and an averaged overall accuracy of 66.4% using the features selection methods included in the CMA package. PGS detected a signature of 14 genes belonging to G0 and 17 genes belonging to G1. The GO of the resulting signature S showed enrichments in response to virus and in cytokines-mediating signaling pathways, confirming the biological meaning of the proposed procedure.  In the cancer case-study PGS detected a signature of 40 genes, 27 in G0 and 13 in G0, whose GO on an Oncogenic Signature database shows enrichment in epidermal growth factor receptors accordingly to recent literature [ 40 ]. The biological processes enriched are related to DNA replication processes in sprouting angiogenesis, but they are not associated with significant adjusted p-values, see S3 File.  In the conclusions reported in [22], the authors underline the significant role of the axons signaling pathway in the survival analysis; interestingly, we found an enrichment in the axon guidance signaling pathway as well, but it wass not associated with a significant adjusted p-value.  The overall accuracy in this case is around 99% for all the feature selection methods tested, result that can be explained with the absolute absence of imbalance in the experimental design along with the fact that the gene expression was taken from normal and tumor tissues, in other words it is a direct measure.    Conclusion\r\n  Microarray experiments measuring gene expression levels are source of important biological informations. Machine-learning algorithms are used to build predictive models and to find biomarker for classification tasks from data sets coming from microarray experiments. The high dimensionality of these technology outputs requires a first step of dimensionality reduction and the necessity of not losing important information gave rise to lots of feature selection approaches.  However, the characteristics of the data set analyzed, such as the biological conditions tested, the source of the genetic material and the human gene expression variability, still have a strong impact on the algorithms affecting their classification performances. When the samples are taken from direct sites of the considered conditions, the available computational approaches are capable of shrink the information contained in the microarray in a small set of genes and detect a biomarker as proved in the cancer related case-study. In vaccine-related studies we need to be more careful and deal with the fact that people undergoing vaccination are healthy, condition that makes difficult to detect a real significant gene expression change. The proposed procedure improve the classification performances in case of imbalanced data sets by selecting genes that are predictive for the two classes separately, reducing the risk of a loss of information about the underrepresented class when compared to other feature selection methods. 15 / 18    Supporting information\r\n  S1 File. Lists of genes selected by the different feature selection procedures for the vaccine S2 File. Lists of genes selected by the different feature selection procedures for the cancer S3 File. Biological Processes enriched on S in Cancer study.    Acknowledgments\r\n  We wish to thank Emilio Siena and Duccio Medini at GSK Vaccines in Siena, for their support and their thoughtful feedbacks and discussion on the manuscript. data sets. (XLSX) data sets. (XLSX) (XLSX)    Author Contributions\r\n  Conceptualization: FM FC MB GB.  Data curation: FM.  Formal analysis: FM FC.  Funding acquisition: FM.  Investigation: FM FC.  Methodology: FM.  Project administration: FC.  Resources: FM FC.  Software: FM.  Supervision: FM FC.  Validation: FM FC.  Visualization: FM FC MB GB.  Writing ± original draft: FM FC.  Writing ± review &amp; editing: FM FC MB GB. 16 / 18 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25.  Liao JG, Chin KV. Logistic regression for disease classification using microarray data: model selection in a large p and small n case. Bioinformatics. 2007; 23(15):1945±1951. https://doi.org/10.1093/bioinformatics/btm287 PMID: 17540680 Kosorok MR, Ma S. Marginal asymptotics for the ªlarge p, small nº paradigm: With applications to microarray data. Ann Statist. 2007; 35(4):1456±1486. https://doi.org/10.1214/009053606000001433 Jirapech-Umpai T, Aitken S. Feature selection and classification for microarray data analysis: evolutionary methods for identifying predictive genes. BMC bioinformatics. 2005; 6:148. https://doi.org/10.1186/1471-2105-6-148 PMID: 15958165 Bø T, Jonassen I. New feature subset selection procedures for classification of expression profiles. Genome biology. 2002; 3(4). PMID: 11983058 Yu L, Liu H. Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution. International Conference on Machine Learning (ICML). 2003; p. 1±8.  Hira Zena M, Gillies DF. A Review of Feature Selection and Feature Extraction Methods Applied on Microarray Data. Advances in Bioinformatics. 2015; 2015(1). https://doi.org/10.1155/2015/198363 PMID: 26170834 He H, Garcia EA. Learning from imbalanced data. IEEE Transactions on Knowledge and Data Engineering. 2009; 21(9):1263±1284. https://doi.org/10.1109/TKDE.2008.239 Dal Pozzolo A, Caelen O, Waterschoot S, Bontempi G. Racing for unbalanced methods selection. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). 2013;8206 LNCS:24±31.  Song L, Li D, Zeng X, Wu Y, Guo L, Zou Q. nDNA-prot: identification of DNA-binding proteins based on unbalanced classification. BMC bioinformatics. 2014; 15(1):298. https://doi.org/10.1186/1471-2105-15298 PMID: 25196432 17 / 18    ",
    "sourceCodeLink": "https://github.com/mbeccuti/PGS",
    "publicationDate": "0",
    "authors": [
      "Federica Martina",
      "Marco Beccuti",
      "Gianfranco Balbo",
      "Francesca Cordero"
    ],
    "status": "Success",
    "toolName": "PGS",
    "homepage": ""
  },
  "94.pdf": {
    "forks": 0,
    "URLs": ["github.com/XiongLi2016/ESMO/tree/master/ESMO-com"],
    "contactInfo": [
      "lx_hncs@163.com",
      "journals.permissions@oup.com"
    ],
    "subscribers": 0,
    "programmingLanguage": "Matlab",
    "shortDescription": "exhausitive search and multiple objectives",
    "publicationTitle": "A fast and exhaustive method for heterogeneity and epistasis analysis based on multi-objective optimization",
    "title": "A fast and exhaustive method for heterogeneity and epistasis analysis based on multi-objective optimization",
    "publicationDOI": "10.1093/bioinformatics/btx339",
    "codeSize": 1993,
    "publicationAbstract": "Motivation: The existing epistasis analysis approaches have been criticized mainly for their: (i) ignoring heterogeneity during epistasis analysis; (ii) high computational costs; and (iii) volatility of performances and results. Therefore, they will not perform well in general, leading to lack of reproducibility and low power in complex disease association studies. In this work, a fast scheme is proposed to accelerate exhaustive searching based on multi-objective optimization named ESMO for concurrently analyzing heterogeneity and epistasis phenomena. In ESMO, mutual entropy and Bayesian network approaches are combined for evaluating epistatic SNP combinations. In order to be compatible with heterogeneity of complex diseases, we designed an adaptive framework based on non-dominant sort and top k selection algorithm with improved time complexity O(k*M*N). Moreover, ESMO is accelerated by strategies such as trading space for time, calculation sharing and parallel computing. Finally, ESMO is nonparametric and model-free. Results:We compared ESMO with other recent or classic methods using different evaluating measures. The experimental results show that our method not only can quickly handle epistasis, but also can effectively detect heterogeneity of complex population structures. Availability and implementation: https://github.com/XiongLi2016/ESMO/tree/master/ESMO-com mon-master. Contact: lx_hncs@163.com VC The Author 2017. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com",
    "dateUpdated": "2017-10-13T07:09:24Z",
    "institutions": ["East China Jiaotong University"],
    "license": "No License",
    "dateCreated": "2017-05-06T02:38:12Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx339   A fast and exhaustive method for heterogeneity and epistasis analysis based on multi-objective optimization     Xiong Li  0    Associate Editor: John Hancock    0  School of Software, East China Jiaotong University ,  Nanchang 330013 ,  China     2017   1  1  8   Motivation: The existing epistasis analysis approaches have been criticized mainly for their: (i) ignoring heterogeneity during epistasis analysis; (ii) high computational costs; and (iii) volatility of performances and results. Therefore, they will not perform well in general, leading to lack of reproducibility and low power in complex disease association studies. In this work, a fast scheme is proposed to accelerate exhaustive searching based on multi-objective optimization named ESMO for concurrently analyzing heterogeneity and epistasis phenomena. In ESMO, mutual entropy and Bayesian network approaches are combined for evaluating epistatic SNP combinations. In order to be compatible with heterogeneity of complex diseases, we designed an adaptive framework based on non-dominant sort and top k selection algorithm with improved time complexity O(k*M*N). Moreover, ESMO is accelerated by strategies such as trading space for time, calculation sharing and parallel computing. Finally, ESMO is nonparametric and model-free. Results:We compared ESMO with other recent or classic methods using different evaluating measures. The experimental results show that our method not only can quickly handle epistasis, but also can effectively detect heterogeneity of complex population structures. Availability and implementation: https://github.com/XiongLi2016/ESMO/tree/master/ESMO-com mon-master. Contact: lx_hncs@163.com VC The Author 2017. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com       1 Introduction\r\n  A central goal for epidemiologists is to understand how the DNA sequence variations influence the progression of complex diseases and predict common complex diseases such as diabetes and cancers and so on. Because the calculation on millions of single nucleotide polymorphisms (SNP) suffered an enormous challenge of computing resources, most of traditional genome-wide association studies adopted single variable strategy to simplify diseases models for saving costs. However, this kind of strategy yields perceived problems such as lack of reproducibility and 'missing heritability'  (Urbanowicz et al., 2013) .  One major reason for the problems is that the development and progression of complex diseases involve multiple SNPs which may be epistatic. Single variable methods aimed at major effects do not work well for analyzing epistasis  (Moore et al., 2010) . Recently, lots of multi-locus epistasis methods have been designed and can be generally categorized as exhaustive search, heuristic search and machining learning methods  (Jing and Shen, 2015; Tuo et al., 2016; Xie et al., 2012) . Exhaustive methods which evaluate the association strength of all possible multi-locus epistatic combinations on disease state have been criticized for their huge computational burden. MDR is the most representative method of exhaustive search, in which multi-locus genotype predictors are effectively induced from n dimensions to one dimension  (Moore et al., 2006) . The crossvalidation and permutation testing of MDR are the key steps for the best model selection. MDR 2.0 beta 8.4, the latest implementation of MDR methodology, can be used for detecting, characterizing and visualizing epistasis in a elegant manner. Although MDR 2.0 is powerful and efficient in a parallel mode, it confronts two major drawbacks: (i) embedded cross-validation and permutation in exhaustive search consume lots of computation resources; (ii) insufficient power in heterogeneous datasets. An approach KNN-MDR basically follows the same pipeline of MDR by modifying majority vote within a set of the K nearest neighbors of the tested individual  (Abo and Farnir, 2017) . For heuristic search, FHSA-SED  (Tuo et al., 2016)  and MACOED  (Jing and Shen, 2015)  combine two objectives to heuristically search two-locus epistatic combination spaces. Based on swarm intelligent algorithm and multi-objective optimization, FHSA-SED and MACOED perform better than previous heuristic approaches. However, they also meet several challenges. Firstly, they are specific for two-locus epistasis. Taking MACOED as an example, the heuristic pheromones of n-1 loci is inefficient for constructing n loci epistasis in pure epistasis analysis, especially for high-order epistasis with huge combination space. This is because for pure epistasis model, the genotypes in n-1 loci may not shown any association with the disease states. Secondly, their performance still needs to be further improved. And, tuning the parameters (such as the number of iterations, the size of swarm and so on) which significantly influence the power and computational costs is a hard work. Finally, due to the randomness of the swarm intelligent algorithm, although running on the same dataset, the results may be inconsistent, leading to clinical researchers to lose confidence in the bioinformatics approach. Machining learning based methods such as logistic regression  (Wu et al., 2009)  and Bayesian network  (Jiang et al., 2011)  are like a black box which is hard to interpret the relationship between epistasis and complex diseases.  Another reason is that the impact of heterogeneity is neglected in most of association studies. Heterogeneity refers to independent effects corresponding to subgroups of complex diseases  (Urbanowicz et al., 2013) . The mixture of subgroups leads to epistatic patterns to be hard to be detected, partly because the sample size of each homogeneous subgroup is reduced and the epistatic signals are interfered with each other. A common strategy for handling heterogeneity is to pure its confounding effect by data stratification  (Fenger et al., 2008) , resulting in the loss of power. Only a few approaches concurrently analyze the phenomena of epistasis and heterogeneity without resorting to some form of stratification  (Urbanowicz et al., 2013) . MDR profiles heterogeneity by returning all underlying epistatic models which may correspond to different subtypes of complex diseases. However, the power on heterogeneous datasets still needs to be improved. The major reason is that the single objective may be partial or insufficient for profiling complex genetic structure of heterogeneity. An adaptive learning classifier systems (LCSs) are a rulebased method that integrate machine learning with evolutionary computing and other heuristics  (Urbanowicz et al., 2013) . LCSs addresses heterogeneity by introducing multiple classification rules which also break single objective paradigm. However, it is not accessible for download.  Undoubtedly, the power of association studies is the first indicator to other measures such as computational costs. In most cases, exhaustive strategy is criticized for its uneconomical computation mode and its infeasibility on genome-wide. For example, the calculation complexity of exhaustive method is O nk S , in which n is the number of SNPs and k is the epistasis order and S is the complexity for model selection. Obviously, in exhaustive search mode, every uneconomical operator will be significantly enlarged. For example, the embedded cross-validation operator is the main obstacles for MDR. Despite this, we insist to apply exhaustive strategy in this study because of three reasons. Firstly, exhaustive search ensures the stability and globalization of solutions. In addition, dimension reduction method can be applied for filtering genome-wide SNPs before searching. Finally, the high performance computing platform (e.g. GPU and cloud computing) further mitigates the computational burden of large scale calculation. For example, to efficiently exploit the whole computational capacity of modern clusters based on GPUs and Xeon Phi coprocessors, pairwise epistasic detection method is run on heterogeneous clusters using both types of accelerators  (Gonzalez-Domınguez et al., 2015) ; epiSNP also has been modified for modern multi-core and highly parallel many-core processors to efficiently handle these large datasets, with exquisite parallelism scheme such as the serial optimizations, dynamic load balancing and so on  (Weeks et al., 2016) .  In this study, we propose a fast evaluation scheme for concurrently handling epistasis and heterogeneity based multi-objective optimization in exhaustive search called ESMO. We compared ESMO with other methods, including MDR, FHSA-SED and MACOED both on pure and heterogeneous datasets. Experiments show that ESMO has practical meanings for epistasis and heterogeneity analysis.    2 Materials and methods\r\n  In ESMO, the model selection step based on multi-objective is embedded in k nesting loop, so that the number of objectives will significantly influence the costs of calculation. Here, we combine only two objectives to evaluate candidate epistatic models. The first objective based on mutual entropy is used to profile the relationship between disease state and each combination of k-epistatic SNPs. For the second objective, Bayesian network derived K2 score is adopted to select models fitting to samples from statistical perspective. It is also worth noting that ESMO is adaptive both on pure and heterogeneous datasets. It means that for pure epistasis datasets, these two objectives are consistent for the best model. For the heterogeneous datasets, these two objectives can profile different genetic models with a better performance.  In order to ease the burden of exhaustive search, a fast scheme including calculation sharing and trading space for time is proposed to accelerate the searching process. Note that, the fast scheme can be supported by parallel computing.   2.1 Mutual entropy and K2 score\r\n  Objective 1: For a case-control study, each sample is labeled as 0 (control) or 1 (case). In this work, the Shannon entropy H(Y) can be used to quantify the uncertainty of the disease state Y in bits as Formula 1.  HðYÞ ¼ 1 X p yi log 2pðyiÞ i¼0 (1) where p(y0) represents the possibility of controls and p(y1) represents the possibility of cases in population.  Joint entropy can be applied to measure the joint uncertainty of k SNPs. Let X be a biallelic SNP with three kinds of genotypes aa (0), aA (1) and AA(2), then the joint entropy of k SNPs can be defined as Formula 2.  HðX1; . . . ; XkÞ ¼ 2 2 X . . . X pðx1; . . . xkÞ log 2pðx1; . . . xkÞ x1¼0 xk¼0 (2)  To quantify the information contribution of a k order epistatic combination to disease state Y (or vice versa), mutual entropy can be calculated as Formula 3.  IðYjX1; . . . XkÞ ¼ HðYÞ þ HðX1; . . . XkÞ HðY; X1; . . . XkÞ (3) where I(YjX1,. . .,Xk) denotes the uncertainty reduction of the disease state when the k-epistatic combination is observed. We set the object 1 score to be reciprocal of I(YjX1,. . .,Xk). Consequently, SNPs with low score are considered to be epistatic.  Objective 2: A Bayesian network (BN) is a probabilistic directed graphic model consisted of a set of nodes (random variables) and edges (conditional dependence) widely used in previous studies  (Jing and Shen, 2015; Skwark et al., 2017; Zeng et al., 2016) . Given the Markov condition, the joint probability distribution for a k þ 1 nodes (k SNP and a disease state) can be calculated as Formula 4. pðX1; X2; . . . ; Xkþ1Þ ¼ kþ1 Y PðXijpðXiÞÞ i¼1 where pðXiÞ represents the set of parents nodes of Xi. If pðXiÞ ¼ 1, PðXijpðXiÞÞ is a marginal distribution. Attributed to Markov condition, for a epistatic model, only edges from a SNP node to disease state need to be considered. A k-epistatic BN model can be seen in Figure 1. There are a total of Ckn combinations in exhaustive search, in which n is the number of all SNPs within samples.  Like the K2 score in  (Jing and Shen, 2015) , K2 score is defined as Formula 5 when the prior distribution is assumed to be a Dirichlet distribution D a11 . . . aij . In this study, assume that no prior knowledge about the relationship between SNPs and disease state, then we set aij ¼ 1.  I K2 ¼ X riþ1  X log ðbÞ i¼1 b¼1 2 rij X X log ðdÞ j¼1 d¼1 ! (4) (5) where I is the total number of combinations and in this study, I ¼ 3k. riis the frequency of ith genotype combination in all samples and rij refers to the number of ith genotype combination in samples with jth state. Although K2 score is defined to be some form of k-locus epistasis in  (Jing and Shen, 2015; Xie et al., 2012) , MACOED and FHSA-SED based on swarm intelligent algorithm are only effective for 2-locus epistasis detection. In this work, k can be 2, 3 and so on if computing resources allowed.    2.2 An adaptive framework for heterogeneity and epistasis analysis\r\n  Two objectives are introduced to select top models in previous sections. In ESMO, candidate epistatic combinations with lower scores on these two objectives have stronger associations with diseases state. Ideally, there is only one best model for pure datasets, while heterogeneous datasets may exist multiple non-dominant solutions corresponding to different epistatic models. Therefore, detecting epistasis and profiling heterogeneity becomes the problem of sorting solutions according to these two objectives. The non-dominant sorting algorithm proposed in NSGA-II  (Deb et al., 2002)  is to find all the solutions on an optimal plane since the objectives conflict with each other and its time complexity of is O M N2 , in which M is the number of objectives and N is the size of solutions. Obviously, for a exhaustive strategy, it is unsuitable. In this study, we firstly design an adaptive framework to concurrently handle heterogeneity and epistasis based on fusing nondominant sort and top k selection algorithm whose time complexity is Oðk M NÞ. The pseudo code of the algorithm is listed as follows:   Non-dominant sort and top k selection algorithm\r\n    Input: the decision space A and the parameter k.\r\n  Output: k solutions set K. selected ¼ 1; While(selected &lt; ¼k)  P ¼ 1; For each objective Ai 2 A  pi ¼ Find the solution with the smallest score in Ai and save its label into the P; End for If all pi 2 P are equal  Add the pi solution into K; selected ¼ selected þ 1;  Delete the pi solution from decision space A;    Else\r\n    Add all these w different solutions into K;\r\n  selected ¼ selected þ w;  Delete the w solutions from decision space A; End if  Return the first k solutions from K; End while  The core ideas of the algorithm are: (i) if a solution has the best scores on all objectives, it certainly has to be considered as a epistatic combination; (ii) if a solution has the best score at least on one objective, it cannot be dominated by any other solutions.  We find that ESMO searches for each objective a set of best solutions separately, which indicates that the non-dominant sort and top k selection algorithm only searches for two separate points rather than the Pareto plane in the decision space. Here, we adhere to this strategy for the following reasons: 1. For the exhaustive scheme, searching all non-dominant solutions in the whole plane needs considerable computing sources.  Therefore, we just focus on several representative solutions. 2. When the epistatic combination space is large, there may be many non-dominant solutions in the whole plane, which could result in high level of false positive. To overcome false positive ratio, clean stage must be adopted such as MACOED. 3. In spite of heterogeneity, the number of subgroups will not be too much. It means that several representative solutions may be enough.     2.3 A fast scheme for accelerating exhaustive search\r\n  As mentioned before, every uneconomical operator will be amplified in exhaustive method. We believe that the major reason for the DG1 DG2 DG3 DG4 DG5 DG6 DG7 DG8 DG9 DG10 DG11 DG12 DG13 DG14 DG15 MAF infeasibility of MDR in larger scale dataset is the huge computation costs caused by cross-validation and permutation. In this study, we use multiple objectives and non-dominant solution searching schemes to ensure the consistency of epistatic models, which are faster than cross-validation and permutation. In addition, we have applied other strategies to save computational costs such as computing share, trading space for time and parallel mode.  Computing share: The open-source ESMO is implemented on MATLAB 2014a. We found that counting the frequencies of genotypes is common between entropy calculation and possibility calculation of BN. Consequently, we extract this common operator to share its computing results.  Trading space for time: From the Formula 5, the logarithm form calculation is the most frequent operator in multi-objective optimization. For example, in exhaustive search, 106 calculations of multi-objective will lead to about 8*108 calculations for factorial values. Actually, we found that the vast majority of factorial calculations are repeated. Therefore, we can calculate all unique factorial values before exhaustive search, so that computing Formula 5 only needs to locate the factorial value in the corresponding location. For a dataset with M samples, we only need to store M factorial values before exhaustive search. This strategy significantly saves the running time cost for ESMO.  Parallel mode: For a large scale dataset or higher order epistasis detection, parallel computing is one effective way to accelerate computing. One of the major feature of the exhaustive search is easy to be parallelism. In this work, we just naively use the simplest 'parfor' structure of MATLAB to illustrate this feature. It is important to remember that a wide variety of parallel techniques is faster than 'parfor'. Note that if there is no parallel computing devices, 'parfor' will degenerate into a serial program like 'for' loops.     3 Datasets and evaluation measures\r\n   3.1 Simulated datasets and case study\r\n  Simulated datasets: In this study, both pure and heterogeneous datasets generated by the tool GAMETES_2.1  (Urbanowicz et al., 2012)  are applied to evaluate different approaches. GAMETES_2.1 can simulate epistatic datasets with a friendly graphic user interface, and customer can set parameters such as MAF, heterogeneity proportion (HP), epistatic order (k), number of SNPs and so on) to determine the architecture of disease models.  In Table 1, there are 15 groups, and each group contains 100 datasets. And there are 800 samples in each dataset. In this study, we simulated two sizes of datasets: 100 and 1000. The replication denotes the number of times the dataset was repeatedly and randomly generated with the same parameters. For example, the replication of DG1 is 2, then with the same parameters there are two randomly groups DG1_1 and DG1_2. When HP equals to 1.0, it means that this kind of dataset is pure. In the pure dataset, k is the order of epistasis and the values of MAF in each parentheses correspond to k loci, respectively. Otherwise, the value of HP in parentheses refers to the proportion of each epistatic model in heterogeneous dataset. In this case, the k and MAF of heterogeneous dataset correspond to different disease model. Taken DG10 as an example, (60%,40%) means that 60% of the data from one model (3-locus epistasis and MAF are 0.2, 0.2 and 0.2)and 40% from the other model (3-locus epistasis and MAF are 0.2, 0.3 and 0.3)  (Urbanowicz et al., 2012) . Note that we only use two epistatic models labeled H1 and H2 to simulate heterogeneous datasets. DG14 and DG15 are more complex situations for heterogeneity, in which there are three different 3-locus diseases models mixed in populations.  Case study: A breast cancer dataset consisted with 10 000 samples (5000 cases and 5000 controls) and each sample has 23 SNPs collected from 6 genes (COMT, CYP19A1, ESR1, PGR, SHBG and STS)  (Yang et al., 2013) . These genes have been proved to be important in steroid hormone metabolism and signaling  (Gabriel et al., 2013) .    3.2 Evaluation measures and parameters setting\r\n  To evaluate the performance of epistasis and heterogeneity detection, we use measures such as the power, loss, running time and speedup. where n indicates the frequency of correctly recognizing the real functional loci in a group. Each group contains a fixed number N of datasets, and in this study N ¼ 100.  Loss: To fairly compare with other methods which is not specific for heterogeneous dataset, the loss is defined as Formula 7. where l is the number of datasets in which none of any epistatic functional loci has been detected.  Running time: We count the seconds of all methods (MDR, MACOED, FHSA-SED and ESMO) to evaluate their efficiency. All these methods are running on our workstation (Windows 10, Intel i7-6800K 6 cores and 16G RAM) and the parameters are listed in Table 2.  Speedup: The speedup is the ratio of sequential calculation running time (Ts) to parallel calculation running time (Tp) as Formula 8.  Ts  Speedup ¼ Tp     4 Experiments and results\r\n   4.1 Experiments on pure datasets\r\n  We simulated two sizes (100 and 1000) of pure datasets such as DG1-3, DG6-9 and DG13. In the following subsections, the power, running time and speedup results are listed to show the performance of ESMO for epistasis detection. (6) (7) (8) Power(%)  n Power ¼ N  l Loss ¼ N the main reason for this is that ESMO and MDR search epistatic combination space exhaustively. Consequently, if the criteria is fit for profiling epistasis, the best model, namely real functional loci, will be precisely detected.  Figure 2 also reflects the volatility of swarm intelligent algorithm based methods FHSA-SED and MACOED. The power of FHSASED is better than MACOED for 100 SNPs dataset, on average. However, for DG6, MACOED achieves far higher power than FHSA-SED. Note that, the performance of FHSA-SED and MACOED also depends on the parameters. It means that in some cases of parameters settings FHSA-SED is better than MACOED, but in other situations, MACOED may be better than FHSA-SED.  From the results of Table 3, whether or not parallel technology is used, the running time of ESMO is the lowest for 100 SNPs datasets. Although MDR and ESMO-parfor both are parallelism, the running time cost of MDR is more than 10 times that of ESMOparfor. It is interesting that the sequential algorithm FHSA-SED is also significantly faster than MDR. However, FHSA-SED is at the expense of the power. For 1000 SNP datasets, the running time of MDR and FHSA-SED are similar, and we can expect that as the size of datasets increases, the advantages of parallelism will gradually show. And, ESMO only needs about 1400 seconds to detect 2-locus epistasis on 1000 SNPs datasets, even with a simple parallel structure 'parfor'.  Although ESMO-unparfor achieves 100% power, it takes a relative longer running time than MDR and FHSA-SED. From the results of the power and running time in Figure 2 and Table 3, we find that MACOED and FHSA may be hard to directly applied to bigger scale of epistasis analysis. 4.1.2 ESMO versus MDR for 3-locus epistasis on pure datasets Because FHSA-SED and MACOED are specific for 2-locus epistasis analysis, in this section, we only compare ESMO with MDR. For pure datasets with 100 SNPs, we find that both MDR and ESMO can completely recognize the true functional 3-locus epistasis, reaching the power 100% as shown in Table 4. However, the running time cost of MDR is about 2 times of ESMO-parfor. For ESMOunparfor, the seconds of handling 3-locus on 100 SNPs datasets is about 2400, which is not listed here.  For a 3-locus epistasis, ESMO-parfor consumes about 743819 seconds (about 8 days) to handle 100 datasets and each dataset contains 800 samples and 1000 SNPs. ESMO also achieve 100% precision for detecting 3-locus epistasis. Here, we do not list the results of MDR, because we rough estimate it may take 500 hours to handle 100 datasets. 4.1.1 ESMO versus other methods for 2-locus epistasis on pure datasets Figure 2 depicts the results of the power on datasets with 100SNPs (DG1-3) and datasets with 1000 SNPs (DG6). The results show that, for pure epistasis analysis, the performance of MDR is very comparable with ESMO and both of them reach 100%. Obviously, 4.1.3 The speedup The time complexity of ESMO is O nk S , in which n is the number of SNPs and k is the epistasis order and S denotes the cost of model selection. In this study, we directly use 'parfor' structure to accelerate the outermost loop of k-nesting loop. The results show that the speedups are bigger than 2.5 on a computing platform with  Speedup 100 1000 6 cores in Figure 3. As the scale of datasets increases, the increment of 2-locus analysis is significant. However, the increment on 3-locus is marginal. The main reason for this is that the most timeconsuming part of the calculation has changed from 2-locus to 3locus analysis. It means that as the order of epistasis grows, the proportion of parts that cannot be parallelized is getting higher. In general, exhaustive method is easy to be parallelism.    4.2 Experiments on heterogeneous datasets\r\n  To show the capability of ESMO to concurrently handle heterogeneity and epistasis, we use both the power and loss evaluation measures. Note that because MDR needs to manually count the results of each single dataset, we only extracted 10%, 20% and 30% of datasets in each group for convenience. Note that just two epistatic model (H1 and H2) are applied to simulate heterogeneous datasets by GAMETES. 4.2.1 ESMO versus MDR for 2-locus epistasis on heterogeneous datasets As defined in Formula 7, the loss denotes the number of true 2-locus epistatic combinations missed. As shown in Table 5, we find that ESMO did not make any missing. For MDR, we investigate all top models and compare all these true epistatic models with top models. In some cases, none of true epistatic models has been recognized by MDR, resulting in the loss.  As shown in Table 6, even for exhaustive search, H1 and H2 cannot be fully recognized due to the mixture of different disease models. However, for a 2-locus heterogeneous datasets, ESMO can achieve the powers higher than 90%. 4.2.2 ESMO versus MDR for 3-locus epistasis on heterogeneous datasets From Table 7, we can see that all the loss values of ESMO are no bigger than MDR. It is also interesting to find that the performance of both ESMO and MDR on DG11 are better than DG10 and DG12, respectively. It indicates that the heterogeneity proportions of heterogeneous datasets will confuse computational method to some degree. For example, the loss of MDR is nearly 50% on DG10 and DG12.  For DG14 and DG15, in these complex situations, the effects of ESMO and MDR are listed in Tables 7 and 8. For the perspective of the loss, as shown in Table 7, the loss of MDR is significantly higher than ESMO both on DG 14 and DG15. Note that the effects of ESMO and MDR are obviously weakened on DG 15 compared with DG14. This phenomenon is likely due to the changes on the HP of each subgroups.  Table 8 shows some kinds of relationship between the power and the HP. For the datasets DG10 and DG12 have the same HP (50%, 50%), which means that epistatic models H1 and H2 occupy the same proportion in the population. Then, the powers of H1 and H2 are similar. But for DG11 with the HP (60%, 40%), the powers of them are significantly different. The reason for this may be that one epistatic model with high proportion has a relative big sample   The proportion of entire datasets (%) DG10 DG11\r\n  DG12  DG14  DG15 ESMO (%) MDR (%) ESMO (%) MDR (%) ESMO (%) MDR (%) ESMO (%) MDR (%) ESMO (%) MDR (%)     4.3 Experiments on a breast cancer dataset\r\n  We conduct heterogeneity and epistasis analysis on a breast cancer dataset both 2-locus and 3-locus. For the non-dominant sorting and top k selection algorithm, we also set the k to be 3. Firstly, after 2-locus epistatic analysis by ESMO, 3 kinds of 2-locus epistasis have been returned such as (rs3020314, rs2017591), (rs1514348,rs2017591) and (rs2077647,rs2017591). Taken a further statistical significance analysis, only (rs3020314, rs2017591) meets the significance level with p-value 0.00038895. With using MACOED and MDR, (rs3020314, rs2017591) is also considered to be strongly associated with breast cancer, which is consistent with the results of ESMO.  ESMO is also used to analyze 3-locus epistasis. (rs3020314, rs1514348, rs2017591), (rs10046, rs3020314, rs2017591) and (rs6269, rs3020314, rs2017591) are considered to be relative with breast cancer on these samples. However, MDR returns (rs3020314, rs9478249, rs2017591) considered to be the best model, which is slightly different with ESMO. The rs3020314 and rs1514348 returned by ESMO are on the estrogen receptor 1 (ESR1) which encodes an estrogen receptor, a ligand-activated transcription factor composed with several domains playing important role in hormone binding, DNA binding, and transcription activation. The results of ESMO mean that these two SNPs located in the same gene ESR1 may interact with rs2017591, resulting in the pathological processes of breast cancer.  It is interesting that rs2017591 is the most common SNP existing in all these epistatic combinations. The rs2017591 locates in gene STS (Chromosome X) which refers to the biosynthetic pathway for estrogen. If these epistatic combinations contribute to different subtypes of breast cancer, thereby rs2017591 is common pathogenic SNP between these subtypes.     5 Discussion and conclusion\r\n  For epistasis analysis, as the order increases, the combination space expands exponentially. It is a great challenge for computational methods to search the true epistasis genotypes in such a huge space. Two main difficulties hindered us from recognizing the functional epistasis: complex disease model and limited calculation resources. Although the time complexity of swarm intelligent algorithm may be lower than exhaustive method in theory, in practice it is very hard to tune the parameters settings and the volatility of solutions is unsuitable for clinic research. More importantly, it is very challenging for swarm intelligent algorithm to pick up k SNPs step by step, because in pure epistasis the subset of k-locus has no advantages than others. It means that no useful information can be provided by the subsets. Therefore, it is difficult to converge to the best solution, especially when the k is big.  In this study, we adopt the exhaustive search like MDR, ensure the power as the first place. The ESMO has 3 major features: (i) Concurrently handle epistasis and heterogeneity based on multiobjective optimization. Here, mutual entropy and Bayesian network derived scores are combined to profile epistatic models from different perspectives; (ii) Exhaustive search has been criticized by their huge computation burden. In this work, unlike MDR with embedded crossvalidation, multi-objective applied to select model is far faster. In addition, a well-designed non-dominant sorting and top k selection algorithm reduces the time complexity from O M N2 to Oðk M NÞ. More importantly, calculation sharing and trading space for time are also applied to accelerate ESMO; 3) Our method ESMO ensures the globalization of solutions, which can significantly improve the confidence of clinical researchers for bioinformatics.  Although the results of 2-locus and 3-locus epistasis analysis show that ESMO is powerful than other methods, there are still some work that needs to be improved. First of all, in nature, there may be k &gt; 3 SNPs involved in some kinds of complex diseases. As the number of SNPs and the epistasis order k increase, the cost of computing resources grows exponentially. Two ways to ease this are dimensions reduction and high performance computing platform.  On the other hand, prior knowledge should be borrowed to quicken the process of search. In addition, prior knowledge may directly reduce the k-locus epistasis to be lower, resulting in narrowed combination space. At the same time, it also enriches biological meanings to computational approaches.    Acknowledgements\r\n  We are grateful to the anonymous reviewers whose suggestions and comments contributed to the significant improvement of this paper.    Funding\r\n  This paper is partially supported by the National Natural Science Foundation of China (Serial No.61602174), the Jiangxi Provincial natural science fund  (No. 20161BAB212052 and 20151BAB217011)  and the Scientific and Technological Research Project of Education Department in Jiangxi Province (GJJ150496).  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/XiongLi2016/ESMO",
    "publicationDate": "0",
    "authors": [
      "Xiong Li",
      "Associate Editor: John Hancock"
    ],
    "status": "Success",
    "toolName": "ESMO",
    "homepage": ""
  },
  "74.pdf": {
    "forks": 11,
    "URLs": [
      "github.com/rrwick/Unicycler",
      "doi.org/10.1371/journal.pcbi.1005595.g001"
    ],
    "contactInfo": ["rrwick@gmail.com"],
    "subscribers": 14,
    "programmingLanguage": "C++",
    "shortDescription": "hybrid assembly pipeline for bacterial genomes",
    "publicationTitle": "Unicycler: Resolving bacterial genome assemblies from short and long sequencing reads",
    "title": "Unicycler: Resolving bacterial genome assemblies from short and long sequencing reads",
    "publicationDOI": "None",
    "codeSize": 38557,
    "publicationAbstract": "The Illumina DNA sequencing platform generates accurate but short reads, which can be used to produce accurate but fragmented genome assemblies. Pacific Biosciences and Oxford Nanopore Technologies DNA sequencing platforms generate long reads that can produce complete genome assemblies, but the sequencing is more expensive and errorprone. There is significant interest in combining data from these complementary sequencing technologies to generate more accurate ªhybridº assemblies. However, few tools exist that truly leverage the benefits of both types of data, namely the accuracy of short reads and the structural resolving power of long reads. Here we present Unicycler, a new tool for assembling bacterial genomes from a combination of short and long reads, which produces assemblies that are accurate, complete and cost-effective. Unicycler builds an initial assembly graph from short reads using the de novo assembler SPAdes and then simplifies the graph using information from short and long reads. Unicycler uses a novel semi-global aligner to align long reads to the assembly graph. Tests on both synthetic and real reads show Unicycler can assemble larger contigs with fewer misassemblies than other hybrid assemblers, even when long-read depth and accuracy are low. Unicycler is open source (GPLv3) and available at github.com/rrwick/Unicycler.",
    "dateUpdated": "2017-10-17T16:52:05Z",
    "institutions": [
      "The University of Melbourne",
      "National Human Genome Research Institute"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-03-14T06:57:02Z",
    "numIssues": 9,
    "downloads": 0,
    "fulltext": "     June      Unicycler: Resolving bacterial genome assemblies from short and long sequencing reads     Ryan R. Wick  rrwick@gmail.com  0  1    Louise M. Judd  0  1    Claire L. Gorrie  0  1    Kathryn E. Holt  0  1    0  Department of Biochemistry and Molecular Biology, Bio21 Molecular Science and Biotechnology Institute, The University of Melbourne ,  Victoria ,  Australia    1  Editor: Adam M. Phillippy, National Human Genome Research Institute ,  UNITED STATES     8  6  2017   8  2017  2  23    22  5  2017    13  1  2017     The Illumina DNA sequencing platform generates accurate but short reads, which can be used to produce accurate but fragmented genome assemblies. Pacific Biosciences and Oxford Nanopore Technologies DNA sequencing platforms generate long reads that can produce complete genome assemblies, but the sequencing is more expensive and errorprone. There is significant interest in combining data from these complementary sequencing technologies to generate more accurate ªhybridº assemblies. However, few tools exist that truly leverage the benefits of both types of data, namely the accuracy of short reads and the structural resolving power of long reads. Here we present Unicycler, a new tool for assembling bacterial genomes from a combination of short and long reads, which produces assemblies that are accurate, complete and cost-effective. Unicycler builds an initial assembly graph from short reads using the de novo assembler SPAdes and then simplifies the graph using information from short and long reads. Unicycler uses a novel semi-global aligner to align long reads to the assembly graph. Tests on both synthetic and real reads show Unicycler can assemble larger contigs with fewer misassemblies than other hybrid assemblers, even when long-read depth and accuracy are low. Unicycler is open source (GPLv3) and available at github.com/rrwick/Unicycler.       Introduction\r\n  Bacterial genomics is currently dominated by Illumina sequencing platforms. Illumina reads are accurate, have a low cost per base and have enabled widespread use of whole genome sequencing. However, much Illumina sequencing uses short fragments (500 bp or less) that are smaller than many repetitive elements in bacterial genomes[ 1 ]. This prevents short-read assembly tools (assemblers) from resolving the full genome, and their assemblies are instead study design, data collection and analysis, decision to publish, or preparation of the manuscript. fragmented into dozens of contiguous sequences (contigs). Consequently, most available bacterial genomes are incomplete, which hinders large-scale comparative genomic studies.  Pacific Biosciences (PacBio) and Oxford Nanopore Technologies (ONT) sequencing platforms can sequence DNA fragments of 10 kbp or longer, but at a higher cost per base than Illumina platforms. PacBio and ONT long reads also have much higher per-base error rates than Illumina reads (5±15% vs &lt;1%), although they are often sufficient to complete bacterial genome assemblies with reasonable consensus accuracy[ 2,3 ]. Hence most researchers must choose between generating fragmented draft assemblies for many isolates with inexpensive Illumina sequencing, or generating complete assemblies for fewer isolates with expensive longread technologies. Hybrid assembly, which uses a combination of short and long reads, offers an alternative. In this approach, short reads are used to produce accurate contigs and long reads provide the information to scaffold them together. This requires relatively few long reads and can thus be the most cost-effective route to a complete bacterial genome.  Despite recent developments in long-read technologies, Illumina reads are widely used in public health and research laboratories[ 4 ], and are likely to remain so for some time due to their high accuracy and low cost. Moreover, Illumina data is already available for hundreds of thousands of bacterial isolates, and most of these are unlikely to be replaced with long-readonly sequencing data. It is therefore probable that research and clinical labs will continue to use low cost Illumina reads for most samples and generate long reads as necessary to complete genomes of interest. Hybrid assembly, which requires fewer long reads than long-read-only assembly, is the most cost-effective means of achieving this goal.  Hybrid assembly can be accomplished with either a short-read-first or long-read-first approach. In the short-read-first method, a scaffolding tool uses long reads to join Illumina contigs together. However, scaffolding mistakes are common and lead to structural errors (misassemblies) in the sequence[ 5 ]. Long-read-first approaches may involve assembly of uncorrected long reads, followed by error-correction of the assembly using short reads[ 3 ]. Alternatively, they may first use short reads to correct errors in long reads, followed by assembly of the corrected long reads[ 6,7 ]. Whether error correction is performed before or after assembly, long-read-first approaches require higher long-read depth than short-read-first approaches.  Here we present Unicycler, a new hybrid assembly pipeline for bacterial isolate genomes. Unicycler first assembles short reads into an accurate and connected assembly graph, a data structure containing both contigs and their interconnections[ 8 ]. It then uses long reads to find the best paths through the graph. By following a short-read-first approach, Unicycler makes effective use of low quantities of long reads, but it can produce a completed assembly (one contig per replicon) if the long-read depth is sufficient. By using the assembly graph connections to constrain the possible scaffolding arrangements, Unicycler achieves lower misassembly rates than alternative short-read-first assemblers.    Design and implementation\r\n  Unicycler encapsulates its entire pipeline (Fig 1) in a single command and automatically determines low-level parameters so users can expect optimal results with default settings[ 9 ].   Short-read assembly\r\n  Unicycler uses SPAdes (v3.6.2 or later) to construct a De Bruijn graph assembly using a wide range of k-mer sizes: 10 values spanning 20±95% of the Illumina read length (not exceeding 127, the largest k-mer possible in SPAdes)[ 10 ]. In SPAdes, large k-mers often result in larger contigs, but excessively large k-mers can cause a fragmented graph with dead ends. Unicycler 2 / 22 Fig 1. Key steps in the Unicycler pipeline.  https://doi.org/10.1371/journal.pcbi.1005595.g001 3 / 22 1 assigns a score (c\u0085d\u00871\u00862) to each k-mer graph based on the number of contigs (c) and the number of dead ends (d). This score function penalises both large numbers of contigs and large numbers of dead ends. Since dead ends are particularly problematic in later Unicycler steps (see Multiplicity and Graph bridging using long-read alignments), the score function scales with the inverse of d2. The highest scoring graph is selected as a balance between minimising both contig count and dead ends (Figs 1A and S1).  As some contamination is possible in sequencing read sets (particularly when multiplexing, which is a common strategy for bacterial isolate sequencing on Illumina[ 11 ]), Unicycler then removes contigs with a depth of less than half the median graph depth, unless doing so would create a dead end. This removes most contamination while leaving important graph structures intact.    Multiplicity\r\n  To resolve the graph as accurately as possible, Unicycler must first determine the multiplicity of contigs in the assembly graph. The most important distinction is between single-copy contigs (sequences that occur once in the genome, multiplicity k = 1) and repeat contigs (sequences that occur multiple times in the genome, multiplicity k &gt; 1). However, determining the correct multiplicity for repeat contigs is also important, as this information can be used when finalising the assembly graph (see Conservative, normal and bold).  When a bacterial genome consists of a single chromosome with no additional replicons, then for each contig x, its median read depth dx is a good indicator of its multiplicity kx. Single-copy contigs will have a median depth dx close to D, the median depth per base across the entire assembly, while repeat contigs will have a median depth near a multiple of that value (i.e. dx ~ kx D). The relationship between median read depth and multiplicity is more complicated when the genome contains multiple replicons present at different copy numbers per cell. For example, small plasmids are often present in multiple copies, while large conjugative plasmids are often present once per cell. The relationship between read depth and multiplicity only holds for replicons which exist in one copy per cell (the same as the chromosome). For example, contigs with depth 2D may be chromosomal and have a multiplicity of two, or they may be in a two-copy-per-cell plasmid and have a multiplicity of one.  In addition to read depth, a contig's graph connections also provide useful information about its multiplicity. Repeat contigs typically have multiple graph connections at their start and end, while single-copy contigs usually have only a single connection at each end. These trends break down when the assembly graph is fragmented, which is one reason why Unicycler aims to minimise the number of dead ends when determining the optimal short-read assembly graph.  To determine multiplicity values, Unicycler therefore uses both depth and connectivity information. Initially, a multiplicity of one is assigned to all contigs that are near the graph's median depth and have no more than one connection at either end. A greedy algorithm then propagates multiplicity where graph connections and depth are in close agreement (Figs 1B and S2). When no more propagation is possible, the largest suitable contig is given a multiplicity of one and the process is repeated. This algorithm can correctly assign multiplicity to highcopy-number plasmid contigs in additional to chromosomal contigs.    Bridges\r\n  Unicycler scaffolds assembly graphs by constructing bridge contigs to connect pairs of singlecopy contigs. Before bridging, single-copy contigs connect via multiple alternative paths containing one or more repeat contigs. After bridging, they connect via a simple, unambiguous 4 / 22 path. Bridges thus simplify the graph by resolving repeats. There are two primary sources of information available for creating bridges: paired-end short reads, which can resolve small repeats, and long reads, which can resolve much larger repeats.    Graph bridging using read pair information\r\n  When SPAdes assembles paired-end reads, it uses its ExSPAnder algorithm to find paths through the assembly graph using read-pair orientation[ 12 ]. This process is known as repeat resolution (RR). SPAdes does not save its post-RR assembly (contigs.fasta) in graph form, but it does save the graph paths used to make post-RR contigs (contigs.paths). Unicycler finds cases where two single-copy contigs are connected in a SPAdes contig path and uses them to build bridges. In Fig 1C, the SPAdes contig path connects contigs 1 and 5 via contig 3. Unicycler's resulting bridge connects contigs 1 and 5 directly with a copy of the contig 3 sequence. When this bridge is applied, contigs 2 and 4 also become connected via an unbranching path, in essence becoming bridged by process of elimination. These indirect graph simplifications may be merged together later in Unicycler's pipeline, depending on the mode (see Conservative, normal and bold). While Unicycler creates bridges at this stage, they are not immediately applied to the graph. This is deferred to a later step where bridges are applied in decreasing order of quality (see Bridge application).    Semi-global long-read alignment\r\n  While short reads can resolve repeats up to the insert size of the library (typically &lt;1000 bp), long reads provide a much more powerful source of scaffolding information. As a first step in long-read bridging, Unicycler aligns all available long reads to the single-copy contigs. Since the long and short reads must be from the same biological sample, there should be no genuine structural discrepancies between the long reads and contigs. Semi-global alignment (i.e. endgap free alignment) is therefore appropriate, where alignments can only terminate when the end of a sequence is reached. Most available long-read alignment tools such as BLASR[ 13 ], BWA-MEM[ 14 ], BLAST[ 15 ] and LAST[ 16 ] perform local alignment, so Unicycler implements semi-global alignment directly using the SeqAn C++ library (S3 Fig)[17].    Graph bridging using long-read alignments\r\n  Long reads that align to multiple single-copy contigs can be used for bridging. Such reads contain a sample of the gap sequence between those contigs, and if multiple long reads connect a pair of contigs, Unicycler uses SeqAn to produce a consensus gap sequence[ 18,19 ]. Unicycler does not directly use this gap sequence in the bridge but instead uses it to find the best graph path connecting the contigs, via a branch and bound algorithm. Thus, the bridge sequence comes from the graph and reflects base calling accuracy of the short reads rather than the long reads that may have much lower accuracy (Fig 1D). Sometimes Unicycler cannot find a graph path connecting two single-copy contigs that are connected via long reads, such as when the short-read graph is incomplete and contains dead ends. In these cases, the long-read consensus sequence is directly used as the bridging sequence. Such bridges are more likely to contain errorsÐanother reason why Unicycler strives to minimise dead ends in the assembly graph.    Bridge application\r\n  Having produced bridges from both short reads (SPAdes RR) and long reads, Unicycler can now apply them to simplify the graph structure (Fig 1E). Since some bridges may be erroneous, Unicycler assigns a quality score to each bridge and applies them in order of decreasing 5 / 22 quality, ensuring that when multiple contradictory bridges exist, the best-supported option is used.  Bridge quality is a function of many factors, depending on the type of bridge (S4 Fig). For long-read bridges, these factors are: the number of reads which support the bridge (more is better); the alignment quality between the read consensus and graph path (higher identity is better); the length of the two contigs to be bridged (longer is better); the length and quality of the read alignments to the contigs (longer and higher identity is better); and the read depth consistency between the contigs (closer agreement is better). An ideal long-read bridge therefore connects two long contigs of the same depth, is supported by many reads with long alignments to the contigs, and has a graph path in close agreement with the read sequences. See S4 Fig for more information on bridge quality functions.    Final steps\r\n  Following bridge application, Unicycler performs several actions to finalise the assembly graph. Contigs that have been used in bridges and no longer provide additional connection information are removed. Simple unbranching paths in the graph are merged to form long contigs (Fig 1F). Overlapping sequences at contig ends (created by the SPAdes assembly process) are removed so each contig's sequence leads directly into its neighbours. If any circular replicon was completely assembled, it will now be a single contig with a link connecting its end to its start. A circular sequence can be shifted to any starting position without changing the biological information. Unicycler therefore uses TBLASTN to search for dnaA or repA alleles in each completed replicon[ 20 ]. If one is found, the sequence is rotated and/or flipped so that it begins with that gene encoded on the forward strand. This provides consistently oriented assemblies and reduces the risk that a gene will be split across the start and end of the sequence. As a final step, Unicycler uses Bowtie2 and Pilon to polish the assembly using shortread alignments, reducing the rate of small errors (Fig 1G)[ 21,22 ].    Conservative, normal and bold\r\n  Unicycler can be run in three different modes that alter its cutoff for minimum acceptable bridge quality: conservative, normal and bold. In conservative mode, the quality cutoff is high (i.e. only very high quality bridges will be used). This mode is least likely to produce a complete assembly but carries a very low risk of misassembly and is appropriate for contexts where assembly accuracy is paramount. In bold mode, the quality cutoff is low (i.e. lower quality bridges will be used). This mode is most likely to complete the assembly but carries greater risk of error. It is suited to cases where completeness is more important than accuracy. Normal mode uses an intermediate cutoff and is appropriate in most cases. The quality threshold value assigned to each mode (25, 10 and 1 for conservative, normal and bold, respectively) is arbitrary and the user can manually specify a different threshold for finer control. Furthermore, in conservative mode, Unicycler excludes all SPAdes contig bridges, as tests reveal SPAdes RR to be a common source of misassembly (see Assembly of simulated short-read datasets).  Unicycler's mode also influences its contig-merging behaviour in bridge finalisation. In bold mode, all possible contigs on unbranching paths are merged, regardless of their multiplicity and whether they are bridges. In conservative mode, Unicycler only merges single-copy contigs and their corresponding bridges. Simple paths indirectly created by bridging (as is the case with contigs 2 and 4 in Fig 1C) will not be merged in conservative mode. In normal mode, single-copy contigs can also be merged with non-bridge contigs, but only if their multiplicity is one. For example, contig 3 in Fig 1C has a multiplicity of two before bridge application and a single instance has been used in the bridge, leaving the contig with a multiplicity of 6 / 22 one after bridge application. Unicycler would therefore merge this path (contigs 2, 3 and 4) in normal mode.    Included tools\r\n  Unicycler's semi-global alignment algorithm is included as a stand-alone command line tool (unicycler_align), making it available for use in other pipelines. Unicycler also comes with a polishing tool (unicycler_polish) which applies variants identified by Pilon, GenomicConsensus[ 2 ] and FreeBayes[ 23 ], and assesses the assembly using ALE[ 24 ]. By iteratively polishing the genome with both short and long reads, this process can correct many remaining errors in a completed assembly, including those in repeat regions.     Results\r\n  Unicycler's performance was evaluated using read sets simulated for eight species and using real read sets from the well-studied E. coli K-12 substr. MG1655. We further demonstrated the utility of Unicycler by assembling the complete genomes of novel isolates of Klebsiella pneumoniae using newly generated Illumina, PacBio and ONT reads.  ABySS does not perform hybrid assembly and was only used in the short-read-only tests. npScarf and Cerulean require long reads and were only used for the hybrid-read tests. SPAdes can assemble with or without long reads and was included in all tests. We used default parameters or recommended settings for all tools (S1 Table). The NaS tool can conduct hybrid assemblies but was excluded from this comparison because it depends on Newbler, a closed-source assembler only supported on RedHat/Fedora Linux[ 25 ]. We also excluded ALLPATHS-LG, which can perform hybrid assemblies but has strict library preparation requirements, restricting its applicability[ 26,27 ].   Metrics\r\n  For both the simulated and real E. coli read tests, assemblies were evaluated by comparison to the corresponding complete reference genome using QUAST (v4.3)[ 28 ]. We focused on the following metrics: misassemblies, small-error rate (mismatches and small indels) and NGA50.  QUAST identifies misassemblies as cases where a contig aligns to the reference genome in multiple pieces, not as a single continuous alignment, indicating a structural error in the contig. QUAST distinguishes between ªlocalº and ªextensiveº misassemblies: local misassemblies have a discrepancy of less than 1 kbp while extensive misassemblies have a larger discrepancy. For our tests, we used the sum of both types to quantify all misassemblies, regardless of size. For the simulated read tests, reads were generated from the reference genome so misassemblies always indicate assembler mistakes. For the E. coli tests, there is not a perfect agreement between the reference genome and reads generated in different laboratories from different subcultures of E. coli K-12 substr. MG1655, so false positive misassemblies are possible.  The well-known N50 metric measures only contig size, not contig correctness, limiting its value. A large N50 can therefore result from inaccurately joining sequences into large misassembled contigs. By aligning contigs to a reference, QUAST produces more useful metrics including NGA50 (GA = ªgenome alignedº). Whereas N50 is based on contig lengths, NGA50 is based on the lengths of contig-to-reference alignments. A correctly assembled contig will have a single, unbroken alignment to the reference; a misassembled contig will be divided into multiple smaller alignments. Hence NGA50 is a metric for completeness that, unlike N50, penalises misassemblies. In our tests, we used QUAST's ªstrict-NAº option to break contigs at all misassembly locations, including local misassemblies, for particularly stringent NGA50 scores. 7 / 22 Acinetobacter baumannii Acinetobacter baumannii Escherichia coli Escherichia coli  MLST sequence type 231 758    Simulated read sets\r\n  To provide a wide range of genome size and complexity, we simulated reads from 12 reference genomes from seven bacterial species (2 Acinetobacter baumannii[ 29,30 ], 2 Escherichia coli [ 31,32 ], 3 Klebsiella pneumoniae[33±35], 1 Mycobacterium tuberculosis[36], 1 Shigella dysenteriae[ 37 ], 1 Shigella sonnei, 1 Streptococcus suis[ 38 ]) and the yeast Saccharomyces cerevisiae[ 39 ] (Table 1). Plasmid and mitochondrial sequences were included at higher read depths, as appropriate.  We used ART (v2.5.8) to generate six synthetic paired-end short-read sets from each reference genome which mimic those from an Illumina HiSeq 2500: 125 bp read length, 400 bp mean insert size, 60 bp insert size standard deviation and 50x read depth[ 40,41 ]. Each synthetic short-read set was assigned a long-read accuracy and mean length: 60% and 10 kbp; 60% and 25 kbp; 75% and 10 kbp; 75% and 25 kbp; 90% and 10 kbp; and 90% and 25 kbp. For each short-read set, we used PBSIM[ 42 ] to generate synthetic long reads at seven depths (0.25x, 0.5x, 1.0x, 2.0x, 4.0x, 8.0x and 16.0x). This yielded six short-read sets and 42 hybrid-read sets per strain. For the short-read sets, we performed five assemblies: Unicycler in each of its modes (conservatives, normal and bold), SPAdes and ABySS. For the hybrid sets, we performed six assemblies: Unicycler (all modes), SPAdes, npScarf and Cerulean. Additionally, all tests were performed in five replicates using separately generated synthetic reads, resulting in 16920 total assemblies. 8 / 22 Fig 2. Simulated short-read assemblies: Errors. Misassembly and small-error (mismatches and indels) rates for assemblies of simulated shortread sets, summarising results across all reference genomes and replicate tests (total 360 per assembler).    Assembly of simulated short-read datasets\r\n  For assemblies of synthetic short-read-only sets, Unicycler outperformed the other assemblers in each QUAST metric (Figs 2 and 3, S2 Table). It is particularly interesting to compare Unicycler to SPAdes, since Unicycler uses SPAdes to build the initial short-read assembly graph.  In normal and bold modes, Unicycler achieved the most complete assemblies, as measured by NGA50. This is attributable to the wide k-mer range used in assembly. Both SPAdes and ABySS allow for manual selection of k-mer size, and providing a higher k-mer would likely improve the contiguity of their assemblies. However, we tested each assembler using the settings recommended in the tool's documentation or provided in example commands. SPAdes was run without defined k-mer sizes, and for the test read sets it automatically selected k21±55.  Fig 3. Simulated short-read assemblies: NGA50. NGA50 for assemblies of simulated short-read sets, summarising results across all reference genomes and replicate tests (total 360 per assembler). 9 / 22 Fig 4. Simulated hybrid assemblies: Errors. Error rates for hybrid assemblies of simulated short-read and long-read sets, summarising results across all reference genomes and replicate tests (total 2520 per assembler).  ABySS was run with a k-mer of 64 (the maximum value with default compilation settings).  Unicycler's automatically-selected k-mer differed between read sets, but was most typically 95, giving it greater power to assemble repetitive regions than SPAdes and ABySS.  The misassembly rate of pre-RR SPAdes assemblies was very low, demonstrating that RR is the source of most misassemblies in SPAdes contigs. In conservative mode, Unicycler does not use SPAdes RR and therefore achieves similarly low misassembly rates. In normal and bold modes, Unicycler does use RR, but only if it exceeds a quality threshold. This selectiveness explains why normal/bold Unicycler assemblies have lower misassembly rates than the SPAdes contig assemblies from which they are derived.  Both Unicycler and SPAdes incorporate a polishing step into their pipeline. Unicycler uses Bowtie2 and Pilon. SPAdes uses MismatchCorrector, which is optional but we included it in our tests because the SPAdes documentation recommends it for small genomes. This polishing likely accounts for Unicycler's and SPAdes' superior performance in the small-errors metric.  ABySS assemblies may show similarly low small-error rates if a subsequent polishing step was performed.    Hybrid assembly of simulated short- and long-read datasets\r\n  Unicycler surpassed other assemblers when conducting hybrid assemblies of synthetic reads (S2 Table). Misassembly rates in the hybrid assemblies were often much higher than in the short-read-only assemblies, illustrating the difficulty of resolving repeats with long reads (Figs 4, S5 and S6). Both npScarf and Cerulean consistently produced assemblies with ten or more misassemblies. SPAdes produced fewer misassemblies, but some genomes resulted in many errors, particularly the Shigella genomes with many high-copy-number ~1 kbp repeats associated with insertion sequences. Unicycler's misassembly rates were the lowest and correlated with the assembly mode (conservative, normal or bold).  Small-error rates (mismatches and small indels) were lowest in Unicycler and SPAdes, as they both derive their final contigs from the short-read assembly graph, not from the longread sequences. Unicycler's and SPAdes' polishing steps may also contribute to their low small-error rate. NGA50 was dependent on the long-read depth, and Unicycler performed 10 / 22 best at all tested depths (Fig 5). This is due to Unicycler's low misassembly rates (other assemblers' NGA50 scores were reduced due to their higher occurrence of misassemblies) and its ability to produce bridges using as few as one long read. In many cases, Unicycler produced complete or near-complete assemblies with only 4x long-read depth.  Theoretical analyses of assembly show that error-prone reads are nearly as informative as error-free reads, suggesting that read accuracy is less important than length[ 43,44 ]. Unicycler's performance on the simulated read sets matched these findings. Read length significantly affected the resulting NGA50 for Unicycler (all modes) and SPAdes (Fig 6). In contrast, read accuracy had a weaker effect on Unicycler's NGA50 values, demonstrating its effectiveness in using long reads regardless of their accuracy.    Computational performance\r\n  The assembly tests were all conducted with eight CPU cores and 16 GB of RAM. Unicycler was slower than the alternative hybrid assemblers, taking a median time of 46 minutes to assemble the 8x long-read depth synthetic tests. SPAdes and npScarf performed the fastest, both having a median time of eight minutes and maximum time of less than 25 minutes on the same data. Cerulean had a median time of 23 minutes, although some Cerulean processes did not complete due to crashes or exceeding the 24-hour time limit. The complex biofilm-associated gene in Acinetobacter baumannii A1 was slow to bridge in Unicycler, resulting in a maximum run time of 13 hours. However, Unicycler did fully assemble this gene sequence in many read sets where the other assemblers produced a fragmented or misassembled result.    Real E. coli K-12 read sets\r\n  We tested the same assemblers using real reads of the E. coli K-12 substr. MG1655 genome.  The short reads for these tests were produced on the Illumina MiSeq platform. Long reads were from five different platforms: ONT R7, ONT R9, PacBio RS, PacBio RS II C2 chemistry and PacBio RS II C3 chemistry (Table 2). The ONT R9 reads were further split into two groups, pass and fail, as determined by ONT's Metrichor software (v0.16.37960). For each platform and long-read depth, we conducted 20 trials using different random subsamples of long reads at the same depths used for simulated data. Accuracy was assessed by comparison to the E. coli K-12 substr. MG1655 reference genome (accession NC_000913.3) generated using Sanger-based capillary sequencing at the University of Wisconsin in 1997[ 31 ].  These tests showed the same trends as the simulated data: Unicycler produced larger contigs at lower long-read depths than other assemblers (Fig 7, S3 Table). Notably, Unicycler performed worst on the set with shortest reads (PacBio RS) not the set with lowest read identity (ONT R7), while it performed best on the set with longest reads (PacBio RS II C3). This matches the simulated results and theoretical predictions, again showing that read length is more important than accuracy for Unicycler.  We also searched for each of the E. coli K-12 substr. MG1655 reference genome's seven RNA operons in the assemblies using BLAST. These repetitive sequences are approximately 5 kbp in length and have enough sequence similarity to each other to create fragmented shortread assemblies[ 45 ]. We produced seven query sequences from the reference genome: each RNA operon along with 2 kbp of neighbouring sequence on each end. If an operon had a BLAST hit exceeding both 95% identity and 95% coverage, then we counted it as found in the assembly. Each assembly could therefore achieve a score from zero (no complete RNA operons found) to seven (all RNA operons found). Unicycler performed best in this analysis, often finding all seven operons when it used 8x or 16x depth of long reads (S8 Fig). 11 / 22 Fig 5. Simulated hybrid assemblies: NGA50 against long-read depth. Mean NGA50 values for hybrid assemblies of simulated read sets. Mean values were calculated across all read lengths, read accuracies and replicate tests for each reference genome (210 hybrid-read sets each); the top panel shows mean values for all 12 reference genomes (2520 hybrid-read sets). Horizontal dashed lines indicate the N50 size of the reference genome. For the bacterial genomes, this is the size of their only chromosome; for Saccharomyces, it is the size of chromosome XIII, an intermediatesized replicon in the genome. 12 / 22 Fig 6. Simulated hybrid assemblies: Read length and accuracy. NGA50 values segregated by read length and read accuracy. These plots summarise results across all reference genomes and replicate tests, but only include the tests of 8x longread depth. For read lengths, the p-value is from a two-tailed t-test. For read accuracies, the p-value is from a one-way ANOVA test. 13 / 22  Read count 68,455 24,649 50,277 42,582 82,590 47,910  Total length (Mbp) 584.7 Mean Median N50 depth length (bp) length  (bp) 125.3 300 300  Mean identity (%) 99.4  The NGA50s for these tests were markedly lower than those obtained with reads simulated from the E. coli K-12 substr. MG1655 reference genome. With simulated reads, both Unicycler and SPAdes were often able to achieve complete or near-complete assemblies. With real reads, Unicycler's and SPAdes' best NGA50 values were 2.0 Mbp and 1.4 Mbp, respectively.  This appears to be due to false positive misassembly calls resulting from genuine differences between the reference genome sequence and the genomes of the isolates that were sequenced using Illumina, PacBio and ONT platforms (S7 Fig). For example, one copy of IS1A in the Illumina read set relocated to a position 105 kbp away from the position in the reference genome; when an assembly spanned this region, QUAST identified the difference as a misassembly, reducing the NGA50.    Klebsiella pneumoniae PacBio assembly case study\r\n  To explore the utility of Unicycler, we used it to assemble the genome of K. pneumoniae isolate INF274, which was difficult to assemble using alternative techniques. INF274 is a multidrugresistant sequence type (ST) 340 strain isolated from the urine of a Melbourne hospital patient who had a urinary tract infection. It belongs to clonal group (CG) 258, a common cause of multidrug-resistant hospital-associated infections globally. This isolate was first sequenced on Illumina HiSeq 2000 (generating 750 Mbp of 125 bp paired-end reads) and then on a PacBio RS II (generating 1.3 Gbp of long reads, many of which exceeded 10 kbp in length). Both reads sets are high quality and are an ideal set of inputs for hybrid assembly. Notably, the library preparation for the PacBio reads followed a standard size-selection protocol that excluded short fragments of DNA, so small plasmids were underrepresented in the long reads.  This sample's PacBio read set was sufficiently deep for both hybrid and long-read-only assembly approaches. For hybrid assembly, we used Unicycler (normal mode) and SPAdes (v3.8.1), the best performing assemblers in our synthetic tests. For long-read-only assemblies, we used HGAP (v3) and Canu (v1.3), both of which are modern implementations of the Celera Assembler designed for high-error long reads[ 2,3,46 ]. HGAP was included because it is developed by Pacific Biosciences and included in their SMRT Analysis software suite. Canu is a similar assembler designed for both PacBio and ONT reads. 14 / 22 Fig 7. E. coli K-12 assemblies: NGA50 against long-read depth. Mean NGA50 values for hybrid assemblies of real E. coli read sets, summarised across 20 replicate tests at each depth. Top panel shows mean values for all six long-read sets.  Since INF274 is a novel isolate, we did not analyse the assembly results with QUAST.  Instead, we qualitatively compared the assemblies and analysed the alignment of Illumina reads to each (Figs 8 and S9). Of the four assemblers, only Unicycler and Canu produce a graph file for their final assembly, but Canu did not circularise any replicons, so the sequences remained linear. When viewed in Bandage[ 8 ], the Unicycler graph clearly distinguished between replicons that formed completed circularised sequences and those that did not. Only plasmids 5 and 6 remained incomplete in the Unicycler assembly, as they contain shared sequence and there was an absence of long reads for these replicons, preventing Unicycler from scaffolding them apart. SPAdes and HGAP output their assemblies only as linear sequences, making it difficult to make the distinction between complete and incomplete replicons. The SPAdes assembly suffered from the same problem as Unicycler with plasmids 5 and 6, and it failed to assemble plasmid 3, which contains a prophage sequence also present in the chromosome. 15 / 22 Fig 8. Klebsiella pneumoniae INF274 assembler comparison. Final assemblies of Klebsiella pneumoniae INF274 produced by Unicycler, SPAdes, HGAP and Canu. The contigs/graph of the assembly are shown on the left, coloured by replicon. The read depth plot of plasmid 1's contig is shown on the right. Low read depth at the ends of the contig is indicative of start-end overlap.  Since Unicycler's graph-based scaffolding naturally results in circular sequences, it did not have duplicated sequences at the start/end of circular replicons. SPAdes suffered from a slight overlap and both HGAP and Canu had significant overlaps, indicated by the drop in read depth near the ends of contigs. These must be repaired manually or with a tool such as Circlator[ 47 ].  Klebsiella pneumoniae ONT assembly case study To assess Unicycler's performance on low-depth ONT datasets, we performed R9 sequencing on K. pneumoniae isolate INF125, a virulent ST45 strain isolated from the urine of a Melbourne hospital patient. Reads were generated over a four-hour period resulting in a total of 16 / 22 156 Mbp of sequence (depth = 28.6x) with an N50 length of 11,470 bp. ONT streams sequence data as it is generated, making it feasible to analyse the data in real time and stop sequencing when a complete assembly is reached.  To investigate each assembler's suitability for such real-time analysis, we generated 240 subsets of reads, one set per minute of sequencing, each containing all reads generated up to that minute (e.g. set 60 contained all reads generated in the first hour of sequencing). All sets were assembled with Unicycler (normal mode), SPAdes, npScarf and miniasm. Unicycler and SPAdes were included due to their high accuracy in the synthetic read tests. npScarf was included because it is explicitly designed for streaming, real-time analysis. Miniasm is a longread-only assembler which, unlike HGAP and Canu, does not produce a consensus sequence [ 48 ]. Rather, its assemblies consist of read fragments, have an error rate similar to the raw reads, and require consensus improvement using a separate tool, such as Racon[ 49 ]. It was included in these tests because of its speedÐit only takes a few minutes to runÐmaking it potentially suitable for real-time analysis.  We assessed each assembly using N50, number of contigs, and error rates when aligning the Illumina reads to the assembly (Fig 9, S4 Table). A high read alignment identity is indicative of a low small-error rate (mismatches and small indels). A high proportion of concordantly aligned reads is indicative of a low misassembly rate. Due to the intrinsically high error rates of its uncorrected assemblies, miniasm was excluded from the read alignment tests.  Unicycler completed the INF125 assembly at with 45 minutes of ONT reads (depth = 5.3x). npScarf required 76 minutes of reads (9.0x) to complete the assembly, SPAdes took 102 minutes of reads (12.1x) and miniasm took 213 minutes of reads (25.3x). All completed assemblies contained three contigs (one chromosome and two plasmids), except for SPAdes assemblies which contained extra, erroneous contigs. Read error metrics show that both Unicycler and SPAdes consistently produced more accurate assemblies than npScarf, although the magnitude of the benefit was small (Fig 9).    Limitations\r\n    Summary\r\n  As Unicycler operates on a short-read assembly graph, it requires high quality short reads. Specifically, it is important that there are very few unsequenced regions of the genome that create dead ends in the assembly graph. The quality of Unicycler's assemblies can suffer if the assembly graph is fragmented and incomplete.  Unicycler performed well on both short-read-only sets and all types of hybrid-read sets, producing larger contigs than other assemblers. Perhaps more importantly, Unicycler produced fewer misassemblies than other assemblers, which often had high error rates. As long-read sequencing becomes more common, so will completed genome assemblies, enabling new research into genome structure. High quality assemblies free of structural errors, such as those produced by Unicycler, will be critical to research in this field.     Availability and future directions\r\n  Unicycler's primary use case is when a researcher wishes to complete the assembly of an isolate for which Illumina reads already exist. To facilitate this, future development of Unicycler will add streaming support for ONT sequencing, using reads to create and update bridges in the graph in real time during a sequencing run. This will allow users to halt sequencing once a genome is sufficiently resolved, conserving sequencing resources for other isolates. This modality is currently possible with npScarf, however in our tests Unicycler was more accurate 17 / 22 Fig 9. Klebsiella pneumoniae INF125 ONT assemblies over sequencing time. Assembly metrics of K. pneumoniae INF125 produced by Unicycler, SPAdes, npScarf and miniasm over a four-hour period of sequencing. Miniasm assemblies contain error rates comparable to that of the raw reads and are therefore excluded from the error rate plots. than npScarf and reached complete assemblies with lower read depths. Future development will also focus on improving Unicycler's computational performance. Unicycler is not currently able to perform large assemblies such as human genomes and metagenomes. Algorithmic improvements to long-read alignment, path finding and graph manipulations will all be required for Unicycler to be appropriate in such cases.  Unicycler is open source (GPLv3) and available at github.com/rrwick/Unicycler.    Supporting information\r\n  S1 Fig. Effect of maximum SPAdes k-mer on short-read assemblies. As the maximum assembly k-mer grows, SPAdes assemblies have decreasing numbers of segments (top row). The assembly graphs have fewest dead ends for moderate k-mers (middle row). Unicycler chooses its ideal maximum k-mer using a score function which takes both segments and dead ends into account (bottom row). (PDF) S2 Fig. Unicycler's multiplicity algorithm and an example of its application on a simple assembly graph. (PDF) S3 Fig. Unicycler's semi-global alignment algorithm and an example of its application on a read-contig pair. (PDF) S4 Fig. Bridge quality score functions. All bridges in Unicycler are assigned a quality score, which allows Unicycler to sort bridges by quality. Quality scores are calculated as the product of multiple score functions, and then transformed into the range 0 to 100. Each score function 18 / 22 quantifies some aspect of the bridge in the range of 0 and 1, and different bridge types use different combinations of these functions in their quality score. The score functions are heuristic, not statistically derived. (PDF) S5 Fig. Simulated hybrid assemblies: Misassemblies per genome. Misassembly rates for hybrid assemblies of simulated short-read and long-read sets, summarising results separately for each reference genome (total 210 results per assembler per reference). (PDF) S6 Fig. Simulated hybrid assemblies: Small errors per genome. Small-error rates for hybrid assemblies of simulated short-read and long-read sets, summarising results separately for each reference genome (total 210 results per assembler per reference). (PDF) S7 Fig. E. coli K-12 assemblies: Errors. Error rates for hybrid assemblies of real E. coli read sets, summarised across 840 results per assembler. (PDF) S8 Fig. E. coli K-12 assemblies: RNA operons against long-read depth. Number of RNA operons found in hybrid assemblies of real E. coli read sets, summarised across 840 results per assembler. (PDF) S9 Fig. Klebsiella pneumoniae INF274 assembly errors. Error rates when aligning Illumina reads to each K. pneumoniae INF274 assembly. An increase in discordant pairs is indicative of a misassembly. An increase in error rate is indicative of small errors (mismatches and small indels). (PDF) S10 Fig. Klebsiella pneumoniae INF125 Illumina assembly comparison. Comparison of assembly graphs made by different assemblers using only short reads for K. pneumoniae INF125. Contigs are coloured based on which replicon they represent. (PDF) S1 Table. Commands used for assembly, evaluation and read simulation. (XLSX) S2 Table. Raw QUAST results for simulated read tests. (TSV) S3 Table. Raw QUAST results for E. coli read tests. (TSV) S4 Table. Raw quality metrics for K. pneumoniae INF125 ONT assemblies over time. (TSV)    Author Contributions\r\n  Conceptualization: RRW KEH.  Data curation: CLG.  Formal analysis: RRW.  Funding acquisition: KEH. 19 / 22 Investigation: RRW LMJ.  Methodology: RRW.  Project administration: KEH.  Resources: LMJ KEH.  Software: RRW.  Supervision: KEH.  Validation: RRW.  Visualization: RRW.  Writing ± original draft: RRW.  Writing ± review &amp; editing: RRW LMJ CLG KEH. 20 / 22 17. Gogol-DoÈring A, Reinert K. Biological sequence analysis using the SeqAn C++ library. CRC Press; 2009. 329 p. 21 / 22    ",
    "sourceCodeLink": "https://github.com/rrwick/Unicycler",
    "publicationDate": "0",
    "authors": [
      "Ryan R. Wick",
      "Louise M. Judd",
      "Claire L. Gorrie",
      "Kathryn E. Holt"
    ],
    "status": "Success",
    "toolName": "Unicycler",
    "homepage": ""
  },
  "31.pdf": {
    "institutions": [
      "Harvard Medical School",
      "Dana-Farber Cancer Institute"
    ],
    "URLs": [
      "sourceforge.net/projects/multiplierz/",
      "github.com/BlaisProteomics/multiplierz;"
    ],
    "contactInfo": ["jarrod_marto@dfci.harvard.edu"],
    "fulltext": "     Colour Online: See the article online to view Figs.     10.1002/pmic.201700091   multiplierz v2.0: A Python-based ecosystem for shared access and analysis of native mass spectrometry data     William  4    M. Alexander  0  1  4    Scott B. Ficarro  0  1  4    Guillaume Adelmant  0  1  4    Jarrod A. Marto  jarrod_marto@dfci.harvard.edu  1  2  3  4    0  Department of Biological Chemistry and Molecular Pharmacology, Harvard Medical School ,  Boston, MA ,  USA    1  Department of Cancer Biology and Blais Proteomics Center, Dana-Farber Cancer Institute ,  Boston, MA ,  USA    2  Department of Oncologic Pathology, Dana-Farber Cancer Institute ,  Boston, MA ,  USA    3  Department of Pathology, Brigham and Women's Hospital, Harvard Medical School ,  Boston, MA ,  USA    4  ogy, Dana-Farber Cancer Institute ,  450 Brookline Avenue, Longwood Center 2208, Boston, MA, 02115-5450 ,  USA     2017   1  2  15  16    28  6  2017    7  3  2017    25  6  2017     The continued evolution of modern mass spectrometry instrumentation and associated methods represents a critical component in efforts to decipher the molecular mechanisms which underlie normal physiology and understand how dysregulation of biological pathways contributes to human disease. The increasing scale of these experiments combined with the technological diversity of mass spectrometers presents several challenges for community-wide data access, analysis, and distribution. Here we detail a redesigned version of multiplierz, our Python software library which leverages our common application programming interface (mzAPI) for analysis and distribution of proteomic data. New features include support for a wider range of native mass spectrometry file types, interfaces to additional database search engines, compatibility with new reporting formats, and high-level tools to perform post-search proteomic analyses. A GUI desktop environment, mzDesktop, provides access to multiplierz functionality through a user friendly interface. multiplierz is available for download from: https://github.com/BlaisProteomics/multiplierz; and mzDesktop is available for download from: https://sourceforge.net/projects/multiplierz/    Application programming interface (API) / Python       -\r\n  In a continued effort to achieve ever-more extensive coverage of the proteome, academic researchers and manufacturers have developed a myriad of mass spectrometer scan functions along with new or refined ionization and fragmentation techniques to improve detection, reproducibility, quantification accuracy and other analytical figures of merit [ 1-12 ]. As a result modern mass spectrometers are quite diverse, comprising a wide range of technologies and performance capabilities, along with a correspondingly diverse array of vendor-specific data formats. Taken together, the proprietary file formats, size, and high-dimensionality of mass spectrometry data, along with continued technological advances, complicates the task of ensuring shared access and reproducible analysis across the research community.  Based on the notion that instrument manufacturers are commercially incentivized to develop and maintain efficient binary file formats for their data, we proposed an open, common application programming interface (API) [13] as an alternative to common file formats [14] for shared access to native mass spectrometry binary files. APIs are a key element of modern software development as they enable expert programmers and novice users to utilize powerful software libraries or other computational infrastructure without intimate knowledge of the underlying implementation. A well-engineered common API for native mass spectrometry    Significance of the study\r\n  We introduce a redesign of our open-source, Python software library and desktop environment which leverages our recently described common application programming interface (mzAPI) for access, analysis, and distribution of mass spectrometry-based proteomic data. Significant extensions include support for a wider range of native mass spectrometry file types, database search engines, and reporting formats. Interactive GUIs are provided for common tasks, while the underlying Python architecture facilitates rapid prototyping of custom workflows. data leverages the existing file indices to provide fast and efficient programmatic access to the binary file architecture, while also ensuring that users can leverage manufacturer data system- and instrument-specific functionality which may be absent from common file format definitions, or simply ignored by users when they utilize file extraction routines. In addition, working exclusively with vendor data files via a common API satisfies regulatory criteria and also minimizes file size 'bloat' which is a frequent by-product when native binary files are extracted to self-describing formats.  Using mzAPI as a foundation for transparent data access we developed multiplierz, a scriptable Python software library for access to and analysis of mass spectrometry data [15]. As an easy to learn scripting language, Python provides an ideal environment for rapid prototyping of data analytic tools [ 16 ], and has evolved to include powerful tools for visualization (matplotlib), data sharing (iPython), statistical analysis (numpy and scipy), and importantly provides a high level of interoperability with R (rpy2), the preferred open-source environment for rigorous analysis of large-scale molecular data [ 17, 18 ]. More recently several proteomics-focused software tools have leveraged Python in an effort to motivate wider acceptance and experimentation across the scientific community [ 19-22 ]. Here we describe improvements to the multiplierz library, including extension of our mzAPI implementation to accommodate a broader range of mass spectrometer file formats, and a library of new, native-Python proteomic analysis tools. This new codebase provides an accessible, opensource ecosystem for proteomic data access, analysis, and distribution. 2    Design and architecture\r\n  Our open-source ecosystem for analysis of native mass spectrometry data is comprised of four major sub-packages (Fig. 1): (i) mzAPI, for providing access to raw mass spectrometry data files [13]; (ii) mzSearch, for programmatically submitting data to various search engines; (iii) mzReport, for viewing and processing search result data; and (iv) mzTools, for post-acquisition data processing and analytic tasks. The most recent build (version 2.0) is written for Python 2 and is distributed under the GNU Lesser General Public License. Primary improvements and additions are described below, 1700091 (3 of 9) with a complete overview of the tools provided in Supporting Information Table 1. 2.1  mzAPI The mzAPI interface abstracts across many instrumentspecific hardware, firmware, and acquisition capabilities to provide users with a powerful environment for programmatic access to mass spectrometry data. In addition to supporting high-dimension LC-MS/MS native data formats from ThermoFisher Scientific (.RAW), Danaher AB Sciex (.WIFF), and Agilent (.D), we have added support for simple MALDI experiments which often consist of individual scans (e.g. no time-dependent separation component). Figure 2 illustrates the function which returns a single spectrum from a 4800 MALDI TOF-TOF instrument (.T2D files). Internally, native data files are accessed through a set of proprietary Windows libraries provided by the respective instrument vendors and included in the multiplerz installer; as such mzAPI is currently limited to Windows data systems. To facilitate interoperability with existing workflows, we included support for the most recent definition of the mzML common file format [ 23 ]. In addition, we provide an embedded peak list extraction function for subsequent database searches. 2.2  mzSearch A new feature in this version of multiplierz is mzSearch, a package to abstract the interface details for protein identification software; mzSearch currently supports three popular engines (Mascot, Comet and XTandem) and is readily extensible to enable complex consensus searchers. A scripted mzSearch session proceeds in three steps; an mzSearch object is initialized that loads a file of parameters for the target search engine; optionally, parameters are accessed or changed by reference to corresponding attributes of the object; finally, a search method is called against one or more data files, which invokes the search engine associated with the object. 2.3  mzReport The third Python module is mzReport, which enables users to easily generate, interrogate, and distribute results from proteomic experiments; mzReport supports multiple formats, including simple CSV or Excel files, interactive .mzD SQLite files [ 24 ], as well as the evolving mzIndentML standard [ 25, 26 ]. As we [ 24 ] and others [ 27-29 ] have previously discussed, rapid technological developments in proteomic research make it difficult to establish stable reporting standards while also maintaining the flexibility to browse the underlying data and test alternative hypotheses as new tools become available. The architecture of our multiplierz environment provides seamless integration with native data files via mzAPI, enabling use, distribution, and further development (through Python) of dynamic reporting formats [ 24 ]. 2.4  mzTools The mzTools package enables users to rapidly prototype new utilities for various aspects of mass spectrometry data analysis. Of particular interest, we have further optimized and expanded our recently described Pep2Pro utility [ 30 ]. This new tool (now termed Pep2Gene) enables rapid annotation of peptides with respect to their source proteins, protein isoforms, and parent genes based on an application-/databasespecific sequence-to-gene dictionary. Briefly, this is an optimized form of the k-mer-to-protein mapping database algorithm previously described, whereby a lookup table is compiled that maps all 4-mers present in a sequence database to the corresponding set of source proteins; decomposing a peptide into successive 4-mers and identifying the corresponding protein sets by lookup vastly reduces the search space required to perform peptide-to-protein mapping. From there, a protein-to-gene mapping function is performed, using data obtained from the Uniprot KnowledgeBase [ 31, 32 ]. Creation of a Pep2Gene database for the human proteome (e.g. UniProt) takes less than 30 min. on a modern desktop computer, after which annotating 50 000 peptides, as is typical in large-scale proteome experiments, can be completed in a few minutes. As a result our implementation of Pep2Gene is well-suited for deployment in data analytic pipelines built around modest processing power. Inspired by efforts in label-free quantification [ 33, 34 ], we have also implemented a sophisticated algorithm to detect and catalog MS and MS/MS spectral features; this algorithm provides a foundation for relative peptide quantification by SILAC (multiplierz currently supports duplex and triplex labeling schemes) or label-free analyses. We provide documentation for a full multiplierz peptide identification and SILAC quantitation workflow performed on a third-party data set (multiplierz Github repository); our analysis agreed well with the original quantification performed via Proteome Discoverer [ 35 ]. For intact protein mass spectrometry applications, we implement spectral deconvolution as originally described by Fenn [ 36 ] and Marshall [ 37 ] (Fig. 3). 2.5  mzDesktop In its original implementation, multiplierz served primarily as a GUI-based application to expose mzAPI and provide a modest set of tools for basic analysis of mass spectrometry data. Beginning with the release described herein we have renamed the GUI application 'mzDesktop' to distinguish it from the broader multiplierz project. In general, most functions now available in the multiplierz Python package have GUI-based equivalents available through mzDesktop: the Data Viewer (Fig. 4) provides direct access to native mass spectrometry files via mzAPI; database search engine interfaces powered by the mzSearch API can be used to manually launch protein/peptide identification jobs; mzReports-compatible files (as well as mzIdentML) can be opened by the mzDesktop file reader, and so on. We have also included several GUI-specific functions. One example is the Peptide Coverage viewer which allows users to quickly ascertain the degree of protein sequence coverage supported by a given set of PSMs and generate presentation-quality graphics (Supporting Information Fig. 1). 3    Analysis of complex mass spectrometry datasets\r\n  We have developed novel high-performance liquid chromatography (LC) assemblies for fully automated single- and multiple-dimension peptide separation coupled directly to mass spectrometry for in-depth interrogation of mammalian proteomes [ 38-43 ]. Optimization of total peak capacity for these high-performance platforms requires systematic analysis of peptide elution profiles across multiple separation dimensions. Moreover, the use of multiplexed isotope labeling reagents to enable higher throughput, quantitative analysis may alter the physicochemical behavior of tryptic peptides.  In an effort to understand how iTRAQ [ 44 ] and TMT [ 45 ] reagents may influence peptide elution in 3-dimension RPSAX-RP-MS/MS experiments, we digested HeLa cell protein lysates and then created equal aliquots of unlabeled, iTRAQ 4Plex, iTRAQ 8-Plex, and TMT 6-Plex labeled tryptic peptides. All peptides were mixed and then analyzed by 3D-RP-SAX-RP at a depth of 32 fractions (.RAW data files available for download at: ftp://massive.ucsd.edu/MSV000081101). Our multiplierz pipeline output the subset of 3127 sequence-unique tryptic peptides identified in common across labeled and unlabeled peptides. We generated XICs via random file access with mzAPI (32 .RAW files, 120 GB) to identify the fraction corresponding to the apex elution for each peptide (documentation available on the multiplierz Github repository). Next, these data were collated to create two-dimensional plots to illustrate the number of peptides identified in each first and second dimension fraction (Fig. 5A-E). As expected the addition of each isobaric tag increased peptide retention compared to unlabeled analogues (Fig. 5F). Approximately one-half of iTRAQ-labeled peptides were more strongly retained compared to their unlabeled analogs. Surprisingly peptides labeled with TMT multiplex reagents exhibited a dramatic shift in retention, requiring higher salt and acetonitrile concentrations for elution from the first- and seconddimension columns, respectively. This effect is presumably 1700091 (5 of 9) due to the greater basicity and hydrophobicity of the dimethylpiperidine in the TMT scaffold compared to iTRAQ's methylpiperazine moiety. This example highlights the ease with which multiplierz can support custom data workflows; in this specific case providing a readily interpretable yet powerful quantitative visualization which informs adjustment of experimental parameters to maintain separation power for label-based or label-free proteomic strategies. 4    Discussion\r\n  Increased breadth and depth of mass spectrometry-based studies continues to drive proliferation of commercial and open-source software tools for customized data analysis, and distribution [ 27,46 ]. Efforts in the former were led by Pedrioli in 2004 through extraction of native data to an XML-based common file format [14]. Human-readable formats provide cross-platform compatibility and can live in-perpetuity independently from the source data and original instrument manufacturer. These formats continue to evolve and are supported by a diverse ecosystem of software tools and frameworks for bioinformatic analysis [ 47-58 ]. These advantages are tempered by the technical difficulties of establishing efficient schemes for random access to high-dimension data incorporated within XML-formatted files. As a result, recent iterations of XML common file formats for mass spectrometry have sought to establish an acceptable compromise between raw-/meta-data content and performance for sequential versus random scan/file access within the system memory typically available to a broad range of end-users [ 23, 59-61 ]. The increasing acquisition speed of modern mass spectrometers coupled with the trend toward more comprehensive studies will accentuate these challenges, and fuel the debate regarding the technical merit of human-readable formats for complex numerical data [ 13, 62-64 ].  As an alternative to common file formats we have promoted the use of a common API [13, 15] for mass spectrometry in order to leverage optimized indexing and architecture of the native data files. Our strategy is well-aligned with trends across a wide array of scientific fields, wherein the domain-specific computational tasks are codified into a series of reusable software libraries; for instance, the particular demands of data analysis in astronomy led to the development of AstroPy, a library facilitating access to efficient binary data formats, unit- and coordinate-aware computation, image convolution tools, and other related capabilities [ 65 ]. It should be noted that our emphasis on a common API sacrifices the permanent archival nature of human-readable formats, although the future maintainability of workflows based around either extraction (XML) or abstraction (API) of native data files relies on continued support from mass spectrometry instrument manufacturers. While community-defined standards in proteomics have coalesced around XML-formatted files, we note that an increasing set of software tools [ 49,66,67 ] designed for mass spectrometry now tout an ability to access and analyze native data files directly, suggesting that a significant subset of researchers are productive using proteomic workflows start-to-finish in the Windows environment. Consistent with this trend, multiplierz and mzDesktop leverage our common API to provide unfettered access to native mass spectrometry data files, and we will continue to expand the set of file formats supported by mzAPI.  High-level scientific software libraries are playing an increasingly important role at the intersection of complex instrumentation, high-dimensional data, and federated computational resources. As currently implemented, multiplierz provides all the components necessary to build a complete front-to-back data analysis workflow, through an interface designed with novice programmers in-mind; common tasks can be performed in several lines of Python code, and scripts can be easily generalized across different instruments and search applications. Moreover, we designed multiplierz to minimize conceptual overhead; details of complex tasks are encapsulated within functions, which fulfill well-understood roles at each stage of proteomic data analysis [ 28, 68 ]. Importantly, native Python data types are returned whenever possible. Experienced programmers can use the algorithms and tools provided as a jumping-off point to write additional functions specifically suited to their requirements, or to build workflows that integrate other scientific computing capabilities available in Python. Finally, by enabling the creation of concise, transparent, and complete data processing scripts, multiplierz allows the details of an analysis to be easily archived and shared, facilitating experimental transparency and reproducibility. In addition to adding capabilities for desktop analysis and increasing support for other native data formats, our future development efforts will include porting 1700091 (7 of 9)  GUI features available in mzDesktop to our mzServer web resource [ 69 ] to enable remote, web-based, and platformindependent access to native mass spectrometry data files.  multiplierz is available for download from: https://github.com/BlaisProteomics/multiplierz; and mzDesktop is available for download from: https://sourceforge.net/projects/multiplierz/  This work was supported by the Dana-Farber Strategic Research Initiative and Barr Program in Basic Research, in addition to the N.I.H. (CA188881 and CA178860), the Honorable Tina Brozman Foundation for Ovarian Cancer Research, and the Michael J. Fox Foundation.  The authors have declared no conflict of interest. 5 characterization and use in the comparative analysis of histone H3 post-translational modifications. J. Proteome Res. 2004, 3, 621-626. [12] Xia, Y., McLuckey, S. A., Evolution of instrumentation for the study of gas-phase ion/ion chemistry via mass spectrometry.  J. Am. Soc. Mass Spectrom. 2008, 19, 173-189. [13] Askenazi, M., Parikh, J. R., Marto, J. A., mzAPI: a new strategy for efficiently sharing mass spectrometry data. Nat. Methods 2009, 6, 240-241. [14] Pedrioli, P. G., Eng, J. K., Hubley, R., Vogelzang, M. et al., A common open representation of mass spectrometry data and its application to proteomics research. Nat. Biotechnol. 2004, 22, 1459-1466. [15] Parikh, J. R., Askenazi, M., Ficarro, S. B., Cashorali, T. et al., multiplierz: an extensible API based desktop environment for proteomics data analysis. BMC Bioinformatics 2009, 10, 364.    ",
    "publicationTitle": "multiplierz v2.0: A Python-based ecosystem for shared access and analysis of native mass spectrometry data",
    "title": "multiplierz v2.0: A Python-based ecosystem for shared access and analysis of native mass spectrometry data",
    "publicationDOI": "10.1002/pmic.201700091",
    "publicationDate": "0",
    "publicationAbstract": "The continued evolution of modern mass spectrometry instrumentation and associated methods represents a critical component in efforts to decipher the molecular mechanisms which underlie normal physiology and understand how dysregulation of biological pathways contributes to human disease. The increasing scale of these experiments combined with the technological diversity of mass spectrometers presents several challenges for community-wide data access, analysis, and distribution. Here we detail a redesigned version of multiplierz, our Python software library which leverages our common application programming interface (mzAPI) for analysis and distribution of proteomic data. New features include support for a wider range of native mass spectrometry file types, interfaces to additional database search engines, compatibility with new reporting formats, and high-level tools to perform post-search proteomic analyses. A GUI desktop environment, mzDesktop, provides access to multiplierz functionality through a user friendly interface. multiplierz is available for download from: https://github.com/BlaisProteomics/multiplierz; and mzDesktop is available for download from: https://sourceforge.net/projects/multiplierz/",
    "authors": [
      "William",
      "M. Alexander",
      "Scott B. Ficarro",
      "Guillaume Adelmant",
      "Jarrod A. Marto"
    ],
    "status": "Success",
    "toolName": "multiplierz v2"
  },
  "80.pdf": {
    "forks": 1,
    "URLs": ["github.com/jijiadong/JDINAC"],
    "contactInfo": ["lxie@iscb.org"],
    "subscribers": 1,
    "programmingLanguage": "R",
    "shortDescription": "Joint density-based Differential Interaction Network Analysis and Classification",
    "publicationTitle": "JDINAC: joint density-based non-parametric differential interaction network analysis and classification using high-dimensional sparse omics data",
    "title": "JDINAC: joint density-based non-parametric differential interaction network analysis and classification using high-dimensional sparse omics data",
    "publicationDOI": "10.1093/bioinformatics/btx360",
    "codeSize": 3,
    "publicationAbstract": "Motivation: A complex disease is usually driven by a number of genes interwoven into networks, rather than a single gene product. Network comparison or differential network analysis has become an important means of revealing the underlying mechanism of pathogenesis and identifying clinical biomarkers for disease classification. Most studies, however, are limited to network correlations that mainly capture the linear relationship among genes, or rely on the assumption of a parametric probability distribution of gene measurements. They are restrictive in real application. Results: We propose a new Joint density based non-parametric Differential Interaction Network Analysis and Classification (JDINAC) method to identify differential interaction patterns of network activation between two groups. At the same time, JDINAC uses the network biomarkers to build a classification model. The novelty of JDINAC lies in its potential to capture non-linear relations between molecular interactions using high-dimensional sparse data as well as to adjust confounding factors, without the need of the assumption of a parametric probability distribution of gene measurements. Simulation studies demonstrate that JDINAC provides more accurate differential network estimation and lower classification error than that achieved by other state-of-the-art methods. We apply JDINAC to a Breast Invasive Carcinoma dataset, which includes 114 patients who have both tumor and matched normal samples. The hub genes and differential interaction patterns identified were consistent with existing experimental studies. Furthermore, JDINAC discriminated the tumor and normal sample with high accuracy by virtue of the identified biomarkers. JDINAC provides a general framework for feature selection and classification using high-dimensional sparse omics data. Availability and implementation: R scripts available at https://github.com/jijiadong/JDINAC Contact: lxie@iscb.org Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-09-05T02:42:10Z",
    "institutions": [
      "Shandong University",
      "The City University of New York",
      "Shandong University of Finance and Economics",
      "Columbia University"
    ],
    "license": "No License",
    "dateCreated": "2017-07-23T03:17:00Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx360   JDINAC: joint density-based non-parametric differential interaction network analysis and classification using high-dimensional sparse omics data     Jiadong Ji  2    Di He  4    Yang Feng  3    Yong He  2    Fuzhong Xue  0    0  Department of Biostatistics, School of Public Health, Shandong University ,  Jinan 250012 ,  China    1  Department of Computer Science, Hunter College, The City University of New York ,  NY 10065 ,  USA    2  Department of Mathematical Statistics, School of Statistics, Shandong University of Finance and Economics ,  Jinan 250014 ,  China    3  Department of Statistics, Columbia University ,  New York, NY 10027 ,  USA    4  Ph.D. Program in Computer Science, The Graduate Center, The City University of New York ,  New York, NY 10016 ,  USA     2017   1  1  8   Motivation: A complex disease is usually driven by a number of genes interwoven into networks, rather than a single gene product. Network comparison or differential network analysis has become an important means of revealing the underlying mechanism of pathogenesis and identifying clinical biomarkers for disease classification. Most studies, however, are limited to network correlations that mainly capture the linear relationship among genes, or rely on the assumption of a parametric probability distribution of gene measurements. They are restrictive in real application. Results: We propose a new Joint density based non-parametric Differential Interaction Network Analysis and Classification (JDINAC) method to identify differential interaction patterns of network activation between two groups. At the same time, JDINAC uses the network biomarkers to build a classification model. The novelty of JDINAC lies in its potential to capture non-linear relations between molecular interactions using high-dimensional sparse data as well as to adjust confounding factors, without the need of the assumption of a parametric probability distribution of gene measurements. Simulation studies demonstrate that JDINAC provides more accurate differential network estimation and lower classification error than that achieved by other state-of-the-art methods. We apply JDINAC to a Breast Invasive Carcinoma dataset, which includes 114 patients who have both tumor and matched normal samples. The hub genes and differential interaction patterns identified were consistent with existing experimental studies. Furthermore, JDINAC discriminated the tumor and normal sample with high accuracy by virtue of the identified biomarkers. JDINAC provides a general framework for feature selection and classification using high-dimensional sparse omics data. Availability and implementation: R scripts available at https://github.com/jijiadong/JDINAC Contact: lxie@iscb.org Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Cenk Sahinalp    1 Introduction\r\n  It is well known that a complex biological process, such as the development and progression of cancer, is seldom attributed to a single molecule. Numerous cellular constituents, such as proteins, DNA, RNA and small molecules do not function in isolation, but rather interact with one another to fulfill particular biological functionality. In the view of network biology  (Yoshimura et al., 1998; Zhou et al., 2011) , a cellular function is a contextual attribute of quantifiable patterns of interactions between myriad of cellular constituents. Such interactions are not static processes, instead they are dynamic in response to changing genetic, epigenetic and environmental factors  (Bandyopadhyay et al., 2010; Califano, 2011) . The molecular interactions can be effectively abstracted as a network. In the biological networks, the nodes represent biomolecules (e.g. genes), and the edges represent functional, causal, or physical interactions between the nodes. Differential network analysis aims to identify the difference between the networks under two conditions. Each of the edges in the differential network indicates a change in the connection between two nodes across the two conditions. Thus, differential network analysis becomes an important tool to understand the roles of different modules in complex biological processes, and draws tremendous attention. Typically, differential genetic interactions are a reflection of which cellular processes are differentially important under the studied condition  (de la Fuente A, 2010; Ideker and Krogan, 2012) .  In the past decade, many methods have been proposed to detect the differential network connection patterns between two conditionspecific groups (e.g. patients and health controls).  Gambardella et al. (2013)  introduced DINA procedure to identify whether a known pathway is differentially co-regulated between different conditions.  Yates and Mukhopadhyay (2013)  provided a dissimilarity measure that incorporates nearby neighborhood information for biological network hypothesis tests. Recently,  Ruan et al. (2015)  developed the dGHD algorithm for detecting differential interaction patterns in two-network comparisons. All of the aforementioned methods endeavor to identify whether the global network topology changed significantly between two groups. However, it will be of benefit to reveal critical pairwise molecular or genetic interactions that are responsible for the different physiological or pathological states of an organism in many applications. The identification of such interactions may help us to illuminate the underlying genetic mechanisms of complex diseases (e.g. cancer), to predict drug offtarget effects  (Evangelidis and Xie, 2014) , to develop multi-target anti-cancer therapy  (Xie and Bourne, 2015) , and to discover clinical biomarkers for disease classification.  To this end, the primary focus in this article is to identify pairwise differential interactions among genes that are most closely related to a certain disease status. Most of such studies first require to divide the data into two separate groups according to the factor of interest. Besides, a specific correlations matrix is often involved to represent the strength of pairwise interaction between nodes in the network. The existing methods mainly fall into two categories. The first category is to compare topological characteristics, such as degree, clustering coefficient of vertices within the network, of the constructed sparse network on grouping specific data  (Reverter et al., 2006; Zhang et al., 2009) . The main challenge of this approach lies in how to select appropriate threshold for constructing sparse network, although there have been miscellaneous methods proposed to address this challenge  (Carter et al., 2004; Elo et al., 2007) . To the best of our knowledge, no commonly feasible approach has been available yet. Approaches in the second category normally handle weighted group-specific network to further construct the differential network. In one manner such approach can only concentrate on edge-level to construct edge-difference based differential network  (Hudson et al., 2009; Liu et al., 2010; Tesson et al., 2010) . On the other hand, it could focus more on finding gene sets and identify correlation pattern's difference between groups. For example, the CoXpress  (Watson, 2006)  first performs hierarchical clustering with correlation matrix obtained from normal samples (or disease sample), then applies statistical test to determine whether the average correlation within one cluster is higher (or lower) than expected by chance and thus finally identifies the differentially co-expressed gene groups. Similarly, DiffCorr  (Fukushima, 2013)  identifies the first principal component based 'eigen-molecules' in the correlation matrices constructed from the grouped dataset, then performs Fisher z-test between the two groups to discover differential correlation. In addition,  Zhao et al. (2014)  proposed a direct estimation method (DEDN), which models each condition-specific network using the precision matrix under Gaussian assumption. However, most of the methods mentioned earlier are based on marginal or partial correlation. It can only capture the linear relationship among genes, which could be restrictive in real applications. It is often the case that nonlinear relationships exist between genes. Another critical but inadequately addressed issue is how to adjust the confounding factors in the differential network analysis. For instance, the condition-specific label is the length of the survival time of cancer patients, one group are patients with longer survival time and the other group are those with short survival time. Then the age of the patients is a potential confounding factor which needs to be adjusted. If the patients' ages are different between two groups, it's hard to know whether the identified differential network is associated with the survival time or the age. Furthermore, how to use the identified network biomarkers to achieve classification still poses great challenge in discriminant analysis especially in high-dimensional settings  (He et al, 2016) .  To address the challenges in differential network analysis and classification mentioned above using high-dimensional sparse omics data, we propose a Joint density based non-parametric Differential Interaction Network Analysis and Classification (JDINAC) method to identify differential patterns of network activation between condition-specific groups (e.g. patients and health controls). The contribution of our work lies in that we can not only deal with the non-linear relationship between the genes but also adjust the confounding factors in the differential network analysis. Furthermore, JDINAC is free of the assumption of a parametric probability distribution of gene measurements. We compare the ability of identifying differential network of our methods with DiffCorr  (Fukushima, 2013) , DEDN  (Zhao et al., 2014)  and Lasso based method. By integrating the logistic regression into our method, our method is capable of accurate classification using high-dimensional sparse data. We also compared the classification performance of our method with Random Forest (RF)  (Breiman, 2001) , Naive Bayes (NB) and Lasso based methods in both simulation studies and real data example.    2 Materials and methods\r\n  Network differential analysis and classification using highdimensional sparse omics data face several challenges. First, the number of data points n is often much smaller than the number of features p, e.g. p n problem. Second, the relationship between two biological variables is often non-linear. Third, confounding factors often need to be adjusted in the differential network analysis and classification. Finally, the underlying distribution of biological variable may not follow Gaussian or other probability distribution on which many algorithms are based. JDINAC is proposed to address these problems.  JDINAC assumes that the network-level difference between two biological states comes from the collective effect of differential pairwise gene-gene interactions that can be characterized by conditional joint density of two genes. Formally, assume that we have observed gene-level activities (such as mRNA, methylation or copy number) for p genes measured over individuals. For individual l (l ¼ 1, 2, , n), ( 0 l 2 class 0 the binary response variable is denoted as Yl ¼ 1 l 2 class 1 the expression level of ith gene is denoted as xli. Let Y ¼ ðY1;Y2;...;YnÞT and X ¼ ðX1;X2;...;XnÞT , Xl ¼ ðxl1;xl2;...;xlpÞT , (l ¼ 1, 2, ,n). X is a n p matrix, p is the total number of genes, and Xl denotes the gene features for individual l. Let P denotes the probability Pr(Y ¼ 1), i.e. P ¼ Pr(Y ¼ 1), and Gi is the ith gene. The JDINAC approach based on the logistic regression model can be constructed as, and logitðPÞ ¼ a0 þ XkK¼1 akZk þ Xip¼1 Xjp&gt;1 bijlngfiijjððGGii;;GGjjÞÞ;Xip¼1 Xp j&gt;1 jbijj c; c &gt; 0; where Zk (k ¼ 1;...;K) denote the covariates (e.g. age and gender). fij and gij denote the class conditional joint density of Gi and Gj for class 1 and class 0, respectively, i.e., ððGi;GjÞjY ¼ 1Þ fij and ððGi;GjÞjY ¼ 0Þ gij. The conditional joint densities fijðGi;GjÞ can indicate the strength of association between Gi and Gj in class 1. Since the number of pairs ðGi;GjÞ can be larger than the sample size, the L1 penalty  (Tibshirani, 1996)  was adopted in this highdimensional setting. Note that the above formulation can be viewed as an extension of the FANS approach  (Fan et al., 2016) . Parameters bij 6¼ 0 indicate differential dependency patterns between conditionspecific groups.  L1 regularized estimate for b: bb¼ argmin nXn k l¼1ðð1  YlÞðaT Zl þ bT ClÞ þ lnð1 þ exp ð aT Zl2bTClÞÞÞ þ kkbk1o where a ¼ ða0; a1; . . . ; aKÞT , Zl ¼ ð1; Z1; . . . ; ZKÞT , b ¼ vecðbijÞj&gt;i, Cl ¼ vec ln gfiijjððxxllii;;xxlljjÞÞ j&gt;i. kk1 denotes L1 norm and the operator vecðAÞj&gt;i stacks the columns of the upper triangular position of matrix A excluding the diagonal elements to a vector (e.g. A ¼ ðaijÞ4 4 is a matrix with four rows and four columns, vecðaijÞj&gt;i ¼ ða12; a13; a23; a14; a24; a34ÞT ).  The advantages of the JDINAC approach over existing methods lie in the following aspects: (i) it can achieve differential network analysis and classification simultaneously; (ii) it can adjust confounding factors in the differential network analysis, for example, if the samples are from cancer patients with different length of survival time, then the age of the patient is a potential confounding factor which needs to be adjusted. (iii) it is a non-parametric approach and can identify the non-linear relationship among variables. Besides, it does not require any conditions on the distribution of the data, which makes it more robust.  JDINAC can be implemented as follows with its workflow shown in Figure 1. Fig. 1. Workflow of JDINAC  Step 1. Given the data of n observations D ¼ fðYl; XlÞ; l ¼ 1; . . . ; ng. Randomly split the data into two parts: D ¼ ðD1; D2Þ.  Step 2. On part D1, estimate the joint kernel density functions fbij and gbij, i; j ¼ 1; . . . ; p, j &gt; i.  Step 3. On part D2, fit an L1 penalized logistic regression logitðPÞ ¼ a0 þ PkK¼1 akZk þ Pip¼1 Pjp&gt;1 bijlnðfbijðGi;GjÞ=gbijðGi;GjÞÞ, using cross validation to get the best penalty parameter.  Step 4. Repeat Steps 1- 3 T times, on tth repetition obtain predicted probability Pbt and coefficient bbij;t, t ¼ 1; 2; . . . ; T.  Step 5. Calculate the average prediction Pb ¼ T 1 PT t¼1 Pbt as the final prediction for classification. Calculate the differential dependency weight of each pair ðGi; GjÞ between two groups, wij ¼ PT  t¼1 Iðbbij;t ¼6 0Þ, i; j ¼ 1; . . . ; p, j &gt; i; where Ið Þ is the indicator function. A differential network is inferred by connecting the pairs with high differential dependency weights through their shared genes.   2.1 Simulation studies\r\n  Four simulation scenarios were designed for assessing the performances of differential network analysis and classification accuracy. In scenarios 1 and 2, the difference of association strength between pairs of genes in a network is caused by the different correlation (Fig. 2a and b). In scenario 3, the differential pairs have the same correlation structure between condition-specific groups but different joint density (Fig. 2c). In scenario 4, the differential strength of association between pairs of genes in a network is caused by the non-linear dependence (Fig. 2d). The differential networks of four simulation scenarios are shown in the Supplementary Figures S1-S4. For scenarios 1-3, we generated 100 pairs of datasets, each representing the case (class 1) and the control (class 0) conditions. Each dataset contains 300 observations with p variables drawn from the multivariate normal distribution with mean 0 and covariance matrix R, that is, X Npð0; RÞ. R consists of 3 blocks along the diagonal, R ¼ diagðR1; R2; R3Þ, R1 ¼ ðrijÞm m, for i; j ¼ 1; . . . ; m; m ¼ 80; R2 ¼ R3 ¼ ðrijÞ10 10.  We compared JDINAC with several existing state-of-the-art methods under the aforementioned four scenarios in differential network analysis and classification. Additional simulation studies are detailed in Supplementary Methods.    2.2 Differential network analysis\r\n  We compare the performance of JDINAC in terms of differential network estimation with DiffCorr  (Fukushima, 2013) , DEDN  (Zhao et al., 2014)  and cross-product penalized logistic regression (cPLR). The cPLR is defined as logitðPÞ ¼ b0 þ  Xp i¼1  Xp j&gt;1 bijGiGj: The L1 penalty function was used to optimize the parameters, which is the same for JDINAC. Parameters bij 6¼ 0 indicate differential dependency patterns between two groups.  True discovery rate (TDR; Precision), true positive rate (TPR; Recall) and true negative rate (TNR) are used to evaluate the performance of different methods. TDR, TPR, and TNR are defined as follows, TDR ¼ P  TP i6¼jIðbdij 6¼ 0Þ ; TPR ¼ P  TP i6¼jIðdij 6¼ 0Þ ; TNR ¼ P  TN i6¼jIðdij ¼ 0Þ ; where TP and TN are the numbers of true positives and true negatives respectively, which are defined as TP ¼ Pi¼6jIðdijbdij ¼6 0Þ, TN ¼ Pi¼6jfIð dij ¼ 0ÞIðbdij ¼ 0Þg respectively. ðdijÞp p is the differential adjacency matrix, dij ¼6 0 indicate the pair ðGi;GjÞ are differential dependency between two groups; ðbdijÞp p is the estimated differential adjacency matrix.    2.3 Classification and evaluation\r\n  We compare the classification performance of JDINAC with RF, NB, cPLR and original penalized logistic regression (oPLR). The oPLR is defined as logitðPÞ ¼ b0 þ b1G1 þ þ bpGp:  Similarly, the L1 penalty function was used to optimize the parameters for high-dimensional data. Both cPLR and oPLR are Lasso based methods.  Receiver operating characteristic (ROC) curve and classification error are used to assess the accuracy of four methods.    2.4 Evaluation of computational complexity\r\n  We carried out additional simulations to estimate the computing time under various numbers of genes with sample size 100 for each group, using a single core node with 2.00 GHz Intel(R) Xeon(R) CPU E5-2430L.    2.5 Application\r\n  Breast Invasive Carcinoma (BRCA) is the most common type of breast cancer. This subtype of breast cancer is able to spread to other parts of the body through the lymphatic system and bloodstream, which makes BRCA potentially a highly lethal killer. Most of the genome-wide studies for BRCA focus on identifying differentially expressed genes. However, BRCA is largely determined by a number of genes that interact in a complex network, rather than a single gene perturbed (gene mutation, expression and methylation etc.). A key but inadequately addressed issue is how to identify the underlying molecular interaction mechanisms. The TCGA BRCA study include 1098 patients, along with their matched mRNA, copy number, methylation and microRNA data. The RNASeq Version 2 expression data and clinical data were downloaded from TCGA through TCGA-Assembler  (Zhu et al., 2014) . In this study, we select 114 patients who have both tumor and matched normal samples as our training subjects. The proposed method was applied to identify differential patterns of network activation between the tumor group and the control group. We focus on the 397 genes listed in the cancer pathway (hsa05200) of KEGG as our candidate gene sets. After filtering those genes which include &gt;30% of zero gene expression values in the training data, we have 373 genes as our final candidate genes. To evaluate the performances of classification, we randomly choose 50 of 114 individuals in each group as our test data set. More detailed data description and processing are provided in Supplementary Methods.     3 Results\r\n   3.1 Simulation\r\n  We calculate the TDR, TPR and TNR of identifying the differential network that corresponds to a given threshold by varying thresholds from 1 to 20 (number of random split was set to be 20 in the Step 4). We average those measures over 100 datasets in each of the four scenarios.  Table 1 presents the TPR, TNR and TDR of the JDINAC, DiffCorr, DEDN and cPLR under different scenarios. It shows that JDINAC significantly outperforms all the other three methods.  Although DiffCorr was set to control the false discovery rate (FDR) &lt; 0.1, the FDR tended to be significantly inflated. In particular, JDINAC performs quite well in scenario 4. The TDR, TPR and TNR of JDINAC are close to 1, but the TDR and TPR of the other three methods are close to 0. It indicates that JDINAC can indeed capture the perturbation of non-linear dependence in the network.  By using repeated procedure, JDINAC allows us to assign a weight to each selected pair of genes, which is the frequency as a pair of genes is selected. Thus we can use the precision-recall curve (PRC) to evaluate the performance of JDINAC under various weight cutoff, and to obtain the differential network by controlling the TNR 97.3 97.1 98.1 99.8 Average of 100 replications, %, the best performance is highlighted in bold. aPair ðGi; GjÞ was taken as differential edge in the network for JDINAC, when the differential dependency weight wij 4. bSet to control the FDR ¼ 0.1. trade-off of precision and recall. Figure 3 illustrates the PRC of JDINAC under different scenarios. The JDINAC has high PRC in all scenarios. The PRC is not included for DiffCorr, DEDN and cPLR, because they cannot report the same weights as JDINAC.  The average ROC curves over 100 replications for the classification using five methods under different scenarios (Fig. 4) show that JDINAC performs the best among the five methods. The fractions of votes were used as the continuous predictions for RF models. After getting the continuous prediction, we used 0.5 as the cutoff of prediction to obtain the classification errors (Table 2). JDINAC is much more accurate than other methods.  To further evaluate the performance of JDINAC, we have simulated other non-linear relationship pattern. The two variables are independent in one group and have exponential relationship in the other group (see Supplementary Fig. S5). Supplementary Figure S6 illustrates the ROC curves in this non-linear scenario. JDINAC still performs the best among the five methods.  We also evaluate how the variance of data distribution affects the performance of JDINAC. As shown in Supplementary Figure S7, the ROC of all five methods goes down when the variance increases.  Again, the proposed method JDINAC still has more robust performance than the other four.  Furthermore, we conducted the simulation study for the case with multidimensional outliers. Five percent variables in one group are randomly chosen to be missed. Then for each missing variable 5% samples are randomly chosen to be missed (see Supplementary Methods S2). The results of classification are shown in Supplementary Figure S8. It indicates that the proposed JDINAC has the best performance. Supplementary Table S1 presents the TPR, TNR and TDR of the JDINAC, DiffCorr, DEDN and cPLR. It shows that JDINAC has the highest TDR and TPR, and acceptable TPR. The results indicate that JDINAC indeed performs well in the case with multidimensional outliers.  As we have balanced number of cases and controls in this study, we used 0.5 as the cutoff of prediction to assign two classes in the classification. However, the optimal cutoff value may depend on ratio of case/control in the training data and application. We conducted another simulation study in imbalanced case/control setting (see Supplementary Fig. S9 and Table S2). The sample size was designed to be 100 for class 0 and 500 for class 1, another scenario is 200 for class 0 and 400 for class 1. The maximum value of Youden index (also called Youden's J statistic, J ¼ Sensitivity þ Specificity  1) was used as a criterion for selecting the optimum cutoff value.  Supplementary Table S2 shows the optimal cutoff point for all methods, as expected, 0.5 is not the optimal cutoff point. Different criteria can lead to different optimal cut off in real world scenario, Youden index puts equal weights to the sensitivity and specificity. In some special diagnostic tests, sensitivity is more important than  RF  NB  cPLR  oPLR  Standard errors are in the parentheses. The best performance is highlighted as bold.  JDINAC  RF  NB oPLR  DiffCorr cPLR specificity, or vice versa. Larger weight should be given to sensitivity or specificity.  Table 3 presents the measured computing time in a single core machine. The time complexity of JDINAC is sub-linear. It is slower than DiffCorr but much faster than DEDN for the differential network analysis. The bottleneck of JDINAC mainly comes from the estimation of the pairwise kernel density and the resampling procedure. As these computations can be easily divided into independent processes, parallel computation with multiple cores and nodes can be employed when the datasets become relatively large.    3.2 Application\r\n  Figure 5 depicts the BRCA differential network estimated by JDINAC, DiffCorr, DEDN and cPLR. Only genes connected with at least one other gene were included in the figure. The top 10 differential dependency pairs identified by JDINAC ordered by weight are shown in Table 4. Figure 6 presents the Venn diagram for the edges in the differential networks identified by different methods JDINAC, DiffCorr, cPLR, and DEDN. There are few overlaps of predicted differential interactions (edges in the network) among these methods. Thus, JDINAC may identify complementary information to the existing methods. The overlapped edges between JDINAC and DiffCorr, JDINAC and cPLR and DiffCorr and cPLR are shown in Supplementary Table S3.  Although there are no common edges shared by all of these methods, several common Gene Ontology  (Ashburner et al., 2000)  terms and a KEGG pathway  (Kanehisa and Goto, 2000)  are enriched by JDINAC, DiffCorr, and cPLR, as identified by R package 'clusterProfiler'  (Yu et al., 2012)  when inspecting the differential network as a whole (Supplementary Table S4). The common biological process and pathway suggest that the change of hemidesmosome assembly and ECM-receptor interaction are important in the etiology of invasive breast cancer. These predictions are supported by the literatures  (Bergstraesser et al., 1995; Oskarsson, 2013) ; both of these processes are the hallmark of invasive breast cancer. It is noted that DEDN does not have enriched terms in the pathway enrichment analysis. Although common individual gene pairs could be missed by different methods due to the multigenic nature of complex diseases such as BRCA, and stochastic process of underlying algorithms, our result suggests that a differential network view may provide more robust biological meaningful signals.  No gold standard is available for evaluating differential network analysis in the real dataset since the true underlying dependence relationships are unknown. We found there are experimental supports for the top ranked pairs by JDINAC. For example, F2R (PAR1) is a G-protein coupled receptor that binds and regulates G-protein. It contributes to tumor progression and metastasis in breast cancer  (Shi et al., 2004) . Meanwhile, GNG11 is a G-protein, plays a role in the transmembrane signaling system. It implies that the molecular role of F2R in the breast cancer progression and metastasis origins from the altered F2R-GNG11 interaction. In other cases, dysregulated pairs may not have direct physical interactions, but strong functional associations. The matrix form of fibronectin (FN1) is believed to support cell adhesion, tumor growth, and inflammation. Fibroblast growth factors (FGF1, FGF2) are important factors regulating expression of FN1 and LAMB3  (Kashpur et al., 2013; Tang et al., 2007) . RAC3 is a GTPase which is related to the cell growth and the activation of protein kinases. Rac GTPase activity and paxillin phosphorylation are elevated in cells from the TPM3 tropomyosin gene deleted mice  (Lees et al., 2013) .  Supplementary Table S5 presents the hub genes and the corresponding number of neighbor genes identified by JDINAC. The hub genes are the ones that have at least three neighbor genes in the differential networks. FGF1 and TGFB3 have the largest number of neighbor genes in the differential networks of BRCA data. FGF1 plays an important role in a variety of biological processes involved in embryonic development, cell growth and differentiation, morphogenesis, tumor growth and invasion  (Zhou et al., 2011) . The expression of FGF1 is dysregulated in breast cancer and contributes to the proliferation of breast cancer cells  (Yoshimura et al., 1998; Zhou et al., 2011) . Laverty et al.  (Ghellal et al., 2000)  reviewed numerous literatures and reported TGFB3 is associated with the progression of breast cancer. PDGFA is confirmed to be one of the progesterone target genes on breast cancer cells  (Soares et al., 2007) . FOXO1 contributes to multiple physiological and pathological processes including cancer, and targeting of FOXO1 by microRNAs may promote the transformation or maintenance of an oncogenic state in breast cancer cells  (Fu and Tindall, 2008; Guttilla and White, 2009) . Moreover, FOXO1 is regulated by AKT  (Tzivion et al., 2011) , and PDGFA is the upstream gene of AKT. Indeed, we identified an edge between PDGFA and FOXO1 (Fig. 5a).  Wendt et al. (2015)  demonstrated that EGFR is a critical gene in primary breast cancer initiation, growth and dissemination. FZD7 plays a critical role in cell proliferation in triple negative breast cancer (TNBC) via Wnt signaling pathway and was considered to be a potential therapeutic target for TNBC  (Yang et al., 2011) . An edge between FZD7 and CTBR2 was identified by JDINAC (Fig. 5a). Actually, CTBP2 is a key gene in Wnt pathway. The identified differential network provides new insight into the underlying genetic mechanisms of BRCA, and testable hypothesis for further experimental validations. The differential interaction patterns and hub genes may serve as biomarkers for early diagnosis or drug targets.  It is quite difficult to quantify the non-linear relationship in real world scenario. We randomly selected 10 genes from the BRCA data, and described the pairwise scatterplot matrix (Supplementary Fig. S10). Overall, there are clear non-linear relationships among these genes. Thus it is necessary to develop methods that can capture the non-linear relationship in the differential network analysis as exampled by JDINAC.  Next, we study the classification performances of methods JDINAC, RF, NB, cPLR and oPLR. The classification errors are shown in Table 5. The classification accuracy of JDINAC is the same with oPLR that uses single genes as features, but better than RF, NB and cPLR, all of which use the pair of genes for the classification. The low error rate of JDINAC implies that the identified differential network could be biological meaningful to distinguish the disease state with the normal one.  To further verify the predictive ability of JDINAC, we carried out Y-randomization experiments. Firstly, 20% of the data as the hold-out test set was randomly select, and the left 80% as training data. Secondly, response variable Y with the training data was randomly permuted 1000 times; a JDINAC model was trained using each permutated data set; and the performance of the trained model over the hold-out data was measured. Finally, the statistical significance of performance measure of JDINAC from the non-permutated data was determined based on the distribution of performance measures from permutated oPLR 1     4 Discussion\r\n  A complex disease phenotype (e.g. cancer) is rarely a consequence of an abnormality in a single gene, but reflects various pathobiological processes that interact in a network  (Barabasi et al., 2011) . Network comparison or differential network analysis has become an important means of revealing the underlying mechanism of pathogenesis. The identified differential interaction patterns between two group-specific biological networks can be taken as candidate biomarkers, and have extensive biomedical and clinical applications  (Ji et al., 2015, 2016; Laenen et al., 2013; Yang et al., 2013) . Although numerous differential network analysis methods  (Fukushima, 2013; Ha et al., 2015; Watson, 2006; Yates and Mukhopadhyay, 2013; Zhao et al., 2014)  have been proposed, most of the methods rely on marginal or partial correlation to measure the strength of connection between pairs of nodes in a network. They usually cannot capture the non-linear relationship among genes, which could be ubiquitous in real applications.  We propose a joint kernel density based method, JDINAC, for identifying differential interaction patterns of networks between condition-specific groups and conducting discriminant analysis simultaneously. A multiple splitting and prediction averaging procedure were employed in the algorithm of JDINAC. It can not only make the approach more robust and accurate, but also make more efficient use of limited data  (Fan et al., 2016) . Moreover, the non-parametric kernel method was used to estimate the joint density, which does not require any conditions on the distribution of the data; this also makes JDINAC more robust and has the ability to capture the non-linear relationship among genes. Extensive simulations were conducted to assess the performances of differential network analysis and classification accuracy. It indicated that JDINAC has high reliability (Fig. 3) and significantly outperforms other state-of-the-art methods, DiffCorr, DEDN and cPLR, especially in scenarios 3 and 4 for the differential network analysis (Table 1). One advantage for JDINAC is that it can achieve classification simultaneously, making it more attractive in practical applications. Figure 4 and Table 2 further highlighted that JDINAC is much more accurate in classification than other methods.  Although JDINAC can in principle be applied to genome-wide data sets, such application may be limited due to high computational costs. In this study, we focus on identifying the differential interaction patterns between genes in a given pathway (or a candidate gene set). JDINAC can be directly used in most cases, since &gt;95% pathways from KEGG database contain &lt;150 genes. Under the scenario when the pathway is too large or in the case of genomewide study, prior knowledge or screening method can be used to shrink the candidate gene pair numbers before applying JDINAC. Although the proposed JDINAC method was applied to gene network differential network analysis in this article, it can be used to incorporate other biological networks, such as metabolic network and brain functional connectivity network. It can also be generalized to identify of between pathway interactions.  The freely available JDINAC software is available as R script at https://github.com/jijiadong/JDINAC.    Acknowledgements\r\n  We thank reviewers for their constructive suggestions in improving the article. The authors would like to acknowledge TCGA for providing the BRCA data. We would also like to thank the patients for access to their study data.    Funding\r\n  This research was supported by the National Library of Medicine of the National Institute of Health under the award number [R01LM011986], the National Institute on Minority Health and Health Disparities of the National Institutes of Health under award number [G12MD007599], National Science Foundation under the award number [CNS-0958379, CNS-0855217, ACI1126113, DMS-1554804], the City University of New York High Performance Computing Center at the College of Staten Island, and National Natural Science Foundation of China [grant number 81573259 and 81673272]. Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/jijiadong/JDINAC",
    "publicationDate": "0",
    "authors": [
      "Jiadong Ji",
      "Di He",
      "Yang Feng",
      "Yong He",
      "Fuzhong Xue"
    ],
    "status": "Success",
    "toolName": "JDINAC",
    "homepage": ""
  },
  "15.pdf": {
    "forks": 0,
    "URLs": ["github.com/khaled-buet/CRISPRpred"],
    "contactInfo": ["khaledur@cse.uiu.ac.bd"],
    "subscribers": 2,
    "programmingLanguage": "R",
    "shortDescription": "",
    "publicationTitle": "CRISPRpred: A flexible and efficient tool for sgRNAs on-target activity prediction in CRISPR/Cas9 systems",
    "title": "CRISPRpred: A flexible and efficient tool for sgRNAs on-target activity prediction in CRISPR/Cas9 systems",
    "publicationDOI": "None",
    "codeSize": 17371,
    "publicationAbstract": "The CRISPR/Cas9-sgRNA system has recently become a popular tool for genome editing and a very hot topic in the field of medical research. In this system, Cas9 protein is directed to a desired location for gene engineering and cleaves target DNA sequence which is complementary to a 20-nucleotide guide sequence found within the sgRNA. A lot of experimental efforts, ranging from in vivo selection to in silico modeling, have been made for efficient designing of sgRNAs in CRISPR/Cas9 system. In this article, we present a novel tool, called CRISPRpred, for efficient in silico prediction of sgRNAs on-target activity which is based on the applications of Support Vector Machine (SVM) model. To conduct experiments, we have used a benchmark dataset of 17 genes and 5310 guide sequences where there are only 20% true values. CRISPRpred achieves Area Under Receiver Operating Characteristics Curve (AUROC-Curve), Area Under Precision Recall Curve (AUPR-Curve) and maximum Matthews Correlation Coefficient (MCC) as 0.85, 0.56 and 0.48, respectively. Our tool shows approximately 5% improvement in AUPR-Curve and after analyzing all evaluation metrics, we find that CRISPRpred is better than the current state-of-the-art. CRISPRpred is enough flexible to extract relevant features and use them in a learning algorithm. The source code of our entire software with relevant dataset can be found in the following link: https://github.com/khaled-buet/CRISPRpred.",
    "dateUpdated": "2017-02-09T17:33:11Z",
    "institutions": [
      "2 Dept. of Computer Science and Engineering, United International University",
      "Indiana University"
    ],
    "license": "No License",
    "dateCreated": "2016-10-28T10:16:23Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     August      CRISPRpred: A flexible and efficient tool for sgRNAs on-target activity prediction in CRISPR/Cas9 systems     Md. Khaledur Rahman  khaledur@cse.uiu.ac.bd  0  1    M. Sohel Rahman  0  1    0  AlEDA group, Dept. of Computer Science and Engineering, Bangladesh University of Engineering and Technology ,  Dhaka ,  Bangladesh ,  2 Dept. of Computer Science and Engineering, United International University ,  Dhaka ,  Bangladesh    1  Editor: Haixu Tang, Indiana University ,  UNITED STATES     2  8  2017   2  2017  4  17    10  7  2017    10  4  2017     The CRISPR/Cas9-sgRNA system has recently become a popular tool for genome editing and a very hot topic in the field of medical research. In this system, Cas9 protein is directed to a desired location for gene engineering and cleaves target DNA sequence which is complementary to a 20-nucleotide guide sequence found within the sgRNA. A lot of experimental efforts, ranging from in vivo selection to in silico modeling, have been made for efficient designing of sgRNAs in CRISPR/Cas9 system. In this article, we present a novel tool, called CRISPRpred, for efficient in silico prediction of sgRNAs on-target activity which is based on the applications of Support Vector Machine (SVM) model. To conduct experiments, we have used a benchmark dataset of 17 genes and 5310 guide sequences where there are only 20% true values. CRISPRpred achieves Area Under Receiver Operating Characteristics Curve (AUROC-Curve), Area Under Precision Recall Curve (AUPR-Curve) and maximum Matthews Correlation Coefficient (MCC) as 0.85, 0.56 and 0.48, respectively. Our tool shows approximately 5% improvement in AUPR-Curve and after analyzing all evaluation metrics, we find that CRISPRpred is better than the current state-of-the-art. CRISPRpred is enough flexible to extract relevant features and use them in a learning algorithm. The source code of our entire software with relevant dataset can be found in the following link: https://github.com/khaled-buet/CRISPRpred.       -\r\n  Data Availability Statement: All of our source code, experimental results, data manipulation code, dataset and figure generation code are available in the following link: https://github.com/khaled-buet/CRISPRpred.  Funding: This research work is part of an ongoing CASR research project of Bangladesh University of Engineering and Technology (BUET). It is also partially supported by an INSPIRE Strategic Partnership Award (M. Sohel Rahman), administered by British Council, Bangladesh. The funders had no role in study design, data collection    Introduction\r\n  Genome-editing technology has become very popular in recent years and it has significantly caught the sight of scientific community [ 1 ]. Rapid growth of a number of development tools makes this interesting biological phenomenon clear and helps us obtain desirable biological systems. The Clustered Regularly Inter-spaced Short Palindromic Repeats (CRISPR) and their associated endonucleas genes (Cas9) have been recently demonstrated to be a revolutionary technology for genome editing in mammalian cells [ 2, 3 ]. CRISPR/Cas9 technology functions against viral infections or other types of horizontal gene transfer by cutting down foreign and analysis, decision to publish, or preparation of the manuscript. DNA which attempts to spread into the host [ 4, 5 ]. This technology is easy to use which is an advantage over its genome-editing predecessors, namely, Zinc Finger Nucleases (ZFNs) and Transcription Activator Like Effector Nucleases (TALENs) [ 6, 7 ]. ZFNs and TALENs both require that scientists create custom proteins for each DNA target which requires much more effort than the painless RNA programming required for CRISPR/Cas9. Basically, Cas9 nucleases persuade double-strand breaks on targeted location of genome directed by single-guide RNAs (sgRNAs). Several studies demonstrated that the Cas9-based genome editing may introduce off-target effects which can result in a significant level of non-specific editing at other unplanned genomic loci [ 8, 9 ]. However, some recent studies have shown that the off-target effects of this technology is not as significant as previously mentioned [ 10, 11 ]. So, the efficacy and specificity of Cas9-sgRNA need to be carefully studied before applying it in vivo.  There are two necessary RNA components for CRISPR/Cas9 in bacteria, namely, CRISPR RNA (crRNA) and trans-activating crRNA (trRNA) which complement each other to activate and guide Cas9 nucleases [ 12 ]. A recent work suggests that a single guide RNA (sgRNA) engineering is functionally equivalent to the combining complex of both crRNA and trRNA [ 13 ]. In CRISPR/Cas9 technology, we need to design an sgRNA to have a nucleotide sequence of around 20 nucleotides which is complementary to the target region. Cas9 protein cuts off specific region of DNA (i.e., invading viral genome) guided by this rationally designed sgRNA. In a nutshell, the flexibility of RNA-guided CRISPR/Cas9 system promotes scientists to perform genome editing for virtually any locus of interest in an easy and quick way by changing the sgRNA in the expression vector. Thus efficient design of sgRNA is in great demand.  A lot of web-based tools and standalone software packages can be found in the literature for efficient designing of sgRNA in CRISPR/Cas9 technology. Some of these popular and interesting tools have been briefly reviewed below. CRISPRseek identifies gRNAs that aim at a given input sequence while minimizing off-target cleavage at other sites within any selected genome [ 14 ]. This tool can generate a cleavage score for potential off-target sequences and rank gRNAs based on the difference among predicted cleavage scores in given input sequence. CHOPCHOP also uses scoring functions, efficient sequence alignment algorithms to minimize search times, and rigorously predicts off-target binding of sgRNAs [ 15 ]. CasOT and Cas-OFFinder search potential off-target sites for any given input genome sequence with given types of Protospacer Adjacent Motif (PAM), and the number of mismatches allowed in the seed and nonseed regions [ 16, 17 ]. CCTop is a web-based tool that reports and orders all possible sgRNAs based on off-target quality and shows full documentation [ 18 ]. The CRISPRscan algorithm designs highly efficient sgRNAs for genome editing analyzing different types of molecular features such as guanine enrichment, adenine depletion, nucleosome positioning, etc.; it also analyzes secondary structures of sgRNAs to extract some thermodynamic features [ 19 ]. The sgRNAcas9 software package searches for CRISPR target sites based on given parameters and minimizes off-target effects [ 20 ]. WU-CRISPR analyzes RNA-seq data and identify some novel features that contribute to designing highly efficient sgRNAs [21]. CRISPR-ERA tool designs efficient sgRNAs specifically for gene editing, repression and activation [ 22 ]. The CRISPR Design tool reports that high concentrations of the enzyme increase off-site target effects, whereas lower concentration of Cas9 increases specificity while reducing on-target cleavage activity [ 8 ]. Chari et al. have developed sgRNA Scorer tool to assess sgRNA activity across 1,400 genomic loci [ 23 ]. Their unraveled underlying nucleotide sequence and epigenetic parameters have contributed to designing an improved mechanism of genome targeting. SSC, RDHA sgRNA and CRISPy-web are some online tools for the design of highly active sgRNAs for any gene of interest [ 10, 24, 25 ]. Azimuth is another important tool for in silico modeling of sgRNAs activity which is currently the state-of-the-art for on-target prediction [ 26 ]. Most of these tools analyze the composition of nucleotides of sgRNAs in animals and report regarding 2 / 14 nucleotide preferences. However, a recent study of plant suggests that there is no such nucleotide preferences in plant sgRNAs [ 27 ].  Our Contributions: On-target activity prediction tools are very important for genome editing. High predictive performance in on-target activity of sgRNAs is highly demanding for CRISPR/Cas9 systems. In this article, we present a new tool, namely, CRISPRpred that predicts on-target activity of in-silico sgRNAs efficiently. Our overall contributions are as follows: · We present a generalized method that produces all possible position specific features and sequence-based features. In addition, our method also produces secondary structure related features. · Our method extracts relevant features based on the importance scores of random forest.  After that a single pass anova test keeps relevant features based on p-value. · We train a Support Vector Machine (SVM) to build a learning model and conduct experiments on publicly available dataset using relevant features that have importance scores above a threshold. · Finally, we compare experimental results of CRISPRpred with Azimuth which is currently the state-of-the-art and show the effectiveness and efficacy of the newly introduced tool.    Results\r\n  It has been observed in the literature that the mutagenic performance of CRISPR/Cas9 system differs significantly due to a small change in sgRNAs [ 8 ]. Mandal et al. has suggested that the on-target performance of site-directed mutation is greatly dependent on the sgRNA provided that sgRNAs targeting the same genomic locus show various activities [ 28 ]. As discussed earlier, a handful of studies have focused on sequence determinants of sgRNAs to predict cleavage activity [ 21, 24, 26 ]. Ma et al. have suggested that the secondary structure of sgRNAs may contribute to editing efficiency [ 29 ]. In our work, we have focused on both position-specific sequences of adjacent nucleotides and secondary structures to construct features from sgRNAs. We have re-analyzed a public dataset of 5310 guide sgRNAs (see S1 File) in total for feature extraction and selection that are highly associated with CRISPR activities.  We have conducted experiments and found that CRISPRpred outperforms Azimuth. It is also important to note that choosing a suitable evaluation metric is equally important like using a suitable method. We have analyzed Receiver Operating Characteristic Curve (ROCCurve), Area Under Precision-Recall Curve (AUPR-Curve), Sensitivity-Specificity Curve (SS-Curve), Matthews Correlation Coefficient Curve (MCC-Curve) and Root Mean Square Error (RMSE) to compare the results [ 30, 31 ].   Experimental setup\r\n  We have used three different machines to conduct the experiments, namely, a Desktop computer with Intel Core i3 CPU @ 1.90GHz x 4, Ubuntu 15.10 64-bit OS and 4 GB RAM, a Desktop computer with Intel Core i7 CPU @ 3.30GHz x 4, Windows 7, 64-bit OS and 8 GB RAM, and a Dell PowerEdge R430 Rack Server with Ubuntu 13.04 OS, Intel1 Xeon1 E5-2620 v4 2.1GHz, 20M Cache and 8GB RDIMM. We have used R language version 3.2.1. In `Methods' section, we have reported all other packages used in our experiments. A procedural flow of our experiments has been shown in Fig 1. In the dataset, there can be some irrelevant information like name of target gene, name of drug and others that are not useful for making prediction.  We have discarded those irrelevant information in the preprocessing step. Generally, a `score' 3 / 14 Fig 1. Workflow of CRISPRpred. In the first step, input dataset is preprocessed to discard any kind of irrelevant information. In the next step, it constructs features based on position-specific sequence, position-independent sequence and secondary structures. In extraction and selection step, a feature selection algorithm is run to select top relevant features. In the next step, SVM is applied in `Apply ML-methods' stage to check the performance. We can also tune different parameters of feature selection algorithms and apply SVM again. In each case, we note important results. is calculated based on how successful a guide sequence is at knocking out its target gene and we do not need to process target gene name after calculating the score. We also filter and preprocess such information from the input dataset. Then, we construct features using our tool as described in the `Methods' sections. After feature construction, we keep all those information in the dataset that are relevant and supposed to be important for the SVM. In the next step, we run random forest algorithm to select the most relevant features. Some features may not contain any information for prediction. For example, we constructed a binary feature that indicates the position of `T' in 26th place in a 30-mer sgRNA; but it does not carry any significant information to predict the activity well. Such features have also been discarded after running the selection process and only relevant features are retained for the next step. We apply SVM on the dataset for different combinations of finally selected features. Different parameters can be tuned for random forest and SVM, e.g., learning rate in SVM, number of trees in random forest, etc. and we also record the results. For each setup of parameters, we run each algorithm 30 times and report the average results. We compute the evaluation metrics using leave-onegene-out cross-validation.    Feature construction, importance and selection\r\n  A plethora of previous studies has shown that both sequence characteristics and structural characteristics of sgRNAs play important roles in target cleavage. So, in this research, we have explored positions of nucleotides as well as secondary structures of sgRNAs to construct features (see `Methods' section). In addition, some factors can trigger a particular chain of events that we need to study as well [ 32 ]. Position specific features are related to positions of nucleotides in sgRNAs and also affect the mutagenic activity. These may include specific position of a single nucleotide in an sgRNA, specific position of a pair of nucleotides, specific position of three consecutive nucleotides, etc. Structural features, also known as thermodynamic properties, include Minimum Free Energy (MFE), most favorable thermodynamic RNA-RNA interaction energy, local pair probabilities and specific heat of the corresponding 30-mer of sgRNAs. There are also some features that are treated as position independent features and are not related to previous types such as GC count, i.e., how many times G and C appear in a 30-mer guide; AT count, i.e., how many times AT pair appears in a 30-mer guide and A count, i.e., how many times A appears in a 30-mer guide. Besides these, we have used two features found in the dataset that also improve the predictive performance. The name of those two 4 / 14 Fig 2. Normalized read count of `A', `C', `G' and `T' in 30-mer sgRNAs. `N' represents any nucleotide. features are `amino acid cut position' and `percent peptide'. Detail information about the dataset can be found in [ 26 ]; however, we have collected the dataset from Azimuth website [ 33 ].  We have reported the normalized read count of single nucleotide position specific features from 30-mer sgRNAs in Fig 2. We see that the 25th position has any nucleotide followed by two G's in the 26th and 27th positions. Here, the motif ªNGGº is the Protospacer Adjacent Motif (PAM), where `N' is any nucleotide of `A', `C', `G' or `T' [ 8 ]. From gene specific frequency distribution of all 30-mer sgRNAs, we can deduce that most of the genes are enriched with `G' in 30-mer sgRNAs (see Fig 3). So `G' enrichment is likely to be an important feature. Thus, normalized read count and gene specific nucleotides distribution give us the idea of position specific features and position independent features, respectively.  All features do not carry the same information to build an efficient prediction model. Thus we need to select important features and discard all irrelevant features. There are various existing machine learning methods such as wrapper or filter method that we can apply to do this job. In this work, we have used random forest algorithm for feature selection based on importance scores (Mean Decrease Gini) where we set a threshold to select all relevant features. We 5 / 14 Fig 3. Gene-wise frequency distribution of nucleotides for all sgRNAs. All genes are enriched with `G' except `CUL3'. have shown category-wise feature importance based on normalized Mean Decrease Gini of Random Forest in Fig 4. Here, single-nucleotide, di-nucleotides, tri-nucleotides and tetranucleotides position specific features are referred to as follows: first three character of each category-name denotes the number of nucleotides appear together in 30-mer sgRNAs; e.g., `3rdPIF' represents summation of importance scores for all position independent features of three consecutive nucleotides (tri-nucleotides).  There are some features like `G_24' and `GAGG_24' with `G' in the 24th position in both cases. Intuitively, one may think that these two features may have no combined effect, i.e., if we remove any one of these features from the feature list then performance metrics will not change. To investigate this issue, we have conducted experiments using linear regression model with 10-fold cross-validation keeping both of these features along with all other features and recorded Root Mean Square Error (RMSE). Then, we have excluded `GAGG_24' from the 6 / 14 Fig 4. Normalized importance values (Mean Decrease Gini of Random Forest) of different category-wise features. `1stPIF', `2ndPIF', `3rdPIF' and `4thPIF' represent category-wise summation of importance values of position independent features where first three character of each category-name denotes the number of nucleotides appear together in 30-mer sgRNAs; e.g., `3rdPIF' represents summation of importance value for all position independent features of three consecutive nucleotides (tri-nucleotides). Similarly, `1stPSF', `2ndPSF', `3rdPSF' and `4thPSF' represent category-wise summation of importance values of position specific features where first three character of each category-name denotes the number of nucleotides appear together in 30-mer sgRNAs; e.g., `4rdPSF' represents summation of importance value for all position specific features where four consecutive nucleotides (tetra-nucleotides) are considered. Thermodynamic features `MFE' and `Heat' mean minimum free energy and heat of 30-mer sgRNAs, respectively. `PP' represents percentage of peptide and `AACP' represents amino acid cut position. feature list and observed that RMSE decreases. This experiment validates that `G_24' and `GAGG_24' position specific features have some combined effect along with all other selected features.  Another interesting finding is that though we extract 9632 features (see S2 File) in total, we can not use some features for training that have importance values greater than zero. This is because we have only 5310 guide sequences to train and test. If the size of features is greater than the size of dataset, then SVM does not work well. Therefore, we do these experiments setting some thresholds for importance values. We have shown different threshold values and corresponding ROC-Curves in Fig 5. When an ROC-curve gets the left top corner of the graph then it indicates better performance [ 30 ]. To conduct experiments, we first set the threshold value to 0.5 and draw the ROC-Curve. Then, we decrease threshold value to 0.4 and draw the corresponding ROC-Curve. In this case, we see that the number of selected features is more than the previous case and performance is also better. We get similar results when we set the threshold to 0.19 and 0.18. But, when we decrease more (i.e., reach 0.17 and 0.1), the performance degrades. This happens presumably because, in this case, the total number of features actually crosses a limit which in some sense is incompatible/unreasoanble with respect to the size of the datasets (i.e., 5310). So, if we get a dataset where 9632 features are reasonable, then we can expect to get even better predictive performance. We have summarized the results in when we set threshold to 0.18. 7 / 14 Fig 5. Comparison among ROC-Curves for different thresholds values of importance scores. For each threshold, we build a linear formula taking corresponding features and train dataset. Then, we perform SVM method based on the formula and check prediction values to draw ROC-Curves. In all experiments, we keep the same parametric values and perform leave-one-gene-out cross-validation.    Evaluation and comparison\r\n  Once we have selected relevant position independent features and position-specific features, we are ready to evaluate the predictive performance of CRISPRpred. We also compare the results with current state-of-the-art method (Azimuth). We have used linear combination of all features for CRISPRpred. We have set default parameters for SVM and we choose threshold value for importance score as 0.18 (Please see Table 1). Unless otherwise noted, we have performed leave-one-gene-out cross-validation over the dataset. Note that, we have restricted the parametric values of all methods to avoid over-fitting, i.e., same parametric values have been used for both training and testing cases. We have reported the results in Figs 6 and 7. We have conducted the anova test on linear regression model and found that some features like `AC_25' and `T_26' are redundant. So, we have excluded those features from the feature list Number of Features 4790 3120 2899 2609 890 606  AUROC 0.72 0.73 0.85 0.84 0.78 0.75 Fig 6. Comparison between CRISPRpred and Azimuth (labeled as Doench et al.). Left figure shows comparison of ROC-Curve and right figure shows comparison of Area Under PR-Curve (AUPR-Curve). and conducted the experiment again to record RMSE. Interestingly, we have found that there is no change in the RMSE and other performance metrics, i.e., those features are indeed redundant. In this way, we deduce that active sgRNA is `G' rich which is supported by CRISPRscan [ 34 ]. Our experimental results show that stable sgRNA is `A' rich (p-value = 1.036413e-78) which is also supported by Azimuth [ 33 ].  We have reported the results of Receiver Operating Characteristics (ROC) curves and Precision-Recall (PR) curves in Fig 6. When ROC-curve gets the left upper corner, then it indicates better performance and when the PR-curve gets the right upper corner of the graph then it shows better performance [ 30, 31 ]. Actually, in those cases, we get higher value for area Fig 7. Comparison between CRISPRpred and Azimuth (labeled as Doench et al.). Left figure shows comparison of Sensitivity Specificity Curve (SS-Curve) and right figure shows comparison of Matthews Correlation Coefficient Curve (MCC-Curve). 9 / 14 under ROC-curve or area under PR-curve. In Fig 6, we see that CRISPRpred beats Azimuth (labeled as Doench et al. [ 26 ]) in both ROC-Curve analysis and AUPR-Curve analysis. The values of area under ROC-Curve (AUROC-Curve) for CRISPRpred and Azimuth are 0.85 and 0.83, respectively. In the dataset (FC_plus_RES.csv) provided in Azimuth website [ 33 ], there are only 20% true values. In such case, AUPR-Curve analysis explains better and we perform this experiment also. We find the values of area under PR-Curve (AUPR-Curve) for CRISPRpred and Azimuth are 0.56 and 0.53, respectively. Clearly, we get approximately 2% improvement in AUROC-Curve and 5% improvement in AUPR-Curve that are very important for CRISPR/Cas9 systems where higher accuracy is highly demanding.  We compare some other metrics in Fig 7. In Sensitivity Specificity Curve (SS-Curve) and Matthews Correlation Coefficient Curve (MCC-Curve), CRISPRpred also performs better than Azimuth. Maximum MCC values for CRISPRpred and Azimuth are 0.48 and 0.43, respectively. We also find CRISPRpred performing better here. We have labeled predictions of CRISPRpred that can be found with the software (see S3 File).  We have examined significantly on in silico predictions, i.e., only on-target prediction has been considered but off-target effects are also worth mentioning for erroneous sgRNAs. To this end, we have conducted a narrow set of experiments where we have used CCtop site [ 35 ] to check off-target effects. The results are not mentionable now; however, we plan to work on it with broader datasets and integrate with CRISPRpred in future to get the best performance for both on-target and off-target predictions.    Limitations\r\n    Conclusion\r\n     Methods\r\n   Dataset\r\n  In this article, our investigation provides two key factors that improve the in silico prediction of sgRNA activity in CRISPR/Cas9 system. First of all, we have incorporated all possible single, di-nucleotides, tri-nucletides and tetra-nucleotides position specific features and position independent features. Our newly introduced tool has definitely increased predictive performance. Secondly, we have examined that in addition to `G'-rich property, active sgRNA is enriched with `A' (p-value = 1.036413e-78) but is `T' depleted. We have selected relevant features to build CRISPRpred that consists of SVM algorithm along with some other functionalities. In our experiments, for CRISPRpred, we have achieved excellent values of 0.85, 0.56 and 0.48 for AUROC-Curve, AUPR-curve and maximum MCC, respectively. In all aspects of ontarget prediction, CRISPRpred performs better than the current state-of-the-art. We have used a combination of two primary datasets available in [ 10 ], which we refer to as ªFC-RESº because the methods used to detect successful knockdowns were Flow Cytometry (FC) and RESistance assays. Notably, both FC and RES datasets are also separately available. The FC-RES dataset consists of 17 genes and 4380 unique guides targeting 11 human genes (CD13, CD15, CD33, CCDC101, MED12, TADA2B, TADA1, HPRT, CUL3, NF1, NF2) and 6 mouse genes (Cd5, Cd28, H2-K, Cd45, Thy1, Cd43) (see S1 File). Unless otherwise noted, while conducting experiments, we have always used one-gene-out cross-validation to see the effectiveness of our tool. More details about the dataset can be found in [ 26 ] and [ 10 ]. A summary of the dataset has been discussed at the beginning of results section. We have re-analyzed 10 / 14 the dataset using our data manipulation code and Vienna package [ 36 ] to identify novel features that are correlated to sgRNA efficacy.    Machine learning algorithms\r\n  We have incorporated Linear Regression (LR), Random Forest (RF) and SVM machine learning algorithms to conduct experiments. We have used linear kernel function with SVM. We have also used evaluation metrics for binary classifier. We have written code in R language for each of these models. For all these algorithms, we have used e1071, randomForest, ROCR, caTools R packages. While conducting experiments, we have performed leave-one-gene-out cross-validation. We allow a maximum number of trees to be 500 while conducting experiments for the RF model. We have observed that predictive ability of the SVM model increases with the increase in the number of relevant features.    Features\r\n  We have constructed features from sgRNA guides using a python program which is available in the software package. We have treated each position of a nucleotide as a binary value and deduced 120 features. For example, we get 4 features for the first position of a sgRNA based on the presence of any of four nucleotides (i.e., any of A, C, G, T) and there are 30 positions in 30-mer sgRNAs. Then we have created another 464 binary features by checking whether there are two same consecutive nucleotides in the first and second positions of a sgRNA, respectively. For example, if first nucleotide is `A' and second nucleotide is `A' in a sgRNA then we treat this as 1, otherwise we simply put a 0 which means the absence of `AA' in the first and second positions in sgRNA. We have also mixed two types of nucleotides that are adjacent and repeated the previous feature construction process to get more binary features. For example, we get a feature considering that the first nucleotide is `A' and the second nucleotide is `C' in 30-mer sgRNAs. Similarly, we get another feature if first nucleotide is `C' and second nucleotide is `A'. In this way, we get 29 features for one type of nucleotide and a total of 29 × (4)2 = 464 features. Then we have mixed three types of nucleotides that are adjacent and repeated the previous feature construction process again to consider 28 × (4)3 = 1792 more features. We have also mixed four types of nucleotides that are adjacent and repeated the previous feature construction process again to consider 27 × (4)4 = 6912 more features. In addition to these, we have deduced position independent features like GC content, AT content, A content, C content, G content, T content, etc. We get four position independent features considering single nucleotide, namely, A, C, G and T. After that we get 16 position independent features considering different combinations of two nucleotides (e.g., AA, AC, AG, etc.). Similarly, we also consider different combinations of three nucleotides and four nucleotides. Finally, we get a total 41 + 42 + 43 + 44 = 340 position independent features. We get two important features, namely, amino acid cut position, and percent peptide from the dataset provided in the Azimuth website.  We have constructed thermodynamic features using ViennaRNA Package version 2.0 [ 37 ]. We have calculated Minimum Free Energy (MFE) for each 30-mer of sgRNAs using RNAfold with default parameters and interpreted this as a feature. We have also calculated specific heat of corresponding 30-mer of sgRNAs using RNAheat.  Thus, we have extracted a total of 120 + 464 + 1792 + 6912 + 340 + 2 + 2 = 9632 features. After considering all these features, we have performed a wrapper algorithm built around random forest [ 38 ] to select relevant features. We have used Boruta package of R language for this purpose. Boruta function reports any of three states (`Confirmed', `Tentative' and `Rejected') for all features. All confirmed features are relevant and we have ranked them based on 11 / 14 importance. We have also used randomForest package for the same purpose and found that this method is more suitable than Boruta. Finally, we have used relevant features to train the machine learning algorithms.    Statistical significance\r\n  We have evaluated statistical significance by performing an anova test. We have also analyzed Root Mean Square Error (RMSE) after appending each statistically significant feature to the feature list. We have used our customized R programs for various estimations that are available online.    Software\r\n  All of our source code, experimental results, data manipulation code, dataset and figure generation code are available at the following link: https://github.com/khaled-buet/CRISPRpred     Supporting information\r\n  S1 File. Dataset. A public dataset that contains 5310 guide sgRNAs. (CSV) S2 File. Extracted features. This file contains 9632 extracted features. (CSV) S3 File. Prediction score. This file contains scores predicted by CRISPRpred. (CSV)    Acknowledgments\r\n  This research work is part of an ongoing CASR research project of Bangladesh University of Engineering and Technology (BUET). It is also partially supported by an INSPIRE Strategic Partnership Award (M. Sohel Rahman), administered by British Council, Bangladesh. Authors would like to thank Dr. Miguel and Dr. Jennifer Listgarten for providing supporting documents and directions.    Author Contributions\r\n  Conceptualization: Md. Khaledur Rahman.  Data curation: Md. Khaledur Rahman.  Formal analysis: Md. Khaledur Rahman, M. Sohel Rahman. Methodology: Md. Khaledur Rahman.  Project administration: M. Sohel Rahman.  Software: Md. Khaledur Rahman.  Supervision: M. Sohel Rahman.  Validation: M. Sohel Rahman.  Visualization: Md. Khaledur Rahman.  Writing ± original draft: Md. Khaledur Rahman.  Writing ± review &amp; editing: Md. Khaledur Rahman, M. Sohel Rahman. 12 / 14 13 / 14 21.    ",
    "sourceCodeLink": "https://github.com/khaled-buet/CRISPRpred",
    "publicationDate": "0",
    "authors": [
      "Md. Khaledur Rahman",
      "M. Sohel Rahman"
    ],
    "status": "Success",
    "toolName": "CRISPRpred",
    "homepage": ""
  },
  "45.pdf": {
    "forks": 2,
    "URLs": [
      "github.com/yuansliu/HiRGC",
      "www.genome.gov/sequencingcosts"
    ],
    "contactInfo": ["jinyan.li@uts.edu.au"],
    "subscribers": 4,
    "programmingLanguage": "C++",
    "shortDescription": "High-speed and high-ratio referential genome compression",
    "publicationTitle": "High-speed and high-ratio referential genome compression",
    "title": "High-speed and high-ratio referential genome compression",
    "publicationDOI": "10.1093/bioinformatics/btx412",
    "codeSize": 768,
    "publicationAbstract": "Motivation: The rapidly increasing number of genomes generated by high-throughput sequencing platforms and assembly algorithms is accompanied by problems in data storage, compression and communication. Traditional compression algorithms are unable to meet the demand of high compression ratio due to the intrinsic challenging features of DNA sequences such as small alphabet size, frequent repeats and palindromes. Reference-based lossless compression, by which only the differences between two similar genomes are stored, is a promising approach with high compression ratio. Results: We present a high-performance referential genome compression algorithm named HiRGC. It is based on a 2-bit encoding scheme and an advanced greedy-matching search on a hash table. We compare the performance of HiRGC with four state-of-the-art compression methods on a benchmark dataset of eight human genomes. HiRGC takes <30 min to compress about 21 gigabytes of each set of the seven target genomes into 96-260 megabytes, achieving compression ratios of 217 to 82 times. This performance is at least 1.9 times better than the best competing algorithm on its best case. Our compression speed is also at least 2.9 times faster. HiRGC is stable and robust to deal with different reference genomes. In contrast, the competing methods' performance varies widely on different reference genomes. More experiments on 100 human genomes from the 1000 Genome Project and on genomes of several other species again demonstrate that HiRGC's performance is consistently excellent. Availability and implementation: The C þþ and Java source codes of our algorithm are freely available for academic and non-commercial use. They can be downloaded from https://github.com/yuansliu/HiRGC. Contact: jinyan.li@uts.edu.au Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-10-05T19:34:14Z",
    "institutions": [
      "University of Technology Sydney",
      "National University of Singapore"
    ],
    "license": "No License",
    "dateCreated": "2017-01-26T02:20:00Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx412   High-speed and high-ratio referential genome compression     Yuansheng Liu  0    Hui Peng  0    Limsoon Wong  1    Jinyan Li  0    0  Advanced Analytics Institute, University of Technology Sydney ,  Broadway, NSW 2007 ,  Australia    1  School of Computing, National University of Singapore ,  Singapore 117417     2017   1  1  9   Motivation: The rapidly increasing number of genomes generated by high-throughput sequencing platforms and assembly algorithms is accompanied by problems in data storage, compression and communication. Traditional compression algorithms are unable to meet the demand of high compression ratio due to the intrinsic challenging features of DNA sequences such as small alphabet size, frequent repeats and palindromes. Reference-based lossless compression, by which only the differences between two similar genomes are stored, is a promising approach with high compression ratio. Results: We present a high-performance referential genome compression algorithm named HiRGC. It is based on a 2-bit encoding scheme and an advanced greedy-matching search on a hash table. We compare the performance of HiRGC with four state-of-the-art compression methods on a benchmark dataset of eight human genomes. HiRGC takes &lt;30 min to compress about 21 gigabytes of each set of the seven target genomes into 96-260 megabytes, achieving compression ratios of 217 to 82 times. This performance is at least 1.9 times better than the best competing algorithm on its best case. Our compression speed is also at least 2.9 times faster. HiRGC is stable and robust to deal with different reference genomes. In contrast, the competing methods' performance varies widely on different reference genomes. More experiments on 100 human genomes from the 1000 Genome Project and on genomes of several other species again demonstrate that HiRGC's performance is consistently excellent. Availability and implementation: The C þþ and Java source codes of our algorithm are freely available for academic and non-commercial use. They can be downloaded from https://github.com/yuansliu/HiRGC. Contact: jinyan.li@uts.edu.au Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Inanc Birol    1 Introduction\r\n  Next-generation sequencing (NGS) technologies have produced huge amounts of sequence reads and many assembled genomes over the past decade   (Goodwin et al., 2016 ). The swift decline of sequencing costs has made whole-genome sequencing commonplace (http://www.genome.gov/sequencingcosts). The accumulated data and the continuing genome generation provide new foundations to gain deeper insights into genomic structures, functions and evolutions   (Mardis, 2008 ). For example, &gt;100 million human genomes are expected to be sequenced by 2025 for precision medicine research   (Stephens et al., 2015 ). As an uncompressed haploid human genome needs roughly three gigabytes (GB) of physical memory   (Deorowicz et al., 2015; Ochoa et al., 2015 ), this big-data has created computational challenges for storage   (Kahn, 2011 ), retrieval   (Williams and Zobel, 2002 ) and privacy   (Huang et al., 2016 ). Efficient and high-ratio data compression is needed to address the exponential growth of NGS genomes   (Numanagic et al., 2016 ). Traditional text data compression tools such as gzip (http://www.gzip.org/) and bzip2 (http://www.bzip.org/) are inefficient for compressing genome data, because they have not exploited the intrinsic features of DNA sequences such as small alphabet size, frequent repeats and palindromes ( (Ochoa et al., 2015; Zhu et al., 2013) . Recently proposed compression algorithms specialized for genome data have achieved higher compression ratios than the traditional algorithms by making use of such biological features ( (Zhu et al., 2013; Giancarlo et al., 2014) .  Referential genome compression is a preferred option for genome data lossless compression  (Wandelt et al., 2014) . The key idea of this technique is to store the differences between the target genome and the reference by replacing the common long substrings with the corresponding positions at the reference sequence as well as their lengths. This compression approach is motivated by the high similarity between the target genomes and the reference of the same species, e.g. the &gt;99% similarity for human genomes  (Lander et al., 2001) . Since the first referential genome compression algorithm DNAZip  (Christley et al., 2009)  was published, the topic has attracted intensive research interests  (Pavlichin et al., 2013; Deorowicz et al., 2013) . One main limitation of these algorithms is that the variation data must be provided  (Deorowicz et al., 2015; Ochoa et al., 2015) . Since genomes can be obtained from different experiments with different reference genomes and also can be de novo assembled, the variation data are not always available. This specific setting limits their applications in practice. Here, we focus on referential genome compression when just a reference genome is given (i.e. the variation data is not needed).  Our algorithm HiRGC is a high-performance referential genome compression algorithm. Its speed and compression ratio are both better than the state-of-the-art algorithms  (Ochoa et al., 2015; Deorowicz et al., 2015; Saha and Rajasekaran, 2015, 2016) . A novel idea in the preprocessing stage is to purify the raw data to get a sequence containing characters only in W ¼ fA; C; G; Tg by separating the auxiliary information in the FASTA-format file. Then, we use a 2-bit encoding scheme to encode the pure base sequence to a 2-bit integer sequence. These preprocessing steps are different from those used by existing methods. And, to our best knowledge, these preprocessing steps are explicitly described and employed for the first time in referential genome compression. After these preprocessing steps, the subsequent main computation is a greedy matching on a hash table constructed from the reference integer sequence.  The matching strategy is new, and quite different from the mapping generation step of iDoComp  (Ochoa et al., 2015)  and is also different from the alignment step of ERGC  (Saha and Rajasekaran, 2015) : (i) Our matching method takes 2-bit integer sequences as input. iDoComp and ERGC have to deal with complex strings constructed from large character sets. (ii) Our matching method runs in a single round. ERGC requires iterations with different lengths of substrings and needs to calculate the edit-distance of the unaligned substrings; iDoComp requires storing matches in memory and then requires combining the consecutive matches, which are very memory- and time-consuming; and GDC-2  (Deorowicz et al., 2015)  performs two-level parsing. (iii) Our matching method operates on a global hash table. ERGC uses a partial hash table; (iv) Each match of our algorithm contains at least k integers (a parameter of our algorithm). iDoComp may generate very short matches with only one or two characters. (v) A mismatch element by our algorithm can be an integer or a subsequence. A mismatched character by iDoComp must be related to a match.  The first two differences imply that our greedy matching strategy (a single-round computation without extra cost) is simpler than the other methods, contributing to HiRGC's fast compression speed. The third point implies that HiRGC can perform better than ERGC when structural variants with translocations exist in genomes. Furthermore, the partitioning strategy of ERGC, which divides long sequences into fixed-size blocks, may result in the splitting of long matches into two or more short ones, whereas HiRGC uses a global-matching approach which has less match information to store and thus saves space. The last two points suggest that iDoComp requires more space to store short matches and mismatched characters, whereas HiRGC largely manages to avoid this. Experiments on benchmark genome datasets have confirmed the effectiveness of our new ideas for improving both speed and compression ratio.    2 Related works\r\n  The most difficult part of referential genome compression is to map the target genome to the reference genome; i.e. how to optimize a subsequence of the target to be represented by a pair of integers-a position number and the length of the subsequence-with regard to the reference genome. Existing algorithms have used various data structures to generate the mapping. There are three basic categories. The first category exploits the structure of suffix array or suffix tree to find the longest subsequence. For instance, RLZ  (Kuruppu et al., 2010)  maintains a self-indices structure on the reference sequence, where the self-indices are evolved from a suffix array  (Navarro and Ma\u20ackinen, 2007) . Then, LZ77 parsing  (Ziv and Lempel, 1977)  is used to find the longest prefix of the target sequence with the help of the self-indices structure. An improved version of RLZ (named RLZ_opt  (Kuruppu et al., 2011) ) uses a non-greedy parsing approach and suffix array. Particularly, it incorporates a local lookahead optimization, a short-factor encoding and longest increasing subsequence computation to improve performance.  Wandelt and Leser (2012)  proposed an algorithm to trade between memory consumption and compression speed. The algorithm divides the reference sequence into blocks of fixed size and then finds the longest prefix match by a depth-first search on a suffix tree constructed from the reference block. FRESCO  (Wandelt and Leser, 2013)  parses the target sequence to the reference sequence by a compressed suffix tree. Reference selection, reference rewriting and secondorder compression techniques are also employed by FRESCO to further improve its compression ratio. iDoComp  (Ochoa et al., 2015)  parses the target to the reference by a suffix array. Its strategy is similar to Chern's algorithm  (Chern et al., 2012) , where some adjacent matches are combined into an approximate match and all the mappings are further compressed by an entropy encoder.  The second category of algorithms all use hash tables to parse a target sequence to a reference sequence. GDC  (Deorowicz and Grabowski, 2011)  is an algorithm similar to RLZ_opt. It first divides a sequence into blocks containing 8192 symbols. Then, GDC performs LZ77 parsing by hashing instead of using suffix array. In addition, GDC selects a reference sequence by a heuristic method and provides fast random access.  Xie et al. (2015)  proposed an algorithm CoGI that conducts alignment based on a local no-collision hash table. The primary difference is that CoGI can be used as a reference-free compression algorithm. GDC-2  (Deorowicz et al., 2015)  performs two-level LZ77 parsing to compress a large collection of genomes. Different contextual encoding strategies are employed to encode positions and lengths of the two-level matches. ERGC  (Saha and Rajasekaran, 2015)  is a greedy compression algorithm. It divides the reference sequence and the target sequence to fixed-size blocks, and a target block is aligned to a corresponding reference block using a hash table constructed from all k-mers of the reference block. NRGC  (Saha and Rajasekaran, 2016)  is a recently improved version of ERGC with a scoring-based placement technique.  The third category of algorithms are not related to the above two data structures (suffix array and hash table). RGS  (Wang and Zhang, 2011)  uses a modified UNIX diff program to find the longest common subsequences. GReEn  (Pinho et al., 2012)  is a compression tool based on arithmetic coding. It has a copy model related to the reference sequence for predicting the characters.  Chern et al. (2012)  developed an algorithm to generate the mapping by an adaptive sliding window-based string matching technique.  FASTA file is a widely adopted file format for storing genome sequences. It contains an identifier and lines of sequence data. The standard IUB/IUPAC amino acids and nucleic acid codes constitute the sequence data. Any loss of information in the file would cause unpredictable outcome in personalized medicine. GReEn and CoGI ignore the unique sequence identifier. GRS and RLZ-opt only consider limited upper-case letters, i.e. X ¼ W [ fNg, and CoGI deals with the letters R ¼ fa; t; g; c; ng. Despite the significant enlargement of character sets, the implementation of iDoComp still processes an superset of ðX [ RÞ rather than all the legal characters. Some characteristics of these algorithms are summarized in Supplementary Table S1.  Algorithms like GDC, ERGC and NRGC divide long sequences into fixed-size blocks to improve compression speed. Such a partitioning strategy may result in the splitting of long matches into two or more short ones. ERGC searches for matches in a local block and can achieve high compression ratio only when the reference and target sequence are highly similar. When the variation between them is large, ERGC has limitation due to many unmatched subsequences. To mitigate this limitation, NRGC introduces a new technique called scoring-based placement to detect better alignments via a precomputed scoring system. However, it is a complicated system. Users must have prior knowledge about sequence similarity to obtain a high compression ratio.    3 Materials and methods\r\n  Our algorithm HiRGC is modularized into three stages: preprocessing, greedy matching and post-processing. A schematic diagram of HiRGC is depicted in Figure 1.   3.1 Preprocessing steps\r\n  In this stage, we first separate the FASTA file into two parts: auxiliary information and a sequence that contains letters only in W ¼ fA; C; G; Tg, and then encode the corresponding sequence to a 2-bit integer sequence. The auxiliary information consists of the identifier, the length of short sequences, position intervals of lowercase letters and the letter 'N', and characters not in X ¼ W [ fNg as well as their corresponding positions. We use other characters to represent characters not in X unless otherwise specified.  The detailed procedure for preprocessing a given target genome FASTA file is as follows:  Step 1. Read the identifier from this FASTA file and save it as a string id.  Step 2. Read the short sequences in each line and save their lengths to an array seq len successively. Then the sequences are concatenated as a long sequence L.  Step 3. Find the intervals of lower-case letters in L and save positions of the beginning of these intervals as well as their lengths to the arrays low pos and low len, respectively. Then, convert   Target genome file\r\n    Reference genome file\r\n    Recording auxiliary information (identifier, length of short sequence, position intervals of lower-case letters and N, and other characters)\r\n    Changing all to upper-case letters, and removing identifier and other characters\r\n    Encoding based on the rule: A-0, C-1, G-2, T-3\r\n    Constructing hash table\r\n    PPMD encoder\r\n    Compressed file\r\n    Searching maximum matching\r\n    Matched information and mismatched subsequences\r\n  Processing positions and other characters Text file (delta encoding; run-length encoding;...) After these, the auxiliary information of the target genome is represented as D ¼ fid; seq len; low pos; low len; N pos; N len; oth pos; oth chg, which is required during decompression. Later, we use some advanced encoding techniques to compress D in the postprocessing stage.  Similarly, for the reference genome FASTA file, we obtain a sequence containing characters only in W for the next stage. Note that the auxiliary information of the reference genome file can be discarded. The above preprocessing stage reduces the alphabet size significantly, and makes it possible to encode every character by a 2-bit scheme.  We use a 2-bit encoding method to translate genome sequences into 2-bit integer sequences for referential genome compression. By 2-bit encoding, each nucleotide A, C, G and T is encoded as a 2-bit integer. Considering the Watson-Crick base-pairing rule and that 0 and 1 are complementary in the binary system, the following 2-bit encoding EðxÞ ¼ 8 0; &gt; &gt; &gt; &gt;&lt;&gt; 1; &gt; 2; &gt; &gt; &gt; &gt;: 3; if x ¼ A; if x ¼ C; if x ¼ G; if x ¼ T; translates our preprocessed sequences (containing characters only in W) into 2-bit integer sequences.  For such an integer sequence, we use k-tuple to denote k contiguous integers. Given an integer sequence fuðiÞgin¼1, where uðiÞ 2 f0; 1; 2; 3g, there exist ðn k þ 1Þ overlapping k-tuples. We use an integer of ð2 kÞ bits, called a tuple value, to represent the i-th tuple. It is computed by  k 1 Viu ¼ X uði þ jÞ j¼0 4j ; (1) where i 2 ½1; n  k þ 1 .     3.2 Advanced greedy matching in a global hash table\r\n  First, a hash table is constructed from the tuple values of a reference sequence. It consists of two data structures: an array of entries pð Þ and an array of header pointers hð Þ. We assume that an entry in the hash table contains a tuple, position of this tuple, its hash value and a next pointer. In our implementation, only the next pointer is stored. The entry array links the entries which have the same hash value by the next pointer. An element in the header pointer array and the chain of entries it points to in the array of entries form a bucket that stores all the entries having the same hash value. The details of this step are described in lines 1-4 of Algorithm 1.  In the hash table, our greedy matching finds the maximum match starting at position 1 of the target sequence. Suppose we are processing the i-th tuple in the target sequence. We first calculate the corresponding tuple value Vit. With the corresponding hash value Vit mod s , we detect its corresponding bucket that stores the entries having a hash value equal to Vit mod s . If there is no entry, the i-th tuple in the target sequence is a mismatched one, and skip to the ði þ 1Þ-th tuple. For each entry in this bucket, if its tuple is identical to the current tuple, we extend this match until a mismatched integer is met. The longest one is the final match. If there are more than one longest matches, we select the first one. The subsequence between the last match and the i-th is a mismatched subsequence. The position and length of the match is the matched information. Then, we skip all the tuples in this match. Since most variations between genomes are single nucleotide polymorphism, the integer after a match is always treated as unmatched. Lines 5-26 of Algorithm 1 state the details of this procedure.  Example 1. Suppose we are given the following reference sequence and target sequence:  Reference : 0032 302230103002021    3.3 Post-processing\r\n  We use delta encoding  (Smith et al., 1997)  to encode most of the data derived the first three stages, including the positions of matches, position intervals of lower-case letters and the letter 'N', and positions of other characters. Furthermore, run-length encoding  (Held and Marshall, 1991)  is employed to encode the lengths of short sequences. The other characters are mapped to digit numbers from 0. For example, if there are three other characters, say K, M  Algorithm 1. The hash-based greedy matching Input: a reference sequence R ¼ frðiÞgin¼r 1, a target sequence n T ¼ ftðiÞgi¼t 1, size of hash table s and the tuple length k. Initialize the header pointer, that h(i) ¼ 0 for 8i 2 f1; . . . ; nrg. 1: for i ¼ 1 to ðnr k þ 1Þ do 2: Calculate Vir ¼ Pjk¼01ðrði þ jÞ 4jÞ. 3: Update the hash table:  pðiÞ ¼ hðVir mod sÞ; hðVir mod sÞ ¼ i: 4: end for 5: Set i ¼ 1 and p ¼ 1. 6: repeat 7: Calculate Vit ¼ Pjk¼01ðtði þ jÞ 4jÞ. 8: Set j ¼ hðVit mod sÞ; pmax ¼ 0 and lmax ¼ 0. 9: while j 6¼ 0 do 10: Set l ¼ 0. 11: while rðj þ lÞ ¼ tði þ lÞ do 12: Update l ¼ l þ 1. 13: end while 14: if l k and l &gt; lmax then 15: pmax ¼ j; lmax ¼ l. 16: end if 17: Update j ¼ pðjÞ. 18: end while 19: if lmax &gt; 0 then 20: Subsequence tðp ; . . . ; i 1Þ is a mismatched one. 21: ðpmax; lmaxÞ is the matched information. 22: Update p ¼ i þ lmax. 23: end if 24: Update i ¼ i þ lmax þ 1. 25: until i &gt; ðnt k þ 1Þ 26: The remaining subsequence is a mismatched subsequence.  Output: matched information and mismatched subsequences. and S, these three are converted to integers 0, 1 and 2, respectively. If the other characters are &lt;10 in type, we save all these characters as a string. Otherwise, we combine them as a sequence of 32-bit integers. Then, the auxiliary information, matched information and the mismatched subsequences are stored as an ASCII text file.  We use the PPMD compression algorithm, a variant of prediction by partial matching data compression algorithm   (Cleary and Witten, 1984; Moffat, 1990 ), to compress the text file. 7-zip (http://www.7-zip.org/) provides an implementation of PPMD and we use it in this stage. When there are more than one chromosomes, we compress their text files as a whole.    3.4 Decompression\r\n  Usually referential compression and decompression are asymmetric. Our decompression procedure is just a simple reversion of the compressed one except that the subsequences are recovered from the reference sequence via the positions and lengths of the matches. PPMD first decompresses the final 7z file. Since PPMD is a lossless compression algorithm  (Cleary and Witten, 1984) , and both delta encoding  (Smith et al., 1997)  and run-length encoding  (Held and Marshall, 1991)  are invertible transformation, HiRGC can recover the auxiliary information, matched information and the mismatched subsequences without any information loss during decompression. According to the greedy matching procedure, the reference sequence, the matched information and the mismatched subsequences can be used to precisely reconstruct the target sequence. The 2-bit encoding is an invertible operation as well. Thus, the sequence L3 can be exactly recovered. As the auxiliary information records all the necessary information of changes we made on the original target genome FASTA file, we can precisely reconstruct the original FASTA file by combing the auxiliary information and the sequence L3. Therefore, HiRGC is a lossless compression algorithm.     4 Results and performance analysis\r\n  We report compression ratio, speed, and memory usage of the algorithms in this section. The performance of our algorithm HiRGC is compared to four recently published algorithms: GDC-2  (Deorowicz et al., 2015) , iDoComp  (Ochoa et al., 2015) , ERGC  (Saha and Rajasekaran, 2015) , and NRGC  (Saha and Rajasekaran, 2016) . All the experiments were carried out on a computing cluster running Red Hat Enterprise Linux 6.7 (64 bit) with 2 3:33 GHz Intel Xeon X5680 (6 cores) and 48 GB RAM.   4.1 Genome datasets and their disk file size\r\n  We thoroughly compared the performance of the five algorithms on eight human genomes (Homo sapiens): hg38, hg19, hg18, hg17, the genome of J. Craig Venter (HuRef)  (Levy et al., 2007) , the genome of a Han Chinese known as YH  (Wang et al., 2008) , KOREF_20090131 (denoted by K131), and KOREF_20090224 (denoted by K224)  (Ahn et al., 2009) . Data of these genomes are complete, all containing chromosomes 1-22 and the X and Y sexchromosomes. Each of the eight datasets has a disk file size of around 3.0 GB. All these genomes are benchmark genomes widely used in the performance evaluation of various state-of-the-art algorithms for referential genome compression  (Pinho et al., 2012; Ochoa et al., 2015; Saha and Rajasekaran, 2016; Deorowicz et al., 2016) . Some other information of these genomes are provided in Supplementary Table S2.  In addition, we compared the five algorithms on 100 human genomes randomly selected from the 1000 Genomes Project  (The 1000 Genomes Project Consortium, 2015)  with the help of the script vcf2fasta provided by GDC-2  (Deorowicz et al., 2015) . Finally, genomes of some other species such as three versions of Caenorhabditis elegans (viz. ce6, ce10 and ce11), two versions of Saccharomyces cerevisiae (viz. sacCer2 and sacCer3), two versions of Arabidopsis thaliana (viz. TAIR9 and TAIR10) and three versions of Oryza sativa (viz. TIGR5.0, TIGR6.0 and TIGR7.0), are also used to assess the performance of the five algorithms. The disk file sizes of these genomes range from tens to hundreds megabytes (MB), much smaller than human genomes.    4.2 High compression ratios on the eight benchmark human genomes\r\n  To test the robustness and effectiveness of the algorithms, each of the eight benchmark human genomes was taken iteratively as the reference genome and the remaining seven were all considered as target genomes (i.e. no bias in the target genome selection). In total, 8 7 ¼ 56 reference-target genome pairs were used to test every algorithm. For each reference genome, the total file size of the seven target genomes is around 21.0 GB. The reference genome is not included in any compressed file as per tradition  (Ochoa et al., 2015; Saha and Rajasekaran, 2015, 2016) .  Our HiRGC algorithm could compress the 21.0 GB target genomes to a file size &lt;260 megabytes (MB) in the worst case when HuRef was set as the reference genome. While for the other reference genomes, the HiRGC compressed files are all &lt;130 MB. These compressed files are about 82 to 217 times smaller than the original files. However, most of the compressed files by GDC-2 are at least one order of magnitude larger than ours except for one case (6 times); cf. the last row of Table 1. All of the compressed files by ERGC are at least one order of magnitude (up to 21 times) larger than ours. The compressed files by NRGC are at least 9.2 times larger than ours, and those by iDoComp are at least 1.9 times (up to 7.7 times) larger.  Detailed results on individual reference-target genome pairs show that HiRGC can indeed very often make huge compression improvement. For the other cases, HiRGC has competitive or close performance with the state-of-the-art algorithms. HiRGC did not make any obviously worse (5 MB less) performance than the existing methods except on one pair (viz. when hg18 is set as reference and hg17 as target); cf. Table 2. In fact, HiRGC achieved better results than GDC-2 in 54 cases. In particular, on 32 cases, HiRGC delivered one order of magnitude improvement. HiRGC performed better than NRGC on 53 of the 56 cases; on the remaining three cases, HiRGC and NRGC were very close (within 2.5 MB range in performance). NRGC exhibited extremely poor performance when hg38 or HuRef was set as the reference or the target genomeNRGC even sometimes could not compress. Compared with iDoComp, HiRGC achieved better performance on 44 cases; on 8 other cases, their performance were competitive (within 5 MB range); on the remaining four reference-target genome pairs when Note: iDoComp could not compress some reference-target genome pairs within 24 h. A '\\' means that the result was not available within 24 h. K131 K224 HuRef  Note: Bold indicates the best result in the row. A shadowed text, called the second best result, indicates the difference between this performance and the best is &lt; 5 MB. A '\\' means that the result was not available within 24 h. hg17 was set as reference, iDoComp could not finish compressing the files within 24 h. HiRGC was better than ERGC on 44 of the 56 target genomes (by one order of magnitude mostly); on the remaining 12 cases, the compression performance of HiRGC and ERGC were close to each other within a 5 MB range. Table 2 also lists the best and the second best algorithms on every target genome. HiRGC won 35 times as the best algorithm, and was essentially the second best in all the other cases except one. ERGC won 12 times as the best algorithms; iDoComp won five times as the best algorithm and four times as the second best algorithms; NRGC and GDC-2 got only one and two best cases, respectively.  A box plot of the sizes of the 56 compressed files by each algorithm is shown in Figure 2. The size variation of the compressed files by the four state-of-the-art methods is in a large range (e.g. from 2.28 to 406.66 MB by iDoComp; from 5.79 to 624.43 MB by ERGC). In contrast, the output file size of HiRGC is very stable, ranging from 8.88 to 41.79 MB over the 56 cases. Thus, our algorithm is so robust that it can handle complex input datasets, where the state-of-the-art methods cannot compress well. The average sizes of all the 56 compressed files by the five algorithms are also plotted in Figure 2. Our algorithm HiRGC obtained an average size of 18.3 MB, which is 12.5, 16.6 and 14.8 times improvement over GDC-2's 227.5 MB, ERGC's 303.7 MB and NRGC's 279.3 MB, respectively. iDoComp achieved an average size 83.3 MB (ignoring the four uncompleted cases), 5.9 times larger than HiRGC's.  We also assessed the relative compression gain by comparing our algorithm with the state-of-the-art methods; cf. Table 1. The relative compression gain is defined as:  Gain ¼ 1  Compressed size of HiRGC Compressed size of other method  100%: The gain quantifies the improvement of our method compared against existing ones. For example, our algorithm achieved the gain of 91% against GDC-2 under the reference genome YH. It means that the size of the compressed file using our method is only ð1 91%Þ ¼ 9% of that of GDC-2. The relative compression gain shows that the percentage improvement HiRGC achieved with respect to GDC-2 and ERGC is at least 91%. Comparing with iDoComp, our HiRGC obtained relative compression gain varying from 49% to 87%. The improvement over NRGC by HiRGC is &gt;86%.  For HiRGC, only one parameter (the tuple length k) influences compression result. To explore the best choice of k, we tuned k within the range [10, 25]. The compression ratio is used to measure compression performance. We used YH and hg18 as the reference genome, and tested eight pairs of reference-target genomes under different values of k. The effect of the tuple length k on the compression ratio is shown in Supplementary Figure S1. HiRGC achieved the best compression ratio on different pairs of reference-target genomes when k was set as 20. When k was smaller than 20, lower performance was observed. There was also a slightly decrease in performance when k was set bigger than 20 in most cases. Therefore we set k ¼ 20 here and also recommend this to users.  We ran all the state-of-the-art algorithms with their default parameters. However, we found that NRGC failed to compress some chromosomes under default parameters. Following the instruction of NRGC, we increased the division length which is one of the parameters of NRGC. Because the parameter setting is complex, we cannot guarantee finding the optimum. For a chromosome, if we could not find a proper division length, we had no other choice but to assert that NRGC could not compress it. We then simply compressed the original file with the PPMD compression algorithm.    4.3 Time complexity and speed performance\r\n  Speed is another important factor to measure the performance of a compression algorithm. For HiRGC, only the size of hash table affects compression speed when the tuple length k is fixed. If the size of hash table s is set to 22 20, the buckets will have no hash collision. Therefore, each bucket has the minimum number of entries. But, considering the limited RAM, we need to use a small s. It is difficult to work out the best size of hash table from theory due to the nonuniform distribution of DNA sequences. We performed some experiments on different reference-target pairs with a different s ranging from 26 to 32. The results are shown in Supplementary Figure S2. We found that HiRGC has a good compression speed when the size of hash table s is set to 230. In the following, we first conduct a complexity analysis of HiRGC, and then compare the running speed of HiRGC with other algorithms.  The time complexity of the first and the third stage of HiRGC are linear. Here, we focus on the time complexity analysis of the greedy matching stage. Let nr and nt represent the length of reference sequence and target sequence after preprocessing. First, the complexity of constructing a hash table is linear time, taking OðnrÞ time. Let nb represent the number of mismatched integers. Then the length of total matched subsequence is ðnt nbÞ. Let mi represent the length of the i-th matched subsequence and n~ denote the number of matched subsequences. Then, it can be derived that Pin~¼1 mi ¼ ðnt nbÞ. Let hi be the number of entries of the selected buckets related to the i-th matching subsequence. Then, the number of elementary operations of the greedy matching process is n~ nb þ Xhi i¼1 maxifhig mi nb þ maixfhig n~ Xmi i¼1 nb þ n~ ! Xmi ¼ maxifhig i¼1 nt: Therefore, the best-case time complexity is OðntÞ when hi ¼ 1, and the worst-case time complexity is Oðd ntÞ, where d is the maximum value of hi. Because frequent repeats exist in DNA sequence, there are many identical k-tuples. In our datasets, the maximum value of d can reach a hundred thousands. In practice, we do not meet the extreme case where all values of hi are very large. Next, we analyze the average time complexity of our algorithm on real genomes.  Our above analysis has already indicated that the number of entries of each bucket can significantly affect the time complexity. To investigate the distribution of entries, we count the number of entries in different buckets in the hash table constructed from chromosome 1 of some real genome (chromosome 1 is the longest chromosome). We calculate the percentage of number of buckets for different thresholds as follows:  Number of bucket having at most x entries PðxÞ ¼ Total number of bucket having at least 1 entry 100%: Because P(x) is observed to increase slowly, we only plot data between 1 and 15; this should not affect our analysis and conclusion. The result shows that about 75% of the buckets has no collision and &gt;90% of the buckets has at most 2 entries. When the threshold increases to 15, only about 3&amp; of the buckets has &gt;15 entries. We conclude that most of buckets have very few entries in practice. We then calculate the average number of entries, which is about 1.7 entries per bucket. Finally, the average time complexity of our greedy matching algorithm is estimated as Oð1:7 ntÞ, which is much better than the worst-case. The result is depicted in Supplementary Figure S3.  The actual compression speed of our algorithm is significantly faster than current state-of-the-art methods. To minimise the influence of programming languages, we provide the result for both C þþ and Java implementations of our algorithm. They had similar speed performance. The compression time of our algorithm was &lt;7 min when using C þþ (for most of the cases, &lt;5 min). For GDC2, the fastest compression time was about half an hour, and on some datasets it took &gt;3 h. Since the time to generate the suffix array is expensive, iDoComp spent about half an hour in most cases. There were four cases which could not be compressed by iDoComp within 24 h. ERGC achieved unstable compression speeds from 10 min to half an hour. It heavily relies on the similarity between the reference genome and the target genome. When NRGC could not compress some chromosomes under the default parameter, it spent much time to choose suitable parameters for those chromosomes. Detailed compression time by the different methods is given in Supplementary Table S3.  As described in Section 3.4, the decompression procedure is much simpler than the compression procedure, and it is a linear time operation. Because NRGC could not compress many chromosomes, the decompression time of NRGC is omitted here. The comparison of decompression time is shown in Supplementary Table S4. We observed that GDC-2 and HiRGC performed better than the other methods. GDC-2 and HiRGC achieved stable decompression time, whereas iDoComp heavily depended on the dataset. HiRGC was faster than these methods for most of the cases.  The authors of iDoComp pointed out that the suffix array can be saved once it is generated and can be reused many times. In our algorithm, the hash table obtained from a reference genome can also be reused to compress many target genomes. We ran another experiment and the comparison result is reported in Supplementary Table S5. For this comparison, iDoComp did not count the time of generating the suffix array, and loaded the suffix array from disk, while our algorithm created the hash table once and compressed the other seven genomes. Though the time required to generate the suffix array was ignored, iDoComp still took about more than an hour to compress the genome set. Our algorithm does not consider saving the hash table because it can be generated very fast. From Supplementary Table S5, we see that HiRGC needed &lt;30 min to compress a genome set containing seven genomes of about 21 GB. Moreover, HiRGC spent &lt;20 min to decompress the genome set, while the decompression time by iDoComp was three times more than HiRGC.    4.4 Memory usage by HiRGC\r\n  Storage of the hash table requires 4 nt þ 232 bytes of RAM, where the first and second terms account for the memory requirements of the entry array and the header pointer array, respectively. For the longest chromosome, we have nt &lt; 228. The highest RAM used to store the hash table is &lt;5 GB. Moreover, we require storage for the two integer sequences and the auxiliary information. About 2 GB memories are used in practice. It should be pointed out that none of the positions, the lengths of matches or the mismatched subsequences need to be stored in RAM. Thus, HiRGC can be suitable for the personal laptop with 8 GB of RAM.    4.5 Compressing 100 genomes from the 1000 Genomes\r\n    Project\r\n  We used HG38 as the reference genome to compress 100 genomes randomly selected from the 1000 Genomes Project. The ID numbers of the 100 genomes are listed at Supplementary Table S6. The raw size of the 100 genomes is about 292 GB. ERGC could not compress these genome datasets, because an infinite loop occurred when compressing chromosome 1. GDC-2 could not compress this genome set within 120 h even when 16 threads were used. iDoComp compressed the 100 genomes to 9.7 GB in 1519 min. HiRGC took 210 min to compress the 100 genomes into a file size of 711 MB. The decompression time by iDoComp and HiRGC was 886 and 109 min, respectively. Overall, our performance is much better than the existing methods on compression ratio, compression speed, and decompression time for these 100 genomes.    4.6 Performance on compressing the genomes of some plants and microbial species\r\n  The file size of these genomes are much smaller than human genomes. The algorithms were tested on 16 pairs of reference-target genomes. All the algorithms have competitive or close performance except ERGC and NRGC which did not perform well for the genomes of Oryza sativa. ERGC, GDC-2, HiRGC and iDoComp achieved 2, 3, 4 and 7 best cases, respectively. Detailed compression results are reported at Supplementary Table S7. A separate issue is that iDoComp could not achieve lossless decompression for some cases (see the '?' symbol in Supplementary Table S7). This also happened to iDoComp for some reference-target pairs of the eight benchmark genomes (see the '?' symbol in Supplementary Table S4).     5 Conclusion and future work\r\n  We have proposed a novel algorithm HiRGC for referential genome compression. The key idea is to search maximum matches from the integer sequence on a hash table by an advanced greedy matching strategy. The compression results obtained from real benchmark datasets show that HiRGC significantly outperforms the state-ofthe-art algorithms in terms of compression ratio. In addition, analysis of the time complexity on real datasets shows that the average time complexity of our algorithm is linear in practice; and HiRGC has achieved substantial improvement on compression speed. HiRGC is also stable and robust to deal with different reference genomes.  Although HiRGC has achieved high compression ratio and fast compression/decompression speed, there are some other aspects that are worth consideration for further improvement. First, HiRGC is currently not capable of verifying the reference genome provided by users. To verify the reference genome, there are two possible solutions: (i) Store some information about the reference sequence in the compressed file; or (ii) store a unique ID in both the reference file and the compressed file. Second, HiRGC does not exploit complex structural variants with inversions. If we make full use of such complex structural variants in the greedy matching, a further improvement on compression ratio is possible. Last, HiRGC does not support indexing and searching for the compressed files. There are some research works proposed for searching and indexing on large genome collections  (Danek et al., 2014; Holley et al., 2016) . It is interesting that little compression ratio can be sacrificed to achieve the high speed search in the compressed files  (Wandelt et al., 2013) . For example, the algorithm GDC  (Deorowicz and Grabowski, 2011)  can support random access. We will investigate these issues to provide new functionalities on top of current HiRGC.    Acknowledgements\r\n  We thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of our manuscript. We are also grateful to Dr Yu Zhang of City University of Hong Kong for his insightful discussion.    Funding\r\n  This work was partly supported by Australia Research Council Discovery Project DP130102124.  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/yuansliu/HiRGC",
    "publicationDate": "0",
    "authors": [
      "Yuansheng Liu",
      "Hui Peng",
      "Limsoon Wong",
      "Jinyan Li"
    ],
    "status": "Success",
    "toolName": "HiRGC",
    "homepage": "https://github.com/yuansliu/HiRGC"
  },
  "5.pdf": {
    "forks": 0,
    "URLs": [
      "downloads.sourceforge.net/project/mirlab/groundtruth_Strong.csv",
      "www.synapse.org/#!",
      "doi.org/10.1371/journal.pgen.1004226.s011",
      "github.com/lingfeiwang/findr"
    ],
    "contactInfo": ["Tom.Michoel@roslin.ed.ac.uk"],
    "subscribers": 2,
    "programmingLanguage": "C",
    "shortDescription": "Fast Inference of Networks from Directed Regulations",
    "publicationTitle": "Efficient and accurate causal inference with hidden confounders from genome- transcriptome variation data",
    "title": "Efficient and accurate causal inference with hidden confounders from genome- transcriptome variation data",
    "publicationDOI": "None",
    "codeSize": 1181,
    "publicationAbstract": "Mapping gene expression as a quantitative trait using whole genome-sequencing and transcriptome analysis allows to discover the functional consequences of genetic variation. We developed a novel method and ultra-fast software Findr for higly accurate causal inference between gene expression traits using cis-regulatory DNA variations as causal anchors, which improves current methods by taking into consideration hidden confounders and weak regulations. Findr outperformed existing methods on the DREAM5 Systems Genetics challenge and on the prediction of microRNA and transcription factor targets in human lymphoblastoid cells, while being nearly a million times faster. Findr is publicly available at https://github.com/lingfeiwang/findr.",
    "dateUpdated": "2017-09-22T11:29:00Z",
    "institutions": ["The University of Edinburgh"],
    "license": "https://github.com/lingfeiwang/findr/blob/master/LICENSE",
    "dateCreated": "2016-05-11T14:52:08Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     August      Efficient and accurate causal inference with hidden confounders from genome- transcriptome variation data     Editor: Jennifer Listgarten  0    Microsoft Research  0    UNITED STATES  0    Lingfei Wang  0    Tom Michoel  Tom.Michoel@roslin.ed.ac.uk  0    0  Division of Genetics and Genomics, The Roslin Institute, The University of Edinburgh ,  Easter Bush, Midlothian ,  United Kingdom     18  8  2017   18  2017  6  31    26  7  2017    2  2  2017     Mapping gene expression as a quantitative trait using whole genome-sequencing and transcriptome analysis allows to discover the functional consequences of genetic variation. We developed a novel method and ultra-fast software Findr for higly accurate causal inference between gene expression traits using cis-regulatory DNA variations as causal anchors, which improves current methods by taking into consideration hidden confounders and weak regulations. Findr outperformed existing methods on the DREAM5 Systems Genetics challenge and on the prediction of microRNA and transcription factor targets in human lymphoblastoid cells, while being nearly a million times faster. Findr is publicly available at https://github.com/lingfeiwang/findr.       -\r\n  Data Availability Statement: The following data were downloaded from public repositories; data processing normalization steps, if any, are detailed in the Methods section. DREAM5 Systems Genetics challenges. Available from: https://www.synapse.org/#!Synapse:syn2820440/wiki/.  Geuvadis genotype data. Available from: ftp://ftp. ebi.ac.uk/pub/databases/microarray/data/ experiment/GEUV/E-GEUV-1/genotypes/. Geuvadis gene expression data. Available from: ftp://ftp.ebi. ac.uk/pub/databases/microarray/data/experiment/ GEUV/E-GEUV-1/analysis_results/GD462.  GeneQuantRPKM.50FN.samplename.resk10.txt.gz. Understanding how genetic variation between individuals determines variation in observable traits or disease risk is one of the core aims of genetics. It is known that genetic variation often affects gene regulatory DNA elements and directly causes variation in expression of nearby genes. This effect in turn cascades down to other genes via the complex pathways and gene interaction networks that ultimately govern how cells operate in an ever changing environment. In theory, when genetic variation and gene expression levels are measured simultaneously in a large number of individuals, the causal effects of genes on each other can be inferred using statistical models similar to those used in randomized controlled trials. We developed a novel method and ultra-fast software Findr which, unlike existing methods, takes into account the complex but unknown network context when predicting causality between specific gene pairs. Findr's predictions have a significantly higher overlap with known gene networks compared to existing methods, using both simulated and real data. Findr is also nearly a million times faster, and hence the only software in its class that can handle modern datasets where the expression levels of ten-thousands of genes are simultaneously measured in hundreds to thousands of individuals. Geuvadis miRNA expression data. Available from: ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/ experiment/GEUV/E-GEUV-2/analysis_results/ GD452.MirnaQuantCount.1.2N.50FN.samplename. resk10.txt. Geuvadis best eQTL data for mRNA.  Available from: ftp://ftp.ebi.ac.uk/pub/databases/ microarray/data/experiment/GEUV/E-GEUV-1/ analysis_results/EUR373.gene.cis.FDR5.best. rs137.txt.gz. Geuvadis best eQTL data for miRNA.  Available from: ftp://ftp.ebi.ac.uk/pub/databases/ microarray/data/experiment/GEUV/E-GEUV-2/ analysis_results/EUR363.mi.cis.FDR5.best.rs137. txt.gz. Groundtruth microRNA target genes.  Available from: https://downloads.sourceforge.net/project/mirlab/groundtruth_Strong.csv. siRNA silencing and DNA binding data of transcription factors in lymphoblastoid cell lines. Available from: https://doi.org/10.1371/journal.pgen.1004226.s011. ENCODE filtered proximal transcription factor - target gene network. Available from: http://encodenets.gersteinlab.org/enets2.Proximal_ filtered.txt.  Funding: This research was supported by grants from the Biotechnology and Biological Sciences Research Council (BBSRC, http://www.bbsrc.ac.uk, grant numbers: BB/J004235/1, BB/M020053/1).  The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.  Competing interests: The authors have declared that no competing interests exist.    Introduction\r\n  Genetic variation in non-coding genomic regions, including at loci associated with complex traits and diseases identified by genome-wide association studies (GWAS), predominantly plays a gene-regulatory role [ 1 ]. Whole genome and transcriptome analysis of natural populations has therefore become a common practice to understand how genetic variation leads to variation in phenotypes [ 2 ]. The number and size of studies mapping genome and transcriptome variation has surged in recent years due to the advent of high-throughput sequencing technologies, and ever more expansive catalogues of expression-associated DNA variants, termed expression quantitative trait loci (eQTLs), are being mapped in humans, model organisms, crops and other species [1, 3±5]. Unravelling the causal hierarchies between DNA variants and their associated genes and phenotypes is now the key challenge to enable the discovery of novel molecular mechanisms, disease biomarkers or candidate drug targets from this type of data [ 6, 7 ].  It is believed that genetic variation can be used to infer the causal directions of regulation between coexpressed genes, based on the principle that genetic variation causes variation in nearby gene expression and acts as a causal anchor for identifying downstream genes [ 8, 9 ]. Although numerous statistical models have been proposed for causal inference with genotype and gene expression data from matching samples [10±15], no software implementation in the public domain is efficient enough to handle the volume of contemporary datasets, hindering any attempts to evaluate their performances. Moreover, existing statistical models rely on a conditional independence test which assumes that no hidden confounding factors affect the coexpression of causally related gene pairs. However gene regulatory networks are known to exhibit redundancy [ 16 ] and are organized into higher order network motifs [ 17 ], suggesting that confounding of causal relations by known or unknown common upstream regulators is the rule rather than the exception. Moreover, it is also known that the conditional independence test is susceptible to variations in relative measurement errors between genes [ 8, 9, 18 ], an inherent feature of both microarray and RNA-seq based expression data [ 19 ].  To investigate and address these issues, we developed Findr (Fast Inference of Networks from Directed Regulations), an ultra-fast software package that incorporates existing and novel statistical causal inference tests. The novel tests were designed to take into account the presence of unknown confounding effects, and were evaluated systematically against multiple existing methods using both simulated and real data.    Results\r\n   Findr incorporates existing and novel causal inference tests\r\n  Findr performs six likelihood ratio tests involving pairs of genes (or exons or transcripts) A, B, and an eQTL E of A (Fig 1, Materials and methods). Findr then calculates Bayesian posterior probabilities of the hypothesis of interest being true based on the observed likelihood ratio test statistics (denoted Pi, i = 0 to 5, 0 Pi 1, Materials and methods). For this purpose, Findr utilizes newly derived analytical formulae for the null distributions of the likelihood ratios of the implemented tests (Materials and methods, S1 Fig). This, together with efficient programming, resulted in a dramatic speedup compared to the standard computationally expensive approach of generating random permutations. The six posterior probabilities are then combined into the traditional causal inference test, our new causal inference test, and separately a correlation test that does not incorporate genotype information (Materials and methods).  Each of these tests verifies whether the data arose from a specific subset of (E, A, B) relations (Fig 1) among the full hypothesis space of all their possible interactions, and results in a 2 / 26 Fig 1. Six likelihood ratio tests are performed to test the regulation A ! B, numbered, named, and defined as shown. E is the best eQTL of A. Arrows in a hypothesis indicate directed regulatory relations. Genes A and B each follow a normal distribution, whose mean depends additively on its regulator(s), as determined in the corresponding hypothesis. The dependency is categorical on discrete regulators (genotypes) and linear on continuous regulators (gene expression levels). The undirected line represents a multi-variate normal distribution between the relevant variables. In order to identify A ! B regulation, we select either the null or the alternative hypothesis depending on the test, as shown. 3 / 26 probability of a causal interaction A ! B being true, which can be used to rank predictions according to significance or to reconstruct directed networks of gene regulations by keeping all interactions exceeding a probability threshold.    The traditional causal inference test fails in the presence of hidden confounders and weak regulations\r\n  Findr's computational speed allowed us to systematically evaluate traditional causal inference methods for the first time. We obtained five datasets with 999 samples simulated from synthetic gene regulatory networks of 1,000 genes with known genetic architecture from the DREAM5 Systems Genetics Challenge, and subsampled each dataset to observe how performance depends on sample size (Materials and methods). The correlation test (P0) does not incorporate genotype information and was used as a benchmark for performance evaluations in terms of areas under the receiver operating characteristic (AUROC) and precision-recall (AUPR) curves (Materials and methods). The traditional method [ 11 ] combines the secondary (P2) and independence (P3) tests sequentially (Fig 1, Materials and methods), and was evaluated by comparing P2 and P2 P3 separately against the correlation test. Both the secondary test alone and the traditional causal inference test combination were found to underperform the correlation test (Fig 2A and 2B). Moreover, the inclusion of the conditional independence test worsened inference accuracy, more so with increasing sample size (Fig 2A and 2B) and increasing number of regulations per gene (S1 Text, S2 Fig). Similar performance drops were also observed for the Causal Inference Test (CIT) [ 13, 15 ] software, which also is based on the conditional independence test (S3 Fig).  We believe that the failure of the traditional causal inference test is due to an elevated false negative rate (FNR) coming from two sources. First, the secondary test is less powerful in identifying weak interactions than the correlation test. In a true regulation E ! A ! B, the secondary linkage (E ! B) is the result of two direct linkages chained together, and is harder to detect than either of them. The secondary test hence picks up fewer true regulations, and consequently has a higher FNR. Second, the conditional independence test is counter-productive in the presence of hidden confounders (i.e. common upstream regulators). In such cases, even if E ! A ! B is genuine, the conditional independence test will find E and B to be still correlated after conditioning on A due to a collider effect (S4 Fig) [ 20 ]. Hence the conditional independence test only reports positive on E ! A ! B relations without any confounder, further raising the FNR. This is supported by the observation of worsening performance with increasing sample size (where confounding effects become more distinguishable) and increasing number of regulations per gene (which leads to more confounding).  To further support this claim, we examined the inference precision among the top predictions from the traditional test, separately for gene pairs directly unconfounded or confounded by at least one gene (Materials and methods). Compared to unconfounded gene pairs, confounded ones resulted in significantly more false positives among the top predictions (Fig 2C). Furthermore, the vast majority of real interactions fell outside the top 1% of predictions (i.e. had small posterior probability) [92% (651/706) for confounded and 86% (609/709) for unconfounded interactions, Fig 2C]. Together, these results again showed the failure of the traditional test on confounded interactions and its high false negative rate overall.    Findr accounts for weak secondary linkage, allows for hidden confounders, and outperforms existing methods on simulated data\r\n  To overcome the limitations of traditional causal inference methods, Findr incorporates two additional tests (Fig 1 and Materials and methods). The relevance test (P4) verifies that B is not 4 / 26 Fig 2. Findr achieves best prediction accuracy on the DREAM5 systems genetics challenge. (A, B) The mean AUROC (A) and AUPR (B) on subsampled data are shown for traditional (P2, P2 P3) and newly proposed (P4, P2 P5, P) causal inference tests against the baseline correlation test (P0). Every marker corresponds to the average AUROC or AUPR at specific sample sizes. Random subsampling at every sample size was performed 100 times. Half widths of the lines and shades are the standard errors and standard deviations respectively. Pi corresponds to test i numbered in Fig 1; P is the new composite test (Materials and methods). This figure is for dataset 4 of the DREAM challenge. For results on other datasets of the same challenge, see S2 Fig. (C, D) Local precision of top predictions (bars top to bottom: 0% to 0.01%, 0.01% to 0.02%, 0.02% to 0.05%, 0.05% to 0.1%, 0.1% to 0.2%, 0.2% to 0.5%, 0.5% to 1%, 1% to 10%, and 10% to 100% top predictions) for the traditional (C) and novel (D) tests for dataset 4 of the DREAM challenge. Gene pairs unconfounded (left, blue) and confounded by a third gene (right, red) are visualized separately. Each full brick corresponds to 10% in precision. Numbers next to each bar (x/y) indicate the number of true regulations (x) and the total number of gene pairs (y) within the respective range of prediction scores. For results on other datasets, see S5E and S5F Fig. (E, F) The average AUROC (E) and AUPR (F) over 5 DREAM datasets with respectively 100, 300 and 999 samples are shown for Findr's new (Findr-P), traditional (Findr-PT), and correlation (Findr-P0) tests, for CIT and for the best scores on the DREAM challenge leaderboad. For individual results on all 15 datasets, see S1 Table. 5 / 26 independent from A and E simultaneously and is more sensitive for picking up weak secondary linkages than the secondary linkage test. The controlled test (P5) ensures that the correlation between A and B cannot be fully explained by E, i.e. excludes pleiotropy. The same subsampling analysis revealed that P4 performed best in terms of AUROC, and AUPR with small sample sizes, whilst the combination P2 P5 achieved highest AUPR for larger sample sizes (Fig 2A and 2B). Most importantly, both tests consistently outperformed the correlation test (P0), particularly for AUPR. This demonstrates conclusively in a comparative setting that the inclusion of genotype data indeed can improve regulatory network inference. These observations are consistent across all five DREAM datasets (S2 Fig).  We combined the advantages of P4 and P2 P5 by averaging them in a composite test (P) (Materials and methods), which outperformed P4 and P2 P5 at all sample sizes (Fig 2 and S2 Fig) and hence was appointed as Findr's new test for causal inference. Findr's new test (P) obtained consistently higher levels of local precision (i.e. one minus local FDR) on confounded and unconfounded gene pairs compared to Findr's traditional causal inference test (PT) (Fig 2C and 2D, S5 Fig), and outperformed the traditional causal inference test (PT), correlation test (P0), CIT, and every participating method of the DREAM5 Systems Genetics Challenge (Materials and methods) in terms of AUROC and AUPR on all 15 datasets (Fig 2E and 2F, S1 Table, S6 Fig).  Specifically, Findr's new test was able to address the inflated FNR of the traditional method due to confounded interactions. It performed almost equally well on confounded and unconfounded gene pairs and, compared to the traditional test, significantly fewer real interactions fell outside the top 1% of predictions (55% vs. 92% for confounded and 45% vs. 86% for unconfounded interactions, Fig 2D, S5 Fig).    The conditional independence test incurs false negatives for unconfounded regulations due to measurement error\r\n  The traditional causal inference method based on the conditional indepedence test results in false negatives for confounded interactions, whose effect was shown significant for the simulated DREAM datasets. However, the traditional test surprisingly reported more confounded gene pairs than the new test in its top predictions (albeit with lower precision), and correspondingly fewer unconfounded gene pairs (Fig 2C and 2D, S5 Fig).  We hypothesized that this inconsistency originated from yet another source of false negatives, where measurement error can confuse the conditional independence test. Measurement error in an upstream variable (called A in Fig 1) does not affect the expression levels of its downstream targets, and hence a more realistic model for gene regulation is E ! A(t) ! B with A(t) ! A, where the measured quantities are E, A, and B, but the true value for A, noted A(t), remains unknown. When the measurement error (in A(t) ! A) is significant, conditioning on A instead of A(t) cannot remove all the correlation between E and B and would therefore report false negatives for unconfounded interactions as well. This effect has been previously studied, for example in epidemiology as the ªspurious appearance of odds-ratio heterogeneityº [ 21 ].  We verified our hypothesis with a simple simulation (Materials and methods). In a typical scenario with 300 samples from a monoallelic species, minor allele frequency 0.1, and a third of the total variance of B coming from A(t), the conditional independence test reported false negatives (likeilihood ratio p-value 1, i.e. rejecting the null hypothesis of conditional indepencence, cf. Fig 1) as long as measurement error contributed more than half of A's total unexplained variance (Fig 3B). False negatives occurred at even weaker measurement errors, when the sample sizes were larger or when stronger A ! B regulations were assumed (S7 Fig). 6 / 26 Fig 3. The conditional independence test yields false negatives for unconfounded regulations in the presence of even minor measurement errors. (A, B, C, D) Null hypothesis p-values of the secondary linkage (A), conditional independence (B), relevance (C), and controlled (D) tests are shown on simulated data from the ground truth model E ! A(t) ! B with A(t) ! A. A(t)'s variance coming from E is set to one, x axis (s2A1) is A(t)'s variance from other sources and y axis (s2A2) is the variance due to measurement noise. A total of 100 values from 10−2 (left, bottom) to 102 (right, top) were taken for s2A1 and s2A2 each to form the 100 × 100 tiles. Tiles that did not produce a significant eQTL relation E ! A with p-value 10−6 were discarded. Contour lines are for the log-average of smoothened tile values. Note that for the conditional independence test (B), the true model corresponds to the null hypothesis, i.e. small (purple) p-values correspond to false negatives, whereas for the other tests the true model corresponds to the alternative hypothesis, i.e. small (purple) p-values correspond to true positives (cf. Fig 1). For details of the simulation and results from other parameter settings, see Materials and methods and S7 Fig respectively. (E) Color bar.  This observation goes beyond the well-known problems that arise from a large measurement error in all variables, which acts like a hidden confounder [ 9 ], or from a much larger measurement error in A than B, which can result in B becoming a better measurement of A(t) than A itself [ 8 ]. In this simulation, the false negatives persisted even if E ! A was observationally much stronger than E ! B, such as when A's measurement error was only 10% (s2A1 \u0088 0:1) compared to up to 67% for B (Fig 3B). This suggested a unique and mostly neglected source of false negatives that would not affect other tests. Indeed, the secondary, relevance, and controlled tests were much less sensitive to such measurement errors (Fig 3A, 3C, and 3D). 7 / 26    Findr outperforms the traditional causal inference test and machine learning methods on microRNA target prediction\r\n  In order to evaluate Findr on a real dataset, we performed causal inference on miRNA and mRNA sequencing data in lymphoblastoid cell lines from 360 European individuals in the Geuvadis study [ 3 ] (Materials and methods). We first tested 55 miRNAs with reported significant cis-eQTLs against 23,722 genes. Since miRNA target predictions from sequence complimentarity alone result in high numbers of false positives, prediction methods based on correlating miRNA and gene expression profiles are of great interest [ 22 ]. Although miRNA target prediction using causal inference from genotype and gene expression data has been considered [ 23 ], it remains unknown whether the inclusion of genotype data improves existing expression-based methods. To compare Findr against the state-of-the-art expression-based miRNA target prediction, we used miRLAB, an integrated database of experimentally confirmed human miRNA target genes with a uniform interface to predict targets using twelve methods, including linear and non-linear, pairwise correlation and multivariate regression methods [ 24 ]. We were able to infer miRNA targets with 11/12 miRLAB methods, and also applied the GENIE3 random forest regression method [ 25 ], CIT, and the three tests in Findr: the new (P) and traditional (PT) causal inference tests and the correlation test (P0) (S1 Text). Findr's new test achieved the highest AUROC and AUPR among the 16 methods attempted. In particular, Findr's new test significantly outperformed the traditional test and CIT, the two other genotype-assisted methods, while also being over 500,000 times faster than CIT (Fig 4, S2 Table, S8 Fig). Findr's correlation test outperformed all other methods not using genotype information, including correlation, regression, and random forest methods, and was 500 to 100,000 times faster (Fig 4, S2 Table, S8 Fig). This further illustrates the power of the Bayesian gene-specific background estimation method implemented in all Findr's tests (Materials and methods).    Findr predicts transcription factor targets with more accurate FDR estimates\r\n  We considered 3,172 genes with significant cis-eQTLs in the Geuvadis data [ 3 ] (Materials and methods) and inferred regulatory interactions to the 23,722 target genes using Findr's traditional (PT), new (P) and correlation (P0) tests, and CIT. Groundtruths of experimentally confirmed causal gene interactions in human, and mammalian systems more generally, are of limited availability and mainly concern transcription or transcription-associated DNA-binding factors (TFs). Here we focused on a set of 25 TFs in the set of eQTL-genes for which either differential expression data following siRNA silencing (6 TFs) or TF-binding data inferred from ChIP-sequencing and/or DNase footprinting (20 TFs) in a lymphoblastoid cell line (GM12878) was available [ 26 ] (Materials and methods). AUPRs and AUROCs did not exhibit substantial differences, other than modest improvement over random predictions (S9 Fig, S3 Table). To test for enrichment of true positives among the top-ranked predictions, which would be missed by global evaluation measures such as AUPR or AUROC, we took advantage of the fact that Findr's probabilities are empirical local precision estimates for each test (Materials and methods), and assessed how estimated local precisions of new, traditional, and correlation tests reflected the actual precision. Findr's new test correctly reflected the precision values at various threshold levels, and was able to identify true regulations at high precision control levels (Fig 5). However, the traditional test significantly underestimated precision due to its elevated FNR. This lead to a lack of predictions at high precision thresholds but enrichment of true regulations at low thresholds, essentially nullifying the statistical meaning of its output probability PT. On the other hand, the correlation test significantly overestimated 8 / 26 Fig 4. Findr achieves highest accuracy and speed on the prediction of miRNA target genes from the Geuvadis data. Shown are the AUROC (A), AUPR (B) and runtime (C) for 16 miRNA target prediction methods. Methods are colored by type: blue, genotype-assisted causal inference methods; red, pairwise correlation methods; yellow, multivariate regression methods; purple, other methods. Dashed lines are the AUROC and AUPR from random predictions. For method details, see S1 Text. 9 / 26 Fig 5. Findr predicts TF targets with more accurate FDR estimates from the Geuvadis data. The precision (i.e. 1-FDR) of TF target predictions is shown at probability cutoffs 0.1 to 0.9 (blue to yellow) with respect to known functional targets from siRNA silencing of 6 TFs (A) and known TF-binding targets of 20 TFs (B). The number above each bar indicates the number of predictions at the corresponding threshold. Dashed lines are precisions from random predictions. precisions because it is unable to distinguish causal, reversed causal or confounded interactions, which raises its FDR. The same results were observed when alternative groundtruth ChIP-sequencing networks were considered (S9 and S10 Figs).     Materials and methods\r\n   Datasets\r\n  We used the following datasets/databases for evaluating causal inference methods: 1. Simulated genotype and transcriptome data of synthetic gene regulatory networks from the  DREAM5 Systems Genetics challenge A (DREAM for short), generated by the SysGenSIM 10 / 26 software [ 27 ]. DREAM provides 15 sub-datasets, obtained by simulating 100, 300, and 999 samples of 5 different networks each, containing 1000 genes in every sub-dataset but more regulations for sub-datasets with higher numbering. In every sub-dataset, each gene has exactly one matching genotype variable. 25% of the genotype variables are cis-expression Quantitative Trait Loci (eQTL), defined in DREAM as: their variation changes the expression level of the corresponding gene directly. The other 75% are trans-eQTLs, defined as: their variation affects the expression levels of only the downstream targets of the corresponding gene, but not the gene itself. Because the identities of cis-eQTLs are unknown, we calculated the P-values of genotype-gene expression associations with kruX [ 28 ], and kept all genes with a P-value less than 1/750 to filter out genes without cis-eQTL. For the subsampling analysis, we restricted the evaluation to the prediction of target genes from these cis-genes only, in line with the assumption that Findr and other causal inference methods require as input a list of genes whose expression is significantly associated with at least one cis-eQTL. For the full comparison of Findr to the DREAM leaderboard results, we predicted target genes for all genes, regardless of whether they had a cis-eQTL. 2. Genotype and transcriptome sequencing data on 465 human lymphoblastoid cell line samples from the Geuvadis project [ 3 ] consisting of the following data products: · Genotype data (ArrayExpress accession E-GEUV-1). · Gene quantification data for 23722 genes from nonredundant unique samples and after quality control and normalization (ArrayExpress accession E-GEUV-1). · Quantification data of miRNA, with the same standard as gene quantification data (ArrayExpress accession E-GEUV-2). · Best eQTLs of mRNAs and miRNAs (ArrayExpress accessions E-GEUV-1 and E-GEUV-2).  We restricted our analysis to 360 European samples which are shared by gene and miRNA quantifications. Excluding invalid eQTLs from the Geuvadis analysis, such as single-valued genotypes, 55 miRNA-eQTL pairs and 3172 gene-eQTL pairs were retained. 3. For validation of predicted miRNA-gene interactions, we extracted the ªstrongº groundtruth table from miRLAB [ 24 ], which contains experimentally confirmed miRNA-gene regulations from the following databases: TarBase [ 29 ], miRecords [ 30 ], miRWalk [ 31 ], and miRTarBase [ 32 ]. The intersection of the Geuvadis and ground-truth table contains 20 miRNAs and 1054 genes with 1217 confirmed regulations, which are considered for prediction validation. Interactions that are present in the ground-truth table are regarded as true while others as false. 4. For verification of predicted gene-gene interactions, we obtained differential expression data following siRNA silencing of 59 transcription-associated factors (TFs) and DNAbinding data of 201 TFs for 8872 genes in a reference lymphoblastoid cell line (GM12878) from [ 26 ]. Six siRNA-targeted TFs, 20 DNA-binding TFs, and 6,790 target genes without missing differential expression data intersected with the set of 3172 eQTL-genes and 23722 target genes in Geuvadis and were considered for validation. We reproduced the pipeline of [ 26 ] with the criteria for true targets as having a False Discovery Rate (FDR) &lt; 0.05 from R package qvalue for differential expression in siRNA silencing, or having at least 2 TFbinding peaks within 10kb of their transcription start site. We also obtained the filtered proximal TF-target network from [ 33 ], which had 14 TFs and 7,000 target genes in common with the Geuvadis data. 11 / 26    General inference algorithm\r\n  Consider a set of observations sampled from a mixture distribution of a null and an alternative hypothesis. For instance in gene regulation, every observation can correspond to expression levels of a pair of genes wich are sampled from a bivariate normal distribution with zero (null hypothesis) or non-zero (alternative hypothesis) correlation coefficient. In Findr, we predict the probability that any sample follows the alternative hypothesis with the following algorithm (based on and modified from [ 11 ]): 1. For robustness against outliers, we convert every continuous variable into standard normally distributed N(0, 1) values using a rank-based inverse normal transformation across all samples. We name this step as supernormalization. 2. We propose a null and an alternative hypothesis for every likelihood ratio test (LRT) of interest where, by definition, the null hypothesis space is a subset of the alternative hypothesis. Model parameters are replaced with their maximum likelihood estimators (MLEs) to obtain the log likelihood ratio (LLR) between the alternative and null hypotheses. 3. We derive the analytical expression for the probablity density function (PDF) of the LLR when samples follow the null hypothesis. 4. We convert LLRs into posterior probabilities of the hypothesis of interest with the empirical estimation of local FDR.  Implementational details can be found in Findr's source code.    Likelihood ratio tests\r\n  Consider correlated genes A, B, and a third variable E upstream of A and B, such as a significant eQTL of A. The eQTLs can be obtained either de novo using eQTL identification tools such as matrix-eQTL [ 34 ] or kruX [ 28 ], or from published analyses. Throughout this article, we assume that E is a significant eQTL of A, whereas extension to other data types is straightforward. We use Ai and Bi for the expression levels of gene A and B respectively, which are assumed to have gone through supernormalization, and optionally the genotypes of the best eQTL of A as Ei, where i = 1, . . ., n across samples. Genotypes are assumed to have a total of na alleles, so Ei 2 {0, . . ., na}. We define the null and alternative hypotheses for a total of six tests, as shown in Fig 1. LLRs of every test are calculated separately as follows: 0. Correlation test: Define the null hypothesis as A and B are independent, and the alternative hypothesis as they are correlated: \u00851\u0086 \u00852\u0086 for i = 1, . . ., n. The null hypothesis corresponds to ρ = 0.  Maximum likelihood estimators (MLE) for the model parameters ρ, σA0, and σB0 are and the LLR is simply In the absence of genotype information, we use nonzero correlation between A and B as the indicator for A ! B regulation, giving the posterior probability LLR\u00850\u0086 \u0088 n2 ln \u00851  r^2\u0086: P\u0085A - B\u0086 \u0088 P\u0085H\u0085a0lt\u0086 j LLR\u00850\u0086\u0086: \u00853\u0086 \u00854\u0086 \u00855\u0086 2. Secondary (linkage) test: The secondary test is identical with the primary test, except it verifies that E regulates B. Hence repeat the primary test on E and B and obtain the MLEs: and the LLR as H\u0085a2lt\u0086 is chosen to verify that E regulates B.  ^nj \u0088 1 Xn 1. Primary (linkage) test: Verify that E regulates A from H\u0085a1lt\u0086 where nj is the sample count by genotype category,  Q nj  X n i\u00881  dEij: The Kronecker delta function is defined as δxy = 1 for x = y, and 0 otherwise. When summing over all genotype values (j = 0, . . ., na), we only pick those that exist (nj &gt; 0) throughout this article. Since the null hypothesis is simply that Ai is sampled from a genotype-independent normal distribution, with MLEs of mean zero and standard deviation one due to supernormalization, the LLR for test 1 becomes By favoring a large LLR(1), we select H\u0085a1lt\u0086 and verify that E regulates A, with 3. (Conditional) independence test: Verify that E and B are independent when conditioning on A. This can be achieved by comparing H\u0085a3lt\u0086 B E ! A ^ \u0085A correlates with B\u0086 against H\u0085n3u\u0086ll E ! A ! B. LLRs close to zero then prefer H\u0085n3u\u0086ll, and ensure that E regulates B only through A:  P\u0085E ? B j A\u0086 \u0088 P\u0085H\u0085n3u\u0086ll j LLR\u00853\u0086\u0086: s2  A rsAsB rsAsB s2  B  A: For H\u0085n3u\u0086ll, the distributions follow Eq 4, as well as Substituting parameters μj, νj, σA, σB, ρ of H\u0085a3lt\u0086 and μj, ρ, σA, σB of H\u0085n3u\u0086ll with their MLEs, we obtain the LLR: LLR\u00853\u0086  \u0088 where and r^ is defined in Eq 2.  Bi j Ai  N\u0085rAi; s2B\u0086: n ln s^2 s^2 2 A B \u0087 n2 ln s^2A \u0087 n2 ln \u00851 \u0085r^ \u0087 sAB  1\u00862 r^2\u0086; sAB 1  Xna nj j\u00880 n m^j^nj; \u00856\u0086 4. Relevance test: Since the indirect regulation E ! B tends to be weaker than any of its direct regulation components (E ! A or A ! B), we propose to test E ! A ! B with indirect regulation E ! B as well as the direct regulation A ! B for stronger distinguishing power on weak regulations. We define H\u0085a4lt\u0086 E ! A ^ E ! B A and H\u0085n4u\u0086ll E ! A B. This simply verifies that B is not independent from both A and E simultaneously. In the alternative hypothesis, B is regulated by E and A, which is modeled as a normal distribution whose mean is additively determined by E categorically and A linearly, i.e.  Bi j Ei; Ai  N\u0085nEi \u0087 rAi; s2B\u0086: We can hence solve its LLR as LLR\u00854\u0086 \u0088 n ln s^2 s^2 2 A B \u0085r^ \u0087 sAB 1\u00862 \u0087 n 2 5. Controlled test: Based on the positives of the secondary test, we can further distinguish the alternative hypothesis H\u0085a5lt\u0086 B E ! A ^ A ! B from the null H\u0085n5u\u0086ll B E ! A 14 / 26 to verify that E does not regulate A and B independently. Its LLR can be solved as LLR\u00855\u0086 \u0088 n2 ln s^2As^2B \u0085r^ \u0087 sAB 1\u00862 \u0087 n2 ln s^2As^2B: Null distributions for the log-likelihood ratios The null distribution of LLR, p\u0085LLR j Hnull\u0086, may be obtained either by simulation or analytically. Simulation, such as random permutations from real data or the generation of random data from statistics of real data, can deal with a much broader range of scenarios in which analytical expressions are unattainable. However, the drawbacks are obvious: simulation can take hundreds of times longer than analytical methods to reach a satisfiable precision. Here we obtained analytical expressions of p\u0085LLR j Hnull\u0086 for all the tests introduced above. 0. Correlation test: H\u0085n0u\u0086ll \u0088 A we can start from  B indicates no correlation between A and B. Therefore,  B~i  i:i:d N\u00850; 1\u0086: In order to simulate the supernormalization step, we normalize B~i into Bi with zero mean and unit variance as:  Bi  B~i sB~  B~i ; 1 Xn n i\u00881  B~i;  2 s~  B 1 Xn n i\u00881 \u0085B~i  B~ \u00862:  Transform the random variables fB~ig by defining \u00857\u0086 \u00858\u0086 \u00859\u0086 \u008510\u0086 \u008511\u0086 \u008512\u0086 \u008513\u0086 \u008514\u0086 12 ln \u00851 Y\u0086 follows the Beta distribution. We define distribution D\u0085k1; k2\u0086 as the distribution of a random variable Z \u0088 for Y * Beta(k1/2, k2/2), i.e.  Z \u0088 and for z 0, p(zjk1, k2) = 0. Here B(a, b) is the Beta function. Therefore the null distribution for the correlation test is simply LLR\u00850\u0086=n 1. Primary test: H\u0085n1u\u0086ll \u0088 E A indicates no regulation from E to A. Therefore, similarly with the correlation test, we start from A~i i:i:d N \u00850; 1\u0086 and normalize them to Ai with zero mean and unit variance.  The expression of LLR(1) then becomes: \u008515\u0086 \u008516\u0086 \u008517\u0086 \u008518\u0086 where  ! Xj2 : for j \u0088 0; . . . ; na; Then we can similarly verify that {Xi} are pairwise independent, and  Xi Xna\u00871  na N\u00850; 1\u0086; for i \u0088 0; . . . ; na; w2\u0085n 1\u0086:  ! w2\u0085n na  1\u0086: Y12 w2\u0085na\u0086; 16 / 26 Some calculation would reveal i.e. To account for genotypes that do not show up in the samples, define nv the number of different genotype values across all samples. Then 2. Secondary test: Since the null hypotheses and LLRs of primary and secondary tests are identical, LLR(2) follows the same null distribution as Eq 19. 3. Independence test: The independence test verifies if E and B are uncorrelated when conditioning on A, with H\u0085n3u\u0086ll \u0088 E ! A ! B. For this purpose, we keep E and A intact while randomizing B~i according to B's correlation with A:  B~i r^Ai \u0087 p\u00811\u0081\u0081\u0081\u0081\u0081\u0081\u0081\u0081r^\u0081\u0081\u00812\u0081 Xi; Xi i:i:d N\u00850; 1\u0086: Then B~i is normalized to Bi according to Eq 8. The null distribution of LLR(3) can be obtained with similar but more complex computations from Eq 6, as LLR\u00853\u0086=n D\u0085nv 1; n nv 1\u0086: 4. Relevance test: The null distribution of LLR(4) can be obtained similarly by randomizing Bi according to Eqs 7 and 8, as LLR\u00854\u0086=n D\u0085nv; n nv 1\u0086: 5. Controlled test: To compute the null distribution for the controlled test, we start from B~i \u0088 ^nEi \u0087 s^BXi;  Xi \u008519\u0086 \u008520\u0086 \u008521\u0086 and then normalize B~i into Bi according to Eq 8. Some calculation reveals the null distribution as LLR\u00855\u0086=n  We verified our analytical method of deriving null distributions by comparing the analytical null distribution v.s. null distribution from permutation for the relevance test.    Bayesian inference of posterior probabilities\r\n  After obtaining the PDFs for the LLRs from real data and the null hypotheses, we can convert LLR values into posterior probabilities P\u0085Halt j LLR\u0086. We use a similar technique as in [ 11 ], which itself was based on a more general framework to estimate local FDRs in genome-wide 17 / 26 studies [ 35 ]. This framework assumes that the real distribution of a certain test statistic forms a mixture distribution of null and alternative hypotheses. After estimating the null distribution, either analytically or by simulation, it can be compared against the real distribution to determine the proportion of null hypotheses, and consequently the posterior probability that the alternative hypothesis is true at any value of the statistic.  To be precise, consider an arbitrary likelihood ratio test. The fundamental assumption is that in the limit LLR ! 0+, all test cases come from the null hypothesis (Hnull), whilst as LLR increases, the proportion of alternative hypotheses (Halt) also grows. The mixture distribution of real LLR values is assumed to have a PDF as  p\u0085LLR\u0086 \u0088 P\u0085Hnull\u0086p\u0085LLR j Hnull\u0086 \u0087 P\u0085Halt\u0086p\u0085LLR j Halt\u0086: The priors P\u0085Hnull\u0086 and P\u0085Halt\u0086 sum to unity and correspond to the proportions of null and alternative hypotheses in the mixture distribution. For any test i = 0, . . ., 5, Bayes' theorem then yields its posterior probability as \u008522\u0086 \u008523\u0086 P\u0085H\u0085ail\u0086t j LLR\u0085i\u0086\u0086 \u0088 p\u0085LLR\u0085i\u0086 j H\u0085ail\u0086t\u0086 P\u0085H\u0085ail\u0086t\u0086:  p\u0085LLR\u0085i\u0086\u0086 Pi P\u0085H\u0085ail\u0086t j LLR\u0085i\u0086\u0086;  i \u0088 0; 1; 2; 4; 5; P\u0085H\u0085niu\u0086ll j LLR\u0085i\u0086\u0086; i \u0088 3: Based on this, we can define the posterior probabilities of the selected hypotheses according to Fig 1, i.e. the alternative for tests 0, 1, 2, 4, 5 and the null for test 3 as  ( After obtaining the LLR distribution of the null hypothesis [p\u0085LLR j Hnull\u0086], we can determine its proportion [P\u0085Hnull\u0086] by aligning p\u0085LLR j Hnull\u0086 with the real distribution p(LLR) at the LLR ! 0+ side. This provides all the prerequisites to perform Bayesian inference and obtain any Pi from Eq 23.  In practice, PDFs are approximated with histograms. This requires proper choices of histogram bin widths, P\u0085Hnull\u0086, and techniques to ensure the conversion from LLR to posterior probability is monotonically increasing and smooth. Implementational details can be found in Findr package and in S1 Text. Distributions can be estimated either separately for every (E, A) pair or by pooling across all (E, A) pairs. In practice, we test on the order of 103 to 104 candidate targets (ªBº) for every (E, A) such that a separate conversion of LLR values to posterior probabilities is both feasible and recommended, as it accounts for different roles of every gene, especially hub genes, through different rates of alternative hypotheses.  Lastly, in a typical application of Findr, inputs of (E, A) pairs will have been pre-determined as the set of significant eQTL-gene pairs from a genome-wide eQTL associaton analysis. In such cases, we may naturally assume P1 = 1 for all considered pairs, and skip the primary test.    Tests to evaluate\r\n  Based on the six tests in Fig 1, we use the following tests and test combinations for the inference of genetic regulations, and evalute them in the results. · The correlation test is introduced as a benchmark, against which we can compare other methods involving genotype information. Pairwise correlation is a simple measure for the probability of two genes being functionally related either through direct or indirect regulation, or through coregulation by a third factor. Bayesian inference additionally considers different gene roles. Its predicted posterior probability for regulation is P0. 18 / 26 · The traditional causal inference test, as explained in [ 11 ], suggested that the regulatory relation E ! A ! B can be confirmed with the combination of three separate tests: E regulates A, E regulates B, and E only regulates B through A (i.e. E and B become independent when conditioning on A). They correspond to the primary, secondary, and independence tests respectively. The regulatory relation E ! A ! B is regarded positive only when all three tests return positive. The three tests filter the initial hypothesis space of all possible relations between E, A, and B, sequentially to E ! A (primary test), E ! A ^ E ! B (secondary test), and E ! A ! B ^ (no confounder for A and B) (conditional independence test). The resulting test is stronger than E ! A ! B by disallowing confounders for A and B. So its probability can be broken down as  PT  P1P2P3: Trigger [ 36 ] is an R package implementation of the method. However, since Trigger integrates eQTL discovery with causal inference, it is not practical for use on modern datasets. For this reason, we reimplemented this method in Findr, and evaluated it with P2 and P2 P3 separately, in order to assess the individual effects of secondary and independence tests. As discussed above, we expect a set of significant eQTLs and their associated genes as input, and therefore P1 = 1 is assured and not calculated in this paper or the package Findr. Note that PT is the estimated local precision, i.e. the probability that tests 2 and 3 are both true. Correspondinly, its local FDR (the probability that one of them is false) is 1 − PT. · The novel test, aimed specifically at addressing the failures of the traditional causal inference test, combines the tests differently:  P 1 2 \u0085P2P5 \u0087 P4\u0086: \u008524\u0086 \u008525\u0086 Specifically, the first term in Eq 25 accounts for hidden confounders. The controlled test replaces the conditional independence test and constrains the hypothesis space more weakly, only requiring the correlation between A and B is not entirely due to pleiotropy. Therefore, P2 P5 (with P1 = 1) verifies the hypothesis that B E ! A ^ \u0085A ? BjE\u0086, a superset of E ! A ! B.  On the other hand, the relevance test in the second term of Eq 25 addresses weak interactions that are undetectable by the secondary test from existing data (P2 close to 0). This term still grants higher-than-null significance to weak interactions, and verifies that E ! A ^ \u0085E ! B _ A - B\u0086, also a superset of E ! A ! B. In the extreme undetectable limit where P2 = 0 but P4 6\u0088 0, the novel test Eq 25 automatically reduces to P \u0088 12 P4, which assumes equal probability of either direction and assigns half of the relevance test probability to A ! B.  The composite design of the novel test aims not to miss any genuine regulation whilst distinguishing the full spectrum of possible interactions. When the signal level is too weak for tests 2 and 5, we expect P4 to still provide distinguishing power better than random predictions. When the interaction is strong, P2 P5 is then able to pick up true targets regardless of the existence of hidden confounders.    Evaluation methods\r\n  Evaluation metrics. Given the predicted posterior probabilities for every pair (A, B) from any test, or more generically a score from any inference method, we evaluated the predictions against the direct regulations in the ground-truth tables with the metrics of Receiver Operating 19 / 26 Characteristic (ROC) and Precision-Recall (PR) curves, as well as the Areas Under the ROC (AUROC) and Precision-Recall (AUPR) curves [ 37 ]. In particular, AUPR is calculated with the Davis-Goadrich nonlinear interpolation [ 38 ] with R package PRROC.  Subsampling. In order to assess the effect of sample size on the performances of inference methods, we performed subsampling evaluations. This is made practically possible by the DREAM datasets which contain 999 samples with sufficient variance, as well as the computational efficiency from Findr which makes subsampling computationally feasible. With a given dataset and ground-truth table, the total number of samples n, and the number of samples of our actual interest N &lt; n, we performed subsampling by repeating following steps k times: 1. Randomly select N samples out of the total n samples without replacement. 2. Infer regulations only based on the selected samples. 3. Compute and record the evaluation metrics of interest (e.g. AUROC and AUPR) with the inference results and ground-truths.  Evaluation metrics are recorded in every loop, and their means, standard deviations, and standard errors over the k runs, are calculated. The mean indicates how the inference method performs on the metric in average, while the standard deviation reflects how every individual subsampling deviates from the average performance.  Local precision of top predictions separately for confounded and unconfounded gene pairs. In order to demonstrate the inferential precision among top predictions for any inference test (here the traditional and novel tests separately), we first ranked all (ordered) gene pairs (A, B) according to the inferred significance for A ! B. All gene pairs were split into groups according to their relative significance ranking (9 groups in Fig 2C and 2D, as top 0% to 0.01%, 0.01% to 0.02%, etc). Each group was divided into two subgroups, based on whether each gene pair shared at least one direct upstream regulator gene (confounded) or not (unconfounded), according to the gold standard. Within each subgroup, the local precision was computed as the number of true directed regulations divided by the total number of gene pairs in the subgroup.    Simulation studies on causal models with measurement error\r\n  We investigated how each statistical test tolerates measurement errors with simulations in a controlled setting. We modelled the causal relation A ! B in a realistic setup as E ! A(t) ! B with A(t) ! A. E remains as the accurately measured genotype values as the eQTL for the primary target gene A. A(t) is the true expression level of gene A, which is not observable. A is the measured expression level for gene A, containing measurement errors. B is the measured expression level for gene B.  For simplicity, we only considered monoallelic species. Therefore the genotype E in each sample followed the Bernoulli distribution, parameterized by the predetermined minor allele frequency. Each regulatory relation (of E ! A(t), A(t) ! A, and A(t) ! B) correponded to a normal distribution whose mean was linearly dependent on the regulator variable. In particular, for sample i: \u008526\u0086 \u008527\u0086 \u008528\u0086 N\u0085~Ai\u0085t\u0086 ; s2B\u0086; in which σA1, σA2, and σB are parameters of the model. Note that s2B is B's variance from all unknown sources, including expression level variations and measurement errors. The tilde normalizes the variable into zero mean and unit variance, as: ~ Xi  X X p\u0081\u0081i\u0081\u0081\u0081\u0081\u0081\u0081\u0081\u0081\u0081\u0081\u0081\u0081 ; Var\u0085X \u0086 \u008529\u0086 where X and Var(X) are the mean and variance of X {Xi} respectively.  Given the five parameters of the model (the number of samples, the minor allele frequency, σA1, σA2, and σB), we could simulate the observed data for E, A, and B, which were then fed into Findr for tests 2±5 and their p-values of the respective null hypotheses. Supernormalization step was replaced with normalization which merely shifted and scaled variables into zero mean and unit variance.  We then chose different configurations on the number of samples, the minor allele frequency, and σB. For each configuration, we varied σA1 and σA2 in a wide range to obtain a 2-dimensional heatmap plot for the p-value of each test, thereby exploring how each test was affected by measurement errors of different strengths. Only tiles with a significant E ! A eQTL relation were retained. The same initial random seed was employed for different configurations to allow for replicability.    Conclusion\r\n  We developed a highly efficient, scalable software package Findr (Fast Inference of Networks from Directed Regulations) implementing novel and existing causal inference tests. Application of Findr on real and simulated genome and transcriptome variation data showed that our novel tests, which account for weak secondary linkage and hidden confounders at the potential cost of an increased number of false positives, resulted in a significantly improved performance to predict known gene regulatory interactions compared to existing methods, particularly traditional methods based on conditional independence tests, which had highly elevated false negative rates.  Causal inference using eQTLs as causal anchors relies on crucial assumptions which have been discussed in-depth elsewhere [ 8, 9 ]. Firstly, it is assumed that genetic variation is always causal for variation in gene expression, or quantitative traits more generally, and is independent of any observed or hidden confounding factors. Although this assumption is valid for randomly sampled individuals, caution is required when this is not the case (e.g. case-control studies). Secondly, measurement error is assumed to be independent and comparable across variables. Correlated measurement error acts like a confounding variable, whereas a much larger measurement error in the source variable A than the target variable B may lead to an inversion of the inferred causal direction. The conditional independence test in particular relies on the unrealistic assumptions that hidden confounders and measurement errors are absent, the violation of which incurs false negatives and a failure to correctly predict causal relations, as shown throughout this paper.  Although the newly proposed test avoids the elevated FNR from the conditional independence test, it is not without its own limitations. Unlike the conditional independence test, the relevance and controlled tests (Fig 1) are symmetric between the two genes considered. Therefore the direction of causality in the new test arises predominantly from using a different eQTL when testing the reverse interaction, potentially leading to a higher FDR as a minor trade-off. About 10% of cis-regulatory eQTLs are linked (as cis-eQTLs) to the expression of more than one gene [ 39 ]. In these cases, it appears that the shared cis-eQTL regulates the genes independently [ 39 ], which in Findr is accounted for by the `controlled' test (Fig 1). When 21 / 26 causality between genes and phenotypes or among phenotypes is tested, sharing or linkage of (e)QTLs will be more common. Resolving causality in these cases will likely require the use of Findr's conservative, traditional causal inference test in conjunction with the new test, and/or the combination of association signals from multiple (e)QTLs [ 40 ]. Lastly, Findr currently operates on individual-level genotype and (molecular or phenotypic) trait data only, and is thus not directly extendable to emerging Mendelian randomization methods that use summary data from independent eQTL and GWAS studies to attempt to infer causality between genes and phenotypic traits [ 40 ].  In this paper we have addressed the challenge of pairwise causal inference, but to reconstruct the actual pathways and networks that affect a phenotypic trait, two important limitations have to be considered. First, linear pathways propagate causality, and may thus appear as densely connected sets of triangles in pairwise causal networks. Secondly, most genes are regulated by multiple upstream factors, and hence some true edges may only have a small posterior probability unless they are considered in an appropriate multivariate context. The most straightforward way to address these issues would be to model the real directed interaction network as a Bayesian network with sparsity constraints. A major advantage of Findr is that it outputs probability values which can be directly incorporated as prior edge probabilities in existing network inference softwares.  In conclusion, Findr is a highly efficient and accurate open source software tool for causal inference from large-scale genome-transcriptome variation data. Its nonparametric nature ensures robust performances across datasets without parameter tuning, with easily interpretable output in the form of accurate precision and FDR estimates. Findr is able to predict causal interactions in the context of complex regulatory networks where unknown upstream regulators confound traditional conditional independence tests, and more generically in any context with discrete or continuous causal anchors.     Supporting information\r\n  S1 Text. Supplementary text. (PDF) S1 Fig. LLR distributions of the relevance test for hsa-miR-200b-3p on 23722 potential targets of Geuvadis dataset. Real, analytical null, and permuted null distributions are demonstrated in the figure, together with the curve of inferred posterior probability of alternative hypothesis. Permutations were randomly conducted on all potential target genes for 100 times. The alignment between analytical and permuted null distributions and the consistent incremental trend of posterior probability verify our method in deriving analytical null distributions. (PDF) S2 Fig. The mean AUROC and AUPR on subsampled data are shown for causal inference with traditional and new tests, together with the baseline correlation test. Every marker corresponds to the average AUROC or AUPR at specific sample sizes. At every sample size we performed 100 subsampling. Half widths of the lines and shades are the standard errors and standard deviations respectively, of AUROC or AUPR. Figures from top to bottom correspond to datasets 1, 2, 3, 5. For dataset 4, see Fig 2. (PDF) S3 Fig. The AUROC and AUPR of CIT are shown for all 15 datasets of DREAM challenge. Every marker corresponds to the AUROC or AUPR of one dataset. CIT is an R package that includes the conditional independence test, along with tests 2 and 5, while also comparing 22 / 26 E ! A ! B against E ! B ! A. The subsampling analysis on CIT was not feasible due to its low speed. (PDF) S4 Fig. The conditional independence test fails in the presence of hidden confounders. When A and B are both regulated by a hidden confounder C, which is independent of E (left), A becomes a collider and conditioning on A would introduce inter-dependency between E and C, which maintains E ! B regulation (right). (PDF) S5 Fig. Local precision of top predictions for the traditional (left) and novel (right) tests for datasets (top to bottom) 1, 2, 3, and 5 of the DREAM challenge. (PDF) S6 Fig. Estimated and real precision-recall curves for dataset 4 of the DREAM challenge. The real precision was computed according to the groundtruth, whilst the estimated precision was obtained from the estimated FDR from the respective inference method (precision = 1 − FDR). Only genes with cis-eQTLs were considered as primary targets in prediction and validation. Both the novel (A, B) and the traditional (C, D) tests were evaluated. In A, C the original groundtruth table was used to validate predictions, whereas in B, D an extended groundtruth was used that also included indirect regulations at any level based on the original groundtruth. (PDF) S7 Fig. Null hypothesis p-values of the conditional independence test on simulated data from the ground truth model E ! A(t) ! B with A(t) ! A under parameter settings other than Fig 3. (A, B) 100 (A) or 999 (B) samples. (C, D) Minor allele frequency is 0.05 (C) or 0.3 (D). (E, F) Regarding B's variance from A(t) ! B as unit variance, B's variance from other sources including measurement errrors is 0.2 (E) or 20 (F). Unmentioned parameters remain the same as in Fig 3. (PDF) S8 Fig. ROC (top) and PR (bottom) curves of miRNA target predictions were compared for Findr's traditional, new, and correlation tests, GENIE3, CIT, and 11 methods in miRLAB, based on Geuvadis data. The solid black lines correspond to expected performances from random predictions. A higher curve indicates better prediction performance. (PDF) S9 Fig. Three methods of causal inference were evaluated and compared against the baseline correlation test method (P0): Findr's new test (P), traditional causal inference test in Findr (PT), and CIT (C). AUROC and AUPR metrics are measured for three inference tasks. MiRNA compares miRNA target predictions based on Geuvadis miRNA and mRNA expression levels against groundtruths from miRLAB. SiRNA and TF-binding compares gene-gene interaction predictions based on Geuvadis gene expression levels against groundtruths from siRNA silencing and TF-binding measurements respectively. ENCODE compares the same gene-gene interaction predictions against TF-binding networks derived from ENCODE data. Dashed lines indicate expected performances from random predictions. (PDF) S10 Fig. Inference precision at estimated precision cutoffs 0.1 to 0.9 with respect to groundtruth network derived from TF binding of 14 TFs from ENCODE data. The number 23 / 26 above each bar indicates the number of positive predictions at the corresponding threshold. The dashed line is precision from random predictions. (PDF) S1 Table. Predictions from Findr's new (P), traditional (PT), and correlation (P0) tests, and CIT were compared against DREAM challenge leaders on AUROC and AUPR for all 15 DREAM datasets. All cis- and trans-genes are included. DREAM challenge constrained the maximum number of submitted regulations by 100,000, which were also applied in our evaluation. Findr's new test consistently obtained higher AUROC and AUPR than all other methods, including the leaders of DREAM challenge. (PDF) S2 Table. AUROCs and AUPRs of miRNA target predictions were compared for Findr's traditional, new, and correlation tests, GENIE3, CIT, and 11 methods in miRLAB, based on Geuvadis data. Higher AUROC and AUPR values signify stronger predictive power. Program running times have units in seconds (s), minutes (m), hours (h), or days (d). Findr outperformed other methods in statistical power and speed, with or without genotype information. (PDF) S3 Table. AUROCs and AUPRs of gene target predictions were compared for a selected subset of methods in S2 Table, also based on Geuvadis data. The three gold standards could not agree on method. (PDF)  Data curation: Lingfei Wang.  Formal analysis: Lingfei Wang, Tom Michoel.  Funding acquisition: Tom Michoel.  Investigation: Lingfei Wang, Tom Michoel.  Methodology: Lingfei Wang, Tom Michoel.  Project administration: Tom Michoel.  Software: Lingfei Wang.  Supervision: Tom Michoel.  Validation: Lingfei Wang, Tom Michoel.  Writing ± original draft: Lingfei Wang, Tom Michoel.  Writing ± review &amp; editing: Lingfei Wang, Tom Michoel. 24 / 26 25 / 26    ",
    "sourceCodeLink": "https://github.com/lingfeiwang/findr",
    "publicationDate": "0",
    "authors": [
      "Editor: Jennifer Listgarten",
      "Microsoft Research",
      "UNITED STATES",
      "Lingfei Wang",
      "Tom Michoel"
    ],
    "status": "Success",
    "toolName": "findr",
    "homepage": ""
  },
  "58.pdf": {
    "forks": 0,
    "URLs": ["github.com/zhqingit/BPP.Contact:"],
    "contactInfo": ["djguo@cuhk.edu.hk"],
    "subscribers": 1,
    "programmingLanguage": "Python",
    "shortDescription": "",
    "publicationTitle": "BPP: a sequence-based algorithm for branch point prediction",
    "title": "BPP: a sequence-based algorithm for branch point prediction",
    "publicationDOI": "10.1093/bioinformatics/btx401",
    "codeSize": 758,
    "publicationAbstract": "Motivation: Although high-throughput sequencing methods have been proposed to identify splicing branch points in the human genome, these methods can only detect a small fraction of the branch points subject to the sequencing depth, experimental cost and the expression level of the mRNA. An accurate computational model for branch point prediction is therefore an ongoing objective in human genome research. Results: We here propose a novel branch point prediction algorithm that utilizes information on the branch point sequence and the polypyrimidine tract. Using experimentally validated data, we demonstrate that our proposed method outperforms existing methods. Availability and implementation: https://github.com/zhqingit/BPP.Contact: djguo@cuhk.edu.hk Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2016-12-29T05:44:16Z",
    "institutions": [
      "Shenzhen University Health Science Center",
      "The Chinese University of Hong Kong",
      "Zhejiang University",
      "School of Life Sciences and the State Key Laboratory of Agrobiotechnology"
    ],
    "license": "https://github.com/zhqingit/BPP/blob/master/LICENSE",
    "dateCreated": "2016-12-18T23:43:47Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx401   BPP: a sequence-based algorithm for branch point prediction     Qing Zhang  3    Xiaodan Fan  1    Yejun Wang  0    Ming-an Sun  3    Jianlin Shao  2    Dianjing Guo  3    0  Department of Cell Biology and Genetics, Shenzhen University Health Science Center ,  Shenzhen 518060 ,  China    1  Department of Statistics, The Chinese University of Hong Kong ,  Shatin, NT, Hong Kong SAR ,  China    2  First Affiliated Hospital, School of Medicine, Zhejiang University ,  Hangzhou ,  China    3  School of Life Sciences and the State Key Laboratory of Agrobiotechnology     2017   1  1  7   Motivation: Although high-throughput sequencing methods have been proposed to identify splicing branch points in the human genome, these methods can only detect a small fraction of the branch points subject to the sequencing depth, experimental cost and the expression level of the mRNA. An accurate computational model for branch point prediction is therefore an ongoing objective in human genome research. Results: We here propose a novel branch point prediction algorithm that utilizes information on the branch point sequence and the polypyrimidine tract. Using experimentally validated data, we demonstrate that our proposed method outperforms existing methods. Availability and implementation: https://github.com/zhqingit/BPP.Contact: djguo@cuhk.edu.hk Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  Eukaryotic pre-mRNA splicing involves a set of reactions catalyzed by the spliceosome, a protein complex consisting of five small nuclear ribonucleoproteins (the U1, U2, U4, U5 and U6 snRNPs) and hundreds of other proteins  (Burge et al., 1999; Jurica and Moore, 2002) . Through alternative splicing, the transcription of a gene can generate multiple isoforms by selectively expressing different exon sequences, which thereby contributes significantly to proteome complexity in metazoan  (Graveley, 2001; Maniatis and Tasic, 2002) . Recent studies suggest that 'spliceosomal mutations' can result in cancer-specific mis-splicing, which can be therapeutically exploited using compounds that influence the splicing process  (Dvinge et al., 2016) . The importance of splicing is also illustrated by the fact that more than 200 human diseases arise from the disruption of splicing by mutations either in the splicing sites or in the cis-acting splicing regulatory sites  (Cieply and Carstens, 2015; Chabot and Shkreta, 2016) .  A key step in determining the intron and exon to be spliced out or retained is the recognition of the branch point sequence (BPS) by the ribonucleoprotein U2 snRNP. The conserved GTAGTA hexanucleotide in U2 snRNA can base-pair with the BPS, forcing the branch point adenosine to flip out and form a bulge between the fifth and the sixth base. This initiates a nucleophilic attack at the intronic 5 splice site, thus starting the first of the two transesterification reactions that mediate splicing. Recently, researchers have attempted to identify branch points (BPs) by high-throughput sequencing approach  (Bitton et al., 2014; Mercer et al., 2015) , in which the intronic lariats were first enriched, and then sequenced to identify the position of the human BPs. However, due to the quick degradation of the lariat structure in the nucleus  (Moore, 2002) , these sequencing methods can only detect a small fraction of the lariats given the moderate sequencing depth. In theory, these sequencing methods also cannot identify the BPs for tissue-specific genes or genes with low expression level. For example,  Mercer et al. (2015)  only identified 59 359 high-confidence BPs out of a total of 300 000 human introns.  Efficient computational tools for human BP prediction would greatly facilitate human genome research. However, in silico, the identification of human BPs is rather challenging, mainly because human BPSs are highly variable and extremely degenerate. BPs have been successfully predicted in fungal species based on the Hamming distance of the BPS to the U2-complementary sequence  (Kupfer et al., 2004) , whereas this approach has proven insufficient for human genomes  (Corvelo et al., 2010) .  Corvelo et al. (2010)  introduced SVM-BPfinder, a method utilizing Support Vector Machines to predict human BPs using a set of high-confidence putative BPSs based on conservation and positional bias across seven mammalian species. SVM-BPfinder performs very well for introns with BPS containing a 'TNA' structure but it is unable to predict non-canonical BPS without the 'TNA' structure. HSF is an online bioinformatics tool to predict the effects of mutations on splicing signals  (Desmet et al., 2009) . It can identify BPS based on a position weight matrix (PWM) from the consensus sequence YNYCRAY. However, the lack of a standalone version limits its wider application.  In this article, we present a novel branch point prediction (BPP) method, integrating the characteristics of BPS and polypyrimidine tract. Our method is innovative in two aspects: (i) by pre-processing the raw data and selecting a suitable start point, degenerate BPS motif in human can be inferred using the mixture model (MM), a popular motif inference method; and (ii) using relative frequencies for different nucleotides instead of the most commonly used arbitrary scores to represent the effects of polypyrimidine tract (PPT). Our model not only estimates the affinity between the protein and the PPT, but also considers the co-evolution between the protein and the binding sequences. Using a set of experimentally validated data, we demonstrate that BPP outperforms previously published methods and thus provides a promising alternative method for BPP for human genome study.    2 Materials and methods\r\n   2.1 Training dataset for BPS motif inference and octanucleotide enrichment\r\n  Firstly, all of the human intron coordinates from the UCSC Genome Browser table were obtained and the intron sequences were extracted. Then the introns used in the testing dataset (introns verified experimentally or by high-throughput sequencing) were removed. Finally, the remaining introns longer than 300 bp were kept to be the training dataset (223 606 introns).    2.2 Inferring the energy motif of a BPS\r\n  According to  Stormo and Fields (1998)  and  Granek and Clarke (2005) , an element in PWM can be interpreted as the contribution of the corresponding base to the relative free energy of the motif. For each position j, we botain a set of equations: ( DG ¼ RTln fjb=pb ;  Pb fjb ¼ 1 (1) where b 2 {A,C,G,T}, j ¼ 1; . . . ; L and L is the length of the motif, R is the gas constant, T is the temperature, fjb is the observed frequency of base b at position j and pb is the background probability of base b. Here T ¼ 300 K is used, and pb is the statistical frequency based on a 50 nt segment located 300 nt upstream of all introns, chosen because no general splicing element is reported in this segment.  The program RNAcofold from the Vienna RNA package  (Hofacker et al., 1994)  was used to calculate the binding free energy between a heptanucleotide and the U2 snRNA. Specifically, heptanucleotides with a fixed base b at position j were forced to undergo a complete pairing with nucleotides (GTAGTA) from the U2 snRNA, with the exception of the BP adenosine, which was forced not to pair with any nucleotide. The average free energy of all of these heptanucleotides was represented as the relative free energy contribution from b at position j. Because DG; R; T and pb are known, fjb was calculated using Equation (1) and normalized by forcing the sum to be unity.    2.3 Updating the motif of BPS using MM\r\n  2.3.1 Mixture model and expectation maximization algorithm The expectation maximization (EM) algorithm searches for maximum likelihood estimates of the parameters of a finite MM, generating a given dataset of sequences  (Bailey et al., 1994) . We assume that a set of heptanucleotides S ¼ fs1; s2; . . . ; sng, where n is the number of heptanucleotides, arises from two mixture components: background and BPS. For convenience, we use 0 and 1 to represent the two components respectively. The probability of discovering si can be written as follows: pðsijMMÞ ¼ k0 p sijhbackground þ k1 pðsijhBPSÞ; (2) where k0 and k1 are the probabilities that the background and BPS components are respectively responsible for generating S, and k0 þ k1 ¼ 1. The parameter hbackground is a vector consisting of the frequencies of four nucleotides (Fb) in the background region; and hBPS represents the PWM of the BPS motif. The PWM expresses the frequency with which nucleotide b appears at position j: fjb, where j ¼ 1; . . . ; L, and L is the length of the motif. Fb expresses the frequency of nucleotide b: f0b.  The EM algorithm for finite MMs is used to find the parameters that maximize the likelihood of the data. For details, please refer to the following articles:  : (Aitkin and Rubin, 1985; Bailey and Elkan, 1995; Bailey et al., 199 4). 2.3.2 Initial value and training dataset To construct the training dataset, we collected a set of human intron sequences longer than 300nt, and extracted three subsequences from upstream of the 3'SS in each sequence: 21-34 nt, 187-200 nt and 316 nt. The three sets of subsequences respectively correspond to the BPS region, background region and PPT region.  The so-called BPS and PPT regions only represent the sequence blocks where BPS and PPT often locate and do not always contain BPS or PPT. The reasons of selecting 14 bp include: (a) many branch point sites locate at the 22nd nucleotide of upstream of the 3SS, so the PPT region can only be defined behind the 21st nucleotide; (b) a space of four nucleotides is given to avoid the repulsion between U2 snRNP and U2AF6B; (c) the last two nucleotides of 3SS locate at the splice donor site. So we have the PPT region from 16 to 3 bp upstream of the 3SS of the introns. To keep consistent, we also pick 14 bp for BPS and background regions.  To enrich the possible BPS, we decomposed the subsequences in each set into heptanucleotides, and compared the frequency of each heptanucleotide across the three sets. Specifically, we performed the Fisher's exact test for each heptanucleotide between the BPS set and the background set, and between the BPS set and the PPT set. Only the heptanucleotides significantly enriched in both tests (P 0.05) were selected to form the training dataset.  An initial value for the EM is essential to discover the desired motif  (Bailey et al., 1994)  due to the highly irregular likelihood landscape in the high dimensional parameter space, especially for degenerative motifs, such as that of human BPS. Here, the energy motif of the BPS and the frequencies of four nucleotides (A,C,G,T) in the background region were used as the initial guess for the EM search.    2.4 Calculating the BPS score of a heptanucleotide based on the PWM of the BPS motif\r\n  For any heptanucleotide, the BPS score is: sbps ¼ 7 Y fibi ; i¼1 where bi is the base in the i-th position of the heptanucleotide, and fibi is the observed frequency of base bi in the i-th position of the BPS motif. BPP estimates the contribution of U2AF6B by summarizing the weighted octanucleotides in the sequence of 20 bp immediately downstream from the candidate BPS: s ¼ 7 Y fibi ; i¼1 where bi is the base in the i-th position of the heptanucleotide, and fibi is the observed frequency of base bi in the i-th position of the BPS motif.    2.5 Generating the weighted octanucleotides\r\n  As shown by  Stormo and Fields (1998)  and  Granek and Clarke (2005) , the relative frequency of the nucleotide at each position in one motif is related to the binding free energy. If we consider the octanucleotide as an unit, its relative frequency should correlate with the binding affinity of U2AF65. To avoid the situation that the count of one ocatanucleotide is very small in the PPT region, but the relative frequency is very high, we also consider the frequency of each ocatanucleotide in the PPT region. We decompose the sequences in the PPT region and background region into octanucleotides, and weight each octanucleotide by  Cppt Cppt wocta ¼ Cbackground P Cppt  ; wocta ¼ wocta= X wocat where Cppt is the octanucleotide count in the PPT set, Cbackground is the octanucleotide count in the background set, and wocat is the normalized weight.    2.6 Finding the AGEZ\r\n  Following  Corvelo et al. (2010)  and  Gooding et al. (2006) , the region between the 3'SS and the first 'AG' dinucleotide whose distance to the 3'SS is longer than 12 nt is defined as the AGEZ in our method.    2.7 The BPP method\r\n  The BPP slides along the AGEZ. At each site, the first seven nucleotides are considered as a candidate BPS, which produces a score based on the motif of the BPS: We respectively normalize the two scores to range from 0 to 1: Finally, the BPP score can be calculated: (3)    2.8 Z-score calculation\r\n  We normalize the BPP score for each position by calculating the Z-score: sppt ¼  L 8þ1  P i¼1  L wocta  : sbps ¼ sbps=sðTACTAACÞ; sppt ¼ sppt=s20 T : s ¼ sbps sppt zi ¼ si r l ; (7) (8) (9) (10) (11) (4) (5) (6) where si is the BPP score of position i; l and r are respectively the mean and the standard deviation of the BPP scores of all positions in the AGEZ.     3 Results\r\n   3.1 BPP provides a sequence-based method for BPP\r\n  BPP is a method for BP prediction in pre-mRNA splicing. The prediction is based on the features of a BPS motif inferred from a MM, and a set of weighted octanucleotides representing the binding affinity between the U2AF65 and the PPT (see Fig. 1a). Using the U2 snRNP sequence, we first infer an intermediate motif, termed the 'energy motif' to represent the free energy (see Section 2). Then, the BPS motif is derived by using an MM, which initializes from the energy motif and is trained on a set of BPS-enriched heptanucleotides. The influence of the PPT is quantified through a set of octanucleotides, weighted by the relative frequencies with which they occur in the PPT and the background regions. When using the trained model for prediction, a sliding window moves along an intron sequence to calculate the score of the BPS and the affinity between the Fig. 1. Flowchart of BPP. (a) Left: Based on the energy motif and the enriched heptanucleotides, the mixture model is trained to infer the motif of the BPS; right: By comparing the octanucleotide frequencies in the background and the PPT regions, each octanucleotide was weighted to represent the affinity of U2AF6B to the PPT. (b) A window (black rectangle) slides along the intron sequence; SBPS is calculated based on the candidate BPS and SPPT is calculated based on the sequence of 20 bp immediately downstream from the candidate BPS downstream sequence and the U2AF65 at each site. Finally, the site with the highest score was selected as the predicted BP site (Fig. 1b, for details see Section 2).    3.2 The inferred energy motif of BPS is consistent with the experimental results\r\n  In this paper, the BPS region is defined as the 21st to the 34th nucleotides (nt) upstream of the 30 splice site (30SS), where most branch points are located  (Mackereth et al., 2011) . The PPT region is defined as the 3rd to the 16th nt upstream of the 30SS. The background region is defined as the 187th to the 200th nt upstream of the 30SS, because no general splicing element is reported in this region. Through their contrast with the BPS and PPT regions, the background regions provide statistical clues about the features of the true signal. Importantly, these defined regions are only used for model training, not for prediction.  MM has been successfully used for motif discover in a set of DNA/RNA sequences  (Bailey et al., 1994) . However, the human BPS motif is highly degenerate  (Gao et al., 2008)  and the fitting of the MM is very sensitive to the initial value. MM is therefore unable to predict the true BPS motif reliably if it is trained using only upstream sequences. Additionally, the co-occurrence of the PPT sequence and the BPS may interfere with the inference process  (Corvelo et al., 2010) . To overcome these problems, we use a set of enriched heptanucleotides in the BPS region to train the MM, which starts from an inferred energy motif of the BPS.  The RNA-RNA base pairing between the GUAGUA motif in the U2 snRNP and the BPS is important in human pre-mRNA splicing  (Wu and Manley, 1989; Zhuang and Weiner, 1989) . To supply the MM with an initial guess as close as possible to the real motif, we first infer an energy motif of the BPS based on the binding energy between 'GTATGA' in the U2 snRNP and all of the heptanucleotides. We name it energy motif to distinguish it from the motif derived from the MM. As the BP is not complementary to any nucleotide in the U2 snRNP, and experimental evidence has shown that the nucleotide Adenine (A) is strongly preferred even though Cytosine (C) and Thymine (T) can also function as the branch nucleotide  (Hartmuth and Barta, 1988) , we set the relative frequencies of the four nucleotides as: A:0.97, C:0.01, G:0.01, T:0.01 (see Fig. 2a).  This energy motif possesses some experimentally validated characteristics. For example, the fourth and seventh positions are mostly pyrimidines  (Gao et al., 2008) , and the second and fifth positions are mostly purine  (Pastuszak et al., 2011) . The heptanucleotide with the highest score (TACTA*C) based on the energy motif is completely complementary to the conserved motif (GTAGTA) in the U2 snRNP, which complies with the finding that TACTAAC is the most efficient BPS for mammalian mRNA splicing  (Zhuang et al., 1989) . The fact that the derived scores based on the inferred energy motif are highly correlated with the binding energy highlights the rationality of this inference (Supplementary Fig. S1).    3.3 Customized MM successfully infers the degenerate\r\n    BPS motif\r\n  The BPS motif inferred by MM is shown in Figure 2b. Compared with the above energy motif, this inferred BPS motif captures most characteristics of the human BPS: (i) the frequency of T at the fourth position reaches 98%, and pyrimidines frequently appear at the third and the seventh positions. This supports the experimental finding that the human branch point consensus sequence is nyUnAyn  (Gao et al., 2008) ; (ii) the frequency of T at the 1st position declines, and the 1st and the 2nd positions carry little information; and (iii) in accordance with results from previous studies  (Corvelo et al., 2010; Harris and Senapathy, 1990) , enriched purines and depleted T are preferentiably found at the fifth position. In contrast, the inferred motif was not successfully located when the MM was trained using an un-enriched dataset and a random start. The relative frequencies of the four nucleotides at the BP site did not affect the final inferred motif (see Supplementary Fig. S2).    3.4 The weighted octanucleotides show binding affinity to the U2AF65\r\n  Because the PPT-U2AF65 binding is essential for efficient BP utilization and 3'SS recognition in metazoans  (Frendewey and Keller, 1985; Garcıa-Blanco et al., 1989; Reed, 1989; Reed and Maniatis, 1985; Roscigno et al., 1993; Ruskin and Green, 1985; Wieringa et al., 1984) , the PPT also incorporates the binding affinity between the U2AF65 and the sequences downstream of the BPS.  In previous studies, the PPT score was calculated as the contribution of U2AF65 based on its pyrimidine content  (Clark and Thanaraj, 2002; Corvelo et al., 2010)  or by statistical tests  (Schwartz et al., 2008) , and these scoring systems are universal across different species. Considering, however, that the PPT may have co-evolved with the U2AF65 RNA recognition motifs  (Schwartz et al., 2008) , we deem it more reasonable to construct a species-specific PPT scoring system.  We use weighted octanucleotides to represent the U2AF65 binding affinity because either octanucleotides or nonanucleotides may be the basic binding elements of U2AF65  (Ito et al., 1999; Mackereth et al., 2011; Sickmier et al., 2006) . The weight score reflects the relative frequency of an octanucleotide in the PPT and in the background regions (Section 2), and therefore, a small fraction of octanucleotides (the long right tail in Supplementary Fig. S3) frequently occur at the PPT region (Supplementary Fig. S2). This fraction includes octanucleotides known to be preferred by human U2AF65, including TTTTTTTT, TTTCTTTT, TTTTCTTT and others. The finding that higher scores correlate with a stronger affinity between the octanucleotide and the U2AF65 validates the effectiveness of our method.    3.5 Performance comparison between BPP and other methods\r\n  A performance comparison was conducted for BPP, SVM-BPfinder and HSF. Of these three methods, both BPP and SVM-BPfinder predict BPs in the AGEZ region  (Gooding et al., 2006)  of the introns. BPP starts from 4 bp upstream of 30SS, and if the distance between the candidate BPS and the 30SS is shorter than 9 bp, then BPP does not integrate the contribution of the PPT. In contrast, SVMBPfinder starts from 15 bp upstream of 30SS. Based on the output of the online system, HSF predicts BPs in the region from 18 to 100 bp upstream of the 30SS.  To comprehensively evaluate the three methods, a set of 86 introns with experimentally verified BPs  (Ajiro and Zheng, 2015; Burrows et al., 1998; Chavanas et al., 1999; ; Corvelo et al., 2010; Darman et al., 2015; Goux-Pelletan et al., 1990; ; Gooding et al., 2006; Gao et al., 2008; Helfman and Ricci, 1989; Janssen et al., 2000; Li and Pritchard, 2000; Maslen et al., 1997; Mayer et al., 2000; Smith and Nadal-Ginard, 1989; Southby et al., 1999; Webb et al., 1996; )  and a set of 47 294 introns with sequencing-verified BPs  (Mercer et al., 2015)  were collected and used for the comparison. The comparisons were conducted in the following three ways: 1. In their default settings, the methods predict the position with the highest score to be the BP. Based on the corresponding results, we compared the correctly predicted introns of each method. 2. A receiver operating characteristic (ROC) curve was built at the intron level for the three methods based on a testing set consisting of positive introns with known BPs (86 experimentally verified and 47 294 NGS verified BPs) and a set of randomly generated negative sequences. 3. A ROC curve was built at the position level for each method, taking the BP and non-BP positions in the introns as the positives and negatives respectively.    3.6 Comparison based on the default setting outputs of each method\r\n  For performance comparison, only real intron sequences were used as inputs and the positions with the highest score for each intron were selected. For HSF, only positions in the AGEZ were selected. Here, precision was defined as the number of introns with a correctly predicted BP divided by the total number of introns. Given the fact that the U2 snRNA can hybridize to the adenosines at either the fifth or the sixth position of the BPS and bulge out of the other  (Query et al., 1994) , and that reverse transcriptase can stop within 1 nt of the branch point,  (Rodriguez et al., 1984; Zeitlin and Efstratiadis, 1984) . The prediction was also considered correct if the predicted BP was 1 or 1 nt away from of the real BP position. We supplied both one nucleotide error tolerant and no error tolerant versions. For introns containing more than one BP, the prediction was marked correct if any one of the BPs was detected. Because SVM-BPfinder only finds BPS containing TNA, its prediction was considered false for introns that did not include any TNAcontaining BPS.  For the 86 experimentally verified introns, the precision levels of BPP, SVM-BPfinder and HSF were 68.6, 61.6 and 30.2%, respectively (Fig. 3a, Supplementary Fig. S4a). Even when only considering the introns with a TNA structure, BPP still outperformed SVMBPfinder: in these cases, BPP correctly predicted 77.6% of the BPs, compared with 59.7% for SVM-BPfinder and 34.2% for HSF. The full list of the 86 introns with the corresponding predictions by the three methods can be accessed in Supplementary Table S1. Subsequently, we compared the three methods based on the 47 294 introns with NGS-verified BPs. Again, BPP outperformed the other methods (Supplementary Fig. S5a and c).    3.7 Comparison using the ROC curves at the intron level\r\n  To build the ROC curves, a set of sequences with the same lengths as the introns in the positive dataset was randomly generated as the negative dataset. For the experimentally verified BPs, the testing dataset consisted of 86 positives and 86 negatives; for the NGSverified BPs, the testing dataset consisted of 47 294 positives and 47 297 negatives. As described earlier, the prediction was considered correct if the predicted BP was located within 1 or 1 nt of the real BP position. The ROC curves under the general and TNA-only conditions are illustrated in Figure 3b (Supplementary Fig. S4b) and Supplementary Figure S5b (Supplementary Fig. S5d), and show that BPP again outperformed the other two methods.    3.8 Comparison using ROC curves at the position level\r\n  For this comparison, two ROCs were created for each method based on two different values: (i) the absolute score of each position and (ii) the ranking of each position based on its score relative to all of the others. The ranking values were used because for each position, the score was inferred independently, i.e. neglecting the competition of other positions, and because the scores across the introns might not be mutually compatible. The true-positive rate was unable to reach 1 because some of the real BP positions lay outside of the regions selected for prediction by the three methods. As shown in Supplementary Figure S4c and d, for all three methods, the evaluation methods by ranking performed better than the one by score, indicating the positions of the BPs were influenced by the competitive effects of other positions. Again, BPP gave the best performance and HSF outperformed SVM-BPfinder.  In general, BPP and SVM-BPfinder are recommended if the goal is to predict the most likely BP for each intron. However, if the goal is to predict multiple BPs in one intron, BPP and HSF should be preferred methods.    3.9 Genome-wide BP prediction in human\r\n  Using BPP, all of the introns in the human genome (N ¼ 281 787) were scanned for BPs (Supplementary Table S2). As shown in Figure 4a, 94.6% (N ¼ 266 707) of the predicted BPs were located within 0-50 upstream of the 30SS, consistent with previous reports  (Corvelo et al., 2010; Mercer et al., 2015) , that most BPs are discovered at 23-25 nt upstream of the 30SS.  The presence of distal BPs in the predictions prompted us to speculate on how they are selected in the splicing process. The distances of the BPs to the 30SS were plotted against their relative BPP scores, i.e. the normalized Z scores across the BPP scores of all positions in the AGEZ (Fig. 4b top). Interestingly, the Z scores of the distal BPs increased rapidly with the distance to the 30SS at distances longer than 100 bp. However, the absolute BPP scores were still very low for most of the distal BPs (Fig. 4b bottom). Hence this suggests that the distal BPs are selected due to the even much lower scores of the positions close to the 30SS. Given the fact the absolute BPP scores reflect the physical affinity between the RNA sequence and the splicing proteins, it seems likely that most of the introns with distal BPs are partially spliced, which is consistent with the conclusion from paper  (Bitton et al., 2014) . We further plotted the frequencies of the alternative ends of the introns under different BP distance to 30SS (Supplementary Fig. S6). If the end of the intron can be found in all the transcripts, we define it as the constitute end, otherwise the alternative end. In accord with the Figure 4b, the frequencies of the alternative ends increase when the distal BPs are used. However it is surprising that we found a minimal value at 122 bp for the downstream alternative ends, which we cannot explain based on known knowledge. But it might be a good start to explore some specific biological functions in the RNA splicing process in the future.  Searching the ClinVar  (Landrum et al., 2014)  database, we found that 42 BPs located in gene related to various human diseases, including hyperoxalurea type III, colorectal cancer, supratentorial primitive neuroectodermal tumours and Alzheimer's disease, among others (Supplementary Table S2). In future, it will be interesting to examine the possibility that the mutation of BPs might alter the mRNA splicing process and result in these diseases.     4 Discussion\r\n  With the development of high-throughput sequencing technology, it is now possible to identify thousands of human BPs in one experiment. However, due to experimental costs and low gene expression levels, only 20% of the total human introns have been validated by the sequencing approach. Computational methods for human BP prediction are complicated by the degeneracy of the motifs and the involvement of other auxiliary elements, such as the PPT. To overcome these limits, we proposed an algorithm (BPP) to predict the branch points of human introns based only on the sequence information. By using a set of experimentally verified BPs, we showed that BPP outperformed the other currently available programs. However, BPP still has some limitations. For example, the BPS motif inference and octanucleotide weighting process are conducted separately, which may hinder the accurate weight estimation of certain heptanucleotides or octanucleotides, and result in loss of information from other elements, because the BPS and the PPT co-exist in a contiguous sequence. In general, a framework combining these two processes into a unified function may further improve the predictive accuracy of BPP. In future work, we will refine the current MM to accommodate the combination of different elements.    Funding\r\n  This work was supported by a grant from the Hong Kong University Grants Committee Hong Kong UGC/AoE Plant &amp; Agricultural  Biotechnology Project grant AoE-B-07/09, a grant from the Shenzhen Science and Technology Committee [grant no. JCYJ20140425184428456] and a grant from the Hong Kong Research Grants Council [Project No. CUHK400913].  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/zhqingit/BPP",
    "publicationDate": "0",
    "authors": [
      "Qing Zhang",
      "Xiaodan Fan",
      "Yejun Wang",
      "Ming-an Sun",
      "Jianlin Shao",
      "Dianjing Guo"
    ],
    "status": "Success",
    "toolName": "BPP",
    "homepage": ""
  },
  "88.pdf": {
    "forks": 0,
    "URLs": ["jpsglouzon.github.io/structurex"],
    "contactInfo": ["shengrui.wang@usherbrooke.ca"],
    "subscribers": 1,
    "programmingLanguage": "JavaScript",
    "shortDescription": "An interactive plateform to explore structural features of RNA secondary structures",
    "publicationTitle": "Structurexplor: a platform for the exploration of structural features of RNA secondary structures",
    "title": "Structurexplor: a platform for the exploration of structural features of RNA secondary structures",
    "publicationDOI": "10.1093/bioinformatics/btx323",
    "codeSize": 39395,
    "publicationAbstract": "Summary: Discovering function-related structural features, such as the cloverleaf shape of transfer RNA secondary structures, is essential to understand RNA function. With this aim, we have developed a platform, named Structurexplor, to facilitate the exploration of structural features in populations of RNA secondary structures. It has been designed and developed to help biologists interactively search for, evaluate and select interesting structural features that can potentially explain RNA functions. Availability and implementation: Structurxplor is a web application available at http://structurex plor.dinf.usherbrooke.ca. The source code can be found at http://jpsglouzon.github.io/structurex plor/. Contact: shengrui.wang@usherbrooke.ca Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2016-12-23T03:26:53Z",
    "institutions": ["Universite ́ de Sherbrooke"],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-09-08T15:21:00Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx323   Structurexplor: a platform for the exploration of structural features of RNA secondary structures     Jean-Pierre Se´ hi Glouzon  0  1    Jean-Pierre Perreault  1    Shengrui Wang  0    0  Department of Computer Science, Faculty of Science, Universite ́ de Sherbrooke ,  Sherbrooke, QC, J1K 2R1   Canada    1  RNA Group, Department of Biochemistry, Faculty of Medicine and Health Sciences, Applied Cancer Research Pavilion, Universite ́ de Sherbrooke ,  Sherbrooke, QC, J1K 2R1 ,  Canada     2017   1  1  4   Summary: Discovering function-related structural features, such as the cloverleaf shape of transfer RNA secondary structures, is essential to understand RNA function. With this aim, we have developed a platform, named Structurexplor, to facilitate the exploration of structural features in populations of RNA secondary structures. It has been designed and developed to help biologists interactively search for, evaluate and select interesting structural features that can potentially explain RNA functions. Availability and implementation: Structurxplor is a web application available at http://structurex plor.dinf.usherbrooke.ca. The source code can be found at http://jpsglouzon.github.io/structurex plor/. Contact: shengrui.wang@usherbrooke.ca Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: Cenk Sahinalp    1 Introduction\r\n  Because structure largely determines RNA function  (Wan et al., 2011) , exploration of structural information in a population of RNA secondary structures to discover features related to specific functions of RNA is essential. A typical example of a functionrelated structural feature is the cloverleaf shape of transfer RNA secondary structures, which is known to play a crucial role in the translation mechanism.  Exploration of structural features is an iterative process in which a structural biologist, the key actor of the exploration process, searches for, evaluates and selects structural features potentially related to RNA functions, which will then be experimentally validated  (Holzinger et al., 2014; Shneiderman, 2002) . Specifically, the exploration process involves searching for and evaluating structural features by first preprocessing, comparing and clustering the structures, and then computing and generating visualizations of structural features to facilitate their interpretation. Interesting structural features can then be selected for further experimental validation.  Exploring structural features is a complex and time-consuming task, especially for those who are not computer science specialists. In fact, a costly investment of effort and time is required to gain the necessary advanced knowledge of languages such as Bash, Python or R, and data analytics such as supervised or unsupervised learning methods  (Holzinger et al., 2014) . Often, one has to use various tools in combination in a pipeline to enable effective exploration of structural features. Most existing tools were designed and developed to solve one particular challenge related to a specific task in the exploration process. For instance, the challenge of finding clusters of similar structures was addressed by Sfold  (Chan et al., 2005) , GraphClust  (Heyne et al., 2012) , NoFold  (Middleton and Kim, 2014) , while the challenge related to visual inspection and manual edition of structures was tackled by tools such as VARNA  (Darty et al., 2009) , Forna  (Kerpedjiev et al., 2015)  or 4SALE  (Wolf et al., 2014) . Programs or methods such as RNAdistance  (Hofacker et al., 1989) , the relaxed base pair measure  (Agius et al., 2010) , ERA  (Zhong and Zhang, 2013) , RNAforester  (Schirmer and Giegerich, 2013) , LocARNA  (Will et al., 2007)  or the super-n-motifs model  (Glouzon et al., 2017)  have been designed to compare structures. The existing tools are unable to assist biologists in the multiple phases of the exploration process.  We propose a new platform, named 'Structurexplor', to facilitate the exploration process for a population of RNA secondary structures. The main contributions of this platform are as follows: \u2022 It facilitates the whole exploration process by assisting the expert in preprocessing and comparing structures, and computing various features such as clusters of structures, representative and unusual structures, and many others. These features are useful since they provide insights into the data. For instance, the shape of the representative structures of clusters sheds light on the main structural shapes of a population of RNA. \u2022 It assists in evaluation and interpretation of the computed features by providing interactive visualization functionalities to efficiently inspect those features. For instance, it provides a way to focus on a specific cluster by zooming in and interactively inspecting the shape of the member structures of this cluster. \u2022 It is versatile, offering the capability to explore structural features of secondary structures from both linear and circular RNA, and to take pseudoknots and G-quadruplexes into account, through its use of the super-n-motifs model  (Glouzon et al., 2017) .  Structurexplor combines a set of tools and models into a unified platform to accelerate the exploration process for RNA secondary structures. The following sections provide a description of the Structurexplor platform and a practical example of its use.    2 Materials and methods\r\n  Structurexplor is a web application mainly written in R  (R Core Team, 2015)  with the Shiny package  (Chang et al., 2016)  providing a fast and responsive interface. It has been deployed using ShinyProxy  (Verbeke and Michielssen, 2016)  which is an alternative open-source program for Shiny server  (Chang et al., 2016) . Structurexplor takes secondary structures in dot-bracket format as input and facilitates the exploration by a series of steps. First, it computes the structural dissimilarities using the super-n-motifs model. It clusters the structures according to their dissimilarity by employing one of the various clustering algorithms including unweighted pair group method using arithmetic mean (UPGMA), Ward or complete linkage  (Pang-Ning et al., 2006) . Then, it computes various structural features such as the representative structure of each cluster. Finally, Structurexplor offers various interactive visualizations that can be used to explore structural features. For instance, it allows interactive visualization of the shape of a representative structure.  Among the structural features output, Structurexplor yields clusters of structures, i.e. groups of structures with similar shapes, indicating either possible functional groups or alternative foldings in the case of exploration of the structure ensemble. Clusters are computed using the stats package  (R Core Team, 2015) . Cluster quality is also assessed, based on the silhouette coefficient  (Pang-Ning et al., 2006)  computed using the cluster package  (Maechler et al., 2015) , which gives information about how compact and well-separated the clusters are. While other indexes such as Calinksi-Habaratz  (Calinski and Harabasz, 2007)  or Davies-Bouldin  (Davies and Bouldin, 1979)  can help to assess clustering quality, the silhouette coefficient has the advantage of being easier to interpret. In fact, the silhouette coefficient is bounded between -1 and 1. It offers a clear interpretation from -1 indicating a low quality clustering to 1 a very high quality of clustering. To facilitate linguistic expression of the clustering assessment, we use quality indices of 'Very high', 'High', 'Medium', 'Low' and 'Very low' corresponding to the silhouette coefficient intervals [1,0.7], [0.7, 0.5], [0.5, 0.3], [0.3, 0], [0, -1]. As an example, a very high clustering quality means that the clusters are far apart and members of a same cluster are very close to each other. The silhouette coefficient requires that the number of members in each cluster be at least 3.  Structurexplor provides cluster and structure hierarchies, which are useful for the study of structural phylogeny. Structurexplor also gives information about the most representative and unusual structural shapes of RNA by the identification of representative and unusual structures of clusters. The package assesses the structural variability of clusters, i.e. how structure shapes may vary within a cluster. Finally, it identifies the region that best describes the clusters. Details about representative and unusual structures, structural variability and the region best describing clusters are provided in the Supplementary Material.  Finally, Structurexplor provides an interactive visualization of the hierarchy, a two dimension representation of structures based on the super-n-motifs representations of structures  (Glouzon et al., 2017) , visualization of the structure shapes and general information on structures and clusters in data table format. These are respectively based on phylotree.js  (Pond et al., 2015) , rCharts  (Vaidyanathan, 2013) , Forna  (Kerpedjiev et al., 2015)  and DT  (Xie, 2015) . Many interactive controls are available to facilitate exploration, such as the capacity to re-root the hierarchy on specific nodes or to focus on a specific cluster by zooming in on the corresponding region of the 2D representation of structures.    3 Example\r\n  When no prior information is available about a population of RNA secondary structures, being able to obtain a clear understanding of the most important structural shapes can be very useful. A typical example is provided to show how Structurexplor can facilitate the identification of these shapes. The example, consisting of 179 various-sized structures from transfer RNA (tRNA) (74-77nt), group-II-D1D4-1 (GII) (70-108nt), 5S ribosomal RNA (5S) (117124nt), Ribonuclease P RNA (RNaseP) (300-330nt), Signal recognition particle RNA (SRP) (317-320nt) and transfer messenger RNA (tmRNA) (362-366nt) families, is available in the 'Prepare' menu. tRNA, 5S, RNaseP, SRP, tmRNA, originated from RNASTRAND database  (Andronescu et al., 2008) . GII secondary structures coming from RFAM database  (Nawrocki et al., 2015)  and representing a conserved structural region from the full structure of Group II Intron has been generated using the consensus structures as a constraint for folding, via the script refold.pl and RNAfold  (Lorenz et al., 2011) . After running the example, Structurexplor switches to the 'Explore' menu, where users can interactively explore various clustering configurations, by changing the clustering algorithm, for instance. The impact of these modifications on the clustering quality can be instantly observed, which is useful for quickly assessing the quality of clustering configurations. After selecting the best clustering configuration, users can then investigate and visualize structures and cluster features.  As mentioned earlier, Structurexplor provides an effective way to visually exploring structural dissimilarity of RNAs. It can generate, in the panel 'Features visualization', a scatter plot in which each Super n-motifs model structure is displayed as a point on a two-dimensional (sub)space (2D) and allows to visually inspect whether the structures are relatively close, i.e. either similar, or distant (Fig. 1). If two structures on the plot are selected, the structural dissimilarity computed on all the dimensions is also displayed to complement the dimensionsrelevant dissimilarity information shown by the relative distance on the plot. Recall that each dimension in the super-n-motifs model represents specific combinations of structural features such as stems or hairpins. Each dimension on the plot is labeled by the amount of associated structural information representing the explained variability, i.e. the strength or the importance of a specific combination of structural features used to represent the structures. To get further information about the computation of the 2D visualization of structures, the structural dissimilarity and the structural information associated with each dimension, readers are referred to the super-nmotifs model  (Glouzon et al., 2017) .  Structurexplor helps identify clusters. Figure 1 shows six clusters, the shapes of the representative structures of clusters 1 and 2, and the structures with unusual shapes. These features are computed taking into account all the dimensions used to represent the secondary structures i.e. the full set of super-n-motifs, and are reported for visualization in the scatter plot. We can see from the typical shapes of the representative structures that clusters 1, 2, 3, 4, 5 and 6, correspond to the functional families tRNA, GII, 5S, RNaseP, tmRNA and SRP. This example illustrates Structurexplor's suitability for discovering useful information, such as identification of functional groups, from RNA secondary structures.    Funding\r\n  This work has been supported by a joint grant from the Fonds de Recherche du Que´bec - Nature et Technologies (FRQ-NT) and the Universite´ de Sherbrooke Research Chair on RNA Structure and Genomics to Prof. Perreault and the Natural Sciences and Engineering Research Council of Canada to Prof. Wang.  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/jpsglouzon/Structurexplor",
    "publicationDate": "0",
    "authors": [
      "Jean-Pierre Se´ hi Glouzon",
      "Jean-Pierre Perreault",
      "Shengrui Wang"
    ],
    "status": "Success",
    "toolName": "structurexplor",
    "homepage": "http://structurexplor.dinf.usherbrooke.ca"
  },
  "100.pdf": {
    "forks": 0,
    "URLs": ["github.com/mgleeming/Xenophile"],
    "contactInfo": [
      "m.leeming@student.unimelb.edu.au",
      "w.donald@unsw.edu.au",
      "rohair@unimelb.edu.au"
    ],
    "subscribers": 1,
    "programmingLanguage": "Python",
    "shortDescription": "Non-targeted identification of xenobiotic-protein adducts",
    "publicationTitle": "Nontargeted Identification of Reactive Metabolite Protein Adducts",
    "title": "Nontargeted Identification of Reactive Metabolite Protein Adducts",
    "publicationDOI": "10.1021/acs.analchem.6b04604",
    "codeSize": 6238,
    "publicationAbstract": "Metabolic bioactivation of many different chemicals results in the formation of highly reactive compounds (chemically reactive metabolites, CRMs) that can lead to toxicity via binding to macromolecular targets (e.g., proteins or DNA). There is a need to develop robust, rapid, and nontargeted analytical techniques to determine the identity of the protein targets of CRMs and their sites of modification. Here, we introduce a nontargeted methodology capable of determining both the identity of a CRM formed from an administered compound as well as the protein targets modified by the reactive metabolite in a single experiment without prior information. Acetaminophen (N-acetyl-p-aminophenol, APAP) and 13C6-APAP were incubated with rat liver microsomes, which are known to bioactivate APAP to the reactive metabolite N-acetyl-p-benzoquinone imine (NAPQI). Global tryptic digestion followed by liquid chromatographic/mass spectrometric (LC/MS) analysis was used to locate \u201ctwin\u201d ion peaks of peptides adducted by NAPQI and for shotgun proteomics via tandem mass spectrometry (MS/MS). By the development of blended data analytics software called Xenophile, the identity of the amino acid residue that was adducted can be established, which eliminates the need for specific parametrization of protein database search algorithms. This combination of experimental design and data analysis software allows the identity of a CRM, the protein target, and the amino acid residues that are modified to be rapidly established directly from experimental data. Xenophile is freely available from https://github.com/mgleeming/Xenophile.",
    "dateUpdated": "2017-03-26T05:06:34Z",
    "institutions": [
      "The University of Melbourne",
      "University of New South Wales"
    ],
    "license": "https://github.com/mgleeming/Xenophile/blob/master/license.txt",
    "dateCreated": "2017-03-26T05:05:35Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Anal. Chem.     10.1021/acs.analchem.6b04604   Nontargeted Identification of Reactive Metabolite Protein Adducts     Michael G. Leeming  0    William A. Donald  1    Richard A. J. O'Hair  0    0  School of Chemistry and Bio21 Institute of Molecular Science and Biotechnology, The University of Melbourne ,  Melbourne, Victoria 3010 ,  Australia    1  School of Chemistry, University of New South Wales ,  Sydney, New South Wales 2052 ,  Australia     2017   89  5748  5756    8  5  2017    21  11  2016     Metabolic bioactivation of many different chemicals results in the formation of highly reactive compounds (chemically reactive metabolites, CRMs) that can lead to toxicity via binding to macromolecular targets (e.g., proteins or DNA). There is a need to develop robust, rapid, and nontargeted analytical techniques to determine the identity of the protein targets of CRMs and their sites of modification. Here, we introduce a nontargeted methodology capable of determining both the identity of a CRM formed from an administered compound as well as the protein targets modified by the reactive metabolite in a single experiment without prior information. Acetaminophen (N-acetyl-p-aminophenol, APAP) and 13C6-APAP were incubated with rat liver microsomes, which are known to bioactivate APAP to the reactive metabolite N-acetyl-p-benzoquinone imine (NAPQI). Global tryptic digestion followed by liquid chromatographic/mass spectrometric (LC/MS) analysis was used to locate \u201ctwin\u201d ion peaks of peptides adducted by NAPQI and for shotgun proteomics via tandem mass spectrometry (MS/MS). By the development of blended data analytics software called Xenophile, the identity of the amino acid residue that was adducted can be established, which eliminates the need for specific parametrization of protein database search algorithms. This combination of experimental design and data analysis software allows the identity of a CRM, the protein target, and the amino acid residues that are modified to be rapidly established directly from experimental data. Xenophile is freely available from https://github.com/mgleeming/Xenophile.       -\r\n  *S Supporting Information M way through metabolic reactions after they enter the ost small organic molecules will be modified in some body. While many metabolites are biochemically inert and readily excreted, it is well-established that so-called \u201cbioactivation\u201d metabolic transformations can result in the formation of highly reactive compounds that can lead to toxicity. This concept was first described by Miller and Miller in the 1940s;1,2 they investigated the binding of the azo dye pdimethylaminoazobenzene to cellular macromolecules in rats and postulated that metabolites of this substance, rather than the substance itself, may be the primary toxicant. Subsequently, many xenobiotics have been found to form reactive metabolites in vivo, including drugs3,4 as well as insecticides and industrial chemicals.5,6  A current theory is that binding of a chemically reactive metabolite (CRM) to cellular macromolecules is a primary event linking bioactivation to the onset of toxicity.7,8 Indeed, an analysis of the covalent binding characteristics of 42 drugs revealed that the extent of protein binding is broadly correlated with the risk of idiosyncratic adverse drug reactions (IADRs).9 While the precise mechanistic details remain to be established, clinical observations suggest that most IADRs are immunerelated.8 One proposed mechanism, the hapten hypothesis,10,11 asserts that modification of a native protein with a CRM, the hapten, triggers the production of antibodies that bind the CRM−protein complex, leading to a potentially severe immune response against the modified protein. Other hypotheses have been put forward,12 such as the danger hypothesis13−15 and the pharmacological interaction with immune receptors (p-i) hypothesis.16,17  Due to the potential for toxicity and the difficulty in predicting the outcomes of CRM formation, general practice in drug development processes has been to minimize the formation of CRMs through chemical modifications to the drug substrate.18 To this end, a vast literature has been assembled on the potential bioactivation processes operative for a wide range of organic functional groups.3,19−21 Assays to determine the global level of protein binding by reactive metabolites are now routinely conducted in drug discovery projects and typically involve incubation of radiolabeled substrate with microsomal fractions or hepatocytes, followed by liquid scintillation counting of washed protein pellets.18  However, there are cases of CRM−protein binding that highlight the complexities inherent in this field. For example, when Roberts et al.22 administered either acetaminophen (Nacetyl-p-aminophenol, APAP) or the structural isomer N-acetylm-aminophenol (AMAP) to hamsters, similar levels of CRMprotein binding were observed, but APAP treatment induced hepatotoxicity while AMAP did not. Due to the close structural similarity of APAP and AMAP, findings such as this have led to the critical-protein hypothesis,7 whereby the exact identity of the proteins adducted is a key determinant of the biological response and varies with even small changes in the structure of the CRM.23 It is interesting to note that differences in the protein adduction profiles of APAP and AMAP have been identified by both radiographic and immunochemical analyses of protein fractions derived from various liver preparations.24,25 Efforts to catalogue the protein targets of reactive metabolites are under way. For example, Hanzlik et al.26,27 maintain a database of proteins known to be modified by CRMs formed from a range of compounds.  There is a pressing need to develop robust, rapid, and nontargeted analytical techniques to determine the identity of the protein targets of reactive metabolites to provide an approach for elucidating the biochemical origins of IADRs.28 A range of analytical methods have been employed to identify these adducts.29,30 For example, antibodies can be raised against protein-bound drug motifs that can be used to detect the modified macromolecules.31 Alternatively, two-dimensional (2D) gel electrophoresis of protein preparations adducted by radiolabeled drug, followed by phosphorimaging, is used to detect gel spots corresponding to adducted proteins.24,32,33 Other methods involving affinity capture of protein digestion products and targeted multiple reaction monitoring (MRM)like experiments have identified modified peptides.34 However, all of these methods are resource-intensive, laborious, or require some degree of prior knowledge about the metabolites likely to be formed. These factors present significant barriers to the routine identification of protein targets of reactive metabolites and likely contribute to the relative rarity of such studies.  One nontargeted method to detect specific chemicals in liquid chromatographic/mass spectrometric (LC/MS) data is to simultaneously administer equal parts of natural abundance and isotopically enriched xenobiotic, which produces a pair of coeluting peaks that have the same abundance and are separated by an m/z spacing corresponding to the mass of the isotopic label.35−38 Recently, we have demonstrated that this so-called twin-ion method, in conjunction with automated data analysis software, enables the nontargeted detection of small-molecule metabolites of APAP in rat blood plasma.39 Here, we extend the twin-ion method to identify the protein targets of CRMs. That is, a nontargeted methodology capable of determining the identity of a CRM formed from an administered compound and the protein targets modified by the reactive metabolite in a single experiment without prior information is introduced. To validate, APAP and 13C6-APAP were bioactivated to the reactive metabolite N-acetyl-pbenzoquinone imine (NAPQI) via incubation with rat liver microsomes.40,41 Global tryptic digestion followed by LC/MS analysis is used to locate twin-ion peaks associated with peptides adducted by NAPQI, while the identity of the proteins is determined by tandem mass spectrometry (MS/MS) shotgun proteomics. By use of blended data analytics, the identity of the amino acids adducted can be established, which removes the need for specific parametrization of protein database search tools. This novel approach does not require reactive metabolite trapping studies, and the identity of the amino acid residue adducted by a reactive metabolite need not be known in advance.42 As such, unexpected or previously unidentified metabolites can still be accounted for in the experimental data.    EXPERIMENTAL METHODS\r\n  Materials. Acetaminophen (APAP), sequencing-grade modified trypsin, iodoacetamide, triethylammonium bicarbonate (TEAB) buffer, formic acid, urea, and reduced nicotinamide ■ adenine dinucleotide 2\u2032-phosphate tetrasodium salt (NADPH) were from Sigma−Aldrich (St. Louis, MO). Pooled rat liver microsomes (20 mg·mL−1 protein, 0.462 nmol of P450·(mg of protein)−1, from 83 uninduced, male Sprague-Dawley donor rats, lot number RT053C) were from Life Technologies (Carlsbad, CA), and stored at −80 °C until use. ring-13C6APAP [&gt;99% 13C enrichment] was from IsoSciences (King of Prussia, PA), and tris-(2-carboxyethyl)phosphine hydrochloride (TCEP) was from Thermo Fisher Scientific (Waltham, MA). Oasis HLB solid-phase extraction cartridges (10 mg) were from Waters (Milford, MA). Acetone and acetonitrile were from Merck (Kenilworth, NJ), and 18 MΩ H2O was obtained from a Milli-Q apparatus  Microsome Incubations. Either aqueous APAP/13C6APAP solution (1:1; 10 μL, 20 mM) or water (10 μL) was combined with microsomes [10 μL, 20 mg·mL−1 protein] in potassium phosphate buffer (170 μL, 100 mM, pH 7.4), and the mixtures were preincubated at 37 °C for 10 min with continuous agitation. Reactions were initiated by addition of NADPH (10 μL, 20 mM) and incubated for 3 h at 37 °C. At both t = 60 and 120 min, an additional aliquot of NADPH (10 μL, 20 mM) was added. Reactions were quenched at t = 180 min via the addition of ice-cold acetone (1 mL). Samples were vortex-mixed for 30 s and then stored at 4 °C overnight. The samples were centrifuged (16000g, 10 min, 4 °C), the supernatant was removed, and the remaining protein pellet was gently washed by overlaying acetone (1 mL). After removal of the supernatant, the pellet was resuspended in TEAB buffer (200 μL, 50 mM, pH 8.5) containing urea (8 M) and TCEP (10 mM) and sonicated until dissolution. The samples were incubated at 37 °C for 45 min and allowed to cool to ambient temperature before iodoacetamide (122 μL, 100 mM) was added. Samples were incubated in darkness for 60 min and then diluted to a final urea concentration of 1 M with TEAB (25 mM). Trypsin (4 μg) was added to each sample, and digestions proceeded at 37 °C for 24 h. The samples were acidified with formic acid to a final concentration of 1% (v/v) and purified by solid-phase extraction as follows: (i) the cartridge was preconditioned with 1 mL of 80% acetonitrile containing 0.1% trifluoroacetic acid (TFA), followed by 2 × 1.2 mL of 0.1% aqueous TFA; (ii) sample was loaded onto the cartridge; (iii) the cartridge was washed with 2 × 1.2 mL of 0.1% TFA; (iv) peptides were eluted from the cartridge with 1 mL of 80% acetonitrile containing 0.1% TFA; and (v) eluent was concentrated on a centrifugal evaporator to approximately half the original volume, then lyophilized overnight and resuspended in 0.1% formic acid (100 μL) and stored at −20 °C until the time of analysis via nano-LC/MS/MS experiments. Experiments were performed in triplicate for both APAP and control treatments.   Liquid Chromatographic/Mass Spectrometric and\r\n     Tandem Mass Spectrometric Experiments. Nano-LC/\r\n  MS/MS analyses of peptides generated from microsomal proteins were performed on a hybrid linear ion trap mass spectrometer and Orbitrap mass spectrometer (Orbitrap Fusion Lumos; Thermo Fisher Scientific) equipped with a nano-HPLC (UltiMate 3000 RSLC; Dionex). The nano-LC system was equipped with a C18 nanotrap column (Acclaim Pepmap, Dionex, 100 Å, 75 μm × 2 cm) and a C18 analytical column (Acclaim Pepmap RSLC, Dionex, 100 Å, 75 μm × 50 cm). Peptides were separated by gradient elution with mobile phases of 0.1% (v/v) aqueous formic acid (solvent A) and 0.1% (v/v) formic acid in CH3CN (solvent B). The gradient timetable was as follows ([time in minutes, %B]): [ 0, 3 ], [ 6, 3 ], [ 55, 25 ], [ 65, 40 ], [70, 80], [75, 80], [ 76, 3 ], and [ 86, 3 ]. Data were collected for each sample in both MS1 and MS/MS mode. For MS/MS runs, higher energy collision-induced dissociation (HCD) spectra were acquired in a data-dependent fashion. Detailed instrument parameters are given in Supporting Information.  Data Analysis. Raw MS1 LC/MS data files were converted to the .mzML format with an intensity threshold of 5000 counts by use of MSConvert43 and scored with HiTIME,39 using Δm/z of 3.01005 and 2.0067 (i.e., the 6.0201 Da difference between APAP and 13C6-APAP in the 2+ and 3+ charge states, respectively). For database searching of MS2 LC/ MS runs, Mascot generic format files were produced by use of MSConvert that contained the 100 most intense fragment ions in each HCD spectrum. Peptide and protein identifications were performed with Mascot44 (v2.4.1), and searches of the Uniprot database45 (release 2015_07) accounted for the possibility of one missed tryptic cleavage, (carbamidomethyl)cysteine, and oxomethionine variable modifications. The MS1 and MS2 m/z tolerances were set to 20 ppm and 0.2 Da, respectively. Further variable modifications were used in some searches as described in the text. Significance thresholds were set to achieve a global false discovery rate of 1% using a targetdecoy approach against sequence-reversed proteins.46 Proteins with two or more significant matches were retained.  Software Development. Xenophile, custom software to facilitate the analysis of CRM−peptide adducts, can be utilized through a graphical user interface and contains methods that allow the user to perform HiTIME searches, control sample baseline subtraction, postprocessing and review of HiTIME search results, nontargeted reactive metabolite detection, and targeted Mascot-HiTIME correlation (see Supporting Information for details).    RESULTS AND DISCUSSION\r\n  Microsomal Bioactivation of Acetaminophen. APAP and 13C6-APAP were incubated with liver microsomal preparation as a source of xenobiotic-metabolizing enzymes, including P450, and are known to bioactivate APAP to the reactive metabolite NAPQI.40,41 Following a 3 h incubation, microsomal proteins were separated by precipitation, digested with trypsin, and analyzed by LC/MS/MS. Mascot analysis of the data against the Uniprot database, allowing variable oxidation of methionine and carbamidomethylation of cysteine, resulted in the identification of 720 and 660 proteins that were common to all three replicates in APAP and control treatment groups, respectively (Figure 1; see Table S1 in Supporting ■ Information for lists of identified proteins). However, adduction of any metabolites formed from APAP would both change the total mass of the peptide and induce an offset in the ladder sequence ions formed in MS2 experiments, resulting in a mismatch between experimental spectra and theoretically predicted fragments generated by database searching. Adducted peptides will remain unassigned and, accordingly, these data do not account for the possibility of reactive metabolites covalently reacting with microsomal proteins.  Several methods could be used to recover the missed CRMmodified peptides. For example, Mascot searches could be directed to consider variable modifications at specific residues for predicted metabolites. However, this requires knowledge of both the identity of the reactive metabolite and the amino acid residue(s) likely to be modified, which runs counter to the goal of nontargeted identification. Blind post-translational modification (PTM) search algorithms47−49 that aim to identify modifications by various approaches based on sequence tags may be adequate in some instances; however, protein−CRM adducts are expected to be rare events relative to the thousands of proteins that can be detected in shotgun experiments, the abundances of modified peptides are likely to be low (often leading to noisy MS/MS spectra), and CRM modifications for a compound of interest are unlikely to be recorded in PTM databases such as UniMod.50 Any search algorithm would therefore be required to handle arbitrary mass offsets and account for modification on any residues, which would result in increased search times, false negatives, and false positives.51 However, reactive metabolites can be considered a distinct subset of all PTMs present in a given sample, and if these could be selectively detected, then global PTM searching would be unnecessary.  Due to the use of both native and 13C6-labeled APAP, this isotopic pattern will be carried through metabolic transformations, providing a signature unique to the products of APAP metabolism. By searching for this signature, peptides modified by APAP CRMs could be detected even though the peptide has not been assigned through database searching and the identity, and site of adduction, of the CRM may be unknown. The MS1 data were thus processed with HiTIME software capable of identifying twin-ion signals.39 The heat maps produced after scoring with an m/z change of 3.01005 and 2.0067 (i.e., the 6.0201 Da difference in the 2+ and 3+ charge states) for all replicates are shown in Figures S1 and S2, respectively. HiTIME scoring reveals multiple bright spots in the APAP incubations (Figure 2A) for which comparable points are not observed in the controls (Figure 2B), indicating that the data in those regions closely match the twin-ion signature. Investigation of highly scoring points reveals that these are true twin-ion hits. For example, the HiTIME hit at m/z 752 and 47.8 min suggests twin ions of m/z 752 and 755. The extracted ion chromatogram (EIC) traces of these ions overlap significantly in the region of 47.8 min (Figure 2C), and two peptide signals, both of the 2+ charge state, are evident in the mass spectrum (Figure 2D). These data indicate that this is a true twin ion corresponding to a peptide modified by a reactive metabolite derived from APAP in the 2+ charge state. For control samples, similarly high-scoring data regions as those corresponding to true twin-ion peptides in APAP treatment data were not observed (Figures S1 and S2), and no convincing case of a twin-ion peptide could be found upon inspection of EIC traces and mass spectra following postprocessing (Table S2).  The observation of numerous twin-ion hits in the drug treatment data indicates the presence of multiple peptides that have been modified by CRMs derived from APAP. Without assuming the identity of the CRM or the reactive site, identification of the modified peptides must be conducted by means other than standard database searching. Since CRM modifications are generally not quantitative and are formed in relatively low abundance, it is reasonable to assume that some quantity of the protein target's unmodified native counterpart will remain. As the native and CRM-modified peptides share the same base structure and differ only in the configuration of various PTMs, MS2 spectra of these peptides are likely to display sequence ions that are either common to both peptides or offset from one another by a constant amount that is related to the mass of the adducted metabolite. With this in mind, nontargeted peptide−CRM identification software was developed.  Nontargeted Peptide−CRM Identification Algorithm. The structural similarity of CRM-modified peptides and their native counterparts facilitates identification of the site of adduction and the mass of the CRM. However, significant amounts of complex data generated by LC/MS, HiTIME scoring, and Mascot analysis make manual interpretation impractical. Thus, semiautomated computer software has been developed that aids verification of twin ions and comparison of MS2 spectra for nontargeted analysis.  In the nontargeted CRM identification algorithm (Scheme 1A), a list of Mascot-assigned peptides is produced that fall within user-defined m/z ranges relative to a verified HiTIME hit (Figure 3). The sequences and PTMs of all Mascot-assigned Scheme 1. Workflow Diagrams for (A) Nontargeted CRM Identification and (B) Twin-ion-directed Targeted Peptide Adduct Identification peptides in this range are used to calculate the m/z values of a set of sequence ions appropriate for the ion activation method used. The match between the experimental MS2 spectrum of the HiTIME hit and each theoretical ion sets is then ranked according to Equation S1. This scoring is complicated by the fact that fragment ions in the HiTIME hit spectrum containing the CRM modification will be offset from those of the native spectrum by a mass related to the CRM. To address this problem, a rolling modification model was used that repeatedly scores each pair of MS2 spectra using m/z offsets for ions in the native peptide corresponding to modification at different sites. A series of theoretical spectra are generated where the m/z offset is iteratively moved along the peptide chain to simulate CRM adduction at different positions (Table S3). The m/z offset is taken to be equal to the mass difference between native and CRM-adducted peptides. Each theoretical spectrum is then independently scored against the experimental HiTIME MS2 spectrum, with the highest scoring match being the lead candidate.  The Mascot-assigned PTM profile of a peptide is used to account for the possibility that CRM adduction precludes other modifications. For example, carbamidomethylation of cysteine residues upon treatment of proteins with iodoacetamide during sample preparation would be prevented if the Cys residue were already modified with a CRM. This results in the mass of the native peptide being increased by 57 Da, and therefore this has to be accounted for when the mass of the CRM is calculated. The CRM mass is thus equal to the mass difference between the modified and native peptides plus the mass of any PTMs precluded by modification with the reactive metabolite.  The CRM mass determined from the most highly correlated MS2 spectra is used to determine possible chemical formulas. Although the number of formulas that are determined for a given Δm/z may be large, many of these will fall beyond the limits of chemically reasonable compositions. To restrict the search space, only formulas that are within user-specified elemental compositions are calculated. Further refinement can be achieved by noting that the chemical structure and composition of the metabolites should have elements in common with the starting material. The user is given the option of specifying the input structure of the test compound by use of a built-in molecule editor. The molecule is then split into fragments at rotatable bonds in an attempt to mimic metabolic reactions such as heteroatom dealkylation (Equation S2). An example is shown in Figure S4. The list of candidate CRM formulas is then compared to the molecular formulas of input molecule fragments and a residual mass error (RME) is calculated according to Equation S3, which quantifies total mass of atoms in the candidate formula that are not accounted for in the closest-matching input molecule fragment. This metric provides lower scores for formulas that more closely resemble the composition of the input molecule (or fragment thereof) and are thus more likely to be the true stoichiometry of the CRM.  Nontargeted Searching of APAP-Microsome Data. To detect the APAP-derived reactive metabolites, the APAP treatment HiTIME and Mascot data were analyzed with the nontargeted CRM detection algorithm, using the parameters provided in Tables S4 and S5. In this process, MS2 spectra associated with twin-ion peptides (unassigned by peptide database searching) are correlated with their non-CRMmodified counterparts, and the mass difference between these highest ranked pairs is taken as a potential CRM (Scheme 1A). The three highest-scoring hits for each of the three APAP treatment replicates are provided in Table 1 (replicate 1) and Tables S6 and S7 (replicates 2 and 3). Taken together, eight of these nine results detect a CRM with a mass of 149.05 Da that, in each case, arises from covalent modification of a cysteine residue. Two possible molecular formulas are identified that fit within the allowed atom stoichiometries and parts per million (ppm) error tolerances. These are C8H7NO2, identified as a possibility in all eight cases, and C6H5N4O, which is identified in five out of the eight cases. The average mass errors for these possible formulas are similar (8.0 and 6.1 ppm, respectively); however, the residual mass errors calculated versus the input APAP structure differ considerably (2 and 86 Da, respectively). This indicates that the former candidate, C8H7NO2, is very similar in composition to the APAP input molecule (APAP: C8H9NO2). Given that (i) C8H7N2O is consistently identified in multiple HiTIME hits and across all replicates, (ii) this formula is closely related to the APAP input molecule, and (iii) no chemically reasonable structure can be devised for the alternative, C8H7N2O is taken as the stoichiometry of the reactive metabolite formed from APAP in these incubations.  To interrogate the data underpinning this result, MS/MS spectra for the CRM-modified and native peptides were extracted and compared. For example, the native peptide EFTPCAQAAFQK (m/z 699.3) and its twin-ion counterpart (m/z 745.3) that were used to assign a CRM mass of 149.0481 Da are shown in Figure 4. A prominent y-ion series is observed, and the m/z values of these sequence ions are common to both spectra for y2 through to y7. However, these diverge beyond the y7 ion at m/z 763 (assigned as [AQAAFQK + H]+). For the native peptide spectrum (Figure 4A), the y8 and y9 ions, which now include a cysteine residue, are located at m/z 923 and 1020, respectively. While no prominent fragments are observed at these positions in the MS/MS spectrum of the twin-ion peptide, two additional fragments appear offset by +92 Da at m/z 1015 and 1112, indicating that the cysteine residue is differentially modified in the two peptides. Because the unmodified peptide is alkylated at the Cys residue (+57 Da aAPAP replicate 1 of 3.  sequence VFANPEDCAGFGK EFTPCAQAAFQK TIQLNVCNSEEVEK modification site  C (8) C (5) C (7) formula C8H7NO2 C8H7NO2 C6H5N4O C6H5N4O C8H7NO2 mass error (ppm)  residual mass (Da) relative to unalkylated peptide) and the twin-ion peptide has a CRM adducted to the Cys residue (+92 Da relative to the alkylated cysteine), the mass of the CRM (149 Da) is calculated from the sum of 57 and 92 Da. The assigned formula, C8H7N2O, is the molecular formula of the well-studied electrophilic NAPQI metabolite of APAP, which is known to bind covalently to the side chains of reduced cysteine residues in proteins.52 It should be emphasized that this CRM has been identified directly from shotgun proteomics data while making essentially no assumptions as to the identity of the metabolite, the protein, or the amino acid target of modification.    Targeted Search of APAP-Microsome MS2 Data.\r\n  Having determined the stoichiometry of the reactive metabolite formed from APAP and the amino acid that was modified by this metabolite, these data can be used to direct subsequent protein database searches. Mascot searches of the LC/MS/MS data were replicated with the addition of C8H7NO2 and 13C6C2H7NO2 (i.e., NAPQI and 13C6-NAPQI) as variable modifications of cysteine in order to directly identify peptides carrying the NAPQI modification.  In all cases, a large number of peptides were identified as APAP adducts by Mascot searching. For the APAP treatment samples, an average of 55 peptides were assigned as APAP or 13C6-APAP adducts and, interestingly, an average of 41 were assigned for control samples that were not exposed to APAP. It therefore seems highly unlikely that all of these can correspond to legitimate APAP−peptide adducts. For control samples, no APAP assigned peptides displayed the twin-ion signature upon manual inspection of their associated MS1 spectra, and this was also true of many hits in the drug treatment samples. These data indicate that targeted database searching alone may not always reliably identify the products of CRM−peptide adduction and may in fact give rise to false positives.  However, in these experiments, true modified peptides should be assigned as such by database search algorithms and also display the expected twin-ion shape. To restrict the data to only those peptides that meet these criteria, a targeted data refinement algorithm was developed that correlates CRM modifications assigned by database searching with HiTIME data (Scheme 1B). When this algorithm was run with HiTIME and Mascot data from microsomal protein digests (Table S8), eight peptides were found to be assigned as CRM adducts by Mascot that also appear as twin ions (Table 2) indicating that these are true CRM-modified peptides. For five of these eight cases, both the light and heavy peptides were separately massselected during data-dependent MS2 acquisition and subjected to HCD. For example, the MS2 spectra of the APAP and 13C6APAP adducts of the peptide EFTPCAQAAFQK2+ are shown in Figure S5 panels A and B, respectively. The series of y ions at m/z 1015, 1112, and 1213 in Figure S5A is offset from the analogous series of ions in Figure S5B at m/z 1021, 1118, and 1219 by 6 Da, which is equal to the nominal mass difference between APAP and 13C6-APAP. Taken together, these data provide a high level of confidence that the peptides assigned as APAP adducts by database searching and as twin ions in MS1 data are indeed true products of reactive metabolite adduction of proteins.  These eight modified peptides comprise partial sequences of seven unique proteins (Table 2). Several of these have been identified as the targets of reactive electrophiles by other researchers, providing additional confidence in these assignments. For example, glutathione S-transferase (GST) was first identified as a target for APAP adduction by radiometric counting in 1981,53 and its constituent peptides VFANPEDCAGFGK and VFANPEDCAGFGKGENAK (and homologous sequences from different species) have since been identified numerous times to be modified at the cysteine residue by NAPQI34,41,54 and electrophilic metabolites of other drug compounds.55 Furthermore, the triazine herbicide atrazine has been found to form an adduct with the HBB1 peptide EFTPCAQAAFQK in rats,56 and the FMO1 peptide  protein hepatic flavin-containing monooxygenase 1 hemoglobin β-1 chain microsomal glutathione S-transferase-I microsomal glutathione S-transferase-I methyltransferase-like protein 7B 3-hydroxybutyrate dehydrogenase glutathione S-transferase Mu 1 betaine-homocysteine S-methyltransferase 1 unique peptides peptide SCDLGGLWR EFTPCAQAAFQK VFANPEDCAGFGK VFANPEDCAGFGKGENAK HIGDGCHLTR TIQLNVCNSEEVEK KHHLCGETEEER QVADEGDALVAGGVSQTPSYLSCK modification APAP L/H C2 APAP L/H C5 APAP L C8 APAP L/H C8 APAP L/H C6 APAP L/H C7 APAP H C5  APAP L C23 aAPAPL and APAPH refer to the natural abundance APAP and 13C6-APAP, respectively. accession no.  FMO1_RAT HBB1_RAT MGST1_RAT MGST1_RAT MET7B_RAT BDH_RAT GSTM1_RAT BHMT1_RAT SCDLGGLWR was identified as a target of reactive metabolites formed from model furan-containing compounds in rat liver microsome incubations.55  The number of proteins identified is similar to and slightly greater than the number of proteins identified in closely related studies that used more targeted methodologies to identify APAP−protein adducts from rat liver microsome incubations.34 In addition, many compounds such as diclofenac and halothane are known to form specific adducts with selected targets despite the presence of large numbers of other proteins.27 These results are consistent with our results, which indicate that NAPQI forms relatively specific adducts with a limited number of microsomal proteins under these conditions.  The global biological and toxicological effects of reactive metabolite adduction to these specific proteins are largely unclear. Previous studies have demonstrated dose-dependent inhibition of the glutathione conjugation activity of GST by APAP which has been linked to NAPQI formation and adduction to Cys4757 and may have an impact on GSH/GSSG balance. Other proteins identified have roles in drug metabolism (FMO1), methionine synthesis (BHMT), and fatty acid catabolism and energy balance (BDH).  False Assignments and Limitations. To investigate the ability of Xenophile to identify twin-ion signals and correctly assign CRM masses and elemental compositions in a controlled manner, semisynthetic data sets were produced by superimposing artificially generated twin-ion signals onto experimental shotgun LC/MS data from the three control treatments for each cysteine-containing peptide, resulting in the incorporation of 560, 610, and 620 signals, respectively (see Supporting Information for details).  These data were scored with HiTIME (Figure S6), and comparison of the locations of synthetic twin ions with local maxima in the HiTIME data revealed that ∼95% of the synthetic signals were identified (Table S9). Many of the remaining 5% were found to heavily overlap with experimental signals, resulting in large distortions to the target peak shape.  False positive rates were found to be less than 10% for approximately the top 90% of targets (Figure S7). Running the nontargeted CRM detection algorithm resulted in ∼88% of the synthetic targets being assigned as NAPQI modifications to within 20 ppm, which increases to 98% upon relaxing this threshold to 100 ppm (Figure S8 and Table S9). The confidence in these assignments was investigated by analyzing the difference in correlation score between the first and second ranked peptide, which provides a metric of the margin by which the top-ranked hit was chosen over the next candidate (Figure S9). Confidence in CRM assignments was ca. 90% or better for the highest-scoring hits and decreased to ca. 60% for lowerscoring hits, indicating that the correct peptide correlations are well separated from their closest counterparts. For large CRMs, many chemical formulas may have the same nominal mass. To profile the number of formulas as a function of molecular weight, various modifications were computationally applied to 1337 marketed drugs, and formulas were generated for the products in a manner analogous to that of the Xenophile software. The number of hits within 100 ppm of the target mass rises rapidly with molecular weight, reaching ∼100 candidates for a mass of 500 Da. However, this decreases to ∼20 hits when tolerances are tightened to 20 ppm (Figure S10). This number can be further refined by use of residual mass values, which quantify the proportion of a CRM formula that is not explained by the administered xenobiotic. These should lie within certain ■ ■ ranges corresponding to common metabolic reactions. By conservative application of this rationale, the number of hits is reduced to ca. 5 for a 500 Da compound, which is feasible for manual user review.  It should be noted that reproducing the complexity and nuances of experimental data with synthetic spectra is difficult, and therefore these results likely represent a best-case scenario.  However, fundamentally, the detection limit for CRM-modified peptides using the software approach presented here should be similar to that for any standard peptides using typical workflows. That is, detection is limited by the ability of the mass spectrometer to ionize, fragment, and detect a peptide. As an approximation of the sensitivity of this experiment, ca. 4 μg of protein was injected onto the column, resulting in the detection of 1021 proteins in each sample (on average) with a mean molecular mass of 52 794 Da. This equates to an average protein loading of ca. 75 fmol. It should be noted that CRMmodified peptides identified here were likely of substantially lower abundances, as CRM adduction generally results in low yields. For example, the APAP-modified peptide VFANPEDCAGFGK (Figure 2C,D) is detected at ca. 20-fold lower abundance than its native counterpart.  One limitation of the software presented here is that reactive metabolites with a heavy/light offset of ca. &lt; 4 Da will produce twin-ion signals with overlapping isotope distributions, which may hinder twin-ion assignments. Computational strategies are currently being planned that may account for these cases and will be included in future Xenophile updates.    CONCLUSION\r\n  A new nontargeted method to identify protein adducts of chemically reactive metabolites from LC/MS data without prior knowledge of the CRM identity has been developed. This method involves two distinct steps. The first step identifies the molecular formula of a CRM as well as the amino acid residue site of adduction on the peptides by searching for pairs of native and CRM-modified peptides that have similar MS2 fragmentation behavior. The second step uses this information to direct further peptide database searches that are specifically parametrized to include the possibility of residue modifications identified in the first step. This methodology was demonstrated for the bioactivation of acetaminophen to the reactive metabolite NAPQI in hepatic microsomes. Nontargeted searching identified a reactive metabolite adducted to cysteine residues with a formula of C8H7NO2, which is consistent with the molecular formula and known residue reactivity of NAPQI. Upon subsequent Mascot searches, eight unique NAPQImodified peptides were identified that arise from seven distinct proteins, including three that are known targets of NAPQI adduction. This approach should prove useful for identifying the protein targets of chemically reactivate metabolites to within a single amino acid residue, without prior knowledge of the metabolite(s) or protein target(s). It is anticipated that this powerful method should be generally applicable to study the mechanisms of toxicity induced by many different compounds.    ASSOCIATED CONTENT\r\n  *S Supporting Information The Supporting Information is available free of charge on the ACS Publications website at DOI: 10.1021/acs.analchem.6b04604. ■  Additional text with experimental details; 10 figures including HiTIME heatmaps for additional replicates of APAP and control microsomal protein digests, histograms of HiTIME scores, and sample MS2 spectra of peptides identified by targeted correlation; eight tables listing possible CRM formulas identified by nontargeted searching and parameters used for nontargeted CRM searching and targeted results correlation; three equations used to score MS2 spectra correlation, identify rotatable bonds, and calculate residual mass error values; six screenshot images of the analysis software user interface (PDF)  One table with lists of proteins identified (XLSX)    AUTHOR INFORMATION\r\n   Corresponding Authors\r\n  *E-mail m.leeming@student.unimelb.edu.au (M.G.L.). *E-mail w.donald@unsw.edu.au (W.A.D.). *E-mail rohair@unimelb.edu.au (R.A.J.O.).    ORCID\r\n  Michael G. Leeming: 0000-0001-8981-0701 Richard A. J. O'Hair: 0000-0002-8044-0502    Notes\r\n  The authors declare no competing financial interest.     ACKNOWLEDGMENTS\r\n  Financial support from the University of Melbourne Interdisciplinary Seed Grant program is gratefully acknowledged. M.G.L. thanks the Elizabeth and Vernon Puzey Foundation for the award of a Ph.D. scholarship and The University of Melbourne for a Norma Hilda Schuster scholarship. W.A.D. thanks the Australian Research Council f o r a D i s c o v e r y E a r l y C a r e e r R e s e a r c h e r A w a r d (DE130100424) and a Discovery Project (DP160102681). We gratefully acknowledge helpful discussions with Ching-Seng Ang. We thank James Ziogas, Andrew Isaac, Bernard Pope, and David Perkins for useful discussions.    ",
    "sourceCodeLink": "https://github.com/mgleeming/Xenophile",
    "publicationDate": "0",
    "authors": [
      "Michael G. Leeming",
      "William A. Donald",
      "Richard A. J. O'Hair"
    ],
    "status": "Success",
    "toolName": "Xenophile",
    "homepage": ""
  },
  "70.pdf": {
    "forks": 459,
    "URLs": [
      "www.continuum.io/downloads",
      "github.com/bioconda/bioconda-recipes",
      "github.com/pnnl/fqc"
    ],
    "contactInfo": ["joseph.brown@pnnl.gov"],
    "subscribers": 24,
    "programmingLanguage": "Shell",
    "shortDescription": "Conda recipes for the bioconda channel.",
    "publicationTitle": "FQC Dashboard: integrates FastQC results into a web-based, interactive, and extensible FASTQ quality control tool",
    "title": "FQC Dashboard: integrates FastQC results into a web-based, interactive, and extensible FASTQ quality control tool",
    "publicationDOI": "10.1093/bioinformatics/btx373",
    "codeSize": 146866,
    "publicationAbstract": "Summary: FQC is software that facilitates quality control of FASTQ files by carrying out a QC protocol using FastQC, parsing results, and aggregating quality metrics into an interactive dashboard designed to richly summarize individual sequencing runs. The dashboard groups samples in dropdowns for navigation among the data sets, utilizes human-readable configuration files to manipulate the pages and tabs, and is extensible with CSV data. Availability and implementation: FQC is implemented in Python 3 and Javascript, and is maintained under an MIT license. Documentation and source code is available at: https://github.com/pnnl/fqc. Contact: joseph.brown@pnnl.gov",
    "dateUpdated": "2017-10-19T11:28:12Z",
    "institutions": ["Pacific Northwest National Laboratory"],
    "license": "No License",
    "dateCreated": "2015-09-12T20:33:30Z",
    "numIssues": 287,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx373   FQC Dashboard: integrates FastQC results into a web-based, interactive, and extensible FASTQ quality control tool     Joseph Brown  1    Meg Pirrung  0    Lee Ann McCue  1    0  Computation and Analytics Division, Pacific Northwest National Laboratory ,  Richland, WA 99352 ,  USA    1  Earth and Biological Sciences Division, Pacific Northwest National Laboratory ,  Richland, WA 99352 ,  USA     2017   1  1  3   Summary: FQC is software that facilitates quality control of FASTQ files by carrying out a QC protocol using FastQC, parsing results, and aggregating quality metrics into an interactive dashboard designed to richly summarize individual sequencing runs. The dashboard groups samples in dropdowns for navigation among the data sets, utilizes human-readable configuration files to manipulate the pages and tabs, and is extensible with CSV data. Availability and implementation: FQC is implemented in Python 3 and Javascript, and is maintained under an MIT license. Documentation and source code is available at: https://github.com/pnnl/fqc. Contact: joseph.brown@pnnl.gov       -\r\n  *To whom correspondence should be addressed. Associate Editor: Jonathan Wren    1 Introduction\r\n  Quality control (QC) tools geared towards next-generation sequencing need to adapt to an ever-changing list of requirements and the expectations of different users. Many tools have been developed that address specific QC requirements, though few provide comprehensive results. We address some of these shortfalls through data aggregation and visualization within an extensible and interactive dashboard that simplifies the identification of points of failure for the individuals generating data as well as providing data quality metrics to the researchers using the data for downstream analyses.  FASTQ files containing nucleotide sequence data with associated quality scores can be summarized in several ways. FastQC (http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) is a widelyadopted application in FASTQ QC as it summarizes read quality by position, informs the user of adapter content in sequences, reports on tetramer frequencies, and many other aspects one would expect to glean from the raw sequence data.  FQC Dashboard addresses QC capturing by utilizing FastQC on the backend, taking advantage of its speed and versatility in analyzing single files, and placing those results within a web server with extensible capabilities. FQC's web framework is built around the same standard metrics obtained from FastQC, but with the ability to aggregate results of multiple sequence runs and easily navigate between them. FQC is intended to be used as an interactive website with dynamically generated plots, which can be readily extended to include additional sequence runs and to add custom plots. This extensibility, and the ability to aggregate multiple sequence runs as individual pages, distinguishes FQC from similar tools  (e.g. MultiQC; Ewels et al., 2016)  designed to summarize FASTQ data sets as individual or groups of samples, but that lack the ability to display multiple, single-sample reports in a unified dashboard; the goal of this design is to enable sequencing facility personnel to identify and track issues with runs.    2 Approach\r\n  FQC Dashboard is a combination of a command line interface (CLI) written in Python 3, which depends on FastQC for processing FASTQ files, a frontend website written in JavaScript, HTML, and CSS, that utilizes Highcharts (http://www.highcharts.com) and D3 (https://d3js.org) Javascript libraries for plotting, and Bootstrap.js (https://getbootstrap.com) for styling and interactivity. These packages are included in the source code repository. The CLI is a single executable with submodules for (i) FASTQ processing of either single- or paired-end data, (ii) batch FASTQ processing based on directory searching and (iii) adding custom plots onto existing dashboard pages. The only dependencies outside of cloning the source code repository are Python 3, which can be installed using Anaconda (https://www.continuum.io/downloads), and FastQC, which can be installed using the Bioconda channel (https://github.com/bioconda/bioconda-recipes).  Plots and tables are dynamically generated from CSV files and configured by the user to be displayed within FQC's supported visualizations: table, line plot, bar plot, area range plot, heatmap or plate heatmap. Configuration is set using JSON files which define the biological samples and sample groups as well as which plots to display and their respective user settings. The default settings for FQC will generate a standard QC analysis from FastQC without the need for the user to edit files, though by modifying the configuration files, the user can personalize and add plots relevant to their analysis protocol.    3 Application\r\n  An FQC analysis starts with the CLI, which simplifies executing QC on FASTQ files by aggregating QC data like quality by position, sequence content, GC content, and adapter content, and generating the FQC dashboard's configuration files. The default configuration files and settings are determined by the results of FastQC per sample and written to the user's server data directory. The CLI handles combining paired-end QC data by compiling data from the forward and reverse strand into single plots where applicable, or adding R1 and R2 tabs for more complex plots that should not be combined, like sequence content.  The dashboard (Fig. 1) provides a single place for laboratory personnel and researchers to explore the data generated from the command line. Dashboard viewing selections are made from groups (Fig. 1B), which are sets of samples, and then from a list of the samples, which can be comprised of single-end or paired-end sequence QC. Samples are selected and explored individually (Fig. 1B) and can contain many plot pages, each of which can have many subplots which are displayed as tabs in the plot window (Fig. 1C). The user has full control over the parameters for every plot, including axes labels, legend colors and labels, plot color zones, titles, subtitles and tab names.  The extensibility of the dashboard provides various ways to employ the visualizations in practice. In a sequencing facility, FQC can be used to plot abundances across reagent plates to show the spatial relationships of read abundance and illustrate effects of a positive control (Fig. 1D) or view amplification bias per plate (Fig. 1E), crucial information for internal QC. The distribution of reads across barcodes can be viewed as a Lorenz curve to quickly evaluate loading concentrations (Fig. 1F). Additional associated data or summary data that is not suitable for plotting can be displayed as a table with pagination and sortable columns.    4 Conclusion\r\n  FQC can track standard FASTQ quality metrics while serving a website and being trivially extensible with additional CSV data. The CLI wraps FastQC and builds the website with default QC metrics upon which one can expand without additional programming. The CLI and dashboard lower the threshold of performing and following up on quality issues that may be apparent upon visual inspection and it promotes evidence-based protocol changes in sequencing facilities to generate better quality data.    Funding\r\n  This research was supported by the Microbiomes in Transition Initiative LDRD Program at the Pacific Northwest National Laboratory, a multi    ",
    "sourceCodeLink": "https://github.com/bioconda/bioconda-recipes",
    "publicationDate": "0",
    "authors": [
      "Joseph Brown",
      "Meg Pirrung",
      "Lee Ann McCue"
    ],
    "status": "Success",
    "toolName": "bioconda-recipes",
    "homepage": "https://bioconda.github.io"
  },
  "4.pdf": {
    "forks": 4,
    "URLs": [
      "egg2.wustl.edu/roadmap/web_portal/",
      "bioconductor.org/packages/devel/bioc/html/motifmatchr.html",
      "CRAN.R-project.org/package=Rtsne",
      "www.github.com/GreenleafLab/motifmatchr",
      "www.github.com/GreenleafLab/chromVa",
      "www.github.com/GreenleafLab/chromVAR",
      "www.nature.com/reprints/index.html"
    ],
    "contactInfo": [],
    "subscribers": 9,
    "programmingLanguage": "C++",
    "shortDescription": "Fast motif matching in R",
    "publicationTitle": "chromVar : inferring transcription-factor- associated accessibility from single-cell epigenomic data",
    "title": "chromVar : inferring transcription-factor- associated accessibility from single-cell epigenomic data",
    "publicationDOI": "10.1038/nmeth.4401",
    "codeSize": 150,
    "publicationAbstract": "single-cell atac -seq (scatac ) yields sparse data that make conventional analysis challenging. We developed chromVar (http://www.github.com/GreenleafLab/chromVa r ), an r package for analyzing sparse chromatin-accessibility data by estimating gain or loss of accessibility within peaks sharing the same motif or annotation while controlling for technical biases. chromVar enables accurate clustering of scatac -seq profiles and characterization of known and de novo sequence motifs associated with variation in chromatin accessibility.",
    "dateUpdated": "2017-08-11T08:34:33Z",
    "institutions": [
      "Broad Institute of MIT and Harvard",
      "Stanford University",
      "Stanford University School of Medicine",
      "Harvard University"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2017-01-19T20:17:16Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     10.1038/nmeth.4401   chromVar : inferring transcription-factor- associated accessibility from single-cell epigenomic data     Alicia N Schep  1  2    Beijing Wu  1  2    Jason D Buenrostro  0  3    William J Greenleaf  1  2    0  Broad Institute of MIT and Harvard ,  Cambridge, Massachusetts ,  USA    1  Center for Personal Dynamic Regulomes, Stanford University ,  Stanford, California ,  USA    2  Department of Genetics, Stanford University School of Medicine ,  Stanford, California ,  USA    3  Harvard Society of Fellows, Harvard University ,  Cambridge, Massachusetts ,  USA. addressed to J.D.B     single-cell atac -seq (scatac ) yields sparse data that make conventional analysis challenging. We developed chromVar (http://www.github.com/GreenleafLab/chromVa r ), an r package for analyzing sparse chromatin-accessibility data by estimating gain or loss of accessibility within peaks sharing the same motif or annotation while controlling for technical biases. chromVar enables accurate clustering of scatac -seq profiles and characterization of known and de novo sequence motifs associated with variation in chromatin accessibility.       -\r\n  The binding of transcription factors to cis-regulatory DNA sequences controls gene expression programs that define cell phenotype. Chromatin-accessibility assays have enabled the discovery of cis-regulatory elements and trans-acting factors across cell states and cell types1, while single-cell sequencing methods have allowed the deconvolution of dynamic or diverse cellular populations2,3. Recently, it has become possible to probe chromatin accessibility within single cells4-6 and thus to potentially identify cis- and trans-regulators that bring about diverse cellular phenotypes.  However, the exceedingly sparse nature of single-cell epigenomic data presents unique computational challenges. Sparsity is intrinsic to this data, since the signal at any genomic locus is fundamentally limited by DNA copy number; only 0, 1 or 2 reads can be generated from elements within a diploid genome. Methods developed for single-cell RNA-seq have shown that measuring the dispersion of gene sets, such as Gene Ontology or coexpression modules, rather than individual genes can be a powerful approach for analyzing sparse data7. In this vein, and building on previous work from our group and others4,8,9, we have developed chromVAR, a versatile R package for analyzing sparse chromatin-accessibility data by measuring the gain or loss of chromatin accessibility within sets of genomic features while controlling for known technical biases in epigenomic data (http://www.github.com/GreenleafLab/chromVAR and Supplementary Software). We show that chromVAR can be used to identify transcription factor (TF) motifs that characterize different cell types and vary within populations.  The chromVAR package takes as inputs (i) aligned sequencing reads, (ii) chromatin-accessibility peaks (determined from either aggregate single-cell data or a bulk reference), and (iii) a set of chromatin features representing either motif position weight matrices (PWMs) or genomic annotations (Fig. 1a, Supplementary Fig. 1 and see Online Methods). As an input of chromatin features, we have curated a set of human and mouse PWMs from the cisBP database10. This set represents a diverse and comprehensive collection of known TF motifs. Alternately, user-provided TF motifs or other genomic annotations, such as enhancer modules, ChIP-seq peaks, or GWAS disease annotations, may be used. chromVAR can also be applied to a collection of k-mers-short DNA sequences of a specific length k-in order to perform an unbiased analysis of DNA sequence features that correlate with chromatin-accessibility variation across cells or samples.  For each motif and cell, chromVAR first computes a 'raw accessibility deviation', the difference between the total number of fragments that map to peaks containing the motif and the expected number of fragments (based on the average of all cells). This aggregation across peaks yields a motif signal that is considerably less sparse than the signal within individual peaks. However, the aggregation can also amplify technical biases between cells that are caused by PCR amplification or variable Tn5 tagmentation conditions (Supplementary Note 1). These biases can lead to differences in the number of observed fragment counts between cells based on the GC content or mean accessibility of a given peak set (Supplementary Fig. 2). To account for these technical confounders, 'background' peak sets are created for each annotation; these sets comprise an equal number of peaks matched for GC content and average accessibility (Supplementary Figs. 2-5). The raw accessibility deviations for background peak sets are used to compute a bias-corrected deviation and z-score for each annotation and cell; this provides a differential measure of the gain or loss of accessibility of a given genomic annotation relative to the average cell profile (see Online Methods). This measure can be used for a number of downstream analyses, including de novo clustering of cells and identification of key regulators that vary within and between cell types. The chromVAR package contains a collection of tools for such downstream analysis; these tools include an interactive web application for exploring the relationship    Bulk\r\n    Chromatin\r\n    Accessibility\r\n    Profile\r\n    TF motifs\r\n    Peaks\r\n  a For every motif, k-mer, or annotation and each cell or sample, compute: . d e v r e s e r s t h g i r l l A . e r u t a N r e g n i r p S f o t r a p , . c n I , a c i r e m A e r u t a N 7 1 0 2 © f igure | chromVAR enables interpretable analysis of sparse chromatin-accessibility data. (a) chromVAR aggregates chromatin accessibility across peaks that share a common feature (e.g., a motif) and applies bias correction to generate scores for each cell or sample that can be used for downstream analysis. (b) Pearson correlation of bias-corrected deviations for 77 samples from different hematopoietic populations before and after downsampling total sequencing reads from full data. Each point represents one of the top 20% most variable motifs (s.d. of z-score); three of the most variable motifs are highlighted. (c) tSNE visualization of different samples using normalized deviations calculated from data downsampled to 10,000 fragments per sample, a typical amount from a single cell. Left panel, cell types of the hematopoietic differentiation hierarchy. HSC, hematopoietic stem cell; MPP, multipotent progenitor cells; CMP, common myeloid progenitor; GMP, granulocye-macrophage progenitor; LMPP, lymphoid-primed multipotential progenitors; CLP, common lymphoid progenitor; Mono, monocyte; MEP, megakaryocytic-erythroid progenitor; Ery, erythroid; B, B cells; CD4, CD4+ T-cells; CD8, CD8+ T cells; NK, natural killer cells. In the left graph, each point (downsampled cell equivalent) is colored by cell type. In all other graphs, cells are colored by the deviation score for the indicated motif. between key TF motifs and cell clusters (Supplementary Fig. 6).  We have also incorporated tools for generating previously described analyses characterizing the correlation and potential cooperativity between two TF-binding sites within the same regulatory element and for computing chromatin variability across regions in cis4.  To test whether our computational workflow could be applied to single-cell data, we measured the robustness of chromVAR outputs to downsampling of bulk ATAC-seq data from a deeply sequenced set of hematopoietic cell types8. TF motif deviations using 106 to 5 × 103 fragments per sample are highly correlated to those determined using the full data set (Fig. 1b and Supplementary Fig. 7). The clustering accuracy using the bias-corrected deviations is also largely preserved after downsampling, and generally outperforms clustering using PCA or other peak-based approaches (Supplementary Fig. 7; see Online Methods).  chromVAR provides robust results for 10,000 fragments per cell, which is a typical yield for a single cell using scATAC-seq4 (Supplementary Fig. 7). By projecting the vector of biascorrected deviations from individual cells onto two dimensions using tSNE11, chromVAR enables the reconstruction of the major hematopoietic lineages using 10,000 fragments per sample. With this analytical framework, we can also visualize the TFs associated with significant chromatin accessibility within each single-cellequivalent epigenome, thereby correctly identifying known master regulators of hematopoiesis, including HOXA9, SPI1, TBX21, and GATA1 (refs. 12-15) (Fig. 1c).  Next, we characterized chromVAR's ability to capture biologically relevant chromatin variability from scATAC-seq data drawn from multiple distinct hematopoiesis-related cell lines and human samples (Supplementary Fig. 8). Using tSNE with bias-corrected deviations for motifs and seven-mers, we clustered single cells into distinct cell types and observed individual motifs that best distinguish each cell type (Fig. 2a). Well defined, distinct clusters are formed in this tSNE projection when using the bias-corrected deviations; whereas clustering is confounded by technical biases when using raw deviations without bias correction. This approach for classifying cell types also compares favorably with performing tSNE on counts within peaks using a variety of alternative similarity measures (Supplementary Fig. 9).  Interestingly, we observe that cells from acute myeloid leukemia (AML) patients cluster between lymphoid-primed multipotent progenitors (LMPPs), monocytes, and HL60 (an AML-derived cancer cell line) cells. In this unsupervised analysis, the AML leukemic stem cells are more similar to LMPPs, while the AML blasts are more similar to monocytes. In addition, patient 1 (AML blast 1) maintains a more stem-like state compared to patient 2 (AML blast 2), as was anticipated from alternate analyses of these cells16. By layering cell-specific z-scores onto this projection, we can identify TFs that may promote the stem-like versus differentiated leukemia phenotype. For example, the master regulators of myeloid cell development SPI1 (PU.1) and CEBPA17 appear to be the most differential motifs between AML leukemic stem cells (LSCs) and blasts (Fig. 2b,c). a b . d e v r e s e r s t h g i r l l A . e r u t a N r e g n i r p S f o t r a p , . c n I , a c i r e m A e r u t a N 7 1 0 2 © f igure | chromVAR can be used to cluster single cells and interpret motifs underlying chromatin-accessibility variation. (a) tSNE visualization of 1,561 single cells based on chromVAR raw (left) or bias-corrected deviations (right) for motifs and seven-mers (see Online Methods).  Top panels, points colored by cell type. Bottom panels, points colored by raw (left) or bias-corrected (right) deviations for a set of random peaks with high GC content and high average accessibility (the bias set).  AML blast, acute myeloid leukemia blast cells; AML LSC, acute myeloid leukemia leukemic stem cells; LMPP, lymphoid-primed multipotential progenitors; Mono, monocyte; HL60, HL-60 promyeloblast cell line; TF1, TF-1 erythroblast cell line; GM, GM12878 lymphoblastoid cell line; BJ, BJ human fibroblast cell line; H1, H1 human embryonic stem cell line. (b) Volcano plot showing the mean difference in bias-corrected accessibility deviations (left) and variability (right) for each motif between the AML blast (n = 122) and LSC cells (n = 144) versus the −log10(P value) for that difference. (c) tSNE with bias-corrected deviations for AML blast and LSC, monocyte, LMPP, and HL60 cells (n = 509). Top panel, points colored by cell type. Lower panels, points colored by deviation z-scores for CEBPA (center) and ZEB1 (bottom).  In addition to cell similarity, we can visualize motif and k-mer activity patterns across cells by inverting the tSNE analysis (Fig. 3a).  By plotting motif or k-mer similarity in tSNE subspace, clusters representing several different TF families can be identified. Different TFs within the same family (e.g., GATA1 and GATA2) often bind highly similar motifs, and therefore chromVAR alone cannot distinguish which regulator binds a particular TF motif. In the motif and k-mer similarity visualization, most, but not all, k-mers cluster with a known motif, suggesting that k-mer analysis may enable de novo discovery of previously unannotated motifs.  By comparing the variation in chromatin accessibility across cells between highly similar k-mers, we can identify critical bases associated with chromatin-accessibility variation. For example, the AGATAAG k-mer, which closely matches the GATA1 motif, is highly variable across single cells, whereas most k-mers differing by one nucleotide share little or no variability (Fig. 3b and Supplementary Fig. 10). The mismatched k-mer with the greatest correlated variability is TGATAAG, which matches the sequence with the second highest score for the GATA1 motif.  Similarly strong sequence specificity is seen across other variable motifs (Supplementary Fig. 10).  We can use these comparisons of variation between highly similar k-mers to construct de novo motifs representing sequences associated with variation in chromatin accessibility. Briefly, we use the covariance between highly variable 'seed' k-mers and k-mers that differ by one mismatch or partially overlap the seed k-mer to assign nucleotide weights at each position of the motif model (Supplementary Fig. 11; see Online Methods). Many de novo motifs assembled using this approach closely match known motifs (Fig. 3c-f and Supplementary Fig. 11). For constructed motifs lacking a close match to a known TF, we confirmed an association with DNase hypersensitivity variation between samples in the Roadmap Epigenomics Project18 (Supplementary Fig. 12). These de novo motifs are thus associated with chromatin accessibility variation in two distinct accessibility assays. To further validate the discovery of these putative trans-regulators, we calculated aggregate TF 'footprints', measures of the DNase or Tn5 cut density around the given motif, and found a diverse set of accessibility profiles (Supplementary Fig. 12). Interestingly, several of these motifs do not match canonical narrow (~20 bp) TF footprints; rather, they are associated with a large footprint (&gt;20 bp) that is potentially indicative of larger regulatory complexes that would protect a larger region of DNA.  We envision that chromVAR will be broadly applicable to single-cell and bulk epigenomics data to provide an unbiased characterization of cell types and the trans-regulators that define them.  In support of this, we analyzed two bulk chromatin-accessibility data sets18,19 downsampled to 10,000 fragments per sample and data from an alternate scATAC-seq approach, and we found that chromVAR generalizes to these additional data (Supplementary Figs. 13-15 and Supplementary Note 3). As methods for measuring the epigenome in single cells and bulk populations continue to improve in throughput and quality, scalable analytical tools are needed. Analysis workflows for ATAC-seq or DNase-seq data often include the identification of motifs enriched in differentially accessible peaks, but such approaches scale poorly to comparisons across many sample types and require sufficient per-locus read depth to determine differential peak accessibility (Supplementary Note 4).  In contrast, chromVAR is highly robust to low sequencing depth and readily scales to hundreds or thousands of cells or samples.  Researchers often face a trade-off between the number of samples to sequence and the sequencing depth for each sample; sparse sequencing coupled with chromVAR analysis may enable the use of 'bulk' ATAC-seq, DNase-seq or other epigenomic methods as large-scale screening tools. We also anticipate that chromVAR will enable additional downstream analyses of single-cell chromatinaccessibility data, as vectors of bias-corrected deviations provide a powerful, dimensionality-reduced input to existing algorithms for inferring spatial and temporal relationships between cells. methods Methods, including statements of data availability and any associated accession codes and references, are available in the online version of the paper.  Note: Any Supplementary Information and Source Data files are available in the online version of the paper. ackno WLedGments This work was supported by National Institutes of Health (NIH) P50HG007735 (to W.J.G.), U19AI057266 (to W.J.G.) HG00943601 (to W.J.G.), the Rita Allen Foundation (to W.J.G.), and the Baxter Foundation Faculty Scholar Grant and    Single-nucleotide mismatch A C\r\n  G T 1 2 3 4 5 6 7    Position\r\n  c De novo 1    GATA1\r\n  De novo 6  NRF1 De novo 2 SPI1 De novo 5    RELA\r\n  De novo 4 ZNF354C De novo 10  PROP1 d e f a ) itry 25 a li m i s if to 0 m ( 2 m id-25 E N S t the Human Frontiers Science Program (to W.J.G). W.J.G. is a Chan Zuckerberg Biohub investigator. J.D.B. acknowledges support from the Harvard Society of Fellows and Broad Institute Fellowship. A.N.S. acknowledges support from the National Science Foundation (NSF) GRFP (DGE-114747). We thank C. Lareau for valuable suggestions for improvements on the package as well as members of the Greenleaf and Buenrostro labs for useful discussions. author contributions A.N.S., J.D.B., and W.J.G. conceived the project and wrote the manuscript. A.N.S. wrote the chromVAR R package and performed the analyses with input from J.D.B. and W.J.G. B.W. generated the scATAC-seq data. comPetin G financia L interests The authors declare competing financial interests: details are available in the online version of the paper. r eprints and permissions information is available online at http://www.nature.com/reprints/index.html. Publisher's note: springer nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. . d e v r e s e r s t h g i r l l A . e r u t a N r e g n i r p S f o t r a p , . c n I , a c i r e m A e r u t a N 7 1 0 2 © . d e v r e s e r s t h g i r l l A . e r u t a N r e g n i r p S f o t r a p , . c n I , a c i r e m A e r u t a N 7 1 0 2 © on Line methods chromVAR algorithm. Bias-corrected deviations and z-scores. For each motif (or k-mer or genomic annotation), a 'raw accessibility deviation' for each cell or sample is computed, representing the total accessibility of peaks with that motif minus the expected count based on the accessibility profile of all cells, all divided by that expected count (Supplementary Fig. 1). Using the matrix of fragment counts in peaks X, where xi,j represents the number of fragments from cell i in peak j, and the matrix of motif matches M, where mk,j is 1 if motif k is present in peak j. The total number of reads mapping to every peak containing motif k in cell i is given by M × XT . For each peak, the expected number of fragments per cell E is computed as the fraction of all fragments across all cells mapping to that peak multiplied by the total number of fragments in peaks for that cell:  E = Σi=1xi, j  × ∑ xi, j Σ j=1Σi=1xi, j j=1 The expected number of fragments mapping to every peak containing motif k in cell i is then given by M × ET, and the raw accessibility deviation Y by:  Y =  M × XT − M × ET  M × ET For each motif or genomic annotation, background peak sets are sampled that match the set of peaks with the motif or genomic annotation in terms of the distribution of GC content and average accessibility. These background peak sets are determined by finding possible background peaks for each peak, as described in the next section. For each background iteration, we can represent the background peak assignments as a matrix B, where bi,j\u2032 is 1 if peak j has peak j\u2032 as its background peak and 0 otherwise. A background motif match matrix M\u2032 is thus computed as M\u2032 = M × B , and a background raw deviation as:  Y \u2032 = (M × B) × XT − (M × B) × ET  M × ET .  Y\u2032 is calculated for each background iteration, and these background deviations are used to compute a bias-corrected deviation as Y−mean (Y\u2032). A deviation z-score is computed by dividing the bias-corrected deviation by the s.d. of the background raw deviations for each iteration:  Y − mean(Y \u2032)  s.d.(Y \u2032)  Background peak selection. The state space of GC content and the log of the average accessibility of peaks is transformed by the Mahalanobis transformation in order to remove the correlation between the two variables. This transformed space is split into an even grid of bins with a specified number of divisions (50) along each axis evenly spaced between the minimum and maximum values. For a peak in a given bin j, the probability of selecting another peak x in bin i is given by: Where f is the probability distribution function of the normal distribution with mean 0 and s.d. w (set to 0.01), and n is the number of peaks in the bin j.  Variability. The variability of a TF motif across samples or cells was determined by computing the s.d. of the z-scores across the cells or samples. The expected value of this metric is 1 if the motif peak sets are no more variable than the background peak sets for that motif.  De novo motif assembly. As a measure of the shared variability in chromatin accessibility between a reference k-mer (or motif ) and other k-mers (or motifs), we compute a normalized covariance based on deviation z-scores. This normalized covariance is simply the covariance of the z-scores across each cell divided by the variance of the z-scores for the reference k-mer (or motif ).  For assembling de novo motifs, we start with the k-mer associated with the greatest variability in chromatin accessibility across the cells as a 'seed' k-mer. We first find the distribution of the normalized covariances between that seed k-mer and all other k-mers with an edit distance from that seed k-mer of at least 3; this distribution is used as a null distribution for testing the significance of the observed covariances for k-mers with a single nucleotide mismatch using a Z-test. For each position along the k-mer, the nucleotide of the seed k-mer is given a weight of 1. Each alternate nucleotide is given a weight of 0 if the P value for the normalized covariance of the k-mer with that mismatch is greater than 0.05; if the P value is less than 0.05, the nucleotide is given a weight equal to the square of the normalized covariance. The weights for each base pair are then normalized to sum to 1. To further extend the de novo motif, we used k-mers overlapping the seed k-mer with an offset of one or two bases. For the two bases immediately outside the seed k-mer, the weighting of each nucleotide is given by x × y2 + (1 − x) × 0.25, where y2 is the square of the normalized covariance for the k-mer with the given nucleotide offset (if significant at 0.05 and otherwise 0), and x is the maximum value of the normalized covariances for the four k-mers (bounded by 0 and 1). For the bases offset by two base pairs from the seed k-mer, the weighting is computed in the same way, except that there are four possible k-mers with a given nucleotide at that position that overlap the seed k-mer; only the k-mer with the maximum normalized covariance with the seed k-mer is used (Supplementary Fig. 11).  Input data and preprocessing. ATAC-seq, scATAC, and DNase data. In addition to the previously published data, we generated three new biological replicates of single-cell K562s (ATCC; validated using STR genotyping (Genetica DNA laboratories)), representing three individual experiments on different days, using the previously published protocol4,8. Bulk ATAC-seq and scATAC-seq data were aligned and filtered as previously described4,8. Uniformly processed DNase data was downloaded from the Roadmap Epigenomics Project Portal18. ATAC-seq data from ref. 19 were obtained from GSE63341 and processed as follows: adapters were trimmed using Cutadapt20, reads were aligned using Bowtie2 (ref. 21) and filtered for mapping quality . d e v r e s e r s t h g i r l l A . e r u t a N r e g n i r p S f o t r a p , . c n I , a c i r e m A e r u t a N 7 1 0 2 © (mapq &gt; 30). For the scATAC-seq data from the GM12878 and HEK293T mixture from the combinatorial indexing approach, a count matrix was obtained from GSM1647122.  Peaks. For the bulk data analysis, we obtained DNase hypersensitivity peaks from the Roadmap Epigenomics Project. MACS2 (ref. 22) peaks for blood cells (primary monocytes from peripheral blood, primary B cells from peripheral blood, primary T cells from peripheral blood, primary natural killer cells from peripheral blood, primary hematopoietic stem cells G-CSF-mobilized female, primary hematopoietic stem cells G-CSF-mobilized male, and monocytes-CD14+ RO01746 Cell line) were downloaded from the Epigenomics Roadmap Portal18. For the singlecell ATAC-seq data, peaks were called for each cell line or type using MACS2 applied to the merged single-cell ATAC-seq data. All peaks were resized to a uniform width of 500 bp, centered at the summit. For both the set of peak calls from the blood cells in Roadmap and the set of peak calls from the scATAC-seq data, peaks were combined by removing any peaks overlapping with a peak with greater signal. Peak width was chosen based on typical sizes of ATAC-seq peaks across a wide collection of experiments, although chromVAR is fairly robust to the exact size of the peaks used (Supplementary Fig. 5 and Supplementary Note 2).  Motif collection. From cisBP, we curated position frequency matrices that represented a total of 15,389 human motifs and 14,367 mouse motifs. To filter motifs to a representative subset, we first categorized motifs as high, medium or low quality, as designated in the cisBP database. We then grouped all 870 unique human or 850 unique mouse TF regulators represented in the database and assigned these regulators to their most representative TF motif(s). To do this, we first iterated through each TF regulator to find all motifs associated with that regulator from the high-quality motif list. For these associated high-quality motifs, we first computed a similarity matrix using the Pearson correlation of the motifs. To calculate the Pearson correlation between pairwise motifs, the shorter motif was padded with an equal distribution of A,C,G,T. Then the Pearson correlation was calculated at every possible offset, and the maximum correlation of all offset comparisons was recorded. To select a representative subset of motifs for each TF regulator, we first found the motif correlated with the most other motifs at R &gt; 0.9. Treating that motif and all of the correlated motifs (R &gt; 0.9) as a group, we next found the motif with the greatest mean correlation to the other members of the group, and we kept that motif as a representative motif for the TF. Motifs highly correlated with that chosen motif (R &gt; 0.9) were then discarded from further analysis, and the process was iterated until no motifs remained. We repeated the process using the medium- and low-quality databases for TF regulators with no associated motifs in the high-quality database. The final curated motif database contains 1,764 human motifs and 1,346 mouse motifs representing 870 human and 850 mouse regulators. The resulting names are formatted as follows: \u201censemble ID\u201d_\u201dunique line number\u201d_\u201dcommon TF name\u201d_\u201ddirect (D) or inferred (I)\u201d_\u201d number of similar motifs grouped\u201d. These position frequency matrices were then converted into position weight matrices (PWMS) by taking the log of the frequency after adding a 0.008 pseudocount and dividing by 0.25.  These PWMs were used for all analyses in main text figures. For Supplementary Figures 2-5 and 13, a smaller set of motifs from the JASPAR CORE database 2016 were used23. nature methods For Supplementary Figure 14, motifs downloaded from http://homer.ucsd.edu/homer/custom.motifs were used24; and for Supplementary Figure 15 motifs downloaded from http://compbio.mit.edu/encode-motifs/ were used25 in order to use the same motifs as the original publication for those data sets.  Motif matching. The MOODS26 C++ library (Version 1.9.3) was used for identifying peaks containing a motif match, using a P value cutoff of 5 × 10−5. As background frequencies we used the nucleotide frequencies across all peaks. We wrapped the MOODS library into an R package, motifmatchr, which enables fast determination of motif presence or positions within genomic regions. The package is available at http://www.github.com/GreenleafLab/motifmatchr and https://bioconductor.org/packages/devel/bioc/html/motifmatchr.html.  Analysis. Downsampling analysis. To downsample a sample with X total fragments to a depth of Y total fragments, we use the fragment count matrix; and for each fragment within a peak we retained each fragment with probability Y/X. Thus the downsampled samples are equivalent to having approximately Y total fragments.  The set of peaks used for the analysis remained the same for each downsampled data set, as the peaks used were from an external data source (Roadmap Epigenomics Project).  For clustering samples using chromVAR results, highly correlated motifs were first removed, and then one minus the Pearson correlation of the bias-corrected deviations was used as the distance matrix for input into hierarchical clustering. For clustering samples using PCA, PCA was performed on the log of the fragment counts for all peaks after normalization for the total number of reads in peaks, and clustering was performed on the Euclidean distance between the first five principal components. Hierarchical clustering was performed with complete linkage, and the resulting dendrogram was cut into 13 groups (the number of cell types). Clustering accuracy was measured using normalized mutual information27.  Differential accessibility and variability. For determining differentially accessible motifs between AML LSC and blast cells, an unequal variances t-test (two sided) was used on the biascorrected deviations. For determining differential variability, a Brown-Forsythe test was used on the deviation z-scores.  Sample similarity tSNE. For performing sample similarity tSNE, highly correlated motifs or k-mers as well as motifs or k-mers with variability below a certain threshold (1.5) were first removed from the bias-corrected deviations matrix. The transpose of that matrix was then used as input to the Rtsne package (http://CRAN.R-project.org/package=Rtsne), with a perplexity parameter of 8 used for the downsampled bulk hematopoiesis data and a parameter of 25 for the single-cell ATAC-seq data.  Motif and k-mer similarity tSNE. For performing motif similarity tSNE, motifs or k-mers with variability below a certain threshold (1.5) were first removed from the bias corrected deviations matrix, which was then used as input to the Rtsne package (http://CRAN.R-project.org/package=Rtsne) with perplexity parameter set to 15.  Motif similarity scores. To score the similarity between a de novo motif and the most similar known motif, we first computed the normalized Euclidean distance between the de novo motif and all the known motifs in our collection using the optimal local alignment with at least five overlapping bases. We then selected the known motif with the lowest distance as the closest match. The similarity score was computed as the negative of the z-score for this distance using the distribution of distances for all the motifs in the collection. Software availability. The chromVAR R package is freely available under the MIT license at http://www.github.com/GreenleafLab/chromVAR and as Supplementary Software. The motifmatchr R package is freely available under a GPL-3 license is available at http://www.github.com/GreenleafLab/motifmatchr and as Supplementary Software.  Data availability statement. The additional K562 scATACseq data have been deposited at GEO with accession number GSE99172. Previously published single-cell ATAC-seq data are available from GSE74310 and GSE65360. Bulk hematopeisis ATAC-seq data are available at GSE74912. Macrophage bulk ATAC-seq data was obtained from GSE63341, combinatorial scATAC-seq from GSM1647122, and Roadmap Epigenomics data from the Roadmap Epigenomics Portal (http://egg2.wustl.edu/roadmap/web_portal/).  A Life Sciences Reporting Summary is available.    ",
    "sourceCodeLink": "https://github.com/GreenleafLab/motifmatchr",
    "publicationDate": "0",
    "authors": [
      "Alicia N Schep",
      "Beijing Wu",
      "Jason D Buenrostro",
      "William J Greenleaf"
    ],
    "status": "Success",
    "toolName": "motifmatchr",
    "homepage": "https://greenleaflab.github.io/motifmatchr/"
  },
  "98.pdf": {
    "forks": 3,
    "URLs": [
      "github.com/qunfengdong/BLCA",
      "ftp.ncbi.nlm.nih.gov/blast/db/16SMicrobial.tar.gz",
      "ftp.ncbi.nih.gov/pub/taxonomy/"
    ],
    "contactInfo": ["qdong@luc.edu"],
    "subscribers": 2,
    "programmingLanguage": "Python",
    "shortDescription": "",
    "publicationTitle": "A Bayesian taxonomic classification method for 16S rRNA gene sequences with improved species-level accuracy",
    "title": "A Bayesian taxonomic classification method for 16S rRNA gene sequences with improved species-level accuracy",
    "publicationDOI": "10.1186/s12859-017-1670-4",
    "codeSize": 7391,
    "publicationAbstract": "Background: Species-level classification for 16S rRNA gene sequences remains a serious challenge for microbiome researchers, because existing taxonomic classification tools for 16S rRNA gene sequences either do not provide specieslevel classification, or their classification results are unreliable. The unreliable results are due to the limitations in the existing methods which either lack solid probabilistic-based criteria to evaluate the confidence of their taxonomic assignments, or use nucleotide k-mer frequency as the proxy for sequence similarity measurement. Results: We have developed a method that shows significantly improved species-level classification results over existing methods. Our method calculates true sequence similarity between query sequences and database hits using pairwise sequence alignment. Taxonomic classifications are assigned from the species to the phylum levels based on the lowest common ancestors of multiple database hits for each query sequence, and further classification reliabilities are evaluated by bootstrap confidence scores. The novelty of our method is that the contribution of each database hit to the taxonomic assignment of the query sequence is weighted by a Bayesian posterior probability based upon the degree of sequence similarity of the database hit to the query sequence. Our method does not need any training datasets specific for different taxonomic groups. Instead only a reference database is required for aligning to the query sequences, making our method easily applicable for different regions of the 16S rRNA gene or other phylogenetic marker genes. Conclusions: Reliable species-level classification for 16S rRNA or other phylogenetic marker genes is critical for microbiome research. Our software shows significantly higher classification accuracy than the existing tools and we provide probabilistic-based confidence scores to evaluate the reliability of our taxonomic classification assignments based on multiple database matches to query sequences. Despite its higher computational costs, our method is still suitable for analyzing large-scale microbiome datasets for practical purposes. Furthermore, our method can be applied for taxonomic classification of any phylogenetic marker gene sequences. Our software, called BLCA, is freely available at https://github.com/qunfengdong/BLCA.",
    "dateUpdated": "2017-07-31T17:46:39Z",
    "institutions": [
      "Loyola University Chicago Lake Shore Campus",
      "Loyola University Chicago Health Sciences Division",
      "Loyola University Chicago Water Tower Campus"
    ],
    "license": "No License",
    "dateCreated": "2016-10-27T20:17:03Z",
    "numIssues": 2,
    "downloads": 0,
    "fulltext": "     Gao et al. BMC Bioinformatics     10.1186/s12859-017-1670-4   A Bayesian taxonomic classification method for 16S rRNA gene sequences with improved species-level accuracy     Xiang Gao  3    Huaiying Lin  1  3    Kashi Revanna  1  3    Qunfeng Dong  qdong@luc.edu  0  1  2  3    0  Bioinformatics Program, Loyola University Chicago Lake Shore Campus ,  Chicago, IL 60660 ,  USA    1  Center for Biomedical Informatics, Loyola University Chicago Health Sciences Division ,  Maywood, IL 60153 ,  USA    2  Department of Computer Science, Loyola University Chicago Water Tower Campus ,  Chicago, IL 60611 ,  USA    3  Department of Public Health Sciences, Loyola University Chicago Health Sciences Division ,  Maywood, IL 60153 ,  USA     2017   18    3  5  2017    18  1  2017     Background: Species-level classification for 16S rRNA gene sequences remains a serious challenge for microbiome researchers, because existing taxonomic classification tools for 16S rRNA gene sequences either do not provide specieslevel classification, or their classification results are unreliable. The unreliable results are due to the limitations in the existing methods which either lack solid probabilistic-based criteria to evaluate the confidence of their taxonomic assignments, or use nucleotide k-mer frequency as the proxy for sequence similarity measurement. Results: We have developed a method that shows significantly improved species-level classification results over existing methods. Our method calculates true sequence similarity between query sequences and database hits using pairwise sequence alignment. Taxonomic classifications are assigned from the species to the phylum levels based on the lowest common ancestors of multiple database hits for each query sequence, and further classification reliabilities are evaluated by bootstrap confidence scores. The novelty of our method is that the contribution of each database hit to the taxonomic assignment of the query sequence is weighted by a Bayesian posterior probability based upon the degree of sequence similarity of the database hit to the query sequence. Our method does not need any training datasets specific for different taxonomic groups. Instead only a reference database is required for aligning to the query sequences, making our method easily applicable for different regions of the 16S rRNA gene or other phylogenetic marker genes. Conclusions: Reliable species-level classification for 16S rRNA or other phylogenetic marker genes is critical for microbiome research. Our software shows significantly higher classification accuracy than the existing tools and we provide probabilistic-based confidence scores to evaluate the reliability of our taxonomic classification assignments based on multiple database matches to query sequences. Despite its higher computational costs, our method is still suitable for analyzing large-scale microbiome datasets for practical purposes. Furthermore, our method can be applied for taxonomic classification of any phylogenetic marker gene sequences. Our software, called BLCA, is freely available at https://github.com/qunfengdong/BLCA.    16S rRNA gene  Taxonomic classification       Background\r\n  High-throughput 16S rRNA gene sequencing is widely used in microbiome studies for characterizing bacterial community compositions. A key computational task is to perform taxonomic classification for 16S rRNA gene sequences, with emphasis increasing on species-level classification [ 1 ]. The published tools dedicated for 16S rRNA gene classification include the RDP Classifier [ 2 ], 16S Classifier [ 3 ] and SPINGO [ 4 ]. There are also software packages or websites that provide 16S classification options, e.g., QIIME [ 5 ] and MG-RAST [ 6 ].  Despite the availability of those taxonomic classification tools, species-level classification for 16S rRNA gene sequences still remains a serious challenge for microbiome researchers. Some of the tools simply do not classify at the species level. For example, the standard version of the widely-used software, RDP Classifier, only classifies 16S rRNA gene sequences from the phylum to genus levels, although the RDP Classifier can be re-trained for species level classification. Another recently published software, the 16S Classifier, is not capable of classifying sequences at the species level either. For the other tools that can classify at the species level, they suffer from at least one of the two major limitations: i) nucleotide k-mer frequency is used for measuring similarity between query and database sequences, a proxy measurement of true sequence similarity; ii) solid probabilistic-based criteria is lacking for evaluating the confidence of taxonomic assignment results, particularly to evaluate whether the best-matched database sequence is significantly better than other database matches for the taxonomic assignments.  Taxonomic classification of 16S gene sequences typically requires comparing query sequences to annotated database sequences. The k-mer based approaches, e.g., the RDP Classifier and SPINGO, compare the frequency of k-mer nucleotides between query and database sequences. The higher degree of shared k-mer nucleotide frequencies, the more similar the two sequences are. The advantage of k-mer based approaches is its fast computational speed. However, k-mer based approaches rely on two key assumptions: i) the k-mer nucleotides in DNA sequences used as discriminating features among different taxa are independent, and ii) the actual nucleotide position of the k-mers in the DNA sequences is not important. In reality, nucleotides in different positions of a gene sequence can be correlated (e.g., to preserve the secondary or higher-dimensional structure of rRNA folding), and gene sequences with the same set of k-mer in different orders are clearly not the same sequences. Therefore, these two assumptions are the theoretical sources of taxonomic misclassification by k-mer based approaches. There is also a nontrivial practical limitation for a k-mer based approach: it is extremely difficult to determine an optimal size of k-mer for discriminating among different species at different regions of 16S sequences. For example, the accuracy of the RDP Classifier, which uses a k-mer size of eight, varies significantly with different types of bacterial taxa at different 16S gene regions [ 7 ]. Therefore, k-mer based approaches rely on a proxy measurement of the sequence similarity between the query and database sequences, which is inherently less accurate than the gold standard sequence-alignment-based method.  As mentioned above, another major limitation for most existing methods is that they lack solid probabilistic-based criteria to evaluate the confidence of their taxonomic assignments. Although all existing methods infer taxonomic classification based on matched database sequences, most of the existing methods do not provide any indication on whether the best-matched database hit sequence is significantly better than other database hits. Since the 16S rRNA gene is highly conserved among different bacterial taxa and the query sequences in microbiome studies are often only a short fragment of the full-length 16S rRNA gene with sequencing errors, it is common to have several database hits from different taxa that may have comparable sequence similarities to the query sequence. Therefore, it is not reliable to simply transfer the taxonomic annotation associated with the best database hit for the query sequence [ 8 ]. Instead, a better method for 16S classification may consider multiple database hits together and evaluate whether the best database hit is significantly better than other database hits.  The Lowest Common Ancestor (LCA) algorithm, implemented in the MEGAN package [ 9 ], provides a natural biological framework to integrate taxonomic annotations associated with multiple database hits when classifying query sequences. In MEGAN, all taxa corresponding to the BLAST [ 10 ] hits are first mapped to NCBI taxonomic trees and the lowest common ancestor of all mapped taxa is then assigned to the query sequence. For example, if a query sequence has two BLAST hits belonging to two different species, e.g., one from Lactobacillus acidophilus and the other one from L. casei, the LCA algorithm assigns the query sequence to the genus Lactobacillus, which is the lowest common taxonomic level of these two species. However, the LCA algorithm fails to consider the differing degrees of similarity between the query and the database hit sequences. In other words, when inferring the LCA for the query, the algorithm acts as if all the hit sequences, affected by an arbitrary sequence similarity threshold, were equally similar to the query sequence, even though in practice they are often not. Biologically speaking, the greater the degree of sequence similarity between the query and the hit sequences, the more likely they may belong to the same taxon, but the current LCA algorithm lacks a quantitative way to incorporate this important information on sequence similarity in its taxonomic assignment.  To overcome the above limitations of the existing software, we have developed a Bayesian-based LCA method, named BLCA. BLCA can perform species and even sub-species level taxonomic classification. It relies on sequence alignment instead of k-mer frequency for sequence similarity measurement; it considers multiple database hits instead of only the best database hit for taxonomic assignment; it provides a probabilistic-based confidence score for evaluating taxonomic assignments. The novelty of our method is that the contribution of each database hit to the taxonomic assignment of the query sequence is weighted by a Bayesian posterior probability based upon the sequence similarity of the database hit to the query. The calculated Bayesian posterior probability implicitly penalizes dissimilar database hit sequences in a quantitative way, which makes our method insensitive to arbitrary sequence similarity thresholds for selecting candidate database hits for each query sequence. We show that BLCA provides significantly more accurate classification results at the species level when compared to all other existing tools.    Implementation\r\n  The BLCA method is implemented as a Python package, which is freely available at https://github.com/qunfengdong/BLCA under the GNU General Public License. An overview of the BLCA method is illustrated in Fig. 1. Users start by comparing the query 16S sequences against entries in an annotated 16S database using BLASTN. The taxonomic lineage of each 16S database sequence is extracted from the NCBI taxonomic database (ftp://ftp.ncbi.nih.gov/pub/taxonomy/). As with MEGAN, we chose the 16S rRNA gene collection from NCBI (ftp://ftp.ncbi.nlm.nih.gov/blast/db/16SMicrobial.tar.gz) as the default database, although users can also use the Greengenes database [  [ 11] or adopt any custom collection of 16S sequences provided that the sequence IDs can be mapped to the NCBI or Greengenes taxonomy. Next, the BLAST hits are extracted; by default, BLCA only extracts the BLAST hits from BLAST pairwise alignments with at least 95% identity and 95% coverage with respect to the query, but users can easily change these parameters using the commandline at execution as well as setting an additional criterion to retain only the BLAST hits whose bit scores are within a certain percentage of difference from the top hits (the same criterion used by MEGAN). Each query sequence and its corresponding BLAST hits are passed as an input to the MUSCLE program [  [ 12] for multiple sequence alignment. Because most 16S query sequences are not full-length gene sequences in practice, BLCA only extracts the relevant subsequences of the hits those that actually align to the query sequences in the BLAST pairwise sequence alignments. An extra 10 nucleotides upstream and downstream relative to the aligned regions from the hit sequences are also included to avoid potential overhangs at the 5\u2032 or 3\u2032 end of the query sequences in the multiple sequence alignment.  We define Pr(Ti | Q) as the Bayesian posterior probability for a taxon Ti being assigned to a given query sequence Q. Based on Bayes' rule, we obtain  PrðT ijQÞ ¼ PrðQjT iÞPrðT iÞ=PrðQÞ ð1Þ wherein Pr(Q | Ti) is the likelihood of observing the sequence Q if it were derived from the taxon Ti. The likelihood can be calculated as the pairwise alignment score between the query sequence Q and the database hit sequence annotated as Ti, divided by the pairwise alignment score between the hit sequence Ti to itself. In other words, the likelihood is defined as the similarity score between the query and the database hit normalized by the maximum possible similarity score between any sequences to the hit sequence. The likelihood Pr(Q | Ti) is a real number between 0 (i.e., no match between the query Q and the database hit Ti) and 1 (i.e., a perfect match between the query and the database hit). Our definition of Pr(Q | Ti) as a likelihood simply reflects the degree of support by the evidence (i.e., similarity between query and the database hit) for the hypothesis (i.e. the query belongs to the taxon of the database hit). In our current implementation, the pairwise alignment score between the query sequence and BLAST hit sequence is computed from the multiple sequence alignment, which tends to be more accurate than the original BLAST pairwise alignment because BLAST alignment performs local alignment, whereas MUSCLE is a global alignment program. Since the alignment is between DNA sequences, the pairwise alignment score can be simply computed with the following criteria: match = +1, mismatch = −2, and gap = −2.5 (these are the exact default scoring criteria used for BLASTN). Pr(Ti) is the prior probability of a particular taxon Ti for the query sequence, which is set to a uniform distribution in our implementation. The uniform prior is a suitable choice for taxonomic classification, since, without knowing the data, we can treat every taxon as equally probable (the same uniform prior is used in the RDP Classifier). If necessary, non-uniform priors can be easily adopted for specific situations where certain taxa are more likely than others in the same Bayesian framework described in this work. Pr(Q) is the marginal distribution of the query sequence Q, which can be calculated as the summation of the product of likelihoods and priors of all the m BLAST hits, i.e., ∑i = 1Pr(Q|Ti)Pr(Ti) for m total BLAST hits, based on the law of total probability. Note that the term Pr(Ti), assumed to be a uniform prior, can be cancelled from the denominator and numerator when calculating Pr(Ti | Q). In addition, sequence similarity estimations might be improved by specifying sequencing error models for both query and database sequences (e.g., a Poisson probability distribution of an observed nucleotide in a DNA sequence being incorrect); these can be incorporated in our Bayesian framework by adjusting the likelihood calculation in Eq. (1).  Since Ti corresponds to the taxonomic annotation for an individual BLAST hit sequence, it represents the leaf node in the NCBI taxonomic tree (e.g., at the species or sub-species level). We also need to compute the posterior probability at higher taxonomic levels, i.e., the internal nodes in the taxonomic tree that correspond to the antecedents of Ti (i.e., the common ancestors of all the Ti). Using the addition rule for probability, the posterior probability of any internal node I, Pr(TI | Q), in the taxonomic tree can be computed by a simple summation of those of all the descendant leaf Ti: PrðT I jQÞ ¼  Xik¼1 PrðT ijQiÞ ð2Þ wherein the internal node I has k total descendant leaf nodes. The Eq. (2) allows us to easily compute the posterior probability of any higher taxonomic level, e.g., from genus to phylum, by simply summing the posterior probabilities associated with all the descendant leaf nodes in the taxonomic trees under any internal nodes. Using the previous example in which a query sequence has one BLAST hit from L. acidophilus and the other from L. casei, the posterior probability for the genus level of Lactobacillus for the query is the sum of the posterior probabilities for L. acidophilus and L. casei, respectively.  Based on the posterior probabilities calculated for all the nodes in the taxonomic tree, a bootstrap confidence score is derived to evaluate the reliability of the taxonomic assignment for each node. Specifically, aligned nucleotide positions in the multiple sequence alignment between query and BLAST hits are randomly sampled with replacement; the total number of sampled nucleotide positions is the same as the length of the query sequence (i.e., a pseudo multiple-sequence alignment is bootstrapped from the original multiple-sequence alignment). Using the pseudo multiple-sequence alignment, the posterior probability of each leaf node in the taxonomic tree is re-computed by the same procedure as described above and the leaf node with the highest posterior probability is identified and tallied as the \u201cwinning\u201d node. The process is repeated 100 times, and the number of times that a leaf node emerged as the winner becomes the confidence score for the taxonomic assignment of the particular node. Similar to the posterior probability calculation, the confidence score for internal nodes can also be obtained by summing up the confidence scores of all their descendent leaf nodes. The RDP Classifier uses a similar bootstrapping strategy to assign confidence scores for its taxonomic classifications. However, unlike the RDP Classifier, which is based on bootstrapping k-mers from query and database sequences, our strategy randomly samples from aligned nucleotides in multiple sequence alignment, a method that is commonly used for evaluating the confidence of branches in molecular phylogenic trees [ 13 ].  To assess the accuracy of a classification tool, we must have a benchmark dataset with known taxonomic annotations for each 16S sequence. Therefore, we extracted the V2, V4, V1-V3, V3-V5, and V6-V9 regions of 16S sequences from 1000 randomly selected bacterial species with known taxonomic annotations in the NCBI database as the benchmark dataset. These variable regions were chosen for testing because they represent typical 16S sequences in real-world microbial studies. Instead of using the exact sequences from those regions for testing, we introduced sequencing errors to each sequence, using a customized Python script to generate an average of 1% random mutation based on a Poisson distribution. The 1% mutation rate is based on the reported upper range of the Illumina MiSeq sequencing platform [ 14 ]. The test sequences, with sequencing errors, were searched against the 16S sequences from NCBI (downloaded on August 5th, 2016) using BLASTN version 2.5.0. For MEGAN parameters, we set the same default settings (e.g., minimum BLAST bit scores, maximum BLAST expected values, and the percent of BLAST hits) for both BLCA and MEGAN. For BLCA, SPINGO, and the RDP Classifier, two sets of confidence score thresholds were used: (i) 0.8-RDP Classifier's default confidence score and (ii) 0.5-RDP Classifier's confidence score threshold recommended for short-read sequences, as written in the RDP Classifier's documentation. Neither MEGAN nor Kraken [ 15 ] have a probabilistic-based parameter for evaluating the assigned taxa, thus we used their default taxonomic assignments for comparison.  For each of the taxa in the benchmark dataset (e.g., a known E. coli sequence), we were able to identify whether the classification results from each software represent a true positive (TP, e.g., the predicted taxonomy is also E. coli), false negative (FN, e.g., the predicted taxonomy is not E. coli), false positive (FP, e.g., other non-E. coli sequences were incorrectly predicted to be E. coli), and true negative (TN, e.g., other non-E. coli sequences were correctly predicted to be non-E. coli). The total amount of TP, FN, FP, and TN are tallied from the 1000 test sequences from the species to the phylum levels. The rates of TP, FN, FP, and TN were used for computing the F-score, which is a standard measure of a classifier's accuracy by combining both the precision and the recall of the classifier [ 16 ]. The procedure above was repeated three times to measure the variability of the classification accuracy.  Besides the above-simulated dataset, we also evaluated the performance of BLCA with a real-world 16S dataset, which was suggested by one of the reviewers of our manuscript. The dataset was originally produced by Pop et al. [ 17 ] and is available in the Bioconductor package (referred as the msd16s dataset) [ 18 ]. The msd16s dataset contains 26,044 species-level operational taxonomic unit (OTU) sequences from the V1V2 rRNA gene region. The original authors used the top BLAST hit against the RDP 16S database [ 19 ] as the taxonomic annotation for each OTU sequence. Since MEGAN and SPINGO can only use NCBI taxonomy nomenclature, we re-annotated the msd16s dataset by using the top BLAST hit against NCBI 16S database (i.e., the same BLAST strategy as in the original study of Pop et al. [ 17 ]) in order to ensure that MEGAN and SPINGO can be compared against BLCA and other programs using the same reference taxonomic annotation.    Results\r\n  To compare BLCA against other software, we reviewed all recently published 16S taxonomic classification tools.  Since BLCA aims to improve species-level classification accuracy compared to existing tools, we excluded the 16S Classifier program since it cannot classify at the species level.  To obtain a fair comparison with MEGAN (version 6.7.1), we used the same default criteria as MEGAN for retaining the BLAST hits. The most important MEGAN parameter for extracting BLAST hits for downstream analysis is the parameter topPercent, used to keep only the BLAST hits whose bit scores are within a given percentage of the best BLAST hit. The default value in MEGAN for this parameter is 10%. For example, if the top BLAST hit has a bit score of 1000, we only retain BLAST hits for downstream analysis if their BLAST bit scores are at least 900 (i.e., 1000-1000*10%). As shown in Table 1, BLCA consistently outperforms MEGAN with all the tested 16S variable regions from the species to the family levels of taxonomic classification. From the order to the phylum levels, the accuracies of BLCA, MEGAN and other software are similar and above 98% (data not shown). More importantly, the accuracy of V6V9 Each entry in the table shows the average and standard deviation of the F-scores for a particular classifier (i.e., rows) at a specific 16S region (i.e., columns) based on three random sets of 1000 test sequences. Two confidence score thresholds (CST), 0.8 and 0.5, were applied for BLCA, RDP Classifier, and SPINGO as described in the main text. The *indicates that the F-scores of BLCA are significantly higher than those of other software, based on a one-tailed paired t-test with a p-value less than 0.05. Similar statistical significance was also obtained using the one-tailed Wilcoxon signed-rank test. Note that the SPINGO program does not produce family-level classification. In addition, Kraken and MEGAN do not provide any probabilistic-based parameters for evaluating the assigned taxa, thus we used their default taxonomic assignments for comparison MEGAN drops significantly when the topPercent filter was relaxed from 5 to 10% and further to 20% (the recommended range by the original MEGAN publication) at both the species and genus levels (Table 2). For example, using V1-V3 sequences, the species-level accuracy of MEGAN, measured by the F-scores, drops from 0.8394 (with topPercent set to 5%) to 0.7071 (with topPercent set to 10%), and further down to 0.4673 (with topPercent set to 20%). Besides V1-V3, these same trends are observed for all other tested 16S regions (Table 2). These results are expected because, by relaxing this parameter, more dissimilar BLAST hits (i.e., potentially \u201cbad\u201d BLAST hits) are included in the analysis and the inclusion of bad BLAST hits leads to erroneous taxonomic assignments. This reveals a fundamental limitation of the MEGAN method: its results are sensitive to which BLAST hits are included for analysis and it lacks a probabilistic method to penalize bad BLAST hits.  Conversely, the results from BLCA, which showed higher accuracy than MEGAN, remained robust to the MEGAN 0.8091 ± 0.0153 The parameter topPercent is for keeping only the BLAST hits whose bit scores are within a given percentage of the best BLAST hit. The larger the parameter is, the more dissimilar database hits are included for taxonomic classification for the query sequence. The default value in MEGAN for this parameter is 10%. In our comparisons, we set the value of topPercent to be 5, 10 and 20% for both BLCA and MEGAN, the recommended range by the original MEGAN publication, to compare the performance of BLCA and MEGAN under different stringencies of retaining BLAST hits. Each table entry shows the average and standard deviation of the F-scores, based on the confidence score threshold of 0.8, for each tested software at the corresponding 16S region. The F-scores of BLCA are much less sensitive to the value of topPercent when compared to MEGAN number of included BLAST hits (Table 2) since bad BLAST hits are penalized using posterior probability scores assigned by the BLCA algorithm. It is worth noting that it is unrealistic to prevent the inclusion of bad BLAST hits in a typical large-scale data analysis since there is no universal cutoff to exclude bad BLAST hits. Any such cutoffs are heuristic in nature, as such, they are inevitably either too stringent or not stringent enough.  The SPINGO program is specifically designed for species-level classification. The authors of SPINGO even showed that SPINGO has superior classification accuracy compared to a customized RDP Classifier and bestmatched BLAST hits at species level [ 4 ]. Like BLCA and MEGAN, SPINGO uses the NCBI taxonomic database for taxonomic assignments. Unlike those tools, however, SPINGO uses a k-mer based approach instead of sequence alignment to measure the similarity between query and database sequences. The only threshold for SPINGO is its confidence score for taxonomic assignments, which is compatible with the BLCA confidence score. Table 1 shows that the accuracy of BLCA is statistically significantly higher than that of SPINGO in all tested 16S regions at the confidence score thresholds of 0.8 and 0.5, respectively. In addition, SPINGO cannot do subspecies classification, nor can it do family or higher level classification, whereas BLCA can classify reads from any level ranging from subspecies to phylum (though there are not enough annotated subspecies datasets at NCBI for evaluating BLCA subspecies-level classification accuracy).  Even though the standard release of the RDP Classifier cannot classify 16S sequences at the species level, we obtained the training script from the RDP Classifier's development team (personal communications) and re-trained the RDP Classifier for species-level classification with the same NCBI 16S database that BLCA uses.  The NCBI 16S database is used because MEGAN and SPINGO must use NCBI taxonomic database. Therefore, the NCBI database provides a common ground for evaluating the results of all of these tools on the basis of their computational algorithms without being influenced by different taxonomic standards. Similar to SPINGO, the RDP Classifier's confidence score is also compatible with the BLCA confidence score. Although the default threshold for the RDP Classifier's confidence score is 0.8, the developers of the RDP Classifier also recommend a threshold of 0.5 for short read classification. Our results show that BLCA has higher accuracy than the RDP Classifier at the thresholds of 0.8 and 0.5 (Table 1).  Besides these 16S-specific classification tools, there are also metagenomic classification tools that are designed for identifying microbial taxa from whole metagenome shotgun (WMS) sequences. We have chosen Kraken [ 15 ] as a representative WMS classification tool to compare with BLCA. Kraken is chosen because of two reasons: i) it has superior or comparable classification accuracy to other existing WMS tools [ 20 ] and ii) to our best knowledge, it is the only WMS tool that has been successfully applied in a published 16S study [ 21 ].  Kraken's default database incorporates reference genome sequences. To have a fair comparison with BLCA, we have replaced Kraken's default database with the same NCBI 16S database used for BLCA, thus increasing its sensitivity to classify a broader range of bacterial taxa.  Kraken, a k-mer based program seeking best database matches, does not provide any confidence score to evaluate the confidence of assigned taxonomies, although Kraken's output can be filtered based on the percent of k-mers matched to each taxa (no guidance is provided by its developer on how to set the filtering threshold). As shown in Table 1, even allowing the maximum sensitivity for Kraken (i.e., without any filtering of Kraken's output), which is the default setting for Kraken, BLCA still significantly outperforms Kraken with all tested 16S regions from the species to the family level.  In addition to using simulated datasets to evaluate BLCA and other software, Table 3 shows that BLCA had either higher or comparable classification accuracies when tested with a real-world 16S dataset. For example, with a confidence score threshold of 0.5 (the recommended threshold for the RDP Classifier for short sequence reads), the species-level classification accuracy of Each entry in the table shows the F-scores for a classifier (i.e., rows) based on all the OTU sequences in the msd16s dataset, as described in the main text. Two confidence score thresholds (CST), 0.8 and 0.5, were applied for BLCA, RDP Classifier, and SPINGO, the thresholds as in Table 1. Note that the SPINGO program does not produce family-level classification. In addition, Kraken and MEGAN do not provide any probabilistic-based parameters for evaluating the assigned taxa, thus we used their default taxonomic assignments for comparison BLCA, measured using an F-score, is 0.716, much higher than the classification accuracy of MEGAN (0.544), the RDP Classifier (0.613), and SPINGO (0.562). The same trends were observed when the default confidence score threshold of 0.8 was applied (Table 3). It is worth noting that, as this is a real-world dataset, the true taxonomic classification is unknown. We had to rely on the top BLAST hit as the reference taxonomic classification when we evaluated the classification accuracies of each software. Nonetheless, the results from the real-world dataset were consistent with those from the simulated datasets, showing that BLCA tends to produce higher taxonomic classification accuracies than currently existing software.    Discussion\r\n  Despite the importance of species-level classification, the existing tools either do not classify 16S sequences at the species level or their taxonomic assignments are not reliable. As discussed above, k-mer based methods are intrinsically less accurate than an alignment-based sequence similarity measurement. The k-mer based approaches may be sufficient for high level taxonomic classification, since sequences from different higher taxonomic levels tend to be very divergent. For lower level taxonomic classification, however, particularly species-level classification, we have shown that BLCA significantly outperforms k-mer based methods (e.g., SPINGO, the RDP Classifier, and Kraken) in classification accuracy.  In addition, the Bayesian posterior probability of BLCA quantitatively measures the difference between the best database hit and other database hits, and the bootstrapping principle, adopted by BLCA for providing confidence score, has solid statistical foundation for measuring prediction errors [ 22 ]. In this study, we have applied 0.5 and 0.8 as thresholds for the BLCA confidence scores for comparison with other software. The confidence score of BLCA is comparable to that of the RDP Classifier and SPINGO. There is no perfect universal threshold that is suitable for all datasets. We recommend that users consider exploring several different thresholds (e.g., 0.6 and 0.8) to examine if their results are consistent under different thresholds. If not, the users need to be wary that their results may be too sensitive based on the particular threshold they have chosen.  It is worth mentioning that BLCA does not require a training process for classification, which can be more convenient for some users when compared to some other software. For example, the 16S Classifier trains a standard machine-learning model, a Random Forest, with k-mer nucleotides from different regions of 16S rRNA genes. We could not even test our V1-V3, V3V5, and V6-V9 datasets with the 16S Classifier because the published software has not been trained for this region, even though these regions are widely used in microbiome studies. In contrast, our BLCA program requires no training process at all since our algorithm is based on the alignment between query and reference database sequences. Therefore, users only need to download reference 16S database sequences for BLCA and this allows our method to be easily applied to any other DNA marker gene families for taxonomic classification (e.g., rpoB or 18S rRNA gene sequences). The accompanying BLCA package includes instructions on how to replace the default 16S sequences with the user's own customized gene family sequences. For example, to demonstrate the flexibility of alternative database sequences, BLCA provides an option to use the Greengenes 16S database and its associated taxonomy [11 11 ] instead of the default NCBI 16S database since many researchers may prefer the Greengenes taxonomy.  We have shown that BLCA has significantly higher accuracy than existing taxonomic classification methods at the species level. This higher accuracy comes with the cost of longer computation time. BLCA is not designed for performing taxonomic classification for raw 16S sequences. Instead, raw 16S sequences should be first clustered into OTUs to eliminate redundant or highly similar sequences before performing taxonomic classification, which is a standard procedure for 16S sequence processing by widely used software packages, e.g., QIIME. With 100,000 OTUs, BLCA can have a run-time of approximately 4 days, which is not unusual for modern-day bioinformatics tasks with large datasets. Considering the significant gains in accuracy with our method, we believe that many researchers will find the time tradeoff to be reasonable. In addition, users can divide the input sequences into multiple files and execute BLCA in parallel on computer clusters to hasten the classification process, if necessary. In addition, not all OTUs require species-level classification in practice. Typically, researchers are only interested in a small subset of OTUs, e.g., a list of OTUs that are differentially abundant in different ecosystems (similar to how molecular biologists are often only interested in detailed gene annotations for a small list of differentially expressed genes instead of all of the genes in an organism). In these cases, BLCA may take only a few minutes to classify a subset of several hundreds of OTUs of interest.    Conclusion\r\n  In summary, we have developed a novel computational method that significantly outperforms previously published software for species-level classification accuracy. Its probabilistic-based confidence score helps users evaluate the confidence of the resulting taxonomic assignments based on multiple database hits. In addition, our methods do not require any training, which makes it easily applicable for different regions of 16S rRNA gene or even different phylogenetic marker genes. Despite its higher computational costs, our method is still suitable for large-scale microbiome datasets, providing a valuable alternative option for microbiome researchers who prefer higher classification accuracy.  Abbreviations FN: False negative; FP: False positive; LCA: Lowest Common Ancestor; OTU: Operational taxonomic unit; rRNA: ribosomal RNA; TN: True negative; TP: True positive Acknowledgements We thank Benli Chai from the RDP research group for providing the training script for the RDP Classifier. We also thank Michael Zhao, Laurynas Kalesinskas and Francis Cocjin for testing the BLCA software package and proofreading the manuscript. We also thank the two anonymous reviewers for their helpful comments and suggestions.  Funding This work was supported by NIH grants 1R01AI116706-01A1, 1P20DK108268, and U01HL121831. The funding bodies had no role in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript. Availability of data and materials Our software is freely available at https://github.com/qunfengdong/BLCA.Authors' contributions XG and QD developed the algorithm, designed computational experiments, analyzed the results, and wrote the manuscript. HL and KR independently implemented the initial version of the computer software, conducted testing, developed documentation, and assisted with manuscript preparation. HL produced the final version of the software, and conducted all the computational experiments. All authors have read and approved the final version of the manuscript. Competing interests The authors declare that they have no competing interests.  Consent for publication Not applicable.  Ethics approval and consent to participate Not applicable.    Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/qunfengdong/BLCA",
    "publicationDate": "0",
    "authors": [
      "Xiang Gao",
      "Huaiying Lin",
      "Kashi Revanna",
      "Qunfeng Dong"
    ],
    "status": "Success",
    "toolName": "BLCA",
    "homepage": ""
  },
  "61.pdf": {
    "forks": 0,
    "URLs": [
      "ftp.ncbi.nlm.nih.gov/genomes/genbank",
      "github.com/jkruppa/kmerPyramid.Contact:"
    ],
    "contactInfo": ["klaus.jung@tiho-hannover.de"],
    "subscribers": 1,
    "programmingLanguage": "R",
    "shortDescription": "R package to visualize the kmer distribution between samples",
    "publicationTitle": "kmerPyramid: an interactive visualization tool for nucleobase and k-mer frequencies",
    "title": "kmerPyramid: an interactive visualization tool for nucleobase and k-mer frequencies",
    "publicationDOI": "10.1093/bioinformatics/btx385",
    "codeSize": 228644,
    "publicationAbstract": "Summary: Bioinformatics methods often incorporate the frequency distribution of nulecobases or k-mers in DNA or RNA sequences, for example as part of metagenomic or phylogenetic analysis. Because the frequency matrix with sequences in the rows and nucleobases in the columns is multidimensional it is hard to visualize. We present the R-package 'kmerPyramid' that allows to display each sequence, based on its nucleobase or k-mer distribution projected to the space of principal components, as a point within a 3-dimensional, interactive pyramid. Using the computer mouse, the user can turn the pyramid's axes, zoom in and out and identify individual points. Additionally, the package provides the k-mer frequency matrices of about 2000 bacteria and 5000 virus reference sequences calculated from the NCBI RefSeq genbank. The 'kmerPyramid' can particularly be used for visualization of intra- and inter species differences. Availability and implementation: The R-package 'kmerPyramid' is available from the GitHub website at https://github.com/jkruppa/kmerPyramid.Contact: klaus.jung@tiho-hannover.de Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-06-27T07:29:16Z",
    "institutions": [
      "University of Veterinary Medicine Hannover",
      "Institute for Animal Breeding and Genetics",
      "Research Center for Emerging Infections and Zoonoses"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-08-04T12:12:02Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx385   kmerPyramid: an interactive visualization tool for nucleobase and k-mer frequencies     Jochen Kruppa  1    Erhard van der Vries  2    Wendy K. Jo  2    Alexander Postel  0    Paul Becher  0    Albert Osterhaus  2    Klaus Jung  1    0  Department of Infectious Diseases, Institute of Virology, University of Veterinary Medicine Hannover ,  Hannover ,  Germany    1  Institute for Animal Breeding and Genetics    2  Research Center for Emerging Infections and Zoonoses ,  RIZ     2017   1  1  2   Summary: Bioinformatics methods often incorporate the frequency distribution of nulecobases or k-mers in DNA or RNA sequences, for example as part of metagenomic or phylogenetic analysis. Because the frequency matrix with sequences in the rows and nucleobases in the columns is multidimensional it is hard to visualize. We present the R-package 'kmerPyramid' that allows to display each sequence, based on its nucleobase or k-mer distribution projected to the space of principal components, as a point within a 3-dimensional, interactive pyramid. Using the computer mouse, the user can turn the pyramid's axes, zoom in and out and identify individual points. Additionally, the package provides the k-mer frequency matrices of about 2000 bacteria and 5000 virus reference sequences calculated from the NCBI RefSeq genbank. The 'kmerPyramid' can particularly be used for visualization of intra- and inter species differences. Availability and implementation: The R-package 'kmerPyramid' is available from the GitHub website at https://github.com/jkruppa/kmerPyramid.Contact: klaus.jung@tiho-hannover.de Supplementary information: Supplementary data are available at Bioinformatics online.       -\r\n  *To whom correspondence should be addressed. Associate Editor: John Hancock    1 Introduction\r\n  Many methods in bioinformatics for the analysis of DNA and RNA sequences make use of the frequency distribution of the four nucleic bases or higher k-mers. For example, the frequency distribution plays a role in metagenomic binning of sequence reads (Dodsworth et al., 2013; Imelfort et al., 2014) and phylogenetic analysis (Podar et al., 2013). Reference sequences of multiple species are available from public data bases or are generated for example by next-generation sequencing (NGS). Frequency matrices of k-mer distributions derived from sequences are often multi-dimensional and thus hard to visualize.  Here, we present the R package 'kmerPyramid' as an interactive visualization tool that can be used for visualization of clustering results, of comparisons between genomic regions, of horizontal gene transfer as well as the display of low complexity regions. The kmerPyramid is based on principal component analysis (PCA) that is used to project the multi-dimensional matrix of nucleobase and k-mer frequencies in the 3-dimensional space. PCA, as a method for dimension reduction, has already been demonstrated to preserve relevant information when exploring k-mer frequencies (Dodsworth et al., 2013; Imelfort et al., 2014; Podar et al., 2013). Our package provides a more comfortable environment for exploring the projected frequency data and allows the user to compare the frequency of his sequences with frequencies of thousands of viral and bacterial reference sequences. For further background we refer to the Supplementary Data.    2 Functionality and examples of application\r\n  In total, the package comprises two main functions, several utility functions and four datasets. The main function pyramid_3d() performs the PCA and plots the projected data of the first three principal components in an interactive plot based on its individual nucleobase frequency distribution. As main argument, the raw (n 4k) data matrix X must be provided, representing the frequencies of the k-mers for n sequences. Functionality to estimate the k-mer frequencies from sequences is offered, too. In addition, a list of colors and labels can be provided. This pyramid plot can then be turned in each direction, the user can zoom into the point cloud and identify individual points by clicking onto them. The second main function pyramid_3d_grid() allows to estimate the k-mer frequencies for a fix k using a sliding window approach, and to plot the projected frequency data in a grid sectioned pyramid.  Since nucleobase frequencies play an important role in the analysis of viruses and bacteria, we determined frequency matrices for ca. 2000 bacteria and 5000 viruses from sequences retrieved from the NCBI genbank (ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank) including species and taxonomic information. The resulting frequency matrices were also added to the package. In addition, two small example datasets including coding and non-coding sequences from the Mavirus and ten exemplary virus sequences are provided.  The kmerPyramid can be used in different fields of application. If the user is interested to visualize bases or k-mers that are very frequent in a sequence, the grid based version of the pyramid can be used. The first three columns of Figure 1 show the grid based pyramid of two papillomavirus sequences (human and monkey) for k ¼ 1, 2, 3. For k ¼ 5, the permutation frequency differences between the two papillomavirus sequences are displayed in another pyramid (top right plot). Those k-mers that are more/less frequent in the human are labelled by a þ/- symbol. Thus, the ordered CGGTT is more frequent in the human strain and the ordered AAGGT in the monkey strain. Bottom right of Figure 1, each point in the basic ACGT-pyramid represents either a coding (o) or a non-coding (x) region of the Mavirus. The user can see that the coding regions have a larger GC-content. A more detailed methods description and a comprehensive list of applications is presented in the Supplementary Material.    3 Summary\r\n  The kmerPyramid package, as a 4-dimensional coordinate system, is a useful tool for visualizing and exploring word frequencies in DNA and RNA sequences. It can be used for visualizing a variety of biological aspects on the sequence level. The two variants of the pyramid can be used to either display the relation of multiple sequences to each other based on their nucleobase frequencies or to visualize the frequency of k-mers within a sequence. The grid based variant can also be used to compare k-mer frequencies between two sequences. Further conclusions and remarks are given in the Supplementary Material.    Funding\r\n  This work was supported by the Niedersachsen-Research Network on Neuroinfectiology (N-RENNT) of the Ministry of Science and Culture of Lower Saxony.  Conflict of Interest: none declared. Dodsworth,J.A. et al. (2013) Single-cell and metagenomic analyses indicate a fermentative and saccharolytic lifestyle for members of the OP9 lineage.  Nat. Commun., 4, 1854.  Imelfort,M. et al. (2014) GroopM: an automated tool for the recovery of population genomes from related metagenomes. PeerJ, 2, e603. Podar,M. et al. (2013) Insights into archaeal evolution and symbiosis from the genomes of a nanoarchaeon and its inferred crenarchaeal host from Obsidian Pool, Yellowstone National Park. Biol. Direct, 8, 9.    ",
    "sourceCodeLink": "https://github.com/jkruppa/kmerPyramid",
    "publicationDate": "0",
    "authors": [
      "Jochen Kruppa",
      "Erhard van der Vries",
      "Wendy K. Jo",
      "Alexander Postel",
      "Paul Becher",
      "Albert Osterhaus",
      "Klaus Jung"
    ],
    "status": "Success",
    "toolName": "kmerPyramid",
    "homepage": ""
  },
  "12.pdf": {
    "forks": 4,
    "URLs": [
      "github.com/bwringe/hybriddetective",
      "github.com/bwringe/parallelnewhybrid"
    ],
    "contactInfo": ["bwringe@gmail.com"],
    "subscribers": 2,
    "programmingLanguage": "R",
    "shortDescription": "",
    "publicationTitle": "l hybriddetective: a workflow and package c to facilitate the detection of hybridization i utsing genomic data in R",
    "title": "l hybriddetective: a workflow and package c to facilitate the detection of hybridization i utsing genomic data in R",
    "publicationDOI": "10.1111/1755-0998.12704",
    "codeSize": 5615,
    "publicationAbstract": "r",
    "dateUpdated": "2017-08-10T21:32:17Z",
    "institutions": [
      "National Oceanic and Atmospheric Administration Southwest Fis",
      "Department of Fisheries and Oceans Canada",
      "heries Science Center",
      "n'sNL"
    ],
    "license": "No License",
    "dateCreated": "2016-02-29T15:52:43Z",
    "numIssues": 2,
    "downloads": 0,
    "fulltext": "     MR. BRENDAN F WRINGE (Orcid ID :     10.1111/1755-0998.12704   l hybriddetective: a workflow and package c to facilitate the detection of hybridization i utsing genomic data in R     Brendan F. Wringe  bwringe@gmail.com  1    Ryan R. E. Stanley  1    Nicholas W. Jeffery  1    Eric C. Anderson  0    Ian R. Bradbury  1    0  Fisheries Ecology Division, National Oceanic and Atmospheric Administration Southwest Fis    1  Science Branch, Department of Fisheries and Oceans Canada ,  80 East White Hills Road, St. Joh    2  heries Science Center ,  Santa Cruz, CA, 95060 , USA    3  n'sNL ,  A1C 5X1    000  0  0002   r    Running title  hybriddetective  hybrid detection workflow p       -\r\n  e The ability to detect and characterize hybridization in nature has long been of interest to many fields of biology and often has direct implications for wildlife management and conservation. The capacity to identify the presence of hybridization, and quantify the numbers of individuals beleonging to different hybrid classes, permits inference on the magnitude of, and time scale over whlich, hybridization has been, or is occurring. Here we present an R package and associated workflow developed for the detection, with estimates of efficiency and accuracy, of multigenerational hybrid individuals using genetic or genomic data in conjunction with the program NEcWHYBRIDS. This package includes functions for the identification and testing of diagnostic paniels of markers, the simulation of multi-generational hybrids, and the quantification and visualization of the efficiency and accuracy with which hybrids can be detected. Overall, this pactkage delivers a streamlined hybrid analysis platform, providing improvements in speed, ease of use and repeatability over current ad hoc approaches. The latest version of the package and associated documentation are available on GitHub (htrtps://github.com/bwringe/hybriddetective).  A    Introduction\r\n  Detecting and elucidating patterns of hybridization between individuals from genetically d distinct populations is of interest in many fields of biology  (Abbott et al. 2013; Payseur &amp; Rieeseberg 2016; Todesco et al. 2016) . Naturally occurring hybrid zones - areas where genetically distinct populations come into contact and create genetically (ad)mixed offspring t are important natural laboratories to study of the interplay between selection and recombination  (Barton &amp; Hewitt 1985; Burke &amp; Arnold 2001; Hilbish et al. 2012) . These areas p have provided opportunities to glean information to further model, and test hypotheses related to especiation  (Abbott et al. 2013; Barton 2013; Dowling &amp; Secor 1997)  and the maintenance of reproductive barriers  (Albrechtová et al. 2012; Griebel et al. 2015; Landry et al. 2007) , natural c selection  (Johnson et al. 2010; Pruvost et al. 2013) , and genetic recombination. Hybridization can also have conservation, regulatory, and legal ramifications related to the genetic structure c and integrity of populations  (Allendorf et al. 2004; Benson et al. 2014; Boyer et al. 2008; Fitzpatrick et al. 2015; Rostgaard Nielsen et al. 2016) , or the introgression of domesticated  A (Fraser et al. 2010; Kidd et al. 2009; Noren et al. 2005) or transgenic  (Oke et al. 2013; Warwick et al. 2003)  alleles into wild populations.  In some cases, hybrid individuals can be identified morphologically (de Oliveira et al. e 2002; Ross &amp; Cavender 1981; Solomon &amp; Child 1978), however morphological classification is l notoriously imperfect (Baumsteiger et al. 2005; Esquer-Garrigos et al. 2015; Hardig et al. 2000; Neff c&amp;Smith 1979) and does not allow for the classification of hybrid category (Lamb &amp; Avise 19i87) or the examination of the effect of genetic dosage (Kierzkowski et al. 2011; Rieseberg 19t95). In contrast, the use of Mendelian genetic markers affords researchers the ability to not only ridentify individuals as hybrid or purebred, but also to characterize them to specific hybrid classes (e.g. pure, F1, F2 and backcrosses). This ability to quantify the types, and numbers of individuals of different hybrid classes present, allows inferences to be made on the magnitude  A of, and time scale over which, hybridization has been, or is occurring  (Anderson &amp; Thompson 2002; Brown et al. 2004; Godinho et al. 2015; Saarman &amp; Pogson 2015) .  dMany statistical approaches have been put forward to use genetic markers to identify hybrids (Anderson 2009), and some of these have been incorporated into widely used, and cited sofetware programs (e.g. NEWHYBRIDS [Anderson &amp; Thompson 2002]; STRUCTURE [Hubisz et al. t2009]; GENODIVE [Meirmans &amp; Van Tienderen 2004]). However, the analyses conducted by these programs is but one step in the path to go from individual genotypes, to the detection and asspignment of those individuals to a hybrid class, with quantifiable levels of certainty. The process of performing hybrid analyses currently entails the use of multiple, standalone proegrams, many of which require data to be provided in a unique format (Lischer &amp; Excoffier 2012; Stanley et al. 2017). Furthermore, the reliance on the user for file management, and for c manually implementing individual analyses with separate programs in addition to affording opcportunity for human error, leads to a disjunct analytical process with a steep learning curve that lacks the efficiency and repeatability of a true workflow.  A  Here we describe the R package hybriddetective and associated workflow for hybrid identification developed in the R computer language (R Development Core Team 2016). The package and workflow encompass every aspect of the hybrid identification procedure. Speecifically, we include functions for (1) panel design, and the quantification of the efficiency, accluracy and power of panels of diagnostic markers; (2) error checking and diagnostics; and (3) qucantification, and visualization of accuracy and assignment power of the selected panel(s). hybiriddetective's simulation and panel selection functions have been designed to work in contcert, as a workflow, to improve the accuracy, and reduce the overestimation of assignment certainty (Anderson &amp; Thompson 2002), and concomitantly reduce high-grading bias r  (described in detail below; Anderson 2010) . This package alleviates much of the complexity in the hybrid detection process, reduces the potential for human error, and at the same time offers sigAnificant speed improvements over previous ad hoc methodologies.  Description of the package  d hybriddetective is compiled as an R package which facilitates a workflow within the R e environment for the detection of hybrids based on genotypic/genomic information using the t program NEWHYRIDS (Anderson &amp; Thompson 2002), and provides a comprehensive and repeatable framework to move from genotypic data to the identification, with quantifiable p certainty, of hybrid individuals. hybriddetective is comprised of 14 functions (Table 1), three exaemple datasets, and a README. Function descriptions (Table 1), example data, and installation instructions are available online https://github.com/bwringe/hybriddetective. For an cexample of the hybriddetective workflow, see Jeffery et al. (2017), and Supplementary Figure 1 . We chose to implement hybrid detection using the program NEWHYRIDS (Anderson &amp; c Thompson 2002) because it permits the assignment of individuals to hybrid class (i.e. purebred, F1, F2, and back-crosses) and does not require a priori knowledge of the allele frequencies  A of the two populations being tested (Anderson &amp; Thompson 2002). Moreover, NEWHYBRIDS is widely used, having been cited over 800 times as of the time of this writing.    Deescription of the workflow\r\n  l The cworkflow can be broken down into three major elements: 1) data preparation, 2) error cheicking and diagnostics, and 3) quantification and analysis. Data preparation encompasses the tprocess of selecting the n most informative loci from amongst the genotypic data available, and the simulation of multi-generational hybrids. After analyzing the simulated data with r NEWHYBRIDS, error checking and diagnostics functions confirm that NEWHYBRIDS MCMC chains reached convergence and quantification and analysis functions test, quantify, and visAualizethe accuracy and assignment power of the selected panel(s). The workflow and the functions used in each step are illustrated and described in in Figure 1, and Table 1, respectively. We have also included a brief section on the implementation of (parallel) NEdWHYBRIDS analyses using the related R package parallelenewhybrid (Wringe et al. 2017).   Daetapreparation\r\n  Patnel selection Panel pselection is the process of selecting from amongst the available markers (i.e. thousands to several hundreds of thousands as produced by RAD-seq) a subset that together permit accurate ideentification of hybrids. In our workflow, the function getTopLoc is used to develop a panel of user defined size, of the most informative (based on global Weir and Cockerham (1984)'s FST) c loci that are not in linkage disequilibrium (LD). Genotype data of individuals known (or suscpected with high certainty (Oliveira et al. 2015)), to be of pure ancestry from the two populations potentially hybridizing are used as input for getTopLoc. getTopLoc first randomly creates two subsamples, each comprised of 50% of the individuals from each of the two  A populations, to create validation and training datasets. To prevent any \u201chigh-grading\u201d bias (i.e. upward bias in the estimation of predictive capacity caused when the same data is used to both select and validate panels of markers), getTopLoc uses subsampling to ensure the same indeividuals are not used to create the panel and to validate it. The function uses the training datlaset to calculate the global, locus-specific Weir and Cockerham's FST and ranks loci by this mectric. Pairwise LD is then calculated using the training dataset for all loci within one or both poipulations at the users' discretion. During this process the r2 threshold above which to contsider a pair of loci to be in LD can be defined by the user. Any loci that are in LD are removed, because NEWHYBRIDS assumes no linkage, and each locus is treated as independent.  r getTopLoc returns a list of panel loci names , a list of individuals (IDs) in the validation dataset, and the genotypes of those individuals at the panel loci. Importantly, random sampling selects Athe individuals in the training and validation datasets, so the individuals and corresponding panel can vary each time the function is run. The variance in global pairwise FST, and hence the loci returned between runs, will likely be greatest where sample sizes for one or both dpopulations are small, and consequently subsampling is more apt to impart stochastic variances in allele/gene frequencies.  e Construction of multi -generational simulated hybrids  t The next step in our workflow is to generate simulated multi-generational hybrid datasets using p the genotypic data from the validation dataset exported by getTopLoc. The two simulation funections, freqbasedsim_GTFreq and freqbasedsim_AlleleSample differ in the way in which they create hybrids. freqbasedsim_GTFreq was designed to simulate individuals within the R envcironment analogously to the commonly used hybrid simulation program HYBRIDLAB (Nielsen et al. 2006). In freqbasedsim_GTFreq, like in HYBRIDLAB, individuals in generation t+1 c are created by sampling one allele per locus from the generation t parental populations, based on the allele frequencies in either population. Unlike HYBRIDLAB, freqbasedsim_GTFreq creates muAlti-generational hybrids, each time it is run, and requires only a single data file to do so. In controlled comparisons with HYBRIDLAB we find freqbasedsim_GTFreq to be more than 20X faster when creating multiple independent simulations (See Supplemental Table 1).  The other hybrid simulation function, freqbasedsim_AlleleSample, was designed with the e intent of providing an additional simulation method. It first randomly subsamples a proportion l of individuals from each of the two populations provided to it and only the alleles of these indcividuals will be available during the subsequent simulation. Secondly, to conduct the actual i simulations, each locus in individuals in generation t+1 is simulated by randomly sampling witthout replacement, one allele from among all the alleles present at that locus from one of the parrental populations at time t, then combining it with an allele chosen in the same manner from the other parental population at time t. In this case, the number of individuals that can be simulated in a given hybrid generation is therefore dependent upon the number of individuals  A sampled in the first step. (Parallel) NEWHYBRIDS analyses  d For actual hybrid identification, we encourage users to take advantage of the R package paerallelnewhybrid, which was developed to run NEWHYBRIDS in parallel thus providing sigtnificant speed improvements (Wringe et al. 2017). Furthermore, the error checking and analytical functions described below were designed to work with the file structure created by paprallelnewhybrid. parallelnewhybrid, and documentation describing its installation and operation can be found at https://github.com/bwringe/parallelnewhybrid.  e c    Error checking and diagnostics\r\n  ChceckMarkov chain convergence  A  As with any MCMC process using Gibbs sampling, chain convergence in NEWHYBRIDS is dependent upon the 'topography' of the probability space relative to the starting point of the chain. Occasionally, the MCMC chains in NEWHYBRIDS analyses will fail to converge. In these caseesNEWHYBRIDS will almost invariably report that (nearly) all individuals have the highest polsterior probability of membership in the F2 hybrid class, a result that is clearly erroneous. To this cend, the function nh_preCheckR quickly checks the results of NEWHYBRIDS flagging those thaitmay have failed to converge, and the function nh_multiplotR complements it by visualizing its tresults. nh_preCheckR inspects the NEWHYBRIDS output and identifies the individuals that are known to be pure-bred in origin, and checks that a user defined proportion of these r individuals have not been assigned posterior probability of assignments (PofZ; Anderson 2003) to the F2 hybrid class in excess of a user defined threshold. If these conditions are violated, the user Aprompted is to verify the(se) result(s). nh_multiplotR permits the user to visualize the cumulative posterior probability of assignment for all genotype frequency classes for each individual. nh_multiplotR can thus be used to confirm and compliment the results of nhd_preCheckR, as well as quickly visualize the results of multiple NEWHYBRIDS analyses.    Queantification and analysis\r\n  Astsess panel accuracy The next step in the workflow, after confirming convergence, is to assess the ability of p NEWHYBRIDS to assign simulated individuals of known hybrid ancestry to the correct genotype freequency class given the genotypes of the individuals at the loci in the selected panel. Because it is impossible to statistically validate the assumed distribution of priors, and the efficacy of the c loci in a panel a priori (Anderson 2003; Nielsen et al. 2006; Oliveira et al. 2008), simulations are often cemployed to evaluate power (Anderson 2003; Nielsen et al. 2006; Vähä &amp; Primmer 2006). Also, Anderson and Thompson (2002), note that the power of NEWHYBRIDS to distinguish among genotype frequencies classes will vary across classes. Thus when evaluating a potential thrAeshold value of posterior probability of assignment for assigning genotype frequency class membership, the effect of choice of posterior probability of assignment value on efficiency, accuracy and overall performance (Vähä &amp; Primmer 2006), as well as on both Type I and Type II error should be considered simultaneously for each genotype frequency class, and for the difeferentiation of purebreds from any type of hybrid.  l  In order to allow researchers to better evaluate the effect of choice of critical posterior procbability of assignment threshold (i.e. posterior probability value above which assignment to i a given hybrid class is accepted) on assignment success, we have developed the function hybtridPowerComp. hybridPowerComp calculates the number of individuals of known hybrid class rcorrectly assigned over the total number of individuals known to belong to that class for posterior probability of assignment thresholds between 0.50 and 1.0 (i.e. number detected / number expected; \"efficiency\" sensu Vähä &amp; Primmer 2006). This is done for each hybrid  A frequency class (Figure 2), as well as separately for the two parental classes, and all hybrids classes considered together (i.e. posterior probability of assignment for hybrid is the sum of all of F1, F2, BC1, BC2). In addition, hybridPowerComp calculates and plots the number of d individuals correctly assigned to a class over the total number of individuals assigned to that class e(i.e. \"accuracy\" sensu Vähä &amp; Primmer 2006)(Figure 3), and the \u201cpower\u201d (i.e. the product of \"efficiency\" and \"accuracy\" sensu Vähä &amp; Primmer 2006) of the panel . Similarly, the number t of individuals wrongly deemed to belong to hybrid genotype frequency classes divided by the total number of known pure individuals  (i.e. type I error; Burgarella et al. 2009) , and the p proportion of individuals misclassified (i.e. type II error) are assessed and plotted. hyberidPowerComp allows visualization of the distribution of posterior probability of assignment values by plotting them for each genotype frequency class, as well as for all hybrid classes concsidered together (refer to Supplementary Table 2 for a list of the plots produced by hybridPowerComp).  c  A  The function nh_panel_delta_plotR complements hybridPowerComp by visualizing the efficacy of different panel sizes for each genotype frequency class and can be used during the assessment of panel accuracy phase of the workflow.  e l Combine simulated and experimental data for analysis c i Once the panel and critical posterior probability of assignment threshold(s) have been finalized, t the experimental/unknown data can be analyzed. Combining simulated data with the unrknown/experimental data (1) assists with the interpretation of results in the absence of known individuals, and (2) allows the user the option to designate the genotype frequency class meAmbership of known individuals, to improve assignment power (Anderson 2003; Anderson &amp; Thompson 2002).  The function nh_analysis_generateR allows researchers to specify both the unknown and expderimental genotype data to analyze and the simulated data to combine with it, thus facilitating reproducibility of analyses as well as the ability to use the same simulated dataset(s) e from which the critical posterior probability of assignment values were determined. The t function nh_analysis_simulateR_generateR permits users to quickly create analysis-ready datasets when panel development and/or more conservative simulation methodology are not p required. This function uses the frequency based simulation algorithm and simulation options of freqbasedsim_GTFreq to create simulated hybrids based on supplied genotype data, and then e merge them with experimental or unknown genotypes.  c     Conclusions\r\n  c Here we have shown that the use of hybriddetective as part of a workflow in the detection of hybAridshas clear and quantifiable benefits over the generally ad hoc, methods normally used. hybriddetective provides researchers an efficient platform for reproducible analyses of hybridization within the R computational language. Furthermore, the interoperability of hybriddetective for the simulation of multi-generation hybrid datasets and the separate R pacekage parallelnewhybrid (Wringe et al. 2017) to efficiently and automatically execute runs of NElWHYBRIDS in parallel, makes it tractable to quantify the expected variability in hybrid asscignment success.   In iconclusion, we have created an R package and associated workflow for the detection, with\r\n  qutantifiable accuracy, efficiency and power, of multi-generational hybrid individuals using genretic or genomic data with the program NEWHYBRIDS. This package includes functions for the development and testing of diagnostic panels of markers, the simulation of multigenerational hybrids, and the quantification and visualization of the accuracy with which  A (simulated) hybrids can be detected. Use of this package offers improvements in the repeatability, speed, and ease of use over conventional approaches.  d     Acknowledgements\r\n  e   The tauthors wish to thank Marion Sinclair-Waters, Justine Létourneau, and Anne-Laure\r\n  Ferchaud for their help bug-checking the code. We also thank Thierry Gosselin for encouraging us ppublish to this package. The manuscript was greatly improved by comments from Sarah Lehnert and three anonymous reviewers. This work was supported by a Natural Sciences and Enegineering Research Council Strategic Project Grant, a Natural Sciences and Engineering Research Discovery Grant, and Canadian Healthy Oceans Network, and Fisheries and Oceans c Canada funding (International Governance Strategy; Programme for Aquaculture Regulatory Recsearch; Genomics Research and Development Initiative) to I.R.B.  A     Author contributions\r\n  B.F.W. wrote the manuscript and the package code, and developed the supporting documentation and example data files hosted on GitHub. R.R.E.S, N.F.W., E.C.A., and I.R.B. all e contributed to the initial concept, development of the code, and associated documentation, as l well as assisting in writing of the manuscript.  c    Datta Accessibility\r\n  r    The package, user manual, README, example workflow, and example data sets are all available online from htAtps://github.com/bwringe/hybriddetective. References\r\n  Anderson EC, Thompson EA (2002) A model-based method for identifying species hybrids using  multilocus genetic data. Genetics 160, 1217-1229.  Barton NH (2013) Does hybridization influence speciation? Journal of Evolutionary Biology 26,  267-269.  BaertonNH, Hewitt GM (1985) Analysis of hybrid zones. Annual Review of Ecology and  l Systematics 16, 113-148.  Baumsteiger J, Hankin D, Loudenslager EJ (2005) Genetic analyses of juvenile steelhead, coastal ccutthroat trout, and their hybrids differ substantially from field identifications.  Transactions of the American Fisheries Society 134, 829-840. de eOliveira AC, Garcia AN, Cristofani M, Machado MA (2002) Identification of citrus hybrids  through the combination of leaf apex morphology and SSR markers. Euphytica 128, 397t403.  Dowling TE, Secor CL (1997) The role of hybridization and introgression in the diversification of  animals. Annual Review of Ecology and Systematics 28, 593-619.  p Esquer-Garrigos Y, Hugueny B, Ibañez C, et al. (2015) Detecting natural hybridization between  two vulnerable Andean pupfishes (Orestias agassizii and O. luteus) representative of the eAltiplano endemic fisheries. Conservation Genetics 16, 717-727.  Fitzpatrick BM, Ryan ME, Johnson JR, Corush J, Carter ET (2015) Hybridization and the species  cproblem in conservation. Current Zoology 61, 206-216.  Fraser DJ, Minto C, Calvert AM, Eddington JD, Hutchings JA (2010) Potential for domesticated  wild interbreeding to induce maladaptive phenology across multiple populations of wild c  Atlantic salmon (Salmo salar). Canadian Journal of Fisheries and Aquatic Sciences 67, 1768-1775.  A  Godinho R, López-Bao JV, Castro D, et al. (2015) Real-time assessment of hybridization between wolves and dogs: combining noninvasive samples with ancestry informative markers.  Molecular Ecology Resources 15, 317-328.  Griebel J, Giessler S, Poxleitner M, et al. (2015) Extreme environments facilitate hybrid esuperiority - the story of a successful Daphnia galeata x longispina hybrid clone. PLoS  One 10, e0140275.  l Hardig TM, Brunsfeld SJ, Fritz RS, Morgan M, Orians CM (2000) Morphological and molecular  evidence for hybridization and introgression in a willow (Salix) hybrid zone. Molecular cEcology 9, 9-24.  Hilibish TJ, Lima FP, Brannock PM, et al. (2012) Change and stasis in marine hybrid zones in  tresponse to climate warming. Journal of Biogeography 39, 676-687.  Hubisz MJ, Falush D, Stephens M, Pritchard JK (2009) Inferring weak population structure with  rthe assistance of sample group information. Molecular Ecology Resources 9, 1322-1332.  Jeffery NW, DiBacco C, Wringe BF, et al. (2017) Genomic evidence of hybridization between two independent invasions of European green crab (Carcinus maenas) in the Northwest  Atlantic. Heredity (Edinb).  A Johnson JR, Fitzpatrick BM, Shaffer HB (2010) Retention of low-fitness genotypes over six decades of admixture between native and introduced tiger salamanders. BMC  Evolutionary Biology 10, 147.  Kidd dAG, Bowman J, Lesbarreres D, Schulte-Hostedde AI (2009) Hybridization between escaped  domestic and wild American mink (Neovison vison). Molecular Ecology 18, 1175-1186.  Kierzkowski P, Pasko L, Rybacki M, Socha M, Ogielska M (2011) Genome dosage effect and ehybrid morphology - the case of the hybridogenetic water frogs of the Pelophylax  esculentus complex. Annales Zoologici Fennici 48, 56-66.  t Lamb T, Avise JC (1987) Morphological variability in genetically defined categories of anuran  hybrids. Evolution 41, 157-165.  LanpdryCR, Hartl DL, Ranz JM (2007) Genome clashes in hybrids: insights from gene expression.  Heredity 99, 483-493.  Lisecher HEL, Excoffier L (2012) PGDSpider: an automated data conversion tool for connecting  population genetics and genomics programs. Bioinformatics 28, 298-299.  Mecirmans PG, Van Tienderen PH (2004) GENOTYPE and GENODIVE: two programs for the  analysis of genetic diversity of asexual organisms. Molecular Ecology Notes 4, 792-794.  Neff NA, Smith GR (1979) Multivariate-analysis of hybrid fishes. Systematic Zoology 28, 176c  196.  Nielsen EE, Bach LA, Kotlicki P (2006) HYBRIDLAB (version 1.0): a program for generating  simulated hybrids from population samples. Molecular Ecology Notes 6, 971-973.  A  Noren K, Dalen L, Kvaloy K, Angerbjorn A (2005) Detection of farm fox and hybrid genotypes  among wild arctic foxes in Scandinavia. Conservation Genetics 6, 885-894.  Oke KB, Westley PAH, Moreau DTR, Fleming IA (2013) Hybridization between genetically  modified Atlantic salmon and wild brown trout reveals novel ecological interactions.  eProceedings of the Royal Society B-Biological Sciences 280.  Olilveira R, Godinho R, Randi E, Alves PC (2008) Hybridization versus conservation: are domestic cats threatening the genetic integrity of wildcats (Felis silvestris silvestris) in  Iberian Peninsula? Philosophical Transactions of the Royal Society B-Biological Sciences c363, 2953-2961.  i Oliveira R, Randi E, Mattucci F, et al. (2015) Toward a genome-wide approach for detecting thybrids: informative SNPs to detect introgression between domestic cats and European  wildcats (Felis silvestris). Heredity 115, 195-205.  Payrseur BA, Rieseberg LH (2016) A genomic perspective on hybridization and speciation.  Molecular Ecology 25, 2337-2360.  Pruvost NBM, Hollinger D, Reyer H-U (2013) Genotype-temperature interactions on larval  performance shape population structure in hybridogenetic water frogs (Pelophylax A  esculentus complex). Functional Ecology 27, 459-471.  R Development Core Team (2016) R: A language and environment for statistical computing R  Foundation for Statistical Computing, Vienna, Austria.  Rieseberg LH (1995) The role of hybridization in evolution: old wine in new skins. American d  Journal of Botany 82, 944-953.  Ross MR, Cavender TM (1981) Morphological Analyses of Four Experimental Intergeneric  eCyprinid Hybrid Crosses. Copeia 1981, 377-387.  Rotstgaard Nielsen L, Brandes U, Dahl Kjaer E, Fjellheim S (2016) Introduced Scotch broom (Cytisus scoparius) invades the genome of native populations in vulnerable heathland habitats. Molecular Ecology 25, 2790-2804.  Saaprman NP, Pogson GH (2015) Introgression between invasive and native blue mussels (genus  Mytilus) in the central California hybrid zone. Molecular Ecology 24, 4723-4738.  Soleomon DJ, Child AR (1978) Identification of juvenile natural hybrids between Atlantic salmon  (Salmo salar L) and trout (Salmo trutta L). Journal of Fish Biology 12, 499-&amp;.  Stacnley RR, Jeffery NW, Wringe BF, DiBacco C, Bradbury IR (2017) genepopedit: a simple and flexible tool for manipulating multilocus molecular data in R. Molecular Ecology  Resources 17, 12-18.  c Todesco M, Pascual MA, Owens GL, et al. (2016) Hybridization and extinction. Evolutionary  Applications.  A  Vähä JP, Primmer CR (2006) Efficiency of model-based Bayesian methods for detecting hybrid individuals under different hybridization scenarios and with different numbers of loci.  Molecular Ecology 15, 63-72.  Warwick SI, Simard MJ, Légère A, et al. (2003) Hybridization between transgenic Brassica napus eL. and its wild relatives: Brassica rapa L., Raphanus raphanistrum L., Sinapis arvensis L.,  and Erucastrum gallicum (Willd.) OE Schulz. Theoretical and Applied Genetics 107, 528l 539.  Weir BS, Cockerham CC (1984) Estimating F-statistics for the analysis of population-structure.  cEvolution 38, 1358-1370.  i Wringe BF, Stanley RR, Jeffery NW, Anderson EC, Bradbury IR (2017) parallelnewhybrid: an R tpackage for the parallelization of hybrid detection using newhybrids. Molecular Ecology  Resources 17, 91-95. Fuinction Name getTopLoc t r freqbasedsim_GTFreq  A freqbasedsim_AlleleSample nh_analysis_generateR  d nhe_analysis_simulateR_genera teR  t nh_subsetR nh_Zcore  p nh_preCheckR e Table 1 - Functions included in the hybriddetective R package, a synopsis of their purpose, and which of the three major elements they are used in. c  Synopsis Main Use Creates a panel comprised of the n (user-specified) most informative (based on Data highest loci-specific FSTs), markers not in linkage disequilibrium. The function Preparation randomly assigns half the individuals in each of the two populations to be used to calculate loci-specific Weir and Cockerham's FSTs1, and returns the genotypes at the n loci of the other half to be used to test the efficacy of the panel to avoid high-grading bias2.  Creates simulated multi-generational (i.e. Pure 1, Pure 2, F1, F2, BC1, BC2) Data hybrids based on the allele frequencies in the two populations provided. The Preparation user can specify the number of individuals in each of the hybrid classes to be created.  Creates simulated multi-generational hybrids by randomly sampling, without Data replacement, two alleles per loci from a proportion of the individual genotypes Preparation provided. The user is able to specify the proportion of genotypes to sample, as well as the number of individuals of each hybrid class to create.  Merges a file composed of simulated hybrid genotypes with a file containing the Data genotypes of unknown/experimental individuals to produce a file suitable to Preparation ascertain the hybrid class of the unknowns. The user is able to specify which hybrid classes from the simulated dataset to include in the output.  Creates a simulated multi-generational hybrid reference dataset from user Data provided data, and then merges it with the genotypes of unknown/experimental Preparation individuals. This function will create a new simulated dataset each time it is run using the same simulation methodology as freqbasedsim_GTFreq.  Removes subsets of desired loci from NEWHYBRIDS formatted files so that the efficacy of panels of various sizes can be assessed.  Allows the user to assign known hybrid category designations to individuals in NEWHYBRIDS formatted files Checks all NEWHYBRIDS results within a directory and flags those that show evidence that the Markov chain may have failed to converge. This is done by evaluating the proportion of known Pure Population 1 or 2 individuals in which the posterior probability of assignment to F2 exceeds a threshold. The user may specify both the proportion of individuals and the PofZ threshold.  Data Preparation Data Preparation Error Checking and Diagnostics This article is protected by copyright. All rights reserved.  c nh_mulitplotR  c nhi_plotR  t hybridPowerComp r  A nh_accuracy_checkR d e nh_panel_delta_plotR nht_build_Example_Data 3 Veahaand Primmer (2006) 4 Burgarella et al. (2009)  Creates a cumulative probability of assignment plot for each NEWHYBRIDS result within a user-specified directory. Compliments preCheckR by allowing visually verification of Markov chain (non-) convergence.  Plots the cumulative probability of assignment of a single NEWHYBRIDS result.  Also allows the user to match plotting colours between analyses when NEWHYBRIDS reverses which population it designates Population 1 and 2.  Evaluates the accuracy3 and efficiency3 with which NEWHYBRIDS assigns individuals of known hybrid class to the correct class across a range of minimum posterior probability thresholds from 0.50 to 0.99. Calculates the number of individuals wrongly assigned to hybrid genotype frequency classes over the total number of known pure individuals (type I error)4 , and the proportion of individuals misclassified (type II error). The distribution of PofZ values for each genotype frequency class, as well as for all hybrid classes considered together is plotted. The effect of varying panel sizes on each of these variables is also evaluated. Plots are returned as .pdf and .jpg files, and all data frames constructed for plotting are exported.  Evaluates the accuracy with which NEWHYBRIDS assigns individuals of known hybrid class to the correct class for a single analysis at three minimum posterior probability thresholds (PofZ &gt;= 0.05, 0.75 and 0.90). This function is meant to compliment hybridPowerComp.  Plots the genotype class assignment (class with max. PofZ) of individuals among panels of different size. Allows visualization of the stability of individual assignments to compliment the proportion of correct assignments returned by hybridPowerComp.  Writes example NEWHYBRIDS results to be evaluated with the function hybridPowerComp Error Checking and Diagnostics Quantification and Analysis Quantification and Analysis Quantification and Analysis Quantification and Analysis This article is protected by copyright. All rights reserved.  c  Pure1 F1  BC1  A 1.00 A 0.75 0.50 0.25 d 0.00 ±ttrrscdeneoCte10..0705 s inm0.50 g s s A itfronoopp0.25 ro0.00 P e1.00 0.75 c0.50 0.25 c 0.00 0.5 0.7 F1  P2 F2 0.8    ",
    "sourceCodeLink": "https://github.com/bwringe/hybriddetective",
    "publicationDate": "0",
    "authors": [
      "Brendan F. Wringe",
      "Ryan R. E. Stanley",
      "Nicholas W. Jeffery",
      "Eric C. Anderson",
      "Ian R. Bradbury"
    ],
    "status": "Success",
    "toolName": "hybriddetective",
    "homepage": ""
  },
  "55.pdf": {
    "forks": 2,
    "URLs": [
      "github.com/aquaskyline/16GT",
      "github.com/aquaskyline/16GT/releases/tag/1.0"
    ],
    "contactInfo": ["rluo5@jhu.edu"],
    "subscribers": 6,
    "programmingLanguage": "Perl",
    "shortDescription": "Simultaneous detection of SNPs and Indels using a 16-genotype probabilistic model",
    "publicationTitle": "16GT: a fast and sensitive variant caller using a 16-genotype probabilistic model",
    "title": "16GT: a fast and sensitive variant caller using a 16-genotype probabilistic model",
    "publicationDOI": "10.1093/gigascience/gix045",
    "codeSize": 1323,
    "publicationAbstract": "16GT is a variant caller for Illumina whole-genome and whole-exome sequencing data. It uses a new 16-genotype probabilistic model to unify single nucleotide polymorphism and insertion and deletion calling in a single variant calling algorithm. In benchmark comparisons with 5 other widely used variant callers on a modern 36-core server, 16GT demonstrated improved sensitivity in calling single nucleotide polymorphisms, and it provided comparable sensitivity and accuracy for calling insertions and deletions as compared to the GATK HaplotypeCaller. 16GT is available at https://github.com/aquaskyline/16GT.",
    "dateUpdated": "2017-08-01T15:05:16Z",
    "institutions": [
      "Johns Hopkins University School of Medicine",
      "Johns Hopkins University"
    ],
    "license": "No License",
    "dateCreated": "2017-01-03T19:55:45Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     10.1093/gigascience/gix045   16GT: a fast and sensitive variant caller using a 16-genotype probabilistic model     Ruibang Luo  rluo5@jhu.edu  0  1    Michael C. Schatz  0  1    Steven L. Salzberg  0  1  2    0  Center for Computational Biology, McKusick-Nathans Institute of Genetic Medicine, Johns Hopkins University School of Medicine ,  Baltimore, MD 21218 ,  USA    1  Department of Computer Science, Johns Hopkins University ,  Baltimore, MD 21218 ,  USA    2  Departments of Biomedical Engineering and Biostatistics, Johns Hopkins University ,  Baltimore, MD 21218 ,  USA     2017     13  6  2017    25  4  2017    22  5  2017     16GT is a variant caller for Illumina whole-genome and whole-exome sequencing data. It uses a new 16-genotype probabilistic model to unify single nucleotide polymorphism and insertion and deletion calling in a single variant calling algorithm. In benchmark comparisons with 5 other widely used variant callers on a modern 36-core server, 16GT demonstrated improved sensitivity in calling single nucleotide polymorphisms, and it provided comparable sensitivity and accuracy for calling insertions and deletions as compared to the GATK HaplotypeCaller. 16GT is available at https://github.com/aquaskyline/16GT.    variant calling  Bayesian model  SNP calling  indel calling       Background\r\n  Single nucleotide polymorphisms (SNPs) and insertions and deletions (indels) that occur at a specific genome position are interdependent; i.e., evidence that elevates the probability of 1 variant type should decrease the probability of other possible variant types, and the probability of all possible alleles should sum to 1. However, widely used tools such as GATK's UniifedGenotyper [  1 ] and SAMtools [ 2 ] use separate models for SNP and indel detection. The model for SNP calling in these 2 tools is nearly identical: both assume all variants are biallelic (i.e., exactly 2 haplotypes are present) and use a probabilistic model allowing for 10 genotypes: AA, AC, AG, AT, CC, CG, CT, GG, GT, TT. For indel calling, the GATK UnifiedGenotyper uses a model from the Dindel's variant caller [ 3 ], while SAMtools' model is from BAQ [ 4 ].    Findings\r\n  In order to detect SNPs and indels with a unified approach, we developed a new 16-genotype probabilistic model and its implementation, named 16GT. Building on an idea first introduced in Luo et al. [ 5 ], 16GT uses an empirically improved model and is the first publicly available implementation. Using X and Y to denote the indels with the highest (X) and second highest (Y) support, we add 6 new genotypes (AX, CX, GX, TX, XX, and XY) to the traditional 10-genotype probabilistic model. The 6 new genotypes include: (i) 1 homozygous indel (XX); (ii) 1 reference allele plus 1 heterozygous indel (AX, CX, GX, TX); (iii) 1 heterozygous SNP plus 1 heterozygous indel (AX, CX, GX, TX, reusing the genotypes in ii); and (iv) 2 heterozygous indels (XY). We exclude the 5 possible combinations AY, CY, GY, TY, and YY because X has higher support than Y. By unifying SNP and indel calling in a single variant calling algorithm, 16GT not only runs 4 times faster, but also demonstrates improved sensitivity in calling SNPs and comparable sensitivity in calling indels to the GATK HaplotypeCaller.  Posterior probabilities of these 16 genotypes are calculated using a Bayesian model P(L|F)∝P(F|L)P(L), where L is an assumed genotype. F refers to the observation of the 6 alleles (A, C, G, T, X, Y) at a given genome position. P(L) is the prior probability of the genotype, P(F|L) is the likelihood of the observed genotype, and P(L|F) is the posterior probability of the genotype. The resulting genotype Lmax is assigned to the genotype with the highest posterior probability. The distance between the highest posterior probability and the second highest posterior probability is used as a quality metric in 16GT, along with some other metrics introduced by GATK (GATK, RRID: SCR 001876) [ 1 ].   Calculating the probability of an observation F given the genotype L\r\n  To test how well an observation fits the expectation of different genotypes, we use a 2-tailed Fisher exact test P and use the resulting P-value as the goodness of fit. When calculating the likelihood of a homozygous genotype, ideally we expect 100% single allele support from the observation. For example, consider genotype \u201cAA\u201d:  P (F | AA ) = Phom (FA) × Pe (FC , FG , FT , FX, FY) , where Pe is the probability of an erroneous base call.  For a heterozygous genotype, 50% support is expected for each allele in the genotype; e.g., consider \u201cCG\u201d:  P (F | C G ) = Phet (FC , FG ) × Pe (FA, FT , FX, FY) , where  Phom (FA) = P  FA  F (1 − Perr ) F F Phet (FC , FG ) = i=C,G  P  Fi  F (0.5 − Perr ) F F Pe (FA, FT , FX, FY) = P  FA + FT + FX + FY F  Perr × F  F n i=1 Fs =  f (Qi , Mi , s) s {A, C, G, T, X, Y} , where s is the allele type, n is the number of reads supporting allele s, Qi is the base quality, and Mi is the mapping quality. f is a function describing how s, Qi, and Mi change the observation: ⎧ α = 0 i f Mi = 0 ⎪⎪ ⎪⎪⎪⎪ α = 1 i f Mi = 0 ⎪⎪ ⎪⎪⎪⎪ β = 0 i f Qi &lt; 10 ⎪⎪ ⎪⎪⎪⎪⎪ β = 1 i f 10 ≤ Qi &lt; 13 ⎪⎪ f (Qi , Mi , s) = α × β × γ ⎨ β = 2 i f 13 ≤ Qi &lt; 17 . ⎪⎪⎪⎪⎪⎪⎪ β = 3 i f 17 ≤ Qi &lt; 20 ⎪⎪⎪⎪ β = 4 i f Qi ≥ 20 ⎪⎪ ⎪⎪⎪⎪⎪⎪⎪⎪⎩ γγ == 11.37i 5f isf s {X, Y} {A, C, G, T} The possible reasons for an observation that does not match the reference genome are (i) a true variant; (ii) an error generated in library construction; (iii) a base calling error; (iv) a mapping error; and (v) an error in the reference genome. Reasons (iii) and (iv) are explicitly captured in our model. For reasons (ii) and (v), we include 2 error probabilities, Ps for SNP error and Pd for indel error. We define Perr as Ps+Pd, where Ps and Pd are set to 0.01 and 0.005, respectively. These 2 values were set empirically based on the observation that SNP errors are more common than indel errors in library construction and in the reference genome.  In addition, most short read aligners use a dynamic programming algorithm to enable gapped alignment, using a scoring scheme that usually penalizes gap opening and extension more than mismatch. Consequently, authentic gaps that occur at an end of a read are more likely to be substituted by a set of false SNPs or alternatively to get trimmed or clipped. Thus, we applied a coefficient γ to weight indel observations more than SNPs in order to increase the sensitivity on indels.    Calculating the probability of the genotype L\r\n  Given (i) a known rate of single nucleotide differences between 2 unrelated haplotypes; (ii) a known rate of single indel differences between 2 unrelated haplotypes; and (iii) a known Transitions to Transversions ratio (Ti/Tv), the 16GT model's prior probabilities are calculated as shown in Table 1.  Given (i) a known rate θ of single nucleotide differences between 2 unrelated haplotypes; (ii) a known rate ω of single indel differences between 2 unrelated haplotypes; and (iii) a known Ti/Tv ε, transition is expected to occur more frequently than transversion under selective pressure. The default known rates for human genome are θ = 0.001, ω = 0.0001, ε = 2.1, where ε is set to the value for human and needs to be changed for other species.   Results\r\n  We benchmarked 16GT with GATK UnifiedGenotyper, GATK HaplotypeCaller (GATK, RRID: SCR 001876) [ 1 ], Freebayes (FreeBayes, RRID: SCR 010761) [ 6 ], Fermikit [ 7 ], ISAAC (Isaac, RRID: SCR 012772) [ 8 ], and VarScan2 [ 9 ] using a set of very highconfidence variants developed by the Genome in a Bottle project for genome NA12878 (Coriell Cat# GM12878, RRID: CVCL 7526; version 2.19) (Additional File 1: Supplementary Note) [ 10 ]. The results are shown in Table 2 and as receiver operating characteristic curves in Supplementary Fig. S1.  For SNPs, 16GT produced the most true positive calls and the fewest false negative calls; i.e., it has the highest sensitivAA GG CC, TT AG AC, AT CG, GT CT AX GX CX, TX XX XY  Hom.  Hom.  Hom.  Het.  Het.  Het.  Het.  Het.  Het.  Het.  Hom.  Het. ity and specificity among all tools. dbSNP version 138 also reported 79% of 16GT's false positive calls, which is the highest among other callers. However, we should point out that the GIAB variant set is biased toward GATK because it was primarily derived from GATK-based analyses, as reported previously [ 11 ]. As an orthogonal test, we further assessed the false positive calls against a set of unbiased calls made by the Illumina Omni 2.5 SNP array (Additional File 1: Supplementary Note). Among the 5346 false positive calls for 16GT, 20 were covered by the Omni array, and all 20 (100%) had the correct genotype. Although limited by the small number of measurable alleles in the Illumina Omni 2.5 SNP array, only allowing us to reassess 20 \u201cfalse positive\u201d calls as true positives, the observation that all 20 genotypes out of the 20 covered alleles are correct suggests that a number of the remaining \u201cfalse positive\u201d calls are actually correct.  For indels, 16GT produced slightly fewer true positive calls and slightly more false negative calls than HaplotypeCaller, but less than half as many false negative calls as UnifiedGenotyper. dbSNP version 138 covered 65% of 16GT's false positive indels. Further investigation into the 1462 false positive indels shows that 981 (67%) of them meet all 3 of the following criteria: (i) at least 3 reads supporting the variant; (ii) at least 1 read supporting both the positive and negative strands; and (iii) in over 80% of the reads that support the variant, there exists no other variant in its flanking 10 bp. This suggests that some of these \u201cfalse positives\u201d might be correct, although further experimental validation would be required to confirm this suggestion. Supplementary Fig. S2 shows 3 examples where the putative false positive from 16GT is likely to be correct.  Conclusions 16GT is the firstly publicly available implementation using a 16genotype probabilistic model for variant calling. Compared with local assembly based variant callers, 16GT provides better sensitivity in SNP calling and comparable sensitivity in indel calling. In the current implementation, 16GT can only be applied to germline variant detection. In the future, we will enhance 16GT to support multi-sample variant calling and GVCF output and to support somatic variant detection and extend the model to support variant calling in species with more than 2 haplotypes.     Additional files\r\n  Additional File 1.docx   Abbreviations\r\n  indel: insertions and deletions; SNP: single nucleotide polymorphism; Ti/Tv: Transitions to Transversions. We thank United Electronics Co. Limited for providing code samples for the bam2snapshot function.      Funding\r\n  This work was supported by the US National Institutes of Health under grants R01-HL129239 and R01-HG006677.    Availability of source code and requirements\r\n  Project name: 16GT Project homepage: https://github.com/aquaskyline/16GT Archived version: https://github.com/aquaskyline/16GT/releases/tag/1.0 Operating system: Platform independent Programming language: C++ and Perl Other requirements: See GitHub page License: GPLv3 Any restrictions to use by non-academics: None    Availability of supporting data and materials\r\n  Snapshots of the code and data are available in the GigaScience repository, GigaDB [ 12 ], and are also available via the Code Ocean reproducibility platform [ 13 ].    Competing interests\r\n  The authors declare that they have no competing interests.    Authors\u2019 contribution\r\n  R.L., M.C.S., and S.L.S. conceived the study. R.L. developed and implemented the 16GT algorithm and benchmarked 16GT with other variant callers. R.L., M.C.S., and S.L.S. wrote the paper. All authors have read and approved the final version of the manuscript.    ",
    "sourceCodeLink": "https://github.com/aquaskyline/16GT",
    "publicationDate": "0",
    "authors": [
      "Ruibang Luo",
      "Michael C. Schatz",
      "Steven L. Salzberg"
    ],
    "status": "Success",
    "toolName": "16GT",
    "homepage": ""
  },
  "21.pdf": {
    "forks": 1,
    "URLs": ["github.com/SharonLutz/Umediation"],
    "contactInfo": ["sharon.lutz@ucdenver.edu"],
    "subscribers": 1,
    "programmingLanguage": "R",
    "shortDescription": "Examines how the results of the mediation analysis would change in the presence of unmeasured confounding.",
    "publicationTitle": "Examining the role of unmeasured confounding in mediation analysis with genetic and genomic applications",
    "title": "Examining the role of unmeasured confounding in mediation analysis with genetic and genomic applications",
    "publicationDOI": "10.1186/s12859-017-1749-y",
    "codeSize": 19,
    "publicationAbstract": "Background: In mediation analysis if unmeasured confounding is present, the estimates for the direct and mediated effects may be over or under estimated. Most methods for the sensitivity analysis of unmeasured confounding in mediation have focused on the mediator-outcome relationship. Results: The Umediation R package enables the user to simulate unmeasured confounding of the exposure-mediator, exposure-outcome, and mediator-outcome relationships in order to see how the results of the mediation analysis would change in the presence of unmeasured confounding. We apply the Umediation package to the Genetic Epidemiology of Chronic Obstructive Pulmonary Disease (COPDGene) study to examine the role of unmeasured confounding due to population stratification on the effect of a single nucleotide polymorphism (SNP) in the CHRNA5/ 3/B4 locus on pulmonary function decline as mediated by cigarette smoking. Conclusions: Umediation is a flexible R package that examines the role of unmeasured confounding in mediation analysis allowing for normally distributed or Bernoulli distributed exposures, outcomes, mediators, measured confounders, and unmeasured confounders. Umediation also accommodates multiple measured confounders, multiple unmeasured confounders, and allows for a mediator-exposure interaction on the outcome. Umediation is available as an R package at https://github.com/SharonLutz/Umediation A tutorial on how to install and use the Umediation package is available in the Additional file 1.",
    "dateUpdated": "2017-07-05T23:48:28Z",
    "institutions": [
      "University of Colorado Anschutz Medical Campus",
      "Children's Hospital Colorado"
    ],
    "license": "No License",
    "dateCreated": "2017-06-21T16:59:13Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Lutz et al. BMC Bioinformatics     10.1186/s12859-017-1749-y   Examining the role of unmeasured confounding in mediation analysis with genetic and genomic applications     Sharon M. Lutz  sharon.lutz@ucdenver.edu  0    Annie Thwing  0    Sarah Schmiege  0    Miranda Kroehl  0    Christopher D. Baker  2    Anne P. Starling  1    John E. Hokanson  1    Debashis Ghosh  0    0  Department of Biostatistics and Informatics, University of Colorado Anschutz Medical Campus ,  13001 E. 17th Place, B119 Bldg. 500 W3128, Aurora, CO 80045 ,  USA    1  Department of Epidemiology, University of Colorado Anschutz Medical Campus ,  Aurora, CO ,  USA    2  Department of Pediatrics and Pulmonary Medicine, Children's Hospital Colorado ,  Aurora, CO ,  USA     2017   18  2  7    4  7  2017    3  4  2017     Background: In mediation analysis if unmeasured confounding is present, the estimates for the direct and mediated effects may be over or under estimated. Most methods for the sensitivity analysis of unmeasured confounding in mediation have focused on the mediator-outcome relationship. Results: The Umediation R package enables the user to simulate unmeasured confounding of the exposure-mediator, exposure-outcome, and mediator-outcome relationships in order to see how the results of the mediation analysis would change in the presence of unmeasured confounding. We apply the Umediation package to the Genetic Epidemiology of Chronic Obstructive Pulmonary Disease (COPDGene) study to examine the role of unmeasured confounding due to population stratification on the effect of a single nucleotide polymorphism (SNP) in the CHRNA5/ 3/B4 locus on pulmonary function decline as mediated by cigarette smoking. Conclusions: Umediation is a flexible R package that examines the role of unmeasured confounding in mediation analysis allowing for normally distributed or Bernoulli distributed exposures, outcomes, mediators, measured confounders, and unmeasured confounders. Umediation also accommodates multiple measured confounders, multiple unmeasured confounders, and allows for a mediator-exposure interaction on the outcome. Umediation is available as an R package at https://github.com/SharonLutz/Umediation A tutorial on how to install and use the Umediation package is available in the Additional file 1.    Mediation analysis  Mediated effects  Direct effects  Unmeasured confounding  Population stratification       Background\r\n  With the recent availability of genome wide genetic and omics data in large population studies, researchers have an opportunity to interrogate the biological pathways by which specific genetic susceptibility is associated with adverse clinical outcomes. For example, there is a replicated genome wide association study (GWAS) signal in the CHRNA5/3/B4 locus on chromosome 15q25.1 that is associated with decreased lung function (FEV1) [ 11 ] and cigarette smoking [ 12 ]. Mediation analysis decomposes this observed effect into a direct effect (i.e. the effect of the single nucleotide polymorphism (SNP) on FEV1 not through the mediator, cigarette smoking) and the mediated effect (i.e. the effect of the SNP on FEV1 through cigarette smoking).  However, the identification of direct and mediated effects relies on strong assumptions, including the assumption of no unmeasured confounding [ 25 ]. Most methods for the sensitivity analysis of unmeasured confounding in mediation have focused on the mediator-outcome relationship. These sensitivity analysis techniques for unmeasured confounding of the mediator-outcome relationship rely on: multiple modeling assumptions [ 6 ], sensitivity parameters involving counterfactual terms [ 21 ], specifying several sensitivity parameters [ 25 ], imposing no assumptions but providing large bounds of the estimates [ 17, 20 ], or imposing no assumptions and providing narrower bounds of the estimates [ 3 ]. The causal inference test (CIT) R package also assumes the exposure is completely randomized and checks the 4 association assumptions of the \u201cCausality Equivalence Theorem\u201d [ 15 ]. The left out variables error (L.O.V.E.) method [ 14 ] has been used to assess the bounds of correlation of a potential unmeasured confounder with the exposure, mediator, and outcome for a single mediator model [ 2 ] and multilevel mediation models [ 24 ]. However, these methods assume that the outcome and mediator are normally distributed and there is no exposure-mediator interaction on the outcome [ 2 ].  Most of these sensitivity analysis methods have focused on the role of unmeasured confounding of the mediator-outcome relationship since it is often assumed that the exposure is completely randomized such that there is no unmeasured confounding of the exposure-mediator and exposure-outcome relationships. However, for population based genetic association studies due to non-random mating, a subject's DNA is not truly randomized. For example, when examining the effect of a SNP in the CHRNA5/3/B4 locus on FEV1 as mediated by average cigarettes per day, unmeasured population stratification can be a confounder of the exposure-outcome and exposuremediator relationships.  The Umediation R package uses simulation studies to examine the role of unmeasured confounding on the estimates for the direct and mediated effects allowing for unmeasured confounding of the exposure-outcome, exposure-mediator, and mediator-outcome relationships. This flexible R package allows for normally distributed or Bernoulli distributed exposures, outcomes, mediators, measured confounders, and unmeasured confounders. Umediation also accommodates multiple measured confounders, multiple unmeasured confounders, and allows for a mediator-exposure interaction on the outcome.    Implementation\r\n  Umediation is available as an R package at https://github.com/SharonLutz/Umediation A full tutorial illustrating how to install and use the Umediation package is available in the Additional file 1.   Input\r\n  The user specifies the relationship between the exposure A, the mediator M, the outcome Y, measured confounders C, and unmeasured confounders U as seen in Fig. 1. Umediation generates the continuous, normally distributed exposure A, mediator M, and outcome Y such that  E½A ¼ γ0 þ γCC þ γU U E½M  ¼ α0 þ αAA þ αCC þ αU U  E½Y ¼ β0 þ βAA þ βMM þ βI A M þ βC C þ βU where the parameters that define the exposure A (γ0 , γC , γU), the mediator M (α0 , αA , αC , αU), and the outcome Y (β0 , βA , βM , βI , βC , βU) are specified by the user. For dichotomous, Bernoulli distributed exposure A, mediator M, and/or outcome Y, the identity link is replaced by the logit link in the above equations. By changing the user specified values of γ , α , β the user is able to change the relationship between the exposure A, mediator M, outcome Y, measured confounders C, and unmeasured confounders U.    Assumptions\r\n  For the models specifying the exposure, mediator, and outcome in the input, we are assuming that the exposure, mediator, and outcome are normally distributed or Bernoulli distributed as specified above. The user needs to make sure that these model assumptions are met and may need to transform the variables accordingly. As a result, these linear regression and logistic regression models are based on the standard assumptions regarding asymptotics and the required sample size for these models should be large enough to meet these assumptions (i.e. sample size &gt;30).    Output\r\n  Once this relationship is specified, the user is able to examine how the results of the mediation analysis [ 23 ] would change if the unmeasured confounder U was included or excluded from the model via simulation studies. The function outputs the proportion of simulations where the mediated or direct effect are significant when the model does not include U versus includes U, as well as the proportion of simulations where the conclusions based on the estimates match whether U is included or excluded. The function also outputs the average estimate of the mediated effect and direct effect when the model does not include U versus includes U and the average absolute difference for the estimates when U is included or excluded from the model. The correlation between all model variables is also given in order to show how the changes in γ , α , β effects the relationship between these variables.    Data analysis example\r\n  In the COPDGene study, the effect of rs16969968 [chromosome 15q25.1] on FEV1 is mediated by average cigarettes smoked per day adjusting for known confounders: age, gender, and genetic ancestry via the first five principal components (PCs) [ 19 ]. It is possible that there is unmeasured confounding due to population stratification that is not accounted for in the first five PCs. In particular, the adjusted R squared for the mediator, average cigarettes per day, as a function of the SNP, age, gender and PCs 1-5 is 0.04 and the adjusted R squared for the outcome FEV1 as a function of the SNP, average cigarettes per day, age, gender, and PCs 1-5 and the exposure-mediator interaction is 0.32. In order to examine how this unmeasured confounding would affect the results of the mediation analysis, we used the Umediation package assuming one to two unmeasured PCs of genetic ancestry. As seen in Fig. 2, the results of the mediation analysis would not change dramatically due to unmeasured confounding of population stratification as long as the unmeasured PC has an effect similar or less than the second strongest measured PC of genetic ancestry. A full tutorial on how to install and use Umediation to recreate this data analysis and Fig. 2 is given in the supplement.     Discussion\r\n  While the assumption of no unmeasured confounding is required for mediation analysis, in reality this assumption may often be violated in large population based genetic studies. Thus, it is critical to assess the likely degree of bias introduced by unmeasured confounders within any given study. With the Umediation package, users may examine the hypothetical influence of an unobserved confounding variable on the estimation of direct and mediated effects, in the presence or absence of a statistical interaction between the exposure and mediator variables. Importantly, the package is flexible enough to allow for confounders of the exposuremediator, mediator-outcome, and exposure-outcome relationships. While the data analysis example focused on population stratification which can confound the SNP-mediator and SNP-outcome associations, the strongest potential for unmeasured confounding is lifestyle and socioeconomic factors of the mediator-outcome association, such as physical activity and education. A strength of the Umediation package is that one can simultaneously account for all of these unmeasured confounders.   Further examples\r\n  While the data analysis example focused on confounding bias due to unobserved population stratification in the decomposition of the total effect of a SNP on lung function (FEV1) through the mediator cigarettes smoking, the Umediation package is applicable and useful in any scenario where there may be unmeasured confounding of the exposure-mediator-outcome relationship (i.e. the exposure was not completely randomized). For example, Umediation has many practical and clinical uses. A relevant clinical example is the pathway by which maternal preeclampsia contributes to the risk of chronic lung disease in the newborn [ 5 ]. The effects of preeclampsia (exposure A) on the developing lung (outcome Y) are mediated by disruptions in angiogenesis (mediator M), indicated by altered pro-angiogenic umbilical cord blood biomarkers [ 4, 13 ]. However, preterm birth (measured confounder C) may confound the mediator-outcome relationship, as the degree of prematurity is associated with both severe lung disease [ 9 ] and levels of pro-angiogenic biomarkers [ 1 ]. Genetic variation affects the risk of maternal preeclampsia (exposure A), angiogenesis in the infant (mediator M), and the risk for preterm lung disease (outcome Y) [ 10 ]. However, specific genetic confounders are often unmeasured (i.e. unmeasured confounders U). While we cannot measure the confounding caused by genetic variation, we can use Umediation to explore whether this confounding is largely responsible for the observed associations. That is, previously published studies may suggest minimum and maximum boundaries of the hypothesized confounder-exposure, confounder-mediator, and confounder-outcome associations. Entering these parameters into Umediation can provide adjusted estimates.  In addition, epigenetic processes have been proposed as plausible mediators linking exposures in one period of life (for example, prenatally) to health outcomes at a later stage of life (such as childhood or adulthood). For example, maternal smoking in pregnancy has been consistently associated with a greater risk of childhood overweight and obesity in the offspring [ 16 ]. Maternal smoking is also associated with numerous detectable changes in DNA methylation in umbilical cord blood at birth [ 8 ]. An investigator may therefore ask how much of the effect of maternal smoking during pregnancy (exposure A) on offspring overweight and obesity (outcome Y) is mediated through changes in DNA methylation detectable in cord blood at birth (mediator M). There may be a number of measured confounders C influencing the probability of maternal smoking and the probability of offspring obesity, such as maternal age at delivery or household income. It is also likely that there are unmeasured confounders U of the associations between maternal smoking and cord blood DNA methylation, between cord blood DNA methylation and offspring obesity, and between maternal smoking and offspring obesity. Genetic variability in the mother and offspring is a possible confounder of each of these associations, and we must consider whether this confounding is likely to be so severe as to challenge our conclusions. Again, we can do so by setting bounds on the probable magnitude of these associations, and by using Umediation to estimate bias-adjusted associations under each of these scenarios.    Limitations\r\n  Umediation examines the role of unmeasured confounding via simulations studies by running mediation analysis [ 23 ] both with and without the unmeasured confounders U for the simulated data based on user input. This is not a theoretical approach but a simulation based approach to examine the role of unmeasured confounding of the exposure-mediator-outcome relationship. As a result, care needs to be given to the interpretation of the results and the number of simulations run. Increasing the number of simulations also increases the running time of the function. For computational efficiency, Umediation can be run in parallel on a cluster by using the seed parameter of the function to run each iteration of the simulation on separate nodes and then compiling the results after the simulations have run. This allows the Umediation function to be run for a large number of simulations in a reasonable amount of time.  Additionally, the Umediation package currently assumes no correlation between the measured confounders C and unmeasured confounders U. While this is a reasonable assumption for the data example where the measured and unmeasured confounders are PCs, the investigator needs to determine if this is a reasonable assumption for their particular question of interest. While the Umediation package allows for an exposure-mediator interaction on the outcome of interest, it is important to note that the Umediation package does not allow for interactions of the measured and unmeasured confounders with the exposure or mediator on the outcome of interest. Also, the Umediation package currently only accommodates one mediator of the exposure-outcome relationship.    Methods that allow for unmeasured confounding\r\n  While we have focused on the effect of unmeasured confounding of the exposure-mediator-outcome relationship in mediation analysis, there are methods that allow for unmeasured confounding, such as instrumental variable methods. There have been instrumental variable methods proposed that can be extended to handle mediation analysis for continuous, normally distributed outcomes [ 7 ] and binary, Bernoulli distributed outcomes [ 18 ].     Conclusions\r\n  Umediation allows investigators to make reasonable quantitative estimates of the magnitude of the effect due to unmeasured confounding of the exposure-mediator, mediator-outcome, or exposure-outcome associations. This R package accommodates multiple unmeasured confounders, which may be either Bernoulli or normally distributed. The utility of Umediation becomes apparent whenever the need arises to examine the impact of unmeasured variables in a mediation analysis: in a post-hoc setting, when data collection and analysis have already been conducted, or in the early stages of study design, when exploring the relative value accrued by collecting data on variables that may be costly or difficult to obtain. By estimating the degree of bias produced at the plausible upper and lower boundaries of the association between each unmeasured confounder and the exposure, mediator, or outcome, investigators will be able to assess whether the mediated or direct effects are likely to be over or under estimated due to unobserved confounders.    Additional file\r\n  Additional file 1: Supplemental Tutorial for Umediation: an R Package for Examining the Role of Unmeasured Confounding in Mediation Analysis with Genetic and Genomic Applications. (PDF 612 kb) Abbreviations COPD: Chronic Obstructive Pulmonary Disorder; FEV1: forced expiratory volume in the first second; GWAS: genome wide association study; PCs: principal components; SNP: single nucleotide polymorphism Acknowledgments We would like to thank the Causal Inference Working Group at the University of Colorado, Anschutz Medical Campus for their help and support. Funding This work was supported by the National Institutes of Health [grant number K01HL125858 (PI: SML), R01HL089897 (COPDGene), and R01Hl089856 (COPDGene)].  Availability of data and materials All data generated or analyzed during this study are included in this published article and the Additional file 1.  Authors' contributions SML and AT created the Umediation package under the input and guidance of SML, SS, MK, CB, AS, JEH, and DG. SML conducted the data analysis. SML, AT, SS, MK, CB, AS, JEH, and DG helped draft the manuscript and were major contributors in writing the manuscript. All authors read and approved the final manuscript.  Ethics approval and consent to participate The COPDGene study was approved by the respective clinical center institutional review boards.  Consent for publication Not applicable.  Competing interests The authors declare that they have no competing interests.    ",
    "sourceCodeLink": "https://github.com/SharonLutz/Umediation",
    "publicationDate": "0",
    "authors": [
      "Sharon M. Lutz",
      "Annie Thwing",
      "Sarah Schmiege",
      "Miranda Kroehl",
      "Christopher D. Baker",
      "Anne P. Starling",
      "John E. Hokanson",
      "Debashis Ghosh"
    ],
    "status": "Success",
    "toolName": "Umediation",
    "homepage": ""
  },
  "64.pdf": {
    "forks": 0,
    "URLs": [
      "doi.org/10.1073/pnas.0506577102",
      "doi.org/10.1371/journal.pone.0012693",
      "www.broadinstitute.org/gsea/doc/GSEAUserGuideFrame",
      "doi.org/10.1371/journal.pone.0037510",
      "doi.org/10.1186/1471-2105-8-431",
      "doi.org/10.1186/1471-2105-8-242",
      "github.com/roqe/T2GA"
    ],
    "contactInfo": [],
    "subscribers": 1,
    "programmingLanguage": "R",
    "shortDescription": "A knowledge-based T^2-statistic to perform pathway analysis for quantitative proteomic data.",
    "publicationTitle": "A knowledge-based T2-statistic to perform pathway analysis for quantitative proteomic data",
    "title": "A knowledge-based T2-statistic to perform pathway analysis for quantitative proteomic data",
    "publicationDOI": "None",
    "codeSize": 4216,
    "publicationAbstract": "Approaches to identify significant pathways from high-throughput quantitative data have been developed in recent years. Still, the analysis of proteomic data stays difficult because of limited sample size. This limitation also leads to the practice of using a competitive null as common approach; which fundamentally implies genes or proteins as independent units. The independent assumption ignores the associations among biomolecules with similar functions or cellular localization, as well as the interactions among them manifested as changes in expression ratios. Consequently, these methods often underestimate the associations among biomolecules and cause false positives in practice. Some studies incorporate the sample covariance matrix into the calculation to address this issue. However, sample covariance may not be a precise estimation if the sample size is very limited, which is usually the case for the data produced by mass spectrometry. In this study, we introduce a multivariate test under a self-contained null to perform pathway analysis for quantitative proteomic data. The covariance matrix used in the test statistic is constructed by the confidence scores retrieved from the STRING database or the HitPredict database. We also design an integrating procedure to retain pathways of sufficient evidence as a pathway group. The performance of the proposed T2-statistic is demonstrated using five published experimental datasets: the T-cell activation, the cAMP/PKA signaling, the myoblast differentiation, and the effect of dasatinib on the BCR-ABL pathway are proteomic datasets produced by mass spectrometry; and the protective effect of myocilin via the MAPK signaling pathway is a gene expression dataset of limited sample size. Compared with other popular statistics, the proposed T2-statistic yields more accurate descriptions in agreement with the discussion of the original publication. We implemented the T2-statistic into an R package T2GA, which is available at https://github.com/roqe/T2GA.",
    "dateUpdated": "2017-08-07T18:19:10Z",
    "institutions": [
      "University of Richmond",
      "3 Institute of Statistical Science"
    ],
    "license": "No License",
    "dateCreated": "2016-12-06T16:07:23Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     June      A knowledge-based T2-statistic to perform pathway analysis for quantitative proteomic data     En-Yu Lai  0  1    Yi-Hau Chen  0    Kun-Pin Wu  0  1    0  Editor: Jacquelyn S. Fetrow, University of Richmond ,  UNITED STATES    1  Institute of Biomedical Informatics, National Yang-Ming University ,  Taipei 11221, Taiwan, 2 Bioinformatics Program ,  Taiwan International Graduate Program, Institute of Information Science ,  Academia Sinica, Taipei 11529, Taiwan ,  3 Institute of Statistical Science ,  Academia Sinica, Taipei 11529 ,  Taiwan     16  6  2017   16  2017    29  5  2017    12  1  2017     Approaches to identify significant pathways from high-throughput quantitative data have been developed in recent years. Still, the analysis of proteomic data stays difficult because of limited sample size. This limitation also leads to the practice of using a competitive null as common approach; which fundamentally implies genes or proteins as independent units. The independent assumption ignores the associations among biomolecules with similar functions or cellular localization, as well as the interactions among them manifested as changes in expression ratios. Consequently, these methods often underestimate the associations among biomolecules and cause false positives in practice. Some studies incorporate the sample covariance matrix into the calculation to address this issue. However, sample covariance may not be a precise estimation if the sample size is very limited, which is usually the case for the data produced by mass spectrometry. In this study, we introduce a multivariate test under a self-contained null to perform pathway analysis for quantitative proteomic data. The covariance matrix used in the test statistic is constructed by the confidence scores retrieved from the STRING database or the HitPredict database. We also design an integrating procedure to retain pathways of sufficient evidence as a pathway group. The performance of the proposed T2-statistic is demonstrated using five published experimental datasets: the T-cell activation, the cAMP/PKA signaling, the myoblast differentiation, and the effect of dasatinib on the BCR-ABL pathway are proteomic datasets produced by mass spectrometry; and the protective effect of myocilin via the MAPK signaling pathway is a gene expression dataset of limited sample size. Compared with other popular statistics, the proposed T2-statistic yields more accurate descriptions in agreement with the discussion of the original publication. We implemented the T2-statistic into an R package T2GA, which is available at https://github.com/roqe/T2GA.       -\r\n  Data Availability Statement: All relevant data are within the paper and its Supporting Information files.  Funding: This work is funded by Ministry of Science and Technology (https://www.most.gov.tw/?l=en). The grant number is 104-2221-E-010009-MY2, and the receiver is KPW. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.  Competing interests: The authors have declared that no competing interests exist.    Author summary\r\n  Pathway analysis is a common approach to quickly access the pathways being regulated in the experiments. There are numerous statistics to perform pathway analysis; most of them assume that the genes or proteins are independent of each other for statistical ease. This assumption, however, is unrealistic to the real biological system and may cause false positives in practice. A standard way to address this issue is to measure the associations among genes or proteins. Unfortunately, the estimation of associations requires sufficient sample size, which is usually not available for proteomic data produced by mass spectrometry. In this study, we propose a T2-statistic, which estimates the associations among gene products, to perform pathway analysis for quantitative proteomic data. Instead of calculating the associations directly from data, we use the confidence scores retrieved from protein-protein interaction databases. We also design an integrating procedure to reserve pathways of sufficient evidence as a regulated pathway group. We compare the proposed T2-statistic to other popular statistics using five published experimental datasets, and the T2-statistic yields more accurate descriptions in agreement with the discussion of the original papers.   Introduction\r\n  Great progress has been made toward the development of high-throughput technologies and their application to biological and clinical research. In a quantitative experiment, genes or proteins with significant changes in expression are potential to have important roles in a given phenotype or phenomenon. Therefore, the analysis of quantitative experimental data generally produces a list of differentially expressed genes or proteins in order. The list may share some insights if the aim of the experiments is restricted to few targets. As regards highthroughput data, the list hardly provides biological understanding of the mechanisms being studied, since the data involve complicated regulations among biomolecules and the number of biomolecules is too large to examine all candidates individually. Systematically investigating the underlying mechanisms from a high-throughput data therefore become a new challenge.  To confront the challenge, one idea is to apply pathway analysis to identify the genes or proteins that are known to be involved in a biological process or interaction based on the existing knowledge. Many approaches of pathway analysis have been developed over years concerning different methodologies. Some general reviews of pathway analysis approaches can be found in [1±4]. These approaches can be broadly divided into two major factions: a competitive null with a gene sampling model and a self-contained null with a subject sampling model. The null hypothesis of a competitive test suggests that the target pathway (or a predefined gene set) is differentially expressed as well as the rest of all pathways. Practically a competitive null is closely tied to (although not necessarily) a gene sampling approach [5]. A gene sampling model principally implies the independence assumption; the assumption presupposes that genes are expressed independently of each other so that these genes can be manipulated as the sampling subject to produce the null distribution. The null hypothesis of a self-contained test, on the other hand, suggests that the target pathway is not differentially expressed between distinct phenotypes. Using the phenotypes of the experiments as the sampling subject is the setup of a subject sampling model. In other words, a competitive null with a gene sampling approach let pathways compete with each other in order to rank these pathways and use the number of genes as the sample size in the meanwhile; whereas a self-contained null with a subject 2 / 29 sampling approach examine each single pathway to determine if the pathway is indeed differentially expressed between phenotypes and use the number of experiments as the sample size. Intuitively, a competitive null aims to find the pathway that is most significant among all pathways; a self-contained null aims to find the pathway that is the most significantly expressed between phenotypes. The classification of null hypotheses and sampling methods is firstly suggested by Geoman et al [5]. Even both categories have their own pitfalls and benefits, the authors suggested using a self-contained null with a subject sampling model in pathway analysis. Competitive null with gene sampling model usually implies the independence assumption which may produce inaccurate small p-values, cause serious false positives [6], and result misleading interpretations [5]. Further methodology issues of using a self-contained null can be found in [7±9], and of using a competitive null can be found in [ 10 ].  Another issue of pathway analysis comes from the construction of test statistics. These statistics can be divided into univariate tests and multivariate tests. Univariate statistics, such as a modification or a weighted summation of the t-scores [11±14], only focus on expression of genes or proteins and assume these biomolecules as independent units for statistical ease. The independence assumption ignores the associations among biomolecules with similar functions or cellular localization, as well as the interactions among them manifested as changes in expression ratios. In contrast, multivariate statistics take into consideration the associations among genes or proteins [15±20]. Some methodology studies [9, 21±23] have evaluated univariate tests and multivariate tests with synthetic and experimental datasets. Compared with multivariate tests, univariate tests generally result a decrease in statistical power [21] and an increase in false positive rate [22] along with the rise of average correlation. Multivariate approaches usually incorporate the sample covariance matrix into calculation to address biological interaction. However, the sample covariance may not be precise enough to estimate the associations if the sample size is very limited. Compared with other gene expression data, proteomic data produced by mass spectrometry are more difficult to analyze systematically due to the limited number of experiments. This limitation causes current multivariate tests incompetent because the sample covariance will not be a robust statistic.  The composition of pathway diagrams also become a challenge to pathway analysis. A pathway is a group of biomolecules that participate in a particular cellular process. The members of a pathway are usually defined by the tradition (i.e., the history of pathway discovery) of molecular biology scientists. The structure of pathway diagrams is not standardized and therefore arises some issues to pathway analysis. First, the same pathway from different databases or other sources may have the same core members but different side members. Under different experimental conditions, the size of accessible biomolecules also changes. For example, the phosphorylation proteomic data may not provide information to the proteins not belong to phosphoproteome. Since the number of members within a pathway is not a constant, using this number as a parameter to determine if the pathway is significant or not may lead to inconsistent results. This issue arises with the assumption of a competitive null. Second, some molecules appear over pathways may play important roles as communication centers. For example, p53 appears in 38 pathways in the KEGG database. The shared members may interact or cooperate with each other and form a functional module. If this module is regulated (i.e., the members within the module are differentially expressed as they cooperate together), the subsets of this module may present in abundant pathways and make these pathways seemingly significant. To identify the regulated pathway among the significant pathways of common modules, biologists usually utilize the distinctive molecules participating in that specific pathway; whereas pathways do not contain distinctive molecules may be irrelevant to the underlying mechanism. However, most of the current approaches do not take consideration of this issue.  The suggestion of irrelevant pathways due to the redundancy over the significant pathways 3 / 29 usually causes confusion to data description. A recent study [ 24 ] also focuses on the second issue. They demonstrate how the shared members affect p-values, and try to address this problem under a competitive null.  In this study, we introduced a multivariate test, based on the Hotelling's T2-statistic, to perform pathway analysis for quantitative proteomic data. The most serious problem of analyzing proteomic data produced by mass spectrometry is the limited number of experiments. We usually obtain only a few replicates (biological or technical) per experimental condition. To manage this issue, we had two special designs in our test. First, instead of using the sample covariance matrix (which is not robust when the sample size is limited), we use the covariance matrix that is constructed of the probabilistic confidence scores provided by the STRING and HitPredict databases. The proposed T2-statistic is then built of the protein expression profile and the covariance matrix to consider the expression level of individual proteins along with the associations among them. Second, we designed a self-contained model to produce a null distribution of altering protein expression while retaining the structure of protein associations. We are not capable of applying a subject sampling model because the number of experiments is too limited. In addition to the settlement of sample size issue, we designed an integrating procedure to categorize significant pathways as well as to avoid redundancy. The performance of the proposed T2-statistic is demonstrated using five public experimental datasets with different levels of biological complexity: the T-cell activation, the cAMP/PKA signaling, the effect of dasatinib on the BCR-ABL pathway, the differentiation process of myoblast, and the protective effect of myocilin via the MAPK signaling pathway. The first four datasets are proteomic data produced by mass spectrometry; the last dataset is a gene expression data of low sample size. We compared T2 with other popular statistics: DPA [ 25 ], GSEA [ 26 ], DAVID [ 27, 28 ], and IPA [ 29 ]. For most of the situations, T2 yielded more accurate descriptions in agreement with the discussion of the original publication.    Materials and methods\r\n     Experimental data\r\n  We took four proteomic datasets of different biological complexity and experimental properties to demonstrate our approach. To be comprehensive, the testing datasets include the case of pathway activation and inhibition; also the case of signaling phosphoproteome and cellular proteome. We only used the final ratios provided by the datasets since they may have different integration approaches (e.g. to combine the results of biological and/or technical repeats, to handle missing data or outliers) under different experimental designs. The summarized ratio is also the most available format for quantitative proteomic data. In this situation, the sample size of data becomes only one, calculating a covariance matrix is not even possible. Our approach provide a solution to undertake this difficulty. We also applied our approach on a gene expression dataset of three samples to demonstrate that the general idea is applicable to other quantitative data of low sample size.  The phosphoproteomic data of TCR signaling. The T-cell receptor (TCR) signaling phosphoproteomic data [ 30 ] aim to reveal system-wide phenomena associated with T cell activation. The authors used OKT3, an antibody specific to CD3 , to initiate the TCR signaling transduction in the human leukemia cell line Jurkat T lymphocyte. They used stable isotope labeling by amino acids in cell culture (SILAC) method to perform large-scale quantitative phosphoproteomic analyses and identified 696 TCR-responsive phosphorylation sites on 453 proteins. Phosphopeptides showing more than 1.85-fold change in abundance are qualified as ªTCR-responsiveº sites, suggested by the authors under their experimental conditions.  We used these 696 proteins since the original publication does not provide raw data, and we 4 / 29 aimed to use this small dataset as a positive control to evaluate the proposed statistic. The dataset contains three time points: 5, 15, and 60 min; the number of UniProt accessions are 30, 376, and 330, respectively. The authors specifically enriched pTyr-contained peptides in 5 and 15 min experiments since pTyr constitutes less than 1% of the total amino acid content; a global phosphopeptide-enrichment approach was used for 15 and 60 min experiments. The 5 min experiment is extremely small because it contained only pTyr-enriched peptides.  The phosphoproteomic data of cAMP/PKA signaling. The cAMP-dependent protein kinase (i.e. protein kinase A, PKA) signaling phosphoproteomic data [ 31 ] aim to provide a resource of substrates of PKA. The authors also used SILAC to profile quantitative changes of potential PKA substrates. Prostaglandin E2 (PGE2) was applied to increase intracellular cAMP and activate PKA in Jurkat T cells. The light cells (L) was used for control, the medium (M) and the heavy (H) were treated with PGE2 for 1 and 60 min, respectively. The authors identified 4284 phosphorylation sites on 607 proteins, and they discussed the dynamic phosphorylation upon stimulation by PGE2. The study considered two expression ratios: M/L and H/L; the number of UniProt accessions are 594 and 595, respectively.  The cellular proteomic data of myogenesis. This cellular proteomic data [ 32 ] monitor the changes in protein expression underlying the phenotypic conversion of human primary myoblasts; from primary mononucleated muscle cells to multinucleated myotubes, using the SILAC method. The authors used human satellite cells isolated from a quadriceps muscle biopsy of a 5-day old infant. The light (L), medium (M) and heavy (H) cells were first treated with the growth medium (serum-supplemented; low glucose), then switched to the differentiation medium (serum-free; high glucose) for 0, 24, and 72 hr, respectively. This study quantified 2240 proteins, in which 2227 unique UniProt identifiers are quantified for both M/L and H/L expression ratios.  The phosphoproteomic data of BCR-ABL signaling for CML treatment. The pathology of chronic myeloid leukemia (CML) is commonly associated with an oncogenic tyrosine kinase BCR-ABL. Dasatinib is an inhibitor of the BCR-ABL and Src family tyrosine kinase, and it serves as a clinical drug for treatment of CML. This phosphoproteomic data [ 33 ] aim to inspect the effects of dasatinib on the entire cell signaling network, using the SILAC method. The authors used the human leukemia cell line K562, which expresses the activated BCR-ABL fusion protein, to examine cellular phosphorylation levels for three conditions. For one hour, the light cells (L) was treated with DMSO only, the medium (M) and the heavy (H) were treated with 5 and 50 nM dasatinib, respectively. The authors identified 5063 phosphorylation sites on 1889 proteins, and they further discussed the mechanisms induced by dasatinib. The study considered two expression ratios: M/L and H/L; the number of UniProt accessions are 5453 and 5443, respectively.  The gene expression data of MAPK signaling. The MAPK signaling gene expression data [ 34 ] aims to understand the functions of myocilin, a causative gene for open angle glaucoma. The authors suggested that myocilin promotes cell proliferation and resistance to apoptosis via the ERK1/2 MAPK signaling pathway. The microarray analysis compared a stably transfected HEK293 cell line expressing myocilin with the control cell lines. The cells were treated with the 10μm MEK inhibitor U0126 two hours prior to the induction of myocilin expression. The gene expression profiling was performed using Human Affymetrix Gene Chip U133 Plus version 2.0. The processed dataset includes three treatment and three control expression values. We took the mean of the triplicated data as the representative expression. Then we took the difference of the treatment group and the control group since the processed data is already normalized. We used the identifier mapping files from the official websites and the final data include 21816 unique UniProt identifiers. 5 / 29    Databases\r\n  To provide more generalized results, we choose two pathway databases and two proteinprotein interaction databases: KEGG and Reactome provide pathway categories served as predefined gene sets, STRING and HitPredict contributes the confidence scores to estimate the covariance between protein expressions.  KEGG. The Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway database [ 35 ] collects manually drawn pathway maps in various topics, especially in metabolism. We downloaded the pathway database in the KGML format from the KEGG website. We used the version last updated on 5th July 2016. The human pathway database (the organism code: hsa) contains 303 pathway diagrams. We excluded the overview maps since they depict very general biological concepts (e.g. one overview map is titled ªMetabolic pathwaysº). There remain 290 pathways for the following analysis. For each pathway, we retrieved the components tagged as gene, enzyme, and group. Those components are associated with gene products so we use them to map the identifiers of experimental data onto pathways.  Reactome. The Reactome pathway database [ 36 ] provides curated pathways of signal transduction, transport, DNA replication, protein synthesis, metabolism and other cellular processes. We downloaded the pathway database in the tab-delimited format from the Reactome website. We used the version last updated on 30th June 2016 (v57). The human pathway database (the organism code: 9606) contains 1618 pathway diagrams of the lowest level (i.e. most specific) and we only used the lowest level pathways for the following analysis.  STRING. The STRING database [ 37 ] scores and integrates known and predicted proteinprotein interactions and gives a global perspective for many organisms. STRING provides eight PPI types: neighborhood, gene fusion, co-occurrence, co-expression, experiments, databases, textmining and homology [ 38 ]. STRING suggests four levels of confidence score: low confidence (0.150), medium confidence (0.400), high confidence (0.700), and highest confidence (0.900). The version for the following analysis is version 9.1, and we only used the interactions above medium confidence since it is the default threshold of the STRING website.The results were produced using the experiment PPI score to avoid artificial correlation due to special interest in the research community.  HitPredict. The HitPredict database [ 39 ] provides the reliability scores of experimentally identified, physical protein-protein interactions (PPI). The database integrates five popular source database: IntAct, BioGrid, HPRD, DIP, and MINT. HitPredict also annotates the confidence scores into two levels: low and high. The version for the following analysis is version 4, and we only retrieved the interactions annotated with high quality.  A knowledge-based T2-statistic  Data processing and pathway mapping. The required input format is a list of protein identifiers (e.g. UniProt accession number) with the expression ratios of the experimental group to the control. The list is subjected to the following data processing steps. First, we perform log2 transformation and winsorization on the data subsequently (if needed). Extreme values beyond the threshold are replaced with a maximum permitted value within the threshold.  For example, if the expression ratios are (−7, −1, 3, 4, 6) and the threshold is set to 5, then the extreme value 6 will be replaced by the maximum permitted value 4, and -7 replaced by -4.  Users can adjust the threshold based on the experimental conditions (i.e., different models or settings of mass spectrometry machines). The reason to set a threshold is to manually control the contribution of proteins with extreme values, since those proteins usually have very a low abundance (maybe beneath the threshold of quantification) in one of the experimental conditions. The default setting of the threshold is 10 and none of the testing data presented in this 6 / 29 paper exceed the threshold. Second, we deal with the problem of multiple identifications and multiple ratios. This problem originates from the mapping of different primary identifiers to UniProt accession numbers. If there exists any ratio with multiple identifiers, we assign the same expression ratio to those identifiers. If there exists any identifier with multiple ratios, we assign the median of the ratios to the identifier. Third, we standardize the data by dividing the standard deviation. Last, we map these proteins with their expression ratios to the pathways.  The identifier mapping tables are downloaded from the KEGG and the Ensembl [ 40 ] official websites.  The proposed T2-statistic. Hotelling's T2-statistic is a multivariate generalization of Student's t statistic. Let x1, . . ., xn be n independent random variables of an m-variate normal distribution, an original Hotelling's T2-statistic for an one-sample T2 test is defined as follows: T2 \u0088 n\u0085x μ0\u0086T S 1\u0085x μ0\u0086 where x is the vector of column means, S is an m × m sample covariance matrix, and μ0 is a given vector. The null hypothesis is that the population mean vector of data μ is equal to a given vector μ0; x is served as an estimation of μ.  Here we introduce the notation to describe the design of the proposed T2-statistic. Let a pathway P be the set of the proteins that are mapped from the data. Then,  P \u0088 fpiji \u0088 1; . . . ; qg where each pi indicates a specific protein with a corresponding ratio xi, and q the number of mapped proteins (i.e., the size of a pathway). To ensure the biological robustness of the expression ratios {xi}, we have another threshold representing our confidence toward the expression direction of the data, the default setting is 1.5. Only the proteins showing more fold change beyond the threshold are qualified to possess their original ratios; the ratios of other proteins are recognized as disturbance and set to zero. We use the proteins with high expression ratios to represent the expression directions of the pathway. If directions of these proteins fit the covariance matrix, then the pathway is more likely to be enriched. In other words, we think the direction is not stable for those proteins of low expression ratios; we only care if the proteins of high expression ratios fit the covariance matrix. Again, users should adjust the threshold based on the experimental conditions.  We collect the processed ratios into a column vector x. The proposed T2-statistic is constructed with the vector x and the covariance matrix S of x, the former represents the significance of protein expression; the latter represents the interaction structure among the proteins. The covariance matrix S is not computed from the experimental data. Due to the limitation of sample size, calculating an informative covariance is infeasible. Our idea is to use the confidence score provided by protein-protein interaction databases to represent the strength of the covariance, and use the expression direction provided by the testing dataset to indicate the sign of the covariance. In order to apply this idea, we have an assumption below:  If pi and pj have a strong interaction supported by experimental evidence, their expression ratios xi and xj will have a high covariance sij with a consistent sign, where xi and xj are any two elements of x and sij is the corresponding covariance of S. To be more precise, we denote the PPI database as the set I and the interaction pairs collected as its 7 / 29 elements:  I \u0088 ( cpvpu cpvpu describing the strength of their interaction: pv and pu are any two distinctive proteins with a confidence score ) On the basis of the confidence scores in I and the protein expression ratios, each element sij of S is determined by the following four rules: 1. For the elements where i = j, the main diagonal of S, namely the variance of x. In this case, there is no evidence score to refer, we have to assign a constant to the diagonal elements.  This constant represents our confidence toward the accuracy of the data, we use the medium confidence level suggested by STRING, which is 0.4. 2. For the elements where i 6\u0088 j, the confidence score between protein pi and pj exist in I , and the ratios xi and xj are of the same sign. In this case, either xi and xj are both up-regulated or both down-regulated, the covariance between them should be positive. We use cpi pj directly as the substitute of the covariance. 3. For the elements where i 6\u0088 j, the confidence score between protein pi and pj exist in I , and the ratios xi and xj are of opposite signs. In this case, one of xi and xj is up-regulated and the other is down-regulated, the covariance between them should be negative. We take the negative value of cpi pj to be the substitute of the covariance. 4. For the elements where i 6\u0088 j, and the confidence score between protein pi and pj does not exist in I . In this case, we assign zero to these elements.  The possible values of sij can be summarized as follows: 8 &gt;&gt;&gt; 0:4 if i \u0088 j: &gt; &gt; &gt; &gt; &gt;&lt; cpipj sij \u0088 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; : 0:0 cpipj if i 6\u0088 j; cpipj 2 I ; and xi xj  0: if i 6\u0088 j; cpipj 2 I ; and xi xj &lt; 0: if i 6\u0088 j and cpipj2=I : Since our aim is to test if a pathway P is differentially expressed, our null hypothesis can be described as follows:  For those proteins mapped to the pathway being evaluated, they are not differentially expressed between distinctive phenotypes under certain structure of protein interaction.  This self-contained null suggests that the expression ratios of data μ is equal to zero (i.e. the experiment and the control have the same expression values) under the same S. We presuppose the sample size is equal to one because in data processing section we already use the median of ratios to represent the expression level for each protein. Therefore we use x as an estimator of μ. The proposed T2-statistic for a specific pathway P is then defined as,  T2 \u0088 xT S 1x  wq2 where x is the vector of expression ratios, xT is the transpose of x, S−1 is the inverse of the covariance matrix S, and q is the number of mapped proteins in P. Since S is constructed from databases, it is possible that S is degenerate. In this case, we construct a Moore-Penrose pseudoinverse of S as a substitute, and q becomes the rank of S. The p-value of the pathway P 8 / 29 Fig 1. Redundant pathway versus non-redundant pathway. The expressed proteins of the pathway B are a subset of the expressed proteins of the pathway A. There is no evidence that pathway B is regulated since the significance may come from the merit of pathway A. On the contrary, some expressed proteins are unique to pathway A to support that pathway A may be regulated as a pathway group. is derived from the wq2 distribution. Note that the p-values under a self-contained null are not for comparison among pathways. The significance only indicates if the pathway being tested, as an individual, is differentially expressed from one phenotype to another.    Pathway integration\r\n  As we mentioned in Introduction, subsets of one common active module may cause a lot of pathways statistically significant. These pathways may only have slight relevance to the target mechanisms (Fig 1). To avoid misinterpretation due to irrelevant pathways being reported, we identify delegates to categorize pathways into pathway groups. A pathway group is a set of pathways, in which the pathway being the superset of other pathways is defined as the delegate. Since other pathways do not show any distinctive proteins to support themselves, the significance of other pathways may simply originate from the regulation of the delegate. In other words, we want to avoid the situation that a pathway is enriched only because of some common proteins that are shared with other pathways.  Our pathway integration procedure operates as follows. The set of the pathways being evaluated is denoted by P and each pathway P 2 P is associated with a p-value. We iteratively perform the following steps until all the pathways in P are assigned to a specific pathway group M. 1. We identify the pathway with the maximum number of mapped proteins Pm from P. 2. We create a new set M \u0088 fPmg for the delegate Pm, then we remove Pm from P.   3. For each pathway P 2 P, if P\r\n    Pm, we put P in M and remove P from P.\r\n  9 / 29 4. We check if there is any pathway remaining in P. If P 6\u0088 ⌀, we go back to the step 1. Otherwise, we check the p-value for every delegate Pm. Only those Pm with a significant p-value are preserved as our final result.  A demonstrative example can be found in S1 Fig.  The pathway integration procedure aims to find the pathways with the most sufficient information to represent current data. Please notice that this idea is not similar to pathway hierarchy in which higher level pathways are defined as the supersets over lower level pathways, in that case only the pathways of highest level are able to be delegates. One can only apply the integration procedure to pathways without hierarchy relationship. In addition, the procedure is only applicable to the statistical test using a self-contained null since the measure of significance is independent for each pathway.   Results and discussion\r\n  To demonstrate the advantages of the proposed T2-statistic, we compared our results with popular pathway analysis services within the community: Ingenuity1 Pathway Analysis (IPA), the Database for Annotation, Visualization and Integrated Discovery (DAVID), Gene Set Enrichment Analysis (GSEA), and Direction Pathway Analysis (DPA). Among these tools, IPA and DAVID use a competitive null; DPA uses a self-contained null; and GSEA uses a hybrid approach by default but suggests to use a gene sampling model when the sample size is smaller than seven [41]. The parameter settings for each tool are described in S1 Text. Their default significance requirements and ranking statistics are listed in Table 1. All of these tools apply univariate statistics, which means they do not consider the associations among gene products.  We do not compare to any other multivariate statistics because using a multivariate statistics requires large sample size to calculate the covariance matrix, so they are not applicable to the proteomic data of only one experiment. In addition, the performance of T2-statistic has been evaluated in [17], which suggested that T2-statistic generally outperforms other tools in all simulation cases in their study. The proposed T2-statistic, DPA, and GSEA were evaluated using the same version of KEGG and Reactome databases; whereas DAVID had their own versions as web services and IPA was bounded to its own curated database. The following discussion mainly focused on the results of KEGG pathways since the they are canonical to the community; the results of Reactome pathways were summarize in S1 Table.  As we mentioned above, different approaches of pathway analysis may have different statistical assumptions; comparing the performance between these approaches becomes even harder. Since there are no accepted gold-standards to evaluate the methods of pathway analysis, we tried to match the results reported by these methods to the biological idea provided by 10 / 29 the original publication. In other words, we took the aspect of explanatory power (i.e. the interpretation of true positive cases). Generally speaking, a good statistic should be able to determine if a pathway is significant: if a statistic reports very little significant pathways (much less than our expectation regarding the data), it might have the problem of false negatives; if a statistic reports a large number of significant pathways, it might have the problem of false positive. Practically the false positive under a competitive null may originate from high mapping rate with low expression level; the false positive under a self-contained null may originate from low mapping rate with high expression level. We supposed that the number of significant pathways is one of the attribute to evaluate these methods, at least for the significance evaluation part (i.e. revealing the consequence of different null hypotheses). Also, a good statistic should be capable of enriching the true positive pathways. Based on these assumptions we had a preliminary evaluation as below.  The numbers of proteins, PPI clusters (i.e. the networks constructed by mapped proteins and interactions), and pathways for each dataset are listed in S2 Table, their trends and correlation are illustrated in Fig 2. With the exception of the TCR dataset, the identified proteins for each data in the same dataset are almost identical, yet with different amount of quantification values; the pathways enriched in the same dataset are similar. The numbers of enriched pathways should be related to the number of quantified proteins; and the number of potential treatment targets should also reveal the complexity of the five datasets. The treatment of the TCR dataset, anti-CD3 , is a monoclonal antibody, whose target is specific and the influence should be limited. The case of the MAPK dataset is similar, U0126 is a highly selective inhibitor for MEK1 and MEK2. In the case of the PKA dataset, four targets of PGE2 are all subtypes of EP receptors (i.e. EP1, EP2, EP3, and EP4). Most of the downstream pathways are regulated by the second messenger cyclic adenosine monophosphate (cAMP) [ 31, 42 ]. On the other hand, dasatinib has about 10 targets of different kinase families [ 43 ] although its main target is BCR-ABL. The myogenesis dataset does not have a clear target, the size of the dataset is between the PKA dataset and the CML dataset. More targets suggest more proteins and pathways could be involved because of the treatment, and the number of enriched pathways should grow as well. Except for the gene expression data, we indeed observed an increasing trend of quantified proteins and PPI clusters (Fig 2a). However, the results of these tools (Fig 2b) do not totally concur with this hypothetical expectation. From Fig 2c we found both T2-statistics with positive coefficients between the data and the result, DPA and DAVID with some positive and some negative coefficients, and GSEA and IPA with negative coefficients all along. Having a negative coefficient between the data and the result is counter-intuitive, but a competitive null tends to behave in this fashion (GSEA in KEGG, GSEA and DAVID in Reactome, and IPA). Besides the null hypothesis, the construction of the statistics may also contribute. Both DPA and GSEA are based on the approach combining many gene-level statistics into a pathway-level statistics. The pathway-level statistic usually take into consideration the number of gene-level statistics it integrates, therefore the effect size for each gene-level statistic becomes important. In simple words, the average expression of the pathway dominates the enrichment result. For example, the TCR dataset is a cleansed dataset (S2a Fig) showed that there are only few proteins are of low expression ratios), almost all proteins are of high expression ratios. In this case, pathways can easily become significant because the average of expression ratios is usually high. In contrast, the PKA, the myogenesis, the CML, and the MAPK datasets have substantial proteins of low expression ratios (S2b, S2c, S2d and S2e Fig). These proteins will dilute the significance of others therefore the pathways can hardly become significant. Both DAVID and IPA are based on Fisher's exact test, a small dataset will result unequally distribution among the cells of the table and pathways can easily become significant consequently. 11 / 29 Fig 2. The consistency between data complexity and the number of enriched pathways. (a) The line chart indicated that the number of mapped IDs and the number of PPI-clusters have a close similarity. (b) The line chart showed an increasing trend of the number of enriched pathways for both T2 approaches; whereas the same trend can be hardly observed for other tools. (c) We compute the correlation coefficient between the number of mapped clusters and the number of enriched pathways. A plus sign (ª+º) indicates a positive coefficient and a minus sign (ª−º) indicates a negative coefficient. 12 / 29  MAPK U0126 10 μM MAPK signaling pathway We had a preliminary evaluation of the pathway enrichment tools using the ranks of the target pathway for each testing dataset. The numerator is the rank value and the denominator is the number of signi®cant pathways.  DAVID performs better because it uses a more stringent post hoc correction toward its significance level.  We determined the target pathway for each dataset according to their original publication: ªTCR signaling pathwayº for the TCR dataset, ªcAMP signaling pathwayº for the PKA dataset, ªECM-receptor interactionº for the myogenesis dataset, ªchronic myeloid leukemiaº for the CML dataset, and ªMAPK signaling pathwayº for the MAPK dataset. Generally the target pathway is directly provoked by the treatment, but in the myogenesis dataset, there does not exist such a clear target since the removal of serum takes away various kinds of growth factors.  Therefore we chose one pathway that is clearly stated to be differentially expressed for both experimental conditions (24hr/0hr and 72hr/0hr). As a preliminary evaluation of performance, we tried to locate the target pathway among the significant pathways reported by each tool. The ranks of the target pathways were summarized in Table 2. Generally speaking, T2 with STRING (T2×ST) and T2 with HitPredict (T2×HP) found all the target pathways significant; and DAVID found eight out of ten; IPA found seven targets and DPA also found six.  GSEA did not perform very well; it might because the design of the statistics is more suitable for the dataset of more experiments.  The mechanisms at pathway level behind the five datasets were depicted: Fig 3 for the TCR dataset, Fig 4 for the PKA dataset, Fig 5 for the myogenesis dataset, Fig 6 for the CML dataset, and Fig 7 for the MAPK dataset. Except for the myogenesis dataset, the target pathway is located in the center, related pathways are either neighbors or joined by arrows. Please note that the arrows between the treatment and the target pathway indicate the type of regulation (i.e. activation or inhibition) but the arrows between pathways indicate the direction of time. Each pathway is depicted by a circle with a ring of six segments: the background of a circle indicates the pathway is either the focus of the original publication or mentioned in pathway databases or literature; the color for each segment represents the significance reported by the certain tool. For example, in Fig 3, the T cell receptor signaling pathway is the focus of the original publication, and it is enriched by T2×ST, T2×HP, GSEA, DAVID, and IPA in the 5 min experiment. Please note that pathways are human-defined biological concepts, the components of one pathway 13 / 29 Fig 3. The results of the TCR dataset in KEGG. There are three possible downstream routes: the IL-2 expression and the cytoskeleton remodeling routes are the targets in the original publication, the former is enriched by T2×ST, T2×HP, DAVID, and IPA; the later is enriched by T2×ST, T2×HP, DAVID, and IPA; the proliferation route is also suggested by T2×ST and IPA. may appear in other pathways and these components usually interact with each other. Our presentation only depicted the main branches directed from the target pathway that are commonly described in the literature. In addition, since the pathway database of IPA is different from others, we used the most similar pathway title instead. Even the pathway is of the same title, the components of the pathway may still differ; the result is obliquely comparable. 14 / 29 Fig 4. The results of the PKA dataset in KEGG. There are four possible downstream routes: the proliferation route is the target in the original publication and enriched by T2×ST, T2×HP, IPA; the glycogen synthesis route is also suggested by IPA; the cytoskeleton remodeling route is suggested by T2×ST, T2×HP, DPA and IPA; and the DNA repair route is suggested by T2×ST, T2×HP and DPA. 15 / 29 Fig 5. The results of the myogenesis dataset in KEGG. There are five routes mentioned in the original publication: the RNA metabolism route is enriched by T2×ST, T2×HP, DAVID and IPA; the cell cycle withdrawal route is enriched by T2×ST, T2×HP and IPA; the proteolysis and cell fusion route are suggested by T2×ST, T2×HP, and DAVID, and IPA; the cell adhesion and migration route are suggested by T2×ST, T2×HP and DAVID; and the myofibril formation and muscle contraction route is only enriched by T2×ST and T2×HP. 16 / 29 Fig 6. The results of the CML dataset in KEGG. There are three possible downstream routes: the cytoskeleton remodeling and the survival are the targets in the original publication, both of them are enriched by T2×ST and T2×HP; and the proliferation route is suggested by T2×ST, T2×HP, DAVID and IPA. 17 / 29 Fig 7. The results of the MAPK dataset in KEGG. There are two possible upstream routes: the GPCR receptors and the notch receptors. The former is enriched by T2×ST, T2×HP, DPA and DAVID; the later is enriched by T2×ST, T2×HP and DAVID. There are two possible downstream routes: both of the proliferation and the survival are the targets in the original publication. The former is enriched by T2×ST, T2×HP and DAVID; the later is roughly enriched by T2×ST, T2×HP, DAVID and IPA. 18 / 29      The phosphoproteomic data of TCR signaling\r\n  We discussed the flow of signal transduction in time order since the dataset is a time-series of 5 min, 15 min, and 60 min experiments. In the beginning, the treatment anti-CD3 activated TCR signaling pathway. The TCR signaling pathway in KEGG and IPA depicts the signals from the TCR receptors all the way to the IL-2 expression. Usually, the response of signal transduction comes rapid, we expected that the TCR signaling pathway should be enriched in early time points; the downstream from the TCR signaling pathway to the IL-2 expression should be enriched in late time points. From Table 2 we found that T2×ST, T2×HP, DAVID, and IPA enriched the TCR signaling pathway in all experiments; GSEA enriched the TCR signaling pathway in 5 and 15 min experiments only. The ranks of the TCR signaling pathway are top in T2, DAVID, and IPA. The downstream of the TCR signaling pathway was illustrated in Fig 3, there are three possible routes directed from TCR signaling. The original publication focused on the TCR/Ras/MAPK route to the IL-2 expression and the cytoskeleton remodeling response. The TCR/PI3K-Akt/mTOR route, on the other hand, is also important and has been discussed as a cluster in literature [44±47]. From Fig 3 we found the enriched pathways by T2×ST were well-grounded for the following reasons: 1. The phosphorylation of Ras/MAPK is an early event [ 30 ]. Most proteins in the Ras/MAPK route are phosphorylated at serine and threonine whereas the 5 min experiment only contains tyrosine-enriched peptides. As a result, related pathways should be enriched in the 15 min data, T2×ST, T2×HP, DAVID, and IPA achieved this goal. 2. Most proteins in the PI3K-Akt/mTOR are phosphorylated at serine and threonine, and the phosphorylation should happen in order. both T2×ST and IPA matched the description, but in IPA these pathways are of low ranks. 3. The regulation of actin as the response of TCR signaling is also a main subject of the original publication. T2×ST, T2×HP, DAVID, and IPA successfully enriched the cytoskeleton remodeling route.  To sum up above, we found the results of T2×ST fit our expectation: T2×ST enriched the TCR signaling pathway in the early time data, the route TCR/Ras/MAPK and TCR/PI3K-Akt/ mTOR in time order, and the response of actin regulation, in both pathway databases. T2×HP, DAVID, and IPA enriched most of the expected pathways in KEGG, although the ranks of these pathways are occasionally low in IPA; T2×ST, T2×HP and DAVID also enriched most of the expected pathways in Reactome (S1 Table); GSEA failed to enrich half of the expected pathways in both pathway databases. We also found that there is no distinguishable difference between the result using a self-contained null (T2×ST, T2×HP, DPA) or a competitive null (GSEA, DAVID, IPA) in the TCR dataset.    The phosphoproteomic data of cAMP/PKA signaling\r\n  The target of the original publication is PKA substrates. As the authors described in their results; PGE2 induced a rapid and maximal increase in phosphorylation level after 1 min, and the level remained high before the number of substrates gradually returned to near-basal conditions after 60 min (S2b Fig). From Table 2 we found T2×ST, T2×HP, DPA, and IPA enriched the cAMP signaling pathway for both 1 min and 60 min experiments. The downstream of the cAMP signaling pathway was illustrated in Fig 4, there are four possible routes directed from cAMP signaling. The proliferation route was suggested by T2×ST, T2×HP, and IPA; and the cytoskeleton remodeling route also, with an addition of DPA. The DNA repair route was enriched by T2×ST, T2×HP, and DPA, in both pathway databases. The glycogen 19 / 29 synthesis route, on the other hand, was enriched only by IPA. The reason is that T2 takes expression ratios as an important feature, whereas the mapped proteins of the ªGlycolysis / Gluconeogenesisº pathway are all of low ratios (min = −0.23710, max = 0.14950, mean = 0.02742).  Briefly, we found the results of T2 reasonable: T2×ST and T2×HP enriched the cAMP signaling pathway for both datasets; the PKA/Rap1/PI3K-Akt route in time order, and also the cytoskeleton remodeling route and the DNA repair route. DPA and IPA also enriched most of the expected pathways. GSEA and DAVID failed to enrich most of the expected pathways in both pathway databases. We also found that the result using a self-contained null (T2×ST, T2×HP, DPA) enriched more expected pathways than a competitive null (GSEA, DAVID, IPA). Since the pathways competes with each others under a competitive null, the success of some pathways will obstacle other pathways. For example, the top 1 enriched pathway provided by DAVID for the 1 min data is ªRibosomeº. The pathway includes 87 proteins, and 51 of them were mapping by the data. The cAMP signaling pathway, on the other hand, includes 73 proteins, but only 8 of them were mapping by the data. The high mapping rate of ªRibosomeº will makes it harder for DAVID to enrich the target cAMP signaling pathway.    The cellular proteomic data of myogenesis\r\n  This study aims to characterize the changes in protein expression underlying the phenotype conversion from mononucleated muscle cells to multinucleated myotubes. According to the their analysis, five functional clusters were identified in this dataset: cell cycle withdrawal (72hr/0hr), cell adhesion and migration (24hr/0hr), RNA metabolism (both 24hr/0hr and 72hr/ 0hr), myofibril formation (72hr/0hr), and proteolysis, fusion, and ECM remodeling (both 24hr/ 0hr and 72hr/0hr). The corresponding KEGG pathways were illustrated in Fig 5. We chose the pathway ªECM-receptor interactionº as the target pathway in Table 2 because it is clearly stated to be differentially expressed in both experiments. From Table 2 we found the ECMreceptor interaction pathway was enriched by T2×ST, T2×HP and DAVID for both experiments. The original publication focused on the change of cellular phenotype accompanying myogenic differentiation and the development of myofibril. From Fig 5 we found the myofibril formation route was enriched by T2×ST, T2×HP, DAVID, and IPA. T2×ST and T2×HP further enriched muscle contraction related pathways, which were associated with myotube maturation as discussed in the original publication. The cell adhesion and migration play an essential role in the fusion of mononucleated myoblasts. Pathways related to adhesion and migration were enriched by T2×ST, T2×HP, and DAVID; those related to fusion were enriched by T2×ST, T2×HP, DAVID, and IPA. The elevation of lysosomal proteins contributed in remodeling intracellular components during the course of myotube formation. All tools enriched the lysosome pathway with an exception of IPA. The RNA metabolism and the cell cycle withdrawal routes represented the termination of proliferation since the growth factors and nutrition were removed from the medium. Related pathways were enriched by T2×ST, T2×HP and IPA. Briefly, T2×ST and T2×HP enriched all the pathways discussed in the original publication; the interpretation made by T2 fit the description of the dataset pretty well. In the meantime, DAVID and IPA also enriched most of the expected pathways, asides from muscle contraction pathways. DPA enriched most of the expected pathways in Reactome, but failed in KEGG; GSEA failed in both pathway databases. We also found that there is no distinguishable difference between the result using a self-contained null (T2×ST, T2×HP, DPA) or a competitive null (GSEA, DAVID, IPA) in the myogenesis dataset. 20 / 29    The phosphoproteomic data of BCR-ABL signaling for CML treatment\r\n  This dataset, unlike the previous, is not a time series; it is a dose-comparison experiment. According to the original publication, two datasets shared nearly all identified proteins although 50nM dataset did down-regulate more phosphopeptides (S2d Fig). Consequently, the authors only paid attention to the proteins that are regulated by both 5nM and 50nM dasatinib. The main target of dasatinib is the BCR-ABL signaling pathway, which is described in the pathway ªChronic myeloid leukemia (CML)º of KEGG. From Table 2 we found the CML pathway was enriched by T2×ST, T2×HP, DAVID and IPA for both experiments. The downstream of the CML pathway was illustrated in Fig 6, there are three possible routes directed from the inhibition of BCR-ABL. Since the two datasets shared nearly all proteins, we used the ranks of 5 nM dataset to represent the common pathways over two datasets. The original publication focused on the BCR-ABL/Ras/MAPK route and the connection between BCR-ABL signaling and apoptosis. From Fig 6 we found the enriched pathways by T2 were plausible for the following reasons: 1. In the case of the BCR-ABL/Ras/MAPK route, T2×ST, T2×HP, DAVID, and IPA all agreed with the description. The BCR-ABL/PI3K-Akt/Apoptosis route was also enriched by T2×ST, T2×HP and IPA. 2. Even though not mentioned in the original publication, the BCR-ABL/JAK-STAT/Apoptosis route is also well-known [48±50]. Among all the tools for comparison, T2×ST and T2×HP were the only tools that enriched the ªJAK-STAT signaling pathwayº in 50nM data. 3. The initiation of focal adhesion components is also related to BCR-ABL. The STRING analyzing result of the original publication also implied the mechanisms of cytoskeleton remodeling. T2×ST, T2×HP, DAVID, and IPA achieved to enrich the pathway ªRegulation of actin cytoskeletonº; but only T2×ST and T2×HP enriched the ªFocal adhesionº pathway.  In short, we found the performance of T2 pleasant: T2×ST and T2×HP enriched the BCR-ABL/Ras/MAPK route, the BCR-ABL/PI3K-Akt/Apoptosis route, the JAK-STAT signaling pathway, and the pathways related to actin response. Both DAVID and IPA enriched some of the expected pathways. GSEA and DPA failed to enrich most of the expected pathways in both pathway databases. Generally there is no distinguishable difference between the result using a self-contained null or a competitive null, but T2 enriched more expected pathways than other methods.    The gene expression data of MAPK signaling\r\n  The dataset includes only one experiment, comparing the gene expression of myocilin expressed cells to control cells, under U0126 treatment. The authors concluded that myocilin has a protective effect to against apoptosis and further promotes cell survival and proliferation via the MAPK signaling pathways. They also experimentally confirmed that the RafMEK-ERK-MAPK cascade was activated by myocilin. From Table 2 we found T2×ST, T2×HP, DPA, and DAVID enriched the MAPK signaling pathway; all three methods under a self-contained null successfully enriched the target pathway, whereas only DAVID is under a competitive null. The upstream and downstream of the MAPK pathway was illustrated in Fig 7. There are two possible upstream receptors of the MAPK signaling pathway: the GPCR receptors and the Notch receptors. In KEGG, both upstreams were enriched by T2×ST, T2×HP and DAVID, DPA only enriched the GPCR/Ras/MAPK route and IPA only enriched the Notch/Ras/MAPK route. In Reactome, both upstreams were enriched by T2×ST and T2×HP, DAVID only enriched the Notch/Ras/MAPK route (S1 Table). The are two downstream routes, both are 21 / 29 supported by the original publication. The results from both T2×ST and T2×HP suggested that the differentially expressed pathways are more upstream. This conclusion actually fit the discussion of the original publication, which suggests that myocilin may also regulate the upstream kinase of the MAPK signaling pathway. Briefly, both T2×ST and T2×HP successfully enriched the MAPK signaling pathway and its upstream; other tools enriched only some of the expected pathways.  Generally speaking, we found that the proposed T2-statistic was able to enrich the pathways in agreement with the original publication, whereas the performances of DPA, GSEA, and DAVID were not stable. IPA, as a commercial software with high cost, also enriched most of the relevant pathways. Nevertheless some of the pathways are low-ranked and the numbers of enriched pathways are enormous. The results suggested that our multivariate design of the proposed T2-statistic does provide important information toward pathway analysis by considering the strength of interactions among proteins. In the meantime, our self-contained null hypothesis is capable of enrich relevant pathways by the significance of protein expression ratios, whereas the focus of the competition among pathways may neglect the clear distance between phenotypes. Both DAVID and IPA are based on the competitive null hypothesis, although DAVID performs a more stringent post hoc correction, they shared the failure of some relevant pathways. The tremendous numbers of enriched pathways also suggested IPA may report more false positive results. GSEA also applies a competitive null when the sample size is limited and its KS statistic is sensitive to small sample size. The unsatisfied results suggested that GSEA is not suitable for data of limited experiments. The design of DPA is similar to the proposed T2-statistic; both DPA and T2 are specifically designed for quantitative proteomic data, and they both use self-contained null hypotheses. The performance difference between DPA and T2 primarily comes from the aspect of statistic construction. The proposed T2-statistic outperformed DPA because it considers the strength of interactions among proteins. Briefly, in five testing datasets, the results using a self-contained null is generally more well founded than the results using a competitive null.    Robustness test: Using permuted and purged confidence scores\r\n  The importance of applying the covariance matrix is to estimate accurate confidence interval.  We illustrated an example in Fig 8 to demonstrate the situation that may cause inaccurate estimation. Both M1 and M2 are accurate null distributions since the data are normalized using proper covariance matrix and the distribution hence follows χ2. M4 indicates that situation that an independent data are misinterpreted as a correlated data. This happens when we have false positive protein-protein interactions in the databases. In order to minimize the risk, we only use the confidence scores derived from directly experimental evidence. M3 represents the case that a correlated data are misinterpreted as an independent data. This may actually happen due to our incomplete knowledge of the biology system. In this case, the null distribution will not follow χ2 and the estimation of p-value will be inaccurate. Even so, for pathways of vary high expression ratios, applying the covariance matrix or not does not change its p-value dramatically. Here we demonstrated the impact toward p-values using permuted and purged confidence scores. We performed 100 experiments with 30% and 60% permuted confidence scores (i.e. we reassign the score using the same score distribution) and another 100 experiments with 30% and 60% purged confidence scores (i.e. we randomly remove the scores from the PPI databases), and we checked if the expected pathways are still significant under current significance level (α = 0.05). From Table 3 we observed that: 1. In practice, most of expected pathways contain proteins of high expression ratios, so the  influence toward their p-values is limited. 22 / 29 Fig 8. A demonstrative example of accurate and inaccurate estimation. We simulated a toy example to demonstrate the situation that may cause inaccurate estimation. The simulation data consisted 10000 random samples; the correlated data are derived from MVN (0, S) and the independent data are from MVN(0, I), where S is a 20 × 20 matrix with diagonal equals to one and off-diagonal equals to 0.5. 2. Some pathways are more easily to be affected by permuted scores rather than purged scores, such as Cell cycle in CML 5 nM experiment and PI3K-Akt signaling pathway in MAPK 10 μM experiment. In this case, the covariance matrix for the pathway is close to an identity matrix. It might be a M3 case that we do not have enough information to construct the covariance matrix, the p-values tend to be smaller, so the interpretation of significant pathways should be careful. 3. Some pathways are more easily to be affected by purged scores rather than permuted scores, such as PI3K-Akt signaling pathway in PKA 1 min experiment and Endocytosis in myogenesis 72 hr experiment. In this case, the covariance matrix for the pathway provides abundant information. The removal of these information will decrease the chance for the pathway to be enriched. It might be a M4 case that we use some fake information to 23 / 29 50 nM 10 μM The percentage indicated the proportion that was successfully enriched under current permutation or removal setting. Pathways highlighted by green background are easily affected by permuted scores than purged scores; pathways highlighted by red background are easily affected by purged score than permuted scores. 24 / 29 construct the covariance matrix, the p-values tend to be larger, so the users should manually examine the pathways of borderline p-values.  The construction of the proposed T2-statistic showed T2 is heavily dependent on expression ratios. After all, the null hypothesis for T2 is to test if the mean vector equals to zero. The contribution of applying the covariance matrix is to estimate p-values in a more accurate manner: to rescue some pathways with moderate expression ratios but their regulation directions are consistent with current knowledge of protein interaction, and to discard some pathways with inconsistency.   Conclusion\r\n  In this study, we presented a knowledge-based T2 approach to perform pathway analysis for quantitative proteomic data of a limited number of experiments. The proposed T2 is constructed as a multivariate statistic and the test of significance is under a self-contained null. We use the probabilistic confidence score provided by the STRING or HitPredict databases to approximate the covariance matrix of the protein profiles. The proposed T2-statistic is therefore able to reveal the influence of protein-protein interactions while performing the analysis. In addition, our pathway integration procedure is able to categorize pathways into pathway groups as well as to avoid redundancy. We performed the T2-statistic on five published quantitative proteomic dataset. In all cases, T2 was able to eliminate irrelevant pathways, as well as correctly identify relevant pathways that had been discussed in the original publication. The idea of incorporating biological evidence into conventional statistic can be widely applied to the analysis of quantitative proteomic data.    Supporting information\r\n  S1 Text. Versions and parameter settings of other tools. (PDF) S1 Fig. A demonstrative example of pathway integration procedure. This diagram used a toy example to illustrate the procedure of data processing, filtering, pathway mapping, statistical testing, and finally pathway integration. (PDF) S2 Fig. Data distribution. (a) The TCR dataset contains three proteomic experiments. The 5 min data describe the initiation of the TCR signaling pathway. The following response interfered lots of downstream proteins, resulting the 15 min data have the largest number of proteins among this dataset. Then the signal was transmitted to the nuclear and the amount of high-ratio proteins decreased, as described in the 60 min data. (b) The PKA dataset contains two proteomic experiments. The initiation of the cAMP signaling pathway came rapid, so the 1 min data almost illustrate all the following events. The 60 min data have fewer high-ratio proteins because the response of the signal had gradually vanished. (c) The myogenesis dataset contains two proteomic experiments. The 24 hr data have more differentially expressed proteins than the 72 hr data. (d) The CML dataset contains two proteomic data. Their distributions look quite alike, despite that the 50 nM treatment did down-regulate more proteins. Most proteins are down-regulated since the experiment aims to repress the BCR-ABL signaling pathway. (e) The MAPK dataset contains one gene expression experiment. The upregulated probes slightly outnumber the down-regulated (54% versus 46%), and there are few probes of high ratios.  (PDF) 25 / 29 S1 Table. The results of all datasets in Reactome. (PDF) S2 Table. The number of proteins and pathways for each dataset. The enriched pathways are the pathways that fulfilled the significance requirements described in Table 1. We listed the number of clusters using different confidence levels. The number of PPI clusters is calculated for clusters with at least two proteins. Anti-CD3 is specific to CD3 ; PGE2 targets four EP receptors but the downstream is almost under control of cAMP; serum starvation may result in cell cycle arrest and turn on muscle regulatory factors to promote myogenesis; dasatinib mainly targets BCR-ABL but has about 10 other targets of different kinase families; U0126 is a highly selective inhibitor of MEK1 and MEK2. (PDF) We thank Fang-Yi Su and Ning Tsao for helping us to describe the pathway mechanisms of    Acknowledgments\r\n  original publications. Conceptualization: EYL.  Data curation: EYL.  Formal analysis: EYL YHC.  Funding acquisition: KPW.  Investigation: EYL.  Methodology: EYL YHC.  Project administration: KPW.  Resources: KPW.  Software: EYL.  Supervision: KPW.  Validation: EYL.  Visualization: EYL KPW.  Writing ± original draft: EYL.  Writing ± review &amp; editing: EYL YHC KPW. 26 / 29 Goeman JJ, Buhlmann P. Analyzing gene expression data in terms of gene sets: methodological issues. Bioinformatics. 2007; 23(8):980±987. https://doi.org/10.1093/bioinformatics/btm051 PMID: 17303618 Gatti D, Barry W, Nobel A, Rusyn I, Wright F. Heading Down the Wrong Pathway: on the Influence of Correlation within Gene Sets. BMC Genomics. 2010; 11(1):574. https://doi.org/10.1186/1471-2164-11574 PMID: 20955544 Liu Q, Dinu I, Adewale A, Potter J, Yasui Y. Comparative evaluation of gene-set analysis methods. BMC Bioinformatics. 2007; 8(1):431. https://doi.org/10.1186/1471-2105-8-431 PMID: 17988400 Fridley BL, Jenkins GD, Biernacka JM. Self-Contained Gene-Set Analysis of Expression Data: An Evaluation of Existing and Novel Methods. PLoS ONE. 2010; 5(9):e12693. https://doi.org/10.1371/journal.pone.0012693 PMID: 20862301 Rahmatallah Y, Emmert-Streib F, Glazko G. Gene set analysis for self-contained tests: complex null and specific alternative hypotheses. Bioinformatics. 2012; 28(23):3073±3080. https://doi.org/10.1093/bioinformatics/bts579 PMID: 23044539 Tripathi S, Glazko GV, Emmert-Streib F. Ensuring the statistical soundness of competitive gene set approaches: gene filtering and genome-scale coverage are essential. Nucleic Acids Research. 2013; 41(7):e82. https://doi.org/10.1093/nar/gkt054 PMID: 23389952 Tian L, Greenberg SA, Kong SW, Altschuler J, Kohane IS, Park PJ. Discovering statistically significant pathways in expression profiling studies. Proceedings of the National Academy of Sciences of the United States of America. 2005; 102(38):13544±13549. https://doi.org/10.1073/pnas.0506577102 PMID: 16174746 Dinu I, Potter J, Mueller T, Liu Q, Adewale A, Jhangri G, et al. Improving gene set analysis of microarray data by SAM-GS. BMC Bioinformatics. 2007; 8(1):242. https://doi.org/10.1186/1471-2105-8-242 PMID: 17612399 Gupta P, Yoshida R, Imoto S, Yamaguchi R, Miyano S. Statistical Absolute Evaluation of Gene Ontology Terms with Gene Expression Data. In: Mandoiu I, Zelikovsky A, editors. Bioinformatics Research and Applications. vol. 4463 of Lecture Notes in Computer Science. Springer Berlin Heidelberg; 2007. p. 146±157.  Hummel M, Meister R, Mansmann U. GlobalANCOVA: exploration and assessment of gene group effects. Bioinformatics. 2008; 24(1):78±85. https://doi.org/10.1093/bioinformatics/btm531 PMID: 18024976 Nettleton D, Recknor J, Reecy JM. Identification of differentially expressed gene categories in microarray studies using nonparametric multivariate analysis. Bioinformatics. 2008; 24(2):192±201. https://doi.org/10.1093/bioinformatics/btm583 PMID: 18042553 Tsai CA, Chen JJ. Multivariate analysis of variance test for gene set analysis. Bioinformatics. 2009; 25(7):897±903. https://doi.org/10.1093/bioinformatics/btp098 PMID: 19254923 Wu D, Lim E, Vaillant F, Asselin-Labat ML, Visvader JE, Smyth GK. ROAST: rotation gene set tests for complex microarray experiments. Bioinformatics. 2010; 26(17):2176±2182. https://doi.org/10.1093/bioinformatics/btq401 PMID: 20610611 Massa M, Chiogna M, Romualdi C. Gene set analysis exploiting the topology of a pathway. BMC Systems Biology. 2010; 4(1):121. https://doi.org/10.1186/1752-0509-4-121 PMID: 20809931 Zhou YH, Barry WT, Wright FA. Empirical pathway analysis, without permutation. Biostatistics. 2013; https://doi.org/10.1093/biostatistics/kxt004 PMID: 23428933 Glazko GV, Emmert-Streib F. Unite and conquer: univariate and multivariate approaches for finding differentially expressed gene sets. Bioinformatics. 2009; 25(18):2348±2354. https://doi.org/10.1093/bioinformatics/btp406 PMID: 19574285 Tripathi S, Emmert-Streib F. Assessment Method for a Power Analysis to Identify Differentially Expressed Pathways. PLoS ONE. 2012; 7(5):e37510. https://doi.org/10.1371/journal.pone.0037510 PMID: 22629411 23. Jung K, Becker B, Brunner E, Beibbarth T. Comparison of global tests for functional gene sets in twogroup designs and selection of potentially effect-causing genes. Bioinformatics. 2011; 27(10): 1377±1383. https://doi.org/10.1093/bioinformatics/btr152 PMID: 21441576 27 / 29 41. The GSEA team. GSEA: User Guide;. http://www.broadinstitute.org/gsea/doc/GSEAUserGuideFrame.  html?Run_GSEA_Page. 28 / 29     ",
    "sourceCodeLink": "https://github.com/roqe/T2GA",
    "publicationDate": "0",
    "authors": [
      "En-Yu Lai",
      "Yi-Hau Chen",
      "Kun-Pin Wu"
    ],
    "status": "Success",
    "toolName": "T2GA",
    "homepage": ""
  },
  "38.pdf": {
    "forks": 0,
    "URLs": [
      "github.com/gohwils/NetProt/releases/,",
      "github.com/gohwils/NetProt/releases/",
      "github.com/gohwils/NetProt/releases/tag/0.1/Data.zip",
      "rpubs.com/gohwils/204259"
    ],
    "contactInfo": [
      "goh.informatics@gmail.com.",
      "wongls@comp.nus.edu.sg."
    ],
    "subscribers": 3,
    "programmingLanguage": "HTML",
    "shortDescription": "",
    "publicationTitle": "NetProt: Complex-based Feature Selection",
    "title": "NetProt: Complex-based Feature Selection",
    "publicationDOI": "10.1021/acs.jproteome.7b00363",
    "codeSize": 2624,
    "publicationAbstract": "Protein complex-based feature selection (PCBFS) provides unparalleled reproducibility with high phenotypic relevance on proteomics data. Currently, there are five PCBFS paradigms, but not all representative methods have been implemented or made readily available. To allow general users to take advantage of these methods, we developed the R-package NetProt, which provides implementations of representative feature-selection methods. NetProt also provides methods for generating simulated differential data and generating pseudocomplexes for complex-based performance benchmarking. The NetProt open source R package is available for download from https://github.com/gohwils/NetProt/releases/, and online documentation is available at http://rpubs.com/gohwils/204259.",
    "dateUpdated": "2017-07-10T17:54:08Z",
    "institutions": [
      "National University of Singapore",
      "Nanyang Technological University",
      "Tianjin University"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2017-05-30T08:24:29Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     J. Proteome Res.     10.1021/acs.jproteome.7b00363   NetProt: Complex-based Feature Selection     Wilson W    n Bin Goh  0  1  2    Limsoon Wong  0    0  Department of Computer Science, National University of Singapore ,  13 Computing Drive ,  Singapore   117417    1  School of Biological Sciences, Nanyang Technological University ,  60 Nanyang Drive ,  Singapore   637551    2  School of Pharmaceutical Science and Technology, Tianjin University ,  92 Weijin Road, Tianjin 300072 ,  China     2017   16  3102  3112    1  6  2017     Protein complex-based feature selection (PCBFS) provides unparalleled reproducibility with high phenotypic relevance on proteomics data. Currently, there are five PCBFS paradigms, but not all representative methods have been implemented or made readily available. To allow general users to take advantage of these methods, we developed the R-package NetProt, which provides implementations of representative feature-selection methods. NetProt also provides methods for generating simulated differential data and generating pseudocomplexes for complex-based performance benchmarking. The NetProt open source R package is available for download from https://github.com/gohwils/NetProt/releases/, and online documentation is available at http://rpubs.com/gohwils/204259.    proteomics  bioinformatics  networks  feature-selection       INTRODUCTION\r\n  Proteomics, as the high-throughput study of proteins in living systems, has advanced greatly. However, while it has achieved unprecedented proteome coverage in recent years, obtaining sufficient intersample consistency and quantitation accuracy for the purpose of selecting relevant proteins out of all observed proteins (feature selection) for the purpose of biomarker development and drug targeting remains challenging.1 Continued efforts to improve scalability and quantitation accuracy are undoubtedly useful, but this only constitutes half of the problem. The other half lies in the inadequacies of contemporary statistics.2 In general, conventional statistical methods falter given relatively small sample sizes, population heterogeneity, technical bias, and missing proteins (not to mention if their fundamental assumptions, e.g., independent and identically distributed (i.i.d.), are also violated). Therefore, statistically significant proteins in one study are nonreproducible in another and also lack predictive power (nongeneralizable).2  Better designed analytical methods combine biological context with protein-based expression data (proteomics data).3 Context encompasses prior knowledge on biological function, including gene groups,4 biological complexes,5,6 and biological networks.7,8 Here gene groups refer to ensembles of genes or proteins where members are expressionally correlated and share common functions; physical interactions are implied but nonessential. Biological complexes (or protein complexes) refer to physical assemblies of proteins; physical interactions are expected, but exact binding configuration needs not be known, and biological networks refer to protein−protein interaction, regulatory, signaling, and metabolic9−11 networks. As it turns out, real protein complexes (as opposed to predicted ones from a biological network) are very useful:12 They are quite stable and are easily obtainable from centralized repositories, for example, CORUM5,6 and MIPS.13,14 They are also highly enriched for biology information. In particular, given a multitude of predictors (expression correlation, genetic interactions, etc.) in yeast, protein complexes are the best class predictors.15 Even merging multiple predictors cannot improve upon protein complex-based predictions, suggesting that these other predictors do not add more information. These findings were independently confirmed in a large-scale analysis by Michaut et al., who used genetic knockout data from yeast spanning 191 890 genetic interactions between 4415 genes.16 Our own experience17 suggests likewise where missing-protein recovery based on real protein complexes are highly sensitive (∼95%) with decent precision (∼50%). In contrast, predicted complexes fall short, with the best outcome having a sensitivity of 45% with precision of 35%.17 Protein-complex prediction algorithms, while diverse and sophisticated, do not meet practical requirements and cannot supersede real complexes.18−20  Aside from high precision and sensitivity, protein complexbased feature selection (PCBFS) is surprisingly robust against technical bias, particularly batch effects.21−23 Batch effects are nonbiological variation and arise from confounding sources such as different experiment times, handlers, reagents, and instruments.24 It is an extremely important problem for contemporary data analysis and is commonly resolved by batch effect-correction algorithms (BECAs) such as ComBat25 and SVA.26 However, BECAs can affect data integrity and introduce false-positives.23 They can also be highly sophisticated but unwieldy in the hands of an amateur analyst.26 In the case of SVA, class effects (i.e., phenotype) can be supplied to the algorithm, thereby explicitly protecting variation correlated to class. SVA identifies and removes surrogate variables uncorrelated with class. The downside of doing this is that overall variation is decreased concomitantly (alongside the actual degrees-of-freedom) such that any standard statistical test is more likely to produce false-positives.26 Leek et al. have designed statistical tests to account for this and have offered an adjustment based on the F-test in their R package.24,27,28 Nonetheless, people manipulating the batch-corrected data matrix may not know how to deal with this problem appropriately. (This issue applies to other BECAs as well.) Another problem is that BECAs may remove other interesting sources of variation (factors) if not explicitly spelled out to the algorithm. For example, if a class-differentiating protein, which has gender-dependent expression levels, is identified postbatch correction, we are likely only able to observe its classdifferentiating effect but not gender effect.26 Hence, it is unsurprisingly that when Jaffe et al. wrote about the limitations of SVA, they advised users to be very careful, understand SVA's limitations, and run iterative analyses, combining class with other potentially interesting factors (age, gender, race, etc.) as input. Unfortunately, this can be complicated, and, as with any sophisticated BECA, this issue is not limited to SVA alone. In contrast, as recently demonstrated, PCBFS is inherently batcheffect-resistant; that is, without using BECAs (and potentially affecting data integrity) and when tested on data where both batch and class effects exist, top protein complex-based features were strongly associated with class but not batch effects, while individual-protein features selected by parametric statistical approaches were strongly correlated with batch effects.23  A good diagnostic signature must be relevant to the phenotype and therefore ought not be outperformed by randomly generated signatures (i.e., random signature superiority). In gene-expression assays, this is a known problem,29 but the same problem also exists in protein-expression assays. Protein complex-based scores, however, are quite robust against random signature superiority.30  Given these many useful properties, it may be beneficial for scientific investigators to consider PCBFS. Hence, we develop the R package, NetProt, which provides representative PCBFS methods from each of five current paradigms31 and also some basic functionalities for benchmarking. We provide guidelines on when to use what, extensions to other useful R modules (for visualization and functional annotation), and some in-depth discussions on limitations and future development. Nonetheless, we note that PCBFS is not a panacea and has disadvantages as well. The most obvious limitation is that not all protein complexes are known, resulting in some functionalities being poorly represented. Fortunately, our collective knowledge on protein complexes (a.k.a. the complexome) is rapidly expanding given the emergence of new databases (e.g., Bioplex32), purification technologies,33 and computational methods.34 The second limitation arises from redundancy: Overlapping complexes within and across different databases remain open problems with no universally accepted solution. (A simple fix is to simply merge when two complexes share at least n common proteins,35 but this may generate irrelevant faux complexes.17,36) Third, not all complexes are ■ expected to be present in some tissue: Just as tissues have tissue-specific expressions, resulting in idiosyncratic cellular networks,37 different tissues should also have unique tissuespecific complexomes, allowing us to match the appropriate protein-complex background against the data being studied.38 Regardless, depending on the goal, users are free to supply real complexes, predicted complexes, and gene-annotation groups, of their choosing, to NetProt's methods. It is not compulsory to use only a fixed set of real complexes.    MATERIALS AND METHODS\r\n   Gene Fuzzy Scoring Normalization\r\n  NetProt provides Belorkar and Wong's Gene Fuzzy Scoring (GFS) as an independent function (gfs) for data processing on omics data without networks/complexes.39 GFS is an example of a Signal Boosting Transformation (SBT), which are data normalization techniques with the following traits: boosting of high confident signals, penalization of low-confident signals and discretization of wide but nonuseful range of measured values.2  In GFS, an expression matrix is transformed by weighting individual variables (viz. genes or proteins) per sample based on expression ranks. GFS uses two thresholds, θ1 (e.g., top 10%) and θ2 (e.g., between top 10−20%). Variables ranked above θ1 are assigned a weight of 1. Variables ranked between θ1 and θ2 are chopped into n equal-width intervals, and those in the same intervals are assigned the same interpolated weight between 0 and 1 (to account for measurements with inherent high variations). Variables with ranks below θ2 are assigned 0 (and effectively ignored). For further details, refer to Belorkar and Wong.39  By itself, GFS transformation improves reproducibility of feature selection, with demonstrable robustness against batch effect.39 It is also noteworthy that several NetProt methods (FSNET/PFSNET/PPFSNET30,40 and QPSP31) improve upon GFS, providing additional power by taking advantage of autocorrelations among proteins within the protein complex,30 with the added benefit of indirectly recovering low-abundance variables lost by GFS, as a significant complex may comprise both high- and low-abundance proteins30,40 (see next paragraph).    Complex-based Feature-Selection Methods\r\n  We provide a quick introduction on the defining characteristics of each paradigm; cf. Figure 1.  (For detailed descriptions, refer to Goh and Wong, 2016.31)  Over-Representation Analysis (ORA) is a two-stage procedure: univariate feature selection on proteins followed by enrichment test. ORA can be highly unstable as it is very sensitive to the test type and stringency conditions in the univariate feature-selection step30,41 as well as the appropriateness of the null hypothesis of the enrichment test.42 As mitigation, Direct Group (DG) analysis does away with the univariate feature-selection step and directly determine whether a complex is differential by comparing the distribution of constituent protein expression between phenotype classes against that of proteins outside the complex. Hit-Rate (HR) analysis is initially conceived to address inconsistency issues in Data-Dependent Acquisition (DDA)-proteomics. Its defining characteristic involves calculation of overlaps (or hit-rates) among detected proteins against a vector of complexes to generate a hit-rate vector, which is used for class discrimination and feature selection.36 Rank-Based Network Analysis (RBNA) incorporates rank weights based on the expression level of identified proteins and class weights based on the proportion of supporting information among samples,30,40 while Network Paired (NP) analysis does not use any rank weights and work by testing the distribution of paired class differences on a given set of subnets or complexes.42  ORA is probably the most well known paradigm, and the Hypergeometric Enrichment (HE) pipeline is its most common realization. HE involves two steps: differential-protein selection via the two-sample t test, followed by enrichment analysis via the hypergeometric test (equivalent to a one-sided Fisher's exact test). While intuitive and simple, its key limitations are its hypersensitivity toward the upstream univariate proteinselection step43 and its enrichment test's error-prone null hypothesis, which assumes proteins have mutually independent expression levels.42  DG avoids these shortcomings via global non-thresholddependent evaluation. Given all observed proteins, it tests whether a protein complex is differential as a whole versus some constructed background. NetProt provides a vanilla implementation of Gene Set Enrichment Analysis (GSEA),4 which is probably the best known DG method. GSEA first ranks all observed proteins based on effect size, for example, the t-statistic (if the comparison is between two classes). This is followed by the Kolmogorov−Smirnov (KS) test to determine if protein ranks in the complex and the ranks of proteins outside the reference complex arise from the same distribution. The significance of the KS test statistic is evaluated against a null distribution obtained by permuting sample class labels. For further details, refer to Subramaniam et al.4  There are two variants of HR: Proteomics Signature Profiling (PSP)44 and its newer counterpart, quantitative PSP (QPSP).36 In the latter, differential protein lists are derived by applying GFS such that each sample is defined by its top-ranked proteins. For PSP, instead of GFS, the data matrix is simply binarized (1 for present, and 0 for absent) before hit-rate vectorization (Figure 2).  Like QPSP, RBNAs also deploy GFS as the initial dataprocessing step. However, it goes one step further and defines a class-representation proportion (class weighting) per gene, given the rationale that a top-ranking gene frequently observed in the top n% of samples in its respective class is more likely a true-positive (Figure 2). Indeed, RBNAs are extremely powerful, greatly improving signal-to-noise ratios over a series of clinical blood cancer genomics data sets45 and also display very high utility on proteomics data.17,41 There are currently four RBNAs: SubNET (SNET),45 Fuzzy SNET (FSNET), and paired FSNET (PFSNET)40,45 and class-paired PFSNET (PPFSNET).21 SNET and FSNET use the same one-sided reciprocal statistical test (Figure 2) but differ in the upstream data transformation: SNET uses a simple binarization procedure, while FSNET uses GFS. PFSNET uses GFS upstream but differs from FSNET by swapping the one-sided test in favor of a single-sample test. PFSNET uses unpaired tests and has reduced power if samples are pairable (e.g., the normal and disease tissues are derived from the same individual), PPFSNET addresses this shortfall by replacing the unpaired tests in PFSNET with the paired version. For detailed descriptions, please refer to Goh and Wong.30,31 NP is the newcomer among PCBFS paradigms, and there is only one representative method so far, Extremely Small SubNET (ESSNET),42 which tests the distribution of paired class differences across the constituent proteins within a complex42 (Figure 2). ESSNET is the only approach that explicitly considers low-abundance proteins.42    Choosing a Complex-Based Feature-Selection Method\r\n  We provide some rough guidelines on when to use certain methods (Table 1) and their key advantages/disadvantages (Table 2). NetProt provides vanilla implementations of ORA (HE) and DG (GSEA). Given recent benchmarking evaluations, we generally do not recommend use of ORA and DG, except for exploratory and comparative purposes, as they are unstable, even between technical replicates.30,31,41 There are optimal conditions, however: For HE, performance improves if sample size is large;30 for GSEA, performance improves if noise is minimal.31  Given a high proportion of data holes (as is typical in DataDependent Acquisition), PSP is ideal as it explicitly deals with data holes: Instead of trying to predict the values of the missing values via missing value imputation (MVI), PSP embraces data holes as informative. MVI is typically carried out using some statistical approach for estimating missing values based on observed values but is generally inaccurate and also inflates the degrees-of-freedom (DOF), potentially increasing both type I and II errors during feature selection.2,46  If the data matrix is mostly complete, then RBNAs and ESSNET are good options (QPSP is outperformed by these).30,31 Among RBNAs, SNET and FSNET are superseded by PFSNET but are included nonetheless for benchmarking and evaluative purposes. ESSNET explicitly considers lowabundance proteins, with slight loss in stability.31 Between PFSNET and ESSNET, the choice boils down to whether sample clustering is desired or inclusion of low-abundance proteins takes priority: ESSNET explicitly considers lowabundance proteins and provides a statistical evaluation of which complexes are significant given two classes but not individual sample scores. PFSNET provides the latter; therefore, the resultant data matrices can be used for clustering (but low-abundance proteins are generally ignored due to GFS).  Differential Data Simulation and Pseudocomplex    Generation\r\n  Complex-based feature selection in proteomics is a relatively new area;3,12 new paradigms and methods are expected to emerge, and appropriate approaches are needed for evaluation and benchmarking. NetProt provides an implementation (generate_proteomics_sim) of the univariate data simulation approach described by Langley and Mayr47 (Figure 3A). On natural proteomics data with only one true class, the user chooses the proportion of randomly selected proteins to insert an effect size (p, which itself is also randomly sampled from a user supplied vector), which samples to assign into the pseudoclasses, and the number of simulated data sets to generate. The effect size is increased in only one of the pseudoclasses and is expressed as  SCi,j\u2032 = SCi,j*(1 + p) where SCi,j and SCi,j\u2032 are, respectively, the original and simulated spectral counts from the jth sample of protein i.  The Langley−Mayr approach, however, only simulates differential proteins but not complexes. Hence, NetProt also provides a method (generate_pseudo_cplx) for generating pseudocomplexes for protein complex-based feature selection from simulated differential data (Figure 3B). Differential proteins are reordered such that those with similar expression pattern are adjacent to each other. This reordered list is then split at regular intervals to generate differential pseudocomplexes (Users may also adjust the proportion of differential proteins, referred to as purity, in differential pseudocomplexes.) An equal number of nondifferential proteins are randomly selected, reordered on expression similarity, and split to generate nondifferential pseudocomplexes. To make it harder to detect a differential complex, constituent significant proteins are randomly selected and the effect size is removed. The proportion of significant proteins to remove is determined by the purity parameter (where 100% purity means the significant pseudocomplex is comprised solely of differential proteins). The differential and nondifferential pseudocomplexes can be used for precision-recall-based performance benchmarking on new data or on new feature-selection methods. ■     RESULTS\r\n   Using NetProt\r\n  NetProt adopts a modular approach, so that users have freedom to modify the methods according to their needs. (For details, refer to the NetProt documentation; http://rpubs.com/gohwils/204259.) For example, if users do not want to use GFS, they may replace GFS in favor of some other dataprocessing approach (e.g., quantile normalization) of their design or choice. If users do not want to use protein complexes, then they can simply use GFS as standalone.  The modular design also clarifies the logic behind some of the newer paradigms, for example, RBNAs. Broadly, NetProt's functions can be generally classified along the following levels: univariate-data transformation, class-proportion weighting, complex-based scoring, statistical test, and meta-methods.  These are explained in Table 3.  NetProt functions (Figure 4, Table 3) are assembled linearly to produce each PCBFS method. NetProt's modular approach facilitates mix and match with functions from other R packages or linking to new methods, and because we place no restriction on the output, users are free to select how to further analyze the outputs using R's diverse clustering, visualization, annotation, and machine-learning packages (see next section).  As for input, NetProt takes in a standard protein-by-sample expression matrix and compares it against a list of complexes (based on CORUM's format). Code examples are provided in the online RPubs documentation (http://rpubs.com/gohwils/204259). NetProt does not provide automapping functions for identifiers, as it is often a clumsy and error-prone process. The identifiers used in the complex vector (whether real or predicted complexes) must be the same as those in the protein expression matrix. If identifier mapping is required, then the Biomart R package in Bioconductor (https://www.*performs a standard two sample t test based on the reciprocal matrices generated by fsnet *performs a one-sample t test based on the reciprocal matrices generated by fsnet *performs a paired t test based on the reciprocal matrices generated by fsnet *performs a one-sample t test based on the delta matrix generated by internal_substration *Generates simulated two class data with randomly insert differential features at the protein level *Takes the output from generate_proteomics_sim, and converts the protein features into complex-based features  used in FSNET/PFSNET/  PPFSNET/QPSP SNET/PSP PSP SNET/FSNET/  PFSNET/  PPFSNET ESSNET SNET/FSNET/  PFSNET SNET/FSNET PFSNET PPFSNET ESSNET any any The R implementation of NetProt does not provide options for clustering, classification, and visualization as users can directly use many R packages for such tasks. With the exception of ESSNET, because the data output is typically a data matrix (sample by complexes), many analytical tasks are possible (using R's well-developed libraries).  A simple but common task is to test whether PCBFSselected features correlate well with known sample classes. This is achievable using any variety of unsupervised clustering methods. We particularly liked R's pheatmap package, which combines unsupervised hierarchical clustering directly with heatmap visualization. In Figure 5A, we applied SNET on the renal cancer data set of Guo et al.48 and CORUM complexes5,6 to obtain a pair of reciprocal matrices. These matrices, in turn, can be clustered to determine whether the samples segregate perfectly into their known respective classes. If so, then we may claim that PCBFS has been somewhat successful, as the selected complexes exhibit good class-discriminatory power.  We may further evaluate if the selected features provide stable discrimination via R's bootstrap clustering tool, pvclust.49  Alternatively, if we are interested to know whether the selected features have any diagnostic power, we may combine and input the SNET data matrices, feed part of it into a machine learning method (e.g., R's eBayes package is an implementation of the nai v̈e Bayes model) for training, and confirm its predictive power (classification) on the remaining samples not used in model training (Figure 5B).    Extension 2 Functional Characterization\r\n  Often, functional analysis requires linking functionalities to the collective set of selected features. On individual features, this is typically achieved by performing the hypergeometric test on a given set of differential proteins against a background of known proteins for each known functional term. Unfortunately, functional annotation strategies based on protein complexes are lacking. Gene Ontology is organized along the lines of individual proteins, not complexes. A crude workaround is to dismantle significant complexes into a protein list and feed it into a protein functional annotation platform, for example, DAVID50 or GO-Term-Finder.51  Currently, CORUM does provide basic protein complex functional annotations, but these are quite limited, and there is room to develop computational approaches for automatic complex-based annotation. It is surprising that although protein complexes are physical assemblies, they are quite poorly annotated, but PCBFS does not absolutely require protein complexes as input. GSEA gene groups are well-annotated assemblies of genes; they may be used in conjunction or in place of protein complexes if functional characterization is the main objective.  There is further development in complexome research, where protein complex-based networks (the physical and functional interactions between complexes instead of individual proteins) are being developed. This is a logical generalization (as proteins work in assemblies rather than individually) and helps to eliminate a high amount of redundancies associated with the analysis of individual proteins. Unfortunately, this is currently better developed for model organisms, for example, Drosophila,52 than in humans. Regardless, such advances are invaluable for helping to understand the interconnections and relationships among a set of differential complexes and can help in functional/mechanistic investigations. As human complexbased networks become available, we may use PCBFS for helping identify the components of the complex-based networks that are altered in response to a disease. ■     DISCUSSIONS\r\n   Data Sets for Benchmarking\r\n  NetProt can be used for analysis of real data sets, but we think it also provides a shared opportunity for developing new benchmarks and PCBFS methods. Previously, we proposed that a PCBFS method can be broken up along the lines of a datapreprocessing transformation, weight assignment, complexscoring method, and a statistical test (Figure 4). Although not every method needs to follow this design exactly, we expect most approaches can be broadly organized as such (even ESSNET and QPSP; Figure 4).  Another key consideration is which data sets are suitable for use as benchmark data. On proteomics data, a simple broad categorization would be simulated or real data (Figure 6A). NetProt includes three real data sets, RCC, RC, and CR. Simulated data sets D1.2/D2.2 and DRCC are available online at https://github.com/gohwils/NetProt/releases/tag/0.1/Data.zip. Every data set has a unique set of pros and cons, and we have listed these in Figure 6A. It is important that users understand these idiosyncrasies before using the data sets (especially the presence of batch effects in some of the data). Incidentally, batch-effect resistance is a useful property of some PCBFS methods. The complexome (or complex list) does change over time, and so NetProt does not include this internally. While an example complex vector is available internally in the package, we advise users to get the latest versions from CORUM itself. If CORUM is deemed too limiting, then another alternative is to derive the protein complex list from the complex census,53 which offers wider coverage but may be less clean due to the incorporation of predicted protein complexes. Annotated gene sets may be obtained from GSEA.4 Finally, if interest is in testing against predicted protein complexes, there is a wide variety of biological network databases (e.g., HPRD,54−56 BioGrid,57 etc.) and protein complex-prediction algorithms.20    Considerations for Data Simulations\r\n  With simulated data, it is necessary to generate differential features for the purpose of method evaluation. At the level of protein expression, there are two dimensions to consider. The first is the origin of the data: completely simulated data or building up from real data (Figure 7A). The second is concerned with how to simulate the magnitude of the differential effects, whether the effect size is fixed or variable. Completely simulated data with fixed effect sizes have very conserved behavior and are easily tractable but fall short of resembling real data. Alternatively, the more complex the simulations, building on top of the behaviors of real data, with variable effect sizes inserted, the more likely unforeseen confounding factors may creep in, with indeterminate effects on the performance of the feature-selection method. Figure 7A lists primary considerations along these two dimensions.  Because we are interested in PCBFS, it is also important to think how to simulate protein complexes. There also are two dimensions to consider. The first is whether to use real or simulated complexes. The second is how to simulate noise/ impurity in the complex, making it more challenging for PCBFS methods to detect differential effects, if they exist (Figure 7B). It is also important to note that completely simulated data sets would not work with real complexes, as the selection of proteins is random and not based on inherent correlations among same-complex proteins. If the decision is to use real complexes, then the insertion of differential effects must be conserved at the level of complexes; that is, a complex is randomly selected to be differential, and the effect size inserted into its constituent proteins, but such approaches must also take into account the fact that many complexes share proteins, and so this approach can also artificially cause nonselected complexes to also become significant consequently. An alternative approach is to simulate effect sizes at the level of proteins first and then assemble them into differential complexes with no overlaps; however, such pseudocomplexes may lack realism and do not have many of the inherent properties of true complexes. An extensive listing of the considerations involved in protein complex-based simulations is discussed in Figure 7B.    Limitations of NetProt and Future Work\r\n  PCBFS is one of three important quantitative applications in network-based proteomics, the other two being class prediction (for diagnostics) and coverage expansion (for predicting missing proteins).12 For PCBFS to become more powerful, two further advances are required: novel annotation strategies at the level of protein complexes and reference protein complex-based networks for understanding the functional relationships among differential protein complexes.  Currently, the R implementation of NetProt does not provide proprietary visualization options and dynamic output. It requires users to be familiar with the manipulation of data matrices as inputs to various data visualization packages, for example, pheatmaps. However, gaining even basic familiarity with R's basic data structures opens the way toward its powerful library of bioinformatics software and tools.  NetProt is currently in its first version of development and includes a limited set of representative methods from each PCBFS paradigm. We expect future upgrades to incorporate more PCBFS methods. We also hope to successfully develop and incorporate appropriate complex-wise functional annotation tests in the near future.     AUTHOR INFORMATION\r\n   Corresponding Authors\r\n  *W.W.B.G.: E-mail: goh.informatics@gmail.com. *L.W.: E-mail: wongls@comp.nus.edu.sg. Tel: +65-65162902. Wilson Wen Bin Goh: 0000-0003-3863-7501 The authors declare no competing financial interest. NetProt is free software and distributed under the terms of the GNU General Public License as published by the Free Software Foundation, version 3. The R source file is available at https://github.com/gohwils/NetProt/releases/. Online documentation is available at http://rpubs.com/gohwils/204259.     ACKNOWLEDGMENTS\r\n  W.W.B.G. and L.W. thank Abha Belorkar, Kevin Lim, and Donny Soh for ideas contributed towards elements of this work. W.W.B.G. is supported by an education grant (2900819000002), Tianjin University, China.   ORCID\r\n    Notes\r\n  ■     ",
    "sourceCodeLink": "https://github.com/gohwils/NetProt",
    "publicationDate": "0",
    "authors": [
      "Wilson W",
      "n Bin Goh",
      "Limsoon Wong"
    ],
    "status": "Success",
    "toolName": "NetProt",
    "homepage": ""
  },
  "47.pdf": {
    "forks": 0,
    "URLs": [
      "www.ncbi.nlm.nih.gov/pubmed",
      "github.com/wangvsa/CMSA"
    ],
    "contactInfo": [],
    "subscribers": 0,
    "programmingLanguage": "C++",
    "shortDescription": "Improved Center Star Algorithm for Multiple Sequences Alignment Based on CUDA",
    "publicationTitle": "CMSA: a heterogeneous CPU/GPU computing system for multiple similar RNA/DNA sequence alignment",
    "title": "CMSA: a heterogeneous CPU/GPU computing system for multiple similar RNA/DNA sequence alignment",
    "publicationDOI": "10.1186/s12859-017-1725-6",
    "codeSize": 27056,
    "publicationAbstract": "Background: The multiple sequence alignment (MSA) is a classic and powerful technique for sequence analysis in bioinformatics. With the rapid growth of biological datasets, MSA parallelization becomes necessary to keep its running time in an acceptable level. Although there are a lot of work on MSA problems, their approaches are either insufficient or contain some implicit assumptions that limit the generality of usage. First, the information of users' sequences, including the sizes of datasets and the lengths of sequences, can be of arbitrary values and are generally unknown before submitted, which are unfortunately ignored by previous work. Second, the center star strategy is suited for aligning similar sequences. But its first stage, center sequence selection, is highly time-consuming and requires further optimization. Moreover, given the heterogeneous CPU/GPU platform, prior studies consider the MSA parallelization on GPU devices only, making the CPUs idle during the computation. Co-run computation, however, can maximize the utilization of the computing resources by enabling the workload computation on both CPU and GPU simultaneously. Results: This paper presents CMSA, a robust and efficient MSA system for large-scale datasets on the heterogeneous CPU/GPU platform. It performs and optimizes multiple sequence alignment automatically for users' submitted sequences without any assumptions. CMSA adopts the co-run computation model so that both CPU and GPU devices are fully utilized. Moreover, CMSA proposes an improved center star strategy that reduces the time complexity of its center sequence selection process from O(mn2) to O(mn). The experimental results show that CMSA achieves an up to 11× speedup and outperforms the state-of-the-art software. Conclusion: CMSA focuses on the multiple similar RNA/DNA sequence alignment and proposes a novel bitmap based algorithm to improve the center star strategy. We can conclude that harvesting the high performance of modern GPU is a promising approach to accelerate multiple sequence alignment. Besides, adopting the co-run computation model can maximize the entire system utilization significantly. The source code is available at https://github.com/wangvsa/CMSA.",
    "dateUpdated": "2016-08-31T01:45:16Z",
    "institutions": ["Tianjin University"],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-03-27T02:06:45Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     Chen et al. BMC Bioinformatics     10.1186/s12859-017-1725-6   CMSA: a heterogeneous CPU/GPU computing system for multiple similar RNA/DNA sequence alignment     Xi Chen  0    Chen Wang  0    Shanjiang Tang  0    Ce Yu  0    Quan Zou  0    0  School of Computer Science and Technology, Tianjin University ,  Yaguan Road, Tianjin ,  China     2017   18    12  6  2017    16  3  2017     Background: The multiple sequence alignment (MSA) is a classic and powerful technique for sequence analysis in bioinformatics. With the rapid growth of biological datasets, MSA parallelization becomes necessary to keep its running time in an acceptable level. Although there are a lot of work on MSA problems, their approaches are either insufficient or contain some implicit assumptions that limit the generality of usage. First, the information of users' sequences, including the sizes of datasets and the lengths of sequences, can be of arbitrary values and are generally unknown before submitted, which are unfortunately ignored by previous work. Second, the center star strategy is suited for aligning similar sequences. But its first stage, center sequence selection, is highly time-consuming and requires further optimization. Moreover, given the heterogeneous CPU/GPU platform, prior studies consider the MSA parallelization on GPU devices only, making the CPUs idle during the computation. Co-run computation, however, can maximize the utilization of the computing resources by enabling the workload computation on both CPU and GPU simultaneously. Results: This paper presents CMSA, a robust and efficient MSA system for large-scale datasets on the heterogeneous CPU/GPU platform. It performs and optimizes multiple sequence alignment automatically for users' submitted sequences without any assumptions. CMSA adopts the co-run computation model so that both CPU and GPU devices are fully utilized. Moreover, CMSA proposes an improved center star strategy that reduces the time complexity of its center sequence selection process from O(mn2) to O(mn). The experimental results show that CMSA achieves an up to 11× speedup and outperforms the state-of-the-art software. Conclusion: CMSA focuses on the multiple similar RNA/DNA sequence alignment and proposes a novel bitmap based algorithm to improve the center star strategy. We can conclude that harvesting the high performance of modern GPU is a promising approach to accelerate multiple sequence alignment. Besides, adopting the co-run computation model can maximize the entire system utilization significantly. The source code is available at https://github.com/wangvsa/CMSA.    Heterogeneous  GPU  Multiple sequence alignment (MSA)  Center star alignment       Background\r\n  Multiple sequence alignment (MSA) refers to the problem of aligning three or more sequences with or without inserting gaps between the symbols [ 1 ]. It is a fundamental tool for similar sequences analysis in computational biology and molecular function prediction. In computational molecular biology, similar DNA sequences are aligned to find out the single nucleotide polymorphism and the copy-number variant, which is the key content in genetics [ 2 ]. In molecular function prediction, large-scale similar DNA sequence alignment is required when addressing the evolutionary analysis of bacterial and viral genomes [ 3 ]. Therefore, MSA software need to be efficient and scalable to handle large-scale datasets, which may contain hundreds of thousands of similar sequences.  MSA is a problem with an exponential time complexity, it has been proven to be NP-complete [ 4 ]. Many heuristic algorithms are developed and implemented by previous studies, including Kalign [ 5 ], MAFFT [ 6 ] and Clustal [ 7 ]. However, our experiments show that none of these heuristic-based softwares can address the alignment of large-scale RNA/DNA datasets with more than 100,000 sequences. Besides, all of these softwares are optimized either for short sequences or long sequences and none of them are designed for any arbitrary lengths of sequences.  On the other hand, heuristic methods model the MSA problem as multiple pairwise alignment problems, and there are two kinds of classic algorithms, i.e., tree-based algorithm and center star algorithm. In the tree-based algorithm, an evolutionary tree may be assumed, with the input sequences assigned to the leaves. Additional reconstructed sequences, corresponding to the internal nodes of the tree, are added to the multiple alignment. A staralignment will denote the special case in which the tree has only one internal node. This is a reasonable model for the evolutionary history of some input sequences where all the observed sequences are assumed to have arisen by separate lineages from a single ancestral sequence [ 8 ]. For this scenario, the center star algorithm reduce the times of pairwise alignment, and both methods could achieve a similar accuracy. So in this paper, we focus on paralleling and optimizing the center star algorithm. A K-band strategy [ 2, 9 ] is proposed to reduce the time and space cost of dynamic programming process and then developed a MSA software named HAlign, which is based on the center star algorithm and K-band strategy. But its time complexity of finding the center sequence is still too high to make it practical for large-scale datasets. Therefore, we believe that it is necessary to further optimize the center star algorithm for large-scale MSA problems.  Recently, Graphic Processing Unit (GPU) with the Compute Unified Device Architecture (CUDA) programming model is widely used as additional accelerators for timeconsuming computations. And heterogeneous CPU and GPU platform is a desirable way to overlap the computation of the CPU and GPU to fully exploit the compute capability and shorten the runtime [ 10 ]. However, in the multiple similar sequence alignment area, few parallel implementations exist that can address large-scale datasets and produce good speedups.  In this paper, we present CMSA, a robust and efficient MSA system for large-scale datasets on the heterogeneous CPU/GPU platform. CMSA is based on the center star strategy and mainly focuses on the alignment of similar RNA/DNA sequences. It can perform and optimize multiple sequence alignment automatically for users' submitted sequences of any arbitrary length and volume. Second, it adopts the co-run computation model that leverages both CPU and GPU for sequence alignment. So it could maximize the entire system utilization. A pre-computation mechanism is developed in CMSA to estimate the computing capacity of CPU and GPU in advance. CMSA then distributes the workloads for CPU and GPU based on this estimation to achieve a better load balance. Furthermore, we propose a novel bitmap based algorithm to find the center sequence, which is the most crucial procedure in the center star strategy. The new algorithm reduces the time complexity from O(mn2) to O(mn) without sacrificing the accuracy. The experiments demonstrate the efficiency and scalability of CMSA. Specifically, it shows that CMSA has a linear scalability as the number of sequences increases and achieves a speedup up to 11×. We also compare CMSA with the state-of-the-art MSA tools including MAFFT, Kalign, and HAlign. The results show that CMSA is much faster than these tools and is able to process largescale datasets in an acceptable time, whereas previous tools cannot.   Multiple similar sequence alignment\r\n  Similar sequences probably have the same ancestor, share the same structure, and have a similar biological function. The biological information associated with similar sequences can provide the necessary foundation for determining the structure and function of the newly discovered ones. For example, in computational molecular biology, the alignment of similar DNA sequences can be used to find single nucleotide polymorphism.  There are several MSA methods that utilize the feature of the similarity between sequences. Progressive MSA methods align the most similar sequences firstly, add then add the less related sequences to the alignment in succession. The basic idea is that long common substrings can be rapidly extracted from the pairwise sequences when the input sequences are highly similar. Thus, we only need to align the remaining short regions. However, few MSA tools are implemented for massive similar sequences alignment. Therefore, we need some methods to solve the MSA problem on similar large-scale datasets.    Center star strategy\r\n  The main approach underlying the center star strategy is to transform the MSA problem into pairwise alignment problems.  For a dataset of n sequences with the average length of m, the ith sequence is define as si, where 1 ≤ i ≤ n. Sij is the similarity score of sequences si and sj. Si is the total similarity score of sequence si. Then the Si is can be computed with the following equation:  Si = n j=1  Sij, j = i  Center star strategy contains three steps: Step 1. Center sequence selection. Compute the total similarity score Si for each sequence and choose the one with a maximum value as the center sequence. Step 2. Pairwise alignment. The center sequence then is pairwise aligned with each of the other sequences. Step 3. Subtotaling the inserted spaces. All of the inserted spaces are summed to obtain the final MSA result.  Now we analyze the time complexity of the center star strategy. The result is shown in Table 1. Suppose we use a dynamic programming method such as NeedlemanWunsch [ 11 ] to align sequences, which demands O(m2) for both time and space. And in the first step, a naive way to find the center sequence is to align each pair of sequences, which costs a total time of O(m2n2). Then in the second step, aligning the center sequence with other n − 1 sequences demands a total time of (mn2). The position information of all inserted gaps can be stored in O(mn) spaces. In the last step, those gaps are summed to generate the final result.  The second column in Table 1 shows these three steps' time complexity of a naive center star strategy. A conclusion can be drawn from the table that most of the time would be used to find the center sequence. To reduce this cost, HAlign [ 9 ] uses trie trees to accelerate the process. The time complexity for building a trie tree for one sequence is O(m). Searching n sequences in a trie tree incurs a time cost of O(mn). These two steps are performed n times to find the center sequence, which requires a total time of O(mn2). But for large-scale datasets where n m, it's still not efficient enough. Therefore, in this paper, we propose a novel bitmap-based algorithm that could reduce the time complexity of the first stage to O(mn) and also achieves a better accuracy compared to HAlign. Our approach will be discussed in detail in \u201cAn improved center star algorithm\u201d section.    Heterogeneous CPU/GPU architecture\r\n  There are several different parallel programming approaches on multi-core systems: (i) Low-level multi-tasking or multi-threading such as  POSIX Thread (pThread) library [ 12 ]. (ii) High-level libraries, such as Intel Threading Building Blocks [ 13 ], which provides certain abstractions and features attempting to simplify concurrent software development. (iii) Programming languages or language extensions developed specifically for concurrency, such as  OpenMP [ 14 ].  Naive center star O(m2n2) O(m2n)  Moreover, GPU now is widely used to accelerate time-consuming tasks. GPU contains a scalable array of multi-threaded processing units known as streaming multi-processors (SM). Although GPU is originally designed to render graphics, general-purpose GPU (GPGPU) breaks this limit, and CUDA [ 15 ] is proposed as a general-purpose programming model for writing highly parallel programs. This model has proven quite successful at programming a broad range of scientific applications [ 16 ].  A heterogeneous CPU/GPU platform is proposed to achieve the best performance. Figure 1 depicts this architecture. CPU and GPU are connected by PCIE and both of them have their own memory space. There are two main methods for heterogeneous CPU/GPU programming. (i) Consider CPU as a master and GPU as a worker.  CPU handles the work assignment, data distribution, etc. GPU is responsible for the whole computation. (ii) CPU still plays the role of a master and at the same time, it shares a portion of GPU's computations.  The former method has a clear work division between CPU and GPU but wastes the computing resource of CPU regrettably. The latter method has a better performance, but it also brings in some tricky issues such as the load balance and extra communications between CPU and GPU.     Challenges and approaches\r\n  There are several key issues that we need to address for MSA in practice. In the following, we highlight these challenges and then give our corresponding solutions. The detailed implementation will be described in \u201cImplementation\u201d section.  The MSA problem on similar RNA/DNA sequence. Most MSA methods and tools ignore the similarity of RNA/DNA sequences, which is an important characteristic in RNA/DNA sequence alignment. Center star strategy is more suited for similar sequence alignment, but its center sequence selection process is very slow especially for large-scale datasets.  Fig. 1 The heterogeneous CPU/GPU architecture. To achieving the best performance, the co-run model of CPU and GPU is adopted  An improved center star algorithm. We have analyzed that the first stage, center sequence selection, is the most time-consuming part of a straightforward implementation of the center star strategy. Therefore, we designed a bitmap liked algorithm to find the center sequence. The time complexity is reduced from O(mn2) to O(mn), as discussed in \u201cCenter star strategy\u201d section.  Low utilization problem on the heterogeneous CPU/GPU platform. To further improve the performance of CMSA, parallelization is a sensible way. However, most GPU based MSA systems perform all computations on GPU only. The CPU source is idle. And these GPU based systems cannot run on different platforms which only contain CPU device. Therefore, it's necessary to exploit the computing power of CPU and GPU at the same time.  Co-run computation model. To fully utilize all available computing capacities in the heterogeneous CPU/GPU platform, it is crucial to enable CPU and GPU work concurrently for workload computations (i.e., co-run computation), which means that CPU also performs a portion of computation instead of waiting for GPU to do all the work. The software designed for heterogeneous CPU/GPU platform can adapt to different computation environment. And when the GPU is not available, the CPU can handle the overall computation. Thus, CMSA can run on different platforms with or without GPUs. We designed a pre-computation mechanism to decide how to distribute workloads between CPU and GPU.  Different lengths of sequences. Previous MSA software mainly focus on either short or long sequences, but no work consider both of them.  Automatical configuration. CMSA could automatically configure the parameters like thread number and block number according to the lengths of sequences. When the space requirement exceeds GPU's global memory limit, the related computation will be executed on CPU only.    Implementation\r\n  In this section, the execution overflow of CMSA is first explained. Then our improved center star algorithm is discussed. At last, the implementation details of CMSA on the heterogeneous CPU/GPU platform is described.   Execution overflow\r\n  CMSA is a heterogeneous CPU/GPU system, using CUDA and OpenMP for parallelization. To reduce the total execution time, CPU also carries out part of the alignment task instead of waiting for GPU to deal with the whole work. The execution overview of CMSA is shown in Fig. 2. It contains following steps: Step 1. Read input sequences. CMSA reads all sequences into the host (CPU) memory. After the pre-computation process, a copy of sequences that would be handled by GPU will be sent to the device (GPU) memory.  Step 2. Select center sequence. We design a bitmap based algorithm to find the center sequence. This process has a time complexity of O(mn) and could be finished within few seconds even with massive sequences, so it is performed on CPU only without any parallelization. The algorithm will be discussed in \u201cAn improved center star algorithm\u201d section. Step 3. Workload allocation. CMSA performs a precomputation process to decide how to distribute workload for CPU and GPU. In this process, a small number of sequences are aligned in advance to evaluate the computing capacity of CPU and GPU. The detailed information of workload allocation will be described in \u201cWorkload distribution\u201d section.  Step 4. Pairwise sequence alignment. CPU and GPU independently execute pairwise alignment of assigned sequences. For better performances, tasks on CPU are executed in parallel by using OpenMP library. On the GPU end, the parameters like the number of threads in a block and the number of blocks in a grid can be automatically configured based on the different lengths of sequences. \u201cParallel optimization of pairwise alignment\u201d section will describe the implementation of both ends. Step 5. Output. When both CPU and GPU finish their job, CMSA gathers the result from GPU and CPU, then merges the inserted gaps to generate the final result.    An improved center star algorithm\r\n  As we discussed in \u201cCenter star strategy\u201d section, a straightforward implementation of center star strategy is time-consuming mainly because its first stage. In spite of an improved method has been proposed by HAlign, which could substantially reduce the time of finding the center sequence, it still has a O(mn2) time complexity. For large-scale datasets where n m, it would become the bottleneck. Thus in this paper, we propose a bitmap based algorithm to further optimize the center sequence selection process.  First, every sequence is partitioned into a series of disjoint segments. Each segment consists of 8 characters. We use 2 bits (binary number) to represent a character. So a segment needs 16 bits space, which then can be stored in one integer. An example is given in Table 2. Suppose characters 'A', 'T', 'C', 'G' are represented by binary numbers 00, 01, 10, and 11, respectively. The binary number of segment \u201cATCGCGAT\u201d is 0001111011100001, which then is transformed into a decimal number 7905.  Second, an array of integers denoted as Occ[ ] is built for recording the time of occurrence of all segments. The decimal numbers of their represented segments are used as indexes. Therefore, the size of this array is 216 (Occ[ 65535]) since the maximum decimal number of a segment is 216. All elements of Occ is initialized with zero. Next, we look through all segments in each sequence and count the occurrences of them. For example, if a sequence has a segment \u201cATCGCGAT\u201d, whose decimal number is 7905, then the value of Occ[ 7905] is increased by one. Notice that same segments in one sequence only count once.  Finally, we find the center sequence with Occ. We calculate a similarity score (SS) for each sequence by accumulate the occurrences of all its segments. Suppose a sequence contains p segments and the decimal numbers of these segments are d1, d2, . . . , dp, then its similarity score is: SS = Occ[ d1] +Occ[ d2] + · · · + Occ[ dp]. After calculating all similarity scores, the sequence with a maximum SS is chosen to be the center sequence.  If there are n sequences with the average length of m, building the Occ array for one sequence needs a time of  O(m). So the process incurs a time cost of O(mn) for n sequences. Besides, calculating a similarity score for one sequence needs to access Occ m times, so all SSs can be calculated in the time of O(mn). Therefore, the total time complexity of center sequence selection is O(mn), which is less than HAlign's O(mn2). In our experiments, the process of finding the center sequence can complete in a few seconds for a dataset with 500 thousands of sequences.  As discussed in \u201cCenter star strategy\u201d section, we apply the dynamic programming method in the second phase of the center star strategy, which requires a time of O(m2n). In other words, the second step of CMSA, i.e. pairwise alignments, is now the most time-consuming phase. Therefore, we only focus on parallelizing the second step.    Workload distribution\r\n  One of the key issues of a heterogeneous system is load balance. Since CPU and GPU differ greatly in computing capability, a heterogeneous system needs a way to estimate this differential to achieve the load balance. Suppose the execution time of CPU and GPU are T1 and T2, then the total time of the pairwise alignment is the maxmum value of T1 and T2. So the best performance is achieved when the computations of CPU and GPU are completely overlapped, which means T1 = T2.  In CMSA, a pre-computation process is performed to decide how to distribute the workload for CPU and GPU. In this process, both CPU and GPU computes the same number of sequences (a small portion of input sequences). CMSA compares the execution time of CPU and GPU (denoted as t1 and t2) to calculate a ratio of computing capability R, R = tt21 . According to this ratio, CMSA then assigns R+n1 sequences to CPU and the rest RR+n1 sequences to GPU, where n is the number of input sequences.    Parallel optimization of pairwise alignment\r\n  In the CPU end, OpenMP is used to accelerate the pairwise alignment in a coarse-grained manner. The computation of the DP matrix and the backtracking of score matrices are mapped onto different threads. In other words, each thread is responsible for aligning the center sequence with a different sequence. Threads are working independently, and each thread handles its own memory space including allocating and releasing the resources. The number of threads is usually set to the number of cores in the CPU.  Typical general-purpose graphics processors consist of multiple identical instances of computation units called Stream Multiprocessors (SM). SM is the unit of computation to which a group of threads, called thread blocks. A CUDA program consists of several kernels. When a kernel is launched, a two-level thread structure is generated. The top level is called grid, which consists of one or more blocks, denoted as B. Each block consists of the same number of threads, denoted as T. The whole block will be assigned to one SM.  Like the implementation on CPU, each thread in a kernel aligns one sequence with the center sequence, which means each kernel computes B × T sequences. As we discussed early, GPU handles RR+n1 sequences totally. So on  Rn the GPU end, (R+1)(BT) kernels are executed. Since each kernel computes the same number of sequences and the DP matrices computed by each kernel are not used in the next kernel, we could recycle these memory resources. Before the first kernel is invoked, CMSA allocates the memory required for storing the DP matrices in one kernel. And when the last kernel finishes, the memory will be released. The DP matrix is stored in a one-dimensional way in the global memory of GPU. For example, there is a 12GB global memory. In theory, each kernel can simultaneously compute 53688 sequences of the length of 200 if each element in the DP matrix contains three short short type digital in this paper.     Results and discussion\r\n  We evaluate CMSA using 16s rRNA sequences on a heterogeneous CPU/GPU workstation. In this section, we first introduce the experimental environments and then evaluate the efficiency and scalability of CMSA along with our bitmap based algorithm. Finally, we compare CMSA with some of state-of-the-art MSA tools.   Experimental setup\r\n   Experimental platform\r\n  The experiments are carried out on a heterogeneous CPU/GPU platform, which has 32 GB RAM, an Intel Xeon E5-2620 2.4 GHz processor and an NVIDIA Tesla K40 graphic card. Centos 6.5 is installed and CUDA Toolkit 6.5 is used to compile the program. The CPU consists of 12 cores. The detailed specifications of Tesla K40 is shown in Table 3.    Datasets\r\n  The BALiBASE is small and is suited only for protein alignment. As there is no benchmark datasets contain large-scale DNA/RNA sequences, we employ human mitochondrial genomes(mt genomes) and 16s rRNA. 16s rRNA sequences are often used to infer phylogenetic relationships and to distinguish species in microbial environmental genome analyses (Hao et al., 2011). All sequences are obtained from NCBI's GenBank database (http://www.ncbi.nlm.nih.gov/pubmed). The mt genomes is a highly similar dataset. To address DNA/RNA sequences with low similarity, we also tested our program on 16s rRNA. We classified these 16s rRNA sequences into three datasets according to their average lengths, named as D1, D2 and D3, respectively, as shown in Table 4.    Metrics\r\n  The sum-of-pairs (SP) score is often chosen to measure the alignment accuracy. The SP score is the sum of every pairwise alignment score from the MSA. But for large-scale datasets, it may be very large and exceeds the computer's limitation. Thus we employ the average SP value, which is simply divided the SP value by the number of sequences, n. The average SP can also describe alignment performance. In the experimental tests, a program, \u201cbali_score\u201d, downloaded from the Balibase benchmark (http://www.lbgi.fr/balibase/) was used to compare the alignment results.    Baselines\r\n  To show the efficiency and accuracy of CMSA, we compare CMSA with state-of-the-art MSA tools including Kalign, MAFFT and HAlign. Most of state-of-the-art MSA software cannot handle large-scale datasets. In order with data handling size, these tools are T-Coffee (small), CLUSTAL (medium), MAFFT (medium-large) and Kalign(large), as suggesting by EMBL-EBI. Therefore, the MAFFT, Kalign v2 is adopted. Besides, HAlign is the state-of-the-art software using center star strategy. Therefore, we use HAlign, MAFFT and Kalign v2 as benchmarks, and default parameters of Kalign v2, MAFFT and HAlign are used. For fairer comparison, all experiments are conducted on one node.     Bitmap based algorithm for selecting the center sequence\r\n  As we discussed in \u201cCenter star strategy\u201d section, both HAlign and CMSA are based on center star strategy. HAlign uses a tire-tree based algorithm to find the center sequence whereas CMSA uses a bitmap based algorithm. To evaluate our new proposed algorithm, we first compare the running time of the first stage of HAlign and CMSA. Then we perform the subsequent steps using the center sequence selected by HAlign and compare its results with ours. In addition to our own datasets, we also test HAlign and CMSA on the human mitochondrial genomes dataset(marked as MT), which is used in HAlign's experiments. The human mitochondrial genome dataset is a highly similar dataset. It has a total of 672 human mitochondrial genomes shown in Table 4.  Table 5 shows the running time and SP score of HAlign and CMSA(CPU) based on different center sequence selection algorithms. For fairness, the HAlign was tested on only one node. The center sequence showed in the table is the zero-based index of sequences. As we can see, CMSA is much faster than HAlign in all experiments since our bitmap based algorithm has a lower time complexity (O(mn)). Also, HAlign runs out of memory when computing dataset D3 with 5000 sequences. When processing the dataset D2 with 1000 sequences and the dataset D3 with 1000 sequences, HAlign and CMSA find the same center sequence. Except these two tests, HAlign and CMSA reach a different result. And when inspecting the average SP score, CMSA performs better than HAlign. Besides, the better average SP score occurs with the datasets of high similarity. Thus we can conclude that our new algorithm used to find the center sequence is efficient and accurate with high and low similarity.    Efficiency and scalability\r\n  As an indication of how CMSA scales with the size of dataset, Fig. 3a shows the running time of CMSA on all three datasets described in Table 4. It's clear that the longer the average length is, the more time it would cost. Moreover, in all three datasets, the running time goes up linearly as the number of sequences increases, which demonstrates a great scalability of CMSA. Figure 3b shows the speedup of the same experiments. The best speedup is not achieved at first since with a low number of sequences, the runtime of the pre-compute and initialization makes up a considerable proportion. With the increase of the number of sequences, the real computation would dominate most of the running time, which in turn reports a better speedup. MT D1 D2 D3  HAlign Step1  We have test the CMSA (CPU/GPU) with different numbers of sequences(average length:252). Table 6 shows the workload ratio (R) described in \u201cWorkload distribution\u201d section. From the table, the values of workload ratio are similar, and the average workload ratio of GPU and CPU is 1.420. We can confirm that CMSA has the good method of workload distribution for CPU and GPU.    Comparison with State-of-the-art tools\r\n  To show the efficiency and accuracy of CMSA, we compare CMSA with state-of-the-art MSA tools. In this setion, CMSA(CPU) and CMSA(CPU/GPU) are both tested.  Table 7 shows the time consumed for three datasets with different number of sequences computed. In our experiments, Kalign cannot handle datasets that consist of more than 100,000 sequences. MAFFT runs without a problem, but it takes too much time, e.g. 18 h for D1 with 100,000 sequences and more than 24 h for D2 and D3 with 100,000 sequences. So we don't record the exact running time of CMSA for D2 and D3 with more than 100,000 sequences. In comparison, both HAlign and CMSA can handle all datasets in an acceptable time. Moreover, in all experiments, CMSA is the fastest one and also the one having the best scalability as the number of sequences increases. When computing D3, CMSA is 13× faster than HAlign when the dataset size is 10,000 and 24× faster when the size increases to 500,000.  Workload radio 1.382  Table 8 shows the comparison result of average SP scores for 16 s rRNA datasets. From Table 8, we can observe that MAFFT produced better alignment results than other state-of-the-art MSA softwares when addressing the large-scale datasets. The average SP of CMSA was lower than that of MAFFT and higher than that of HAlign. Therefore, we confirm the robustness of CMSA, whether with large-scale or small datasets.     Related work\r\n  There are a number of work on MSA problems and many parallel techniques as well as optimization methods have been proposed to accelerate MSA algorithms. In this section, we review them from two aspects.  MSA software and algorithms. MSA software can be classified into two categories based on their underlying algorithms: heuristic based or combinatorial optimization based. Many popular MSA tools like T-Coffee [ 17 ], CLUSTAL [ 7 ], Kalign [ 5 ] and MAFFT [ 6 ] are based on heuristic methods. T-Coffee can make accurate alignments of very divergent proteins but only for small sets of sequences, given its high computational cost. CLUSTAL is suitable for medium-large alignments. On a single machine, it is possible to take over an hour to compute 10,000 sequences with a more accurate method of CLUSTAL. Kalign is as accurate as other methods on small alignments, but is significantly more accurate when aligning large and distantly related sets of sequences. MAFFT uses fast fourier transforms, which can handle medium-large file sizes and align many thousands of sequences. ClustalW [ 18 ] has more than 52,400 citations now and is considered the most popular MSA tool. A commercial parallel version of ClustalW is designed for expensive SGI shared memory multiprocessor machines [ 19 ]. ClustalW-MPI [ 20 ] targets distributed memory workstation clusters using MPI but parallelize only Stages 1 and 3 of ClustalW. It achieves speedup of 4.3 using 16 processors on the 500-sequences test data. MSA-CUDA [ 21 ] parallelizes all three stages of the ClustalW processing pipeline using CUDA and achieves average speedup of 18.74 for average-length protein sequences compared to the sequential ClustalW. CUDA MAFFT [ 22 ] also uses CUDA to accelerate MAFFT that can achieve speedup up to 19.58 on a NVIDIA Tesla C2050 GPU compared to the sequential and multi-thread MAFFT.  Center star algorithm. The center star algorithm is a combinatorial optimization method and it is much more suited for aligning similar sequences. Then, K-band [ 2 ] is proposed to reduce the space and time cost of the pairwise alignment stage of the center star strategy. Based on the fact that for similar sequences, the backtracking often runs along the diagonal, so the lower left quarter and the upper right quarter in dynamic programming table are not taken into consideration. Therefore the K-band method only computes the band of which the width is k nearby the diagonal of the dynamic programming table. HAlign [ 9 ]then further optimized the center star algorithm with a trie-tree data structure, as we discussed in \u201cCenter star strategy\u201d section. But this method still requires a time cost of O(mn2) to find the center sequence, which is not efficient enough to handle large-scale datasets. Because of this, their Hadoop version skips the center sequence selection process and just designate the first sequence as the center sequence. Moreover, to our best knowledge, there are no work exist on accelerating the center star algorithm with CUDA enabled GPUs.    Conclusion\r\n  In this paper, we designed CMSA, a robust and efficient MSA system for large-scale datasets on the heterogeneous CPU/GPU system. CMSA is based on an improved center star strategy, for which we proposed a novel bitmap based algorithm to find the center sequence. The new algorithm reduces the time complexity from O(mn2) to O(mn). Moreover, CMSA is capable of aligning a large number of sequences with different lengths, which extends the generality of previous studies in MSA. In addition, to fully utilize the computing devices, CMSA takes co-run computation model so that the workloads are assigned and computed on both CPU and GPU devices simultaneously. Specifically, we proposed a pre-computation mechanism in CMSA to distribute workloads to CPU and GPU based on their computing capacity. Moreover, the more accurate mechanism will be future work to be performed for CMSA.  The experiment results demonstrated the efficiency and scalability of CMSA for large-scale datasets. CMSA achieved a speedup of 11 at best and can handle a large dataset with 500,000 sequences in half an hour. We also evaluated our center sequence selection algorithm. It is much faster and accurate that trie-tree based algorithm proposed in HAlign. Besides, we compared CMSA with some state-of-the-art MSA tools including Kalign, HAlign and MAFFT. In all our experiments, CMSA outperformed those software both in average SP scores and in the execution times. Abbreviations CPU: Central processing unit; CUDA: Compute unified device architecture; GPU: Graphics processing unit; MPI: Message passing interface; MSA: Multiple sequence alignment; NCBI: National center for biotechnology information; OpenMP: Open multiprocessing; PCIe: Peripheral component interconnect express; pthread: POSIX thread Acknowledgements We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research. Besides, Shanjiang Tang is one of the corresponding authors.  Funding This work is supported by the National Natural Science Foundation of China (61602336, 61370010, U1531111).  Authors' contributions XC conceptualized the study, carried out the design and implementation of the algorithm, analyzed the results and drafted the manuscript; CW implemented the parallel part of CMSA, designed the experiments and revised for important intellectual content. SJT, CY provided expertise on the GPU, participated in analysis of the results and contributed to revise the manuscript. QZ provided expertise on the MSA and contributed to revise the manuscript. All authors read and approved the final manuscript.  Competing interests The authors declare that they have no competing interests.  Consent for publication Not applicable.  Ethics approval and consent to participate Not applicable.    Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/wangvsa/CMSA",
    "publicationDate": "0",
    "authors": [
      "Xi Chen",
      "Chen Wang",
      "Shanjiang Tang",
      "Ce Yu",
      "Quan Zou"
    ],
    "status": "Success",
    "toolName": "CMSA",
    "homepage": ""
  },
  "2.pdf": {
    "forks": 0,
    "URLs": [
      "orcid.org/0000-0002-5532-1640",
      "github.com/dpxiong/HHConPred"
    ],
    "contactInfo": ["hgong@tsinghua.edu.cn"],
    "subscribers": 1,
    "programmingLanguage": "Python",
    "shortDescription": "",
    "publicationTitle": "Predicting the helix-helix interactions from correlated residue mutations",
    "title": "Predicting the helix-helix interactions from correlated residue mutations",
    "publicationDOI": "10.1002/prot.25370",
    "codeSize": 1736,
    "publicationAbstract": "Helix-helix interactions are crucial in the structure assembly, stability and function of helix-rich proteins including many membrane proteins. In spite of remarkable progresses over the past decades, the accuracy of predicting protein structures from their amino acid sequences is still far from satisfaction. In this work, we focused on a simpler problem, the prediction of helix-helix interactions, the results of which could facilitate practical protein structure prediction by constraining the sampling space. Specifically, we started from the noisy 2D residue contact maps derived from correlated residue mutations, and utilized ridge detection to identify the characteristic residue contact patterns for helix-helix interactions. The ridge information as well as a few additional features were then fed into a machine learning model HHConPred to predict interactions between helix pairs. In an independent test, our method achieved an F-measure of 60% for predicting helixhelix interactions. Moreover, although the model was trained mainly using soluble proteins, it could be extended to membrane proteins with at least comparable performance relatively to previous approaches that were generated purely using membrane proteins. All data and source codes are available at http://166.111.152.91/Downloads.html or https://github.com/dpxiong/HHConPred.",
    "dateUpdated": "2017-07-31T03:08:24Z",
    "institutions": ["Tsinghua University"],
    "license": "No License",
    "dateCreated": "2017-07-31T03:00:31Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Barth P, Schonbrun J, Baker D. Toward high-resolution prediction\r\nand design of transmembrane helical protein structures. Proc Natl\r\nAcad Sci USA.     10.1002/prot.25370   Predicting the helix-helix interactions from correlated residue mutations     Dapeng Xiong  0  1    Haipeng Gong  hgong@tsinghua.edu.cn  0  1    0  Beijing Innovation Center of Structural Biology, Tsinghua University ,  Beijing ,  China    1  MOE Key Laboratory of Bioinformatics, School of Life Sciences, Tsinghua University ,  Beijing ,  China     2017   104  40  1  8   Helix-helix interactions are crucial in the structure assembly, stability and function of helix-rich proteins including many membrane proteins. In spite of remarkable progresses over the past decades, the accuracy of predicting protein structures from their amino acid sequences is still far from satisfaction. In this work, we focused on a simpler problem, the prediction of helix-helix interactions, the results of which could facilitate practical protein structure prediction by constraining the sampling space. Specifically, we started from the noisy 2D residue contact maps derived from correlated residue mutations, and utilized ridge detection to identify the characteristic residue contact patterns for helix-helix interactions. The ridge information as well as a few additional features were then fed into a machine learning model HHConPred to predict interactions between helix pairs. In an independent test, our method achieved an F-measure of 60% for predicting helixhelix interactions. Moreover, although the model was trained mainly using soluble proteins, it could be extended to membrane proteins with at least comparable performance relatively to previous approaches that were generated purely using membrane proteins. All data and source codes are available at http://166.111.152.91/Downloads.html or https://github.com/dpxiong/HHConPred.       -\r\n  |  K E Y W O R D S helix-helix interactions, machine learning, ridge detection, residue contact map, protein structure prediction Predicting the structures of proteins from amino acid sequences is one of the most challenging open problems in computational biology and chemistry.1-3 Despite the rapid deposition of experimentally determined protein structures into the Protein Data Bank (PDB),4 the gap between available sequences and structures is continuously broadening, owing to the even more rapid development of sequencing techniques.3,5,6 Accurate and reliable methods to predict protein structures from sequences are thus in urgent demand. Prevalent protein structure prediction algorithms can be classified into two categories, templatebased and de novo methods, which make the prediction from known structures and from scratch, respectively.2,7 The de novo methods have attracted more interests in the past decade, because of the lack of limitation in practice, especially for protein targets without Dapeng Xiong and Wenzhi Mao contributed equally to this work. homologous structural templates in the PDB database.5,7 Despite the recent progresses,8-10 the usefulness of de novo methods is, however, severely limited by the poor accuracy and reliability.5,11  Indeed, protein fold prediction can hardly be cast as a multistep regression task in the multivariate space, mainly because many effective loss functions for quality evaluation may vary during translations and rotations while protein structures do not.3 In a different approach, one can predict intermediate structural representations that are invariant in translations and rotations and are thus capable of constraining the space of conformational sampling in de novo protein structure predictions. One of such structural representations is the map of residueresidue contacts in proteins,3 and residue contact prediction has thus become a regular subject in the latest Critical Assessment of protein Structure Prediction (CASP) competitions.12 However, reconstructing 3D structures that exactly match a given residue contact map is a wellknown NP-hard problem in computational geometry,13,14 which often cannot achieve accurate solution within an acceptable period of time, especially for large proteins. More importantly, the accuracy of contemporary residue contact prediction algorithms is still far from satisfaction. Particularly, no convincing methods can reliably differentiate errors in the predicted contact maps, and the introduction of erroneous residue contact information in the subsequent protein structure prediction will inevitably impairs the accuracy of predicted structure models.15  Despite the high error rate for the contact prediction of individual residue pairs, interactions between secondary structure elements could be more reliably identified from the predicted residue contact maps because of the presence of repetitive residue contacting patterns.  Here, we focused on the prediction of helix-helix interactions, which could effectively facilitate the de novo structure prediction,16,17 the rational design18,19 and the folding study20,21 of helix-rich proteins.  Interestingly, a recent study reported that structural models obtained using predicted contacts of the same level of precision as restraints are more accurate for mainly a-proteins than for b-proteins.22,23  Furthermore, information of helix-helix interactions is particularly useful for the structural reconstruction of membrane proteins.24 Despite importance in a large variety of essential cellular functions,25 membrane proteins only occupy &lt;1% in the PDB database,26 due to technical difficulty in structural determination.27 Most membrane proteins have their transmembrane (TM) regions folded into helices, and interactions between these TM helices are therefore important determinants of their folding and stabilization.28,29 Several methods have been proposed to predict helix-helix interactions of helical membrane proteins, such as TMHcon,30 TMhit,29 TMhhcp,31 MemBrain,32 and MemConP.33 However, all of these methods were constructed from a limited number of available membrane proteins in the PDB database, which may impair the model robustness.  In this work, we presented a method, HHConPred, to identify the helix-helix interactions from predicted residue contact maps that were derived from correlated residue mutations within the multiple sequence alignment (MSA). In specific, we utilized the ridge detection to capture the characteristic patterns for interacting helix pairs in the noisy 2D contact maps, and then fed the ridge information as well as a few addition features into an adaptive boosting (AdaBoost) algorithm for prediction. Although the model was optimized using a set of nonredundant protein structures mainly composed of soluble proteins, we believe in its applicability in membrane proteins due to the following reasons: (1) The packing of a helix pair should follow similar general principles in soluble and membrane proteins, considering the similar hydrophobicity of the interiors of globular proteins and lipid membrane; (2) Our model mainly utilized the information of correlated residue mutations, a feature independent of the difference in folding of soluble and membrane proteins; (3) A recent research showed success of applying residue contact maps that were predicted from models constructed upon soluble proteins in the structural modeling of membrane proteins.34 In the independent benchmark tests, our method HHConPred not only showed good performance on soluble proteins, but also exhibited at least comparable prediction powers relatively to previous algorithms constructed purely from membrane proteins when predicting helix interactions for membrane proteins. 2 | M A T E R I A L S A N D M E T H O D S    2.1 | Datasets\r\n  The datasets in this study were derived from the database of Structure Classification of Proteins-extended (SCOPe).35 In specific, we extracted the data from SCOPe release 2.06 and removed domains that contain &lt;2 helices following the DSSP definition,36 that are &lt;50 residues, and that have multiple structures or contain missing backbone atoms. Redundancy was then removed using BLASTCLUST,37 by clustering the domains using 20% sequence identity and choosing only one representative (the shortest one) from each cluster. The dataset was further filtered, by retaining only one representative per SCOPe family.  The complete dataset contained 2293 protein domains.  The complete dataset was then divided into two mutually exclusive groups: a training set for model optimization/cross-validation and a testing set for independent benchmark test. To ensure the model robustness for newly discovered targets, 1918 domains released in the old SCOPe version 1.75 were assigned to the training set, while the remaining 375 domains were assigned to the testing set.  For each protein, all helices were extracted with short ones ( 6 residues) neglected. The combination of all helix pairs within each protein in the training and testing protein sets thus jointly composed the corresponding datasets for helix-helix interactions. Helix pairs separated by &lt; 2 residues were ignored to avoid ambiguity. A pair of helices was defined as interacting if there were 3 inter-helix residue-residue contacts with the distal contacting points separated by at least one residue in both helices and the scalar angle between orientation vectors of two helices was &lt;708, where the orientation vector of each helix was computed from the centers of mass of Ca atoms in the first and second halves.  In the evaluation of our method on the helical membrane proteins, we removed all non-soluble proteins from the training dataset for model construction. The testing dataset was obtained from the published article of MemConP,33 which contained 30 membrane proteins.  We also collected membrane protein structures that were deposited into the PDBTM database38 after the data extraction of MemConP to construct a new testing protein set, following the same criterion by MemConP to strictly control the redundancy in sequence and structure. All new testing proteins were published between June 2015 and February 2017 and were \u201cdissimilar\u201d to members of the MemConP training and testing sets following the criterion of MemConP: two proteins were regarded as similar if the sequence identity exceeded 35%, or the TM-score39 exceeded 0.5, or they belonged to the same PFAM family or clan.40 Structures with resolution worse than 3.5 Å, containing &lt; 3 helices, or including unknown residues in sequences were then removed. The new testing protein set finally contained 11 structures. F I G U R E 1 An example of residue contact map (1G73B). (A) The raw contact map obtained from CCMpred. (B-E) Images sub-pooled by retaining the rows/columns separated by 1, 2, 3, or 4 residues, respectively. [Color figure can be viewed at wileyonlinelibrary.com]    2.2 | Feature vectors\r\n  The secondary structure information required for feature extraction was predicted from the amino acid sequences by DeepCNF.41 For each residue, DeepCNF reported the predicted probabilities of three types of secondary structures (H, E and C), but we only adopted the secondary structure type with the largest probability. Thus, the secondary structure information we used here was a single secondary structure sequence for each protein target. The noisy residue contact map for each protein was derived based on correlated mutations in MSA by CCMpred.42 Given these information, we designed the following features for a helix pair denoted as Hm and Hn: 1. F1: ridge information from the predicted residue contact map In mathematics, ridges of a smooth function of two variables are a set of curves with points reaching local maxima in at least one dimension.43 Ridge detection has been successfully applied in computer vision, particularly in capturing the elongated objects on a 2D image.44 Here, we took the predicted residue-residue contact map as a 2D image with the size of L 3 L, where L is the length of protein sequence. The intensity at each position was thus the probability assigned by the residue contact predictor CCMpred.42 We thus could utilize the ridge detection algorithm to capture the repetitive residue contacts between a pair of interacting helices. Conventional ridge detection algorithms require continuous distribution of signal intensity. However, a pair of interacting helices exhibits repetitive but discontinuous native contacts in the diagonal or off-diagonal directions (Figure 1A). To solve this problem, we sub-pooled the raw image by retaining the rows/columns separated by 1, 2, 3, or 4 residues, respectively. The periodicity of 3.6 residues for helices allowed residue contacts in a pair of helices to exhibit continuous elongated distributions in at least one of the sub-pooled images (Figure 1B-E).  A main problem with the fixed-scale ridge definition is that a single strength level cannot capture all major ridges. A number of ridge descriptors have been introduced for automatic scale selection when no a priori information is available.44 In this study, we adopted three measures of ridge strengths, including AL, ML, and NL.44 Because ridge detection is very sensitive to the noise level of the raw image, here, we further developed a de-noised version of ridge strength (see Supplementary Materials for detailed introduction).  By this means, we extracted the ridge information, including the height, width and angle of each ridge, from the raw image and the sub-pooled images of retaining rows/columns separated by 1, 2, 3, or 4 amino acids, respectively. The feature dimensions (height/ width/angle) of ridge information for these images were 1/1/1, 4/ 4/4, 9/9/9, 16/16/16, and 25/25/25, respectively. According to 5-fold cross validation, the ridge measure NL was chosen as the final measure of ridge detection.   2. F2: Length of intervening sequence between Hm and Hn\r\n  This feature was represented as a vector of the binary states within 8 intervals (1-2, 3-7, 8-10, 11-23, 24-34, 35-52, 53-70, and 71).    3. F3: Number of helices between Hm and Hn\r\n  Similar to F2, the number of helices was represented by a vector of binary states in 8 intervals ( 0, 1, 2, 3-9, 10-13, 14-17, 18-20, 21 ). 4. F4: Numbers of residues in three consecutive helices centered at Hm and Hn This feature was extracted as a 6-dimensional vector (one entry for each helix). 5. F5: Flags to label the first, second, second-to-last and last helices in the sequence for Hm and Hn This feature contained only 6 flags, because the helix on the N-/Cterminal side within the pair is impossible to be the last/first one. Combining all features, we constructed a 193-dimensional complete feature vector for each helix pair. The performance of HHConPred was evaluated using 5-fold cross validation on the training dataset. Notably, in practical prediction, values of F2-F5 were obtained from the predicted secondary structures.     2.3 | Model selection\r\n  We chose the AdaBoost algorithm as our classification paradigm, which has shown strong ability in finding global classification solutions.45 The AdaBoost algorithm was implemented using the scikit-learn package (version 0.18.1),46 with decision tree chosen as the weak classifiers (see Supplementary Materials for detailed introduction). The maximum number of estimators at which boosting is terminated was set to 130, and all the other hyper-parameters were set to the default values. Notably, hyperparameters (including the choice of AL, ML, and NL ridge measures) were chosen purely based on cross validation on the training set.    2.4 | Feature importance evaluation\r\n  Feature selection can identify the optimal subset of input features and thus can roughly evaluate feature importance,47,48 We adopted the group MCP49 in feature selection (see Supplementary Materials for detailed introduction). The group MCP algorithm was implemented using the grMCP R package (2.8.1).49,50 The weight of regularization parameters of the group and L2 penalties as well as the maximum number of iterations were set to 0.5 and 1 00 000, respectively, following 5-fold cross validation on the training dataset, while all other parameters were set to the default values. In addition to feature selection, we also evaluated the contribution of each individual feature by iteratively removing this feature from the complete feature set.    2.5 | Performance evaluation\r\n  Several evaluation measures, including Precision (positive prediction value, PPV), Recall (true positive rate, TPR), accuracy (ACC), Matthews where TP, FN, TN, and FP refer to true positives, false negatives, true negatives and false positives, respectively. Precision and Recall measure the proportions of correctly identified helix pairs within the predicted and real interacting ones, respectively, while ACC estimates the overall accuracy of both interacting and non-interacting helix pairs. MCC indicates the degree of correlation between the real and the predicted interacting status of the helix pairs. F-measure is the harmonic mean of Precision and Recall and is generally believed as a more comprehensive and effective evaluator. In this work, we chose F-measure as the primary evaluation criterion. In the evaluation for soluble proteins, considering that many globular proteins contain very few interacting helix pairs, the performance measures were reported on the level of helix pairs rather than averaged over protein targets.    3.1 | Performance evaluation on the training dataset\r\n  We adopted three measures of ridge strength to extract the ridge information. Results obtained through these measures were compared in the 5-fold cross validation on the training dataset. As shown in Table 1, according to the primary evaluator F-measure, results derived from AL and ML are very close, both inferior to NL. Thus, the measure of NL was chosen as the final measure, and our program achieved an ACC of 88.15%, an MCC of 0.5262 and an F-measure of 59.13% (Precision 5 53.82%, Recall 5 65.59%).  Moreover, we also constructed models with the same feature set but using different learning strategies, including random forest (RF), support vector machine (SVM), back-propagation neural networks (BPNN), naïve Bayes (NB), and deep belief networks (DBN). The technical details of all comparison methods are described in the  Learning strategy AdaBoost RF Supplementary Materials. As summarized in Table 2, AdaBoost remarkably outperforms the other ones in respect of all evaluators. Figure 2 shows the Precision-Recall (PR) curves for learning strategies tested here, and the area under the PR curve (AUPRC) was used to quantify performance. The AUPRC values of AdaBoost, RF, SVM, DBN, BPNN, and NB models are 0.5956, 0.6021, 0.5399, 0.5039, 0.4160, and 0.1338, respectively, which suggests the stronger prediction powers of AdaBoost and RF. Combining F-measure and AUPRC, AdaBoost is the best model in predicting helix-helix interactions.    3.2 | Feature importance\r\n  We performed feature selection to identify the optimal feature subset. Unexpectedly, all features were retained after feature selection, which indicates the usefulness of all features adopted in this work. Subsequently, in order to further quantify the contribution of individual features, we iteratively removed each individual one from the feature set and reconstructed the model for performance evaluation. From Tables 2 and 3, removing any feature, except the complete 165-dimension ridge information (F1), slightly weakens performance (in terms of Fmeasure), which reinforces the importance of features proposed in this work. As for ridge information, although each component (F1_X, where X 5 0 represents the raw image while X 5 1, 2, 3, 4 represents different sub-pooling images) makes nearly equal amount of small contribution to the overall F-measure, they jointly contribute to 9.14% in Fmeasure. Therefore, the ridge detection that we proposed to extract information from the predicted residue contact maps plays an important role in the prediction of helix-helix interactions.    3.3 | Evaluation on the benchmark testing dataset\r\n  We then evaluated the performance of HHConPred on the independent testing dataset, in which all targets are novel folds (in the SCOPe definition) to the training dataset. In the testing dataset, HHConPred yields an ACC of 87.23%, an MCC of 0.5468 and an F-measure of 61.81% (Precision 5 55.92%, Recall 5 69.07%), very close to the crossvalidation results. The steady and slightly higher performance in novel folds supports the reliability and robustness of our method.  Our method was developed using the contact maps predicted from CCMpred. To evaluate improvement, we calculated the performance of CCMpred predictions for residues in the helical regions. As expected, | 5 the raw residue contact prediction is poor with ACC 5 40.01%, MCC 5 0.1388 and F-measure 5 28.63% (Precision 5 17.18%, Recall5 85.77%). However, by combining ridge detection and machine learning, we successfully extracted the coarse contact information for helix pairs from noisy residue contact maps with greatly improved precision. Although helix-helix interactions are less informative than residue contacts, their significantly improved precision renders the usefulness in practical protein structure prediction, for instance, as additional constraints in pseudo energies to effectively constrain conformational sampling.    3.4 | Evaluation on the membrane protein dataset\r\n  Because helix packing may follow similar rules in the hydrophobic interiors of globular proteins and in membrane, and because the raw information (correlated residue mutations) in our model is independent of the difference in folding mechanisms between soluble and membrane proteins, we propose that our model may be extended to prediction of helix-helix interactions in membrane proteins. We removed all membrane proteins from the training dataset and reconstructed the model to validate this proposition. Here, we compared the performance of HHConPred against two popular helix-helix interaction predictors for membrane proteins, TMhhcp and MemConP.  Table 4 summarizes the performance evaluation using the testing dataset of the MemConP article. Clearly, the relatively higher values of Precision, ACC and MCC in TMhhcp and MemConP were obtained at the sacrifice of Recall, which may limit the general usefulness of these two programs in membrane proteins. Conversely, HHConPred could find 44.78% of the true helix pairs (Recall), albeit with lower Precision. In respect of F-measure that balances Recall and Precision, HHConPred outperforms the other programs by &gt;6%. Notably, ACC and MCC, evaluators that include true negatives, should be less seriously considered than F-measure here, because positive samples are greatly outnumbered by negative samples in the case of helix-helix interactions. Overall, our program could achieve at least a comparable F I G U R E 2 The Precision-Recall (PR) curves for all methods in the 5-fold cross validation performance in predicting helix-helix interactions for membrane proteins, which supports our idea that data from soluble proteins could be used for the prediction of membrane proteins as long as they are reasonably utilized.  Moreover, since our program was trained in the dataset of soluble proteins, the plenty of samples can ensure the model robustness. In contrast, both TMhhcp and MemConP were trained using a small set of membrane proteins. To further evaluate their robustness, we collected the membrane protein structures determined after the data extraction of MemConP, and constructed a new testing dataset, where all proteins are dissimilar to members of the training and testing datasets of MemConP in terms of both sequence and structure. Performance evaluation on this new dataset was summarized in Table 5.  Clearly, in contrast to the large variations in the results of TMhhcp and MemConP between the two protein sets, HHConPred exhibited a more robust performance. Again, our program achieved an improvement of &gt;8% in F-measure.  In conclusion, in this work, we presented a novel method, HHConPred, to predict helix-helix interactions in both soluble and membrane proteins, given the noisy 2D contact maps generated from correlated mutations. The novel features proposed here, especially the ridge information, allow reliable prediction of helix-helix interactions in both soluble and membrane proteins.  A C K N O W L E D G M E N T S This work was supported by the funds from the National Natural Science Foundation of China (#31670723) and from the Beijing Innovation Center of Structural Biology. The authors declare no conflict of interest.  O R C I D R E F E R E N C E S Haipeng Gong  http://orcid.org/0000-0002-5532-1640 [1] Floudas CA. Computational methods in protein structure prediction.  Biotechnol Bioeng. 2007;97(2):207-213. [2] Baker D, Sali A. Protein structure prediction and structural genomics. Science. 2001;294(5540):93. [3] Vullo A, Frasconi P. Prediction of protein coarse contact maps.  J Bioinf Comput Biol. 2003;01(02):411-431. [4] Berman HM, Westbrook J, Feng Z, et al. The protein data bank.  Nucleic Acids Res. 2000;28(1):235-242. [5] Cao R, Bhattacharya D, Adhikari B, Li J, Cheng J. Large-scale model quality assessment for improving protein tertiary structure prediction. Bioinformatics. 2015;31( 12 ):i116-i123. [6] Lee J, Wu S, Zhang Y. Ab initio protein structure prediction. In: Rigden DJ, ed. From Protein Structure to Function with Bioinformatics.  Dordrecht, The Netherlands: Springer; 2009:3-25. [7] Zhang Y. Progress and challenges in protein structure prediction.  Curr Opin Struct Biol. 2008;18(3):342-348. [8] Rohl CA, Strauss CEM, Misura KMS, Baker D. Protein structure prediction using rosetta. Methods Enzymol. 2004;383:66-93. [9] Yang J, Yan R, Roy A, Xu D, Poisson J, Zhang Y. The I-TASSER Suite: protein structure and function prediction. Nat Methods. 2015; 12(1):7-8. Additional Supporting Information may be found online in the supporting information tab for this article.    ",
    "sourceCodeLink": "https://github.com/dpxiong/HHConPred",
    "publicationDate": "0",
    "authors": [
      "Dapeng Xiong",
      "Haipeng Gong"
    ],
    "status": "Success",
    "toolName": "HHConPred",
    "homepage": ""
  },
  "72.pdf": {
    "forks": 1,
    "URLs": ["github.com/insilichem/gaudi"],
    "contactInfo": [],
    "subscribers": 2,
    "programmingLanguage": "Python",
    "shortDescription": "A modular optimization platform for molecular design",
    "publicationTitle": "GaudiMM: A Modular Multi-Objective Platform for Molecular Modeling",
    "title": "GaudiMM: A Modular Multi-Objective Platform for Molecular Modeling",
    "publicationDOI": "10.1002/jcc.24847",
    "codeSize": 3669,
    "publicationAbstract": "GaudiMM (for Genetic Algorithms with Unrestricted Descriptors for Intuitive Molecular Modeling) is here presented as a modular platform for rapid 3D sketching of molecular systems. It combines a Multi-Objective Genetic Algorithm with diverse molecular descriptors to overcome the difficulty of generating candidate models for systems with scarce structural data. Its grounds consist in transforming any molecular descriptor (i.e. those generally used for analysis of data) as a guiding objective for PES explorations. The platform is written in Python with flexibility in mind: the user can choose which descriptors",
    "dateUpdated": "2017-09-02T18:14:01Z",
    "institutions": [],
    "license": "https://github.com/insilichem/gaudi/blob/master/LICENSE",
    "dateCreated": "2017-04-18T13:26:42Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     Journal of Computational Chemistry     10.1002/jcc.24847   GaudiMM: A Modular Multi-Objective Platform for Molecular Modeling     0  J.   R.-G. Pedregal, G. Sciortino, J. Guasp, Martı Municoy, J.-D. Marechal Departament de Quımica ,  Universitat Autonoma de Barcelona, Cerdanyola del Valle`s ,  Barcelona 08193 ,  Spain     2017   38  2118  2126    10  5  2017    2  2  2017    27  4  2017     GaudiMM (for Genetic Algorithms with Unrestricted Descriptors for Intuitive Molecular Modeling) is here presented as a modular platform for rapid 3D sketching of molecular systems. It combines a Multi-Objective Genetic Algorithm with diverse molecular descriptors to overcome the difficulty of generating candidate models for systems with scarce structural data. Its grounds consist in transforming any molecular descriptor (i.e. those generally used for analysis of data) as a guiding objective for PES explorations. The platform is written in Python with flexibility in mind: the user can choose which descriptors       Introduction\r\n  Molecular modeling has become a major tool in many fields of chemistry and its interfaces. Its final objective is to accurately describe the structural and energetic properties of molecular systems; ultimately from scratch. Most of nowadays computational chemistry exercises need to start with clear structural data on the system of interest (i.e., X-ray or NMR structures). Unfortunately, this first piece of information is frequently missing or highly incomplete and in fact many projects strive to start because of the complexity to find convenient starting points in a reasonably short amount of time. When modelers try to overcome this \u201cblank sheet\u201d syndrome, they generally go through an iterative process of trial and error and manual adjustments, with the only guidance of his or her chemical intuition and/or experimental observations.  Most of the procedures used to rapidly identify physically sound initial models of a molecular system stand on finding the way to ally the exploration of wide conformational spaces and the adequate guiding variables. Among the most frequent tools for this task are Monte Carlo,[ 1,2 ] Random Walk,[ 3 ] Simulated Annealing,[ 4-6 ] and evolutionary algorithms (EA).[ 7,8 ] In those approaches, the way to bias the exploration generally stands on (1) using energetic evaluation of the molecular geometry (i.e., force field) and (2) impose additional simple Euclidian restraints like distances, angles, or dihedrals that could account on structural aspects hypothesis. The construction and assessment of an initial molecular candidate is therefore generally limited to potential energy surface (PES) considerations, eventually accounting for complex force field parametrization, and few guiding elements. However, there is much more structural information that the researchers could account on that are generally neglected at the moment. to use for each problem and is even encouraged to code custom ones. Illustrative cases of its potential applications are included to demonstrate the flexibility of this approach, including metal coordination of multidentate ligands, peptide folding, and protein-ligand docking. GaudiMM is available free of charge from https://github.com/insilichem/gaudi. VC 2017 Wiley Periodicals, Inc.  Genetic Algorithms (GA)[ 9 ] are a kind of EA that have been increasingly applied for complex molecular problems such as molecular matching,[ 10,11 ] protein-ligand docking,[ 12,13 ] or conformational searches.[ 14,15 ] The most popular implementation in these applications are GAs that use a single objective strategy with a unique fitness function targeted. However, for years now, multi-objective genetic algorithms (MOGA) like NSGA[ 16-18 ] and SPEA[ 19-21 ] families are readily available and could bring novelties in the way we deal with molecular modeling. MOGAs are particularly helpful when different variables of the system fight ones against others to reach a tradeoff, especially if their importance is not known beforehand-a prototypical situation in initial model building. MOGAs tend to be applied when substantially different solutions could exist for the same problem. In principle, this kind of approaches could be useful on complex molecular modeling exercises when only partial information is available at the starting point of the study. The potential of multi-objective optimization in molecular modeling has been demonstrated by some recent developments,[ 22,23 ] but there is plenty of room for advances of MOGA applications in Molecular computational chemistry and more particularly in providing a modular platform able to deal with relevant molecular descriptors.  Here, we present GaudiMM (for Genetic Algorithms with Unrestricted Descriptors for Intuitive Molecular Modeling), a GA-based platform for 3D sketching of molecular systems that expands the idea of PES guiding using molecular objectives. WWW.CHEMISTRYVIEWS.COM GaudiMM aims at generating accurate enough geometric candidates on systems where pre-existing structural data is limited. The architecture of this modular platform is written in Python and mainly combines a MOGA with molecular descriptors to explore physically sound geometries by escaping usual force field and scoring function limitations. GaudiMM is therefore an interesting tool to perform hypothesis driven traversal of the conformational landscape of a molecular system with the support of the chemical intuition and experimental knowledge of the researcher. To illustrate its potential, we present several practical cases mainly focusing on biochemical systems and recognition processes.    Methods\r\n  GaudiMM is built on top of a modular NSGA-II multi-objective GA, which has been thoroughly tested and benchmarked in well-characterized multi-objective problems.[ 17 ] Multi-objective means that it can optimize all the needed variables (objectives) at once, without compromising any of them over the rest. A MOGA starts with the generation of a random set of potential solutions (individuals) which comprise the so-called initial population. This first set of individuals is then evaluated with several objectives and each one is assigned a fitness value to find the best ones, which are then allowed to recombine and mutate. The results of these variations can be better or worse than their preceding individuals (parents), but only the best of both the offspring and the parental generation (l 1 k strategy) will be selected by the algorithm and propagated to the next generation. After a number of iterations, the initial population will evolve and, eventually, answer with a number of reasonable solutions to the problem. A flowchart of the NSGA-II algorithm is shown in Figure 1.   Implementation details\r\n  On one hand, the core GA is based on the implementation found in the Deap Python package.[ 24 ] On top of Deap, GaudiMM provides an object-oriented programming interface that emphasizes the conceptual difference between problem exploration and evaluation: that is, between genes and objectives. Diversity is ensured through a fitness-tie and structural similarity analysis. If two candidate solutions share the same fitness value and their RMSD value is under a threshold, one of them will be discarded. As the user can specify both the RMSD threshold and the decimal precision of the reported fitness values, different levels of diversity and/or crowding can be achieved easily.  On the other hand, UCSF Chimera[ 25 ] provides a robust molecular framework that allows rapid development of such genes and objectives, and also doubles as powerful visualization tool. While most genes and objectives are currently wrappers or extended interfaces of parts of UCSF Chimera, additional third-party libraries can be used, like OpenMM for force field energies,[ 26 ] DrugScoreX[ 27 ] and IMP[ 28 ] for docking scoring functions, or ProDy for normal modes analysis.[ 29 ] Exploration: Individuals and genes. Each possible solution to the problem is represented by an Individual object. These objects contain a sorted list of genes and a series of helper methods. In our implementation, a gene describes a molecular feature, such as the topology of the molecule itself, a flexibility model (rotamers, torsions, normal modes), or the spatial orientation. As each gene is a separate module and class, they can be loaded only when required, as many times as needed. This also allows to offer custom mutation and recombination strategies to suit the nature of the feature, instead of using a single global strategy for all genes.  By creating different Individual objects, each with the same type of genes but with different values or alleles, we get to explore the biochemical and conformational search spaces. As a result, depending on the problem at hand, an Individual can be as simple as a rigid molecule that only moves around, or as complex as a two competing, dynamically built peptide with backbone torsions and rotameric side chains (Table 1). Evaluation: Environment and objectives. After creating a number of individuals, those must be evaluated to tell how good of a solution they are. All the individuals are subject to the same conditions: the environment, represented by an Environment class that wraps a sorted list of objectives each individual must face. Each objective is separate module and class, which defines a single method named evaluate that contains the fitness assessment code. As long as this function returns a quantity that can be maximized or minimized, it will be a valid objective. The same approach works equally for energy estimations, structure-focused optimizations and trivial restraints (distances, angles, dihedrals, surface areas, volume. . .). The code itself can be as complex as a H-bond detection engine, or a simple wrapper around a well-known executable that takes Protein Data Bank[ 30 ] files as inputs (Table 2).  As a result, any geometric or energetic parameters that could describe a molecular system can be used as objectives to drive the GA exploration. This allows us to turn the tables on routine protocols based on computing energetic optimizations and then analyzing the results in hopes of finding a suitable model that fits the intended restraints; that is, those same analysis tools can guide the optimization process from the beginning.    Usability\r\n  GaudiMM only requires a plain text YAML[ 31 ] input file, which specifies the genes that describe the molecular system, the objectives that will guide the simulated evolution, and the output parameters. (i.e., where to write the results files). A prototypical example of an input file can be found in the Supporting Information. Calculations are then started from the command line with gaudi run input_file.yaml, or from an inhouse developed GUI, GAUDInspect, that will also help you configure the input file. The results can be analyzed with that same GUI, or with the help of GaudiView, another GUI built as an UCSF Chimera extension.[ 32 ]  Being user-friendly does not mean developers are not welcome in this project. As GaudiMM is designed with extensibility in mind, the users are able and encouraged to develop their own genes and objectives if the built-in ones do not fulfill their requirements. Ultimately, such extensibility could be taken even further to provide different MOGAs for the users to choose, or allow them to code their own ones.  Further details on how to create input files, analyze results or code custom genes or objectives can be found in the attached documentation in the Supporting Information.     Results on Illustrative Cases\r\n   Peptide folding under a given volume\r\n  Peptides are receiving an increasing amount of interest at the interface between biology and chemistry. From models of Name Angle Contacts Coordination Distance DSX Energy HBonds Inertia LigScore Solvation Volume  Description Optimize angle of three atoms, or dihedral of four atoms Minimize steric clashes, maximize hydrophobic interactions Optimize coordination geometry of metal center Optimize distance between two or more atoms Docking scoring function Minimize molecular mechanics potential energy Detect hydrogen bonds Align axes of inertia of two or more molecules Docking scoring function Measure solvent accessible solvent area Measure volume occupied by molecule Depends on UCSF Chimera UCSF Chimera In-house UCSF Chimera DrugScoreX OpenMM UCSF Chimera In-house IMP UCSF Chimera UCSF Chimera WWW.CHEMISTRYVIEWS.COM protein behavior to their role as drugs or biosensors, the use of peptides is constantly growing.[ 33-39 ] However, it is wellknown that peptides are small polymers with a high degree of flexibility and the exploration of relevant conformations for a given purpose can be particularly complex. After all, only certain conformations are usually biologically active.  To illustrate how molecular objectives could be interesting in the context of a hypothesis-driven simulation, we decided to look at the capabilities of GaudiMM to generate structures of a peptide so that it could fit into a particular volume.  As a case study, calculations were performed on the sequence for Alzheimer b-amyloid structure 1-16 (bA1-16) starting from a purely linear geometry as provided by the peptide builder of UCSF Chimera. The input data consisted of three genes-(1) the linear peptide structure, (2) backbone torsion of the peptide, (3) rotameric exploration of sidechains of the peptide- and four objectives: (1) minimization of clashes to rapidly discard unfeasible conformations, (2) minimization of the potential energy as calculated within the AMBER99SBILDN force field,[ 40 ] and (3) a spatial optimization to match 15,854 A˚ 3. This value was selected because it represents the average volume occupied by the Zn-folded NMR structures of the same peptide[ 41 ] and so intrinsically shows the possible pre-organization of the isolated peptide for metal binding (PDB ID: 1ZE9). The complete input is reported in Supporting Information.  As all the objectives competed between them during the optimization in GaudiMM, after 60,300 evaluations, there was obviously not a unique solution for this problem. However, by filtering the proposed solutions of the multi-objective run with reasonable cutoffs (volumetric overlap of Van der Waals spheres under 100 A˚ 3), one could clearly see that the solutions with a good compromise between all the different objectives present a relatively good folding geometry when compared to the experiment.  From the different solutions, most were able to generate a geometry that satisfy the volumetric objective with a difference between the final objective and the best solution being within a 16% deviation from the target value. As a final analysis, we compared the geometry obtained with the zinc bound system of the beta amyloid peptide. As one would expect, the geometry was not a perfect match as the metal was not explicitly considered in the calculation. Even under those limitations, the mean RMSD fell within the 3.4-3.8 A˚ range (Fig. 2). In Table 3, we report the best solutions of the simulation. A complete sample can be found in Supporting Information table ESI.1.    Prediction of metal bound form of siderophore\r\n  Truth to be said, GaudiMM was born based on our experience in dealing with bioinorganic systems and more particularly in our axes of research toward artificial metalloenzymes and metallodrugs. On these systems, one of the most interesting aspects consists of rapidly generating structures for metal coordinating organic molecules or biomolecules circumventing the major complexities of parametrizing metal moieties for a given force field at the early stage of the modeling.  As GaudiMM offers a flexible platform based on modules, the user can mix non-trivial geometrical objectives with standard energetic terms. In this problem, the geometry of the non-metallic part could be dealt with a standard force field and the metallic part could be computed with carefully selected geometric terms. As a result, predicting the structure of a multichelating ligand to a metal becomes a feasible exercise: the only requirement is to select well those geometric terms.  The coordination geometry of said complex can be identified by comparing the positions of the potential ligand atoms around the metal center against the vertices of a number of polyhedra and checking if the directionality of ligand-metal interactions is correct.  To test how GaudiMM could answer to the question, we intended to reproduce the experimental structure of the enterobactin siderophore using only the isolated minimized structure of the apo enterobactin (PubChem CID: 34231)[ 42 ] with an iron atom as the starting structure. GaudiMM was therefore run using four genes-(1) a Molecule containing the initial structure of the siderophore as provided by the PubChem file, but with one less bond in the central ring to allow bond torsions, (2) said dihedral torsions for rotatable bonds in siderophore, (3) another Molecule with a bare iron ion, (4) free movement of the iron ion within 5 A˚ from the center of mass of the initial structure- and four objectives-(1) minimization of clashes to discard non-feasible torsions, (2) optimization of the first coordination sphere of the iron so it matches a tetrahedral geometry as observed in the siderophores found on E. coli holo FepB (PDB ID: 3TLK),[ 43 ] (3) a helper distance optimization to get the potential ligand atoms closer to the metal at around 2.75 A˚ , and (4) another distance minimization so the opened ring of the siderophore does not unfold and stays at bonding distance. The complete input is reported in Supporting Information.  After 751,500 evaluations, the resulting solutions proposed 135 models that matched the experimental siderophore structures with an average RMSD value of 0.95 A˚ (Fig. 3). The best 20 solutions found by GaudiMM are reported in Table 4 and the complete set of 135 structures is available in Supporting Information table ESI.2.  To the best of our knowledge, GaudiMM is the first 3D builder to account in a relative straightforward manner with large multidentate ligand with metals.    Multi-objective protein-ligand docking\r\n  To further assess the interest of exploring the conformational space of a molecular system by competitive objectives optimization, we decided to test our approach in recognition processes. One of the most widely spread in chemical biology, and more particularly drug design projects, is protein-ligand docking, which is aimed at predicting the geometric and energetic poses that a given ligand adopts within the binding cavity of a host. Although standard protein-ligand docking is not the target application of GaudiMM and many excellent programs provide excellent yields in this field,[ 12,44,45 ] this is still a natural way forward to test how its multi-objective capabilities would behave on this kind. Indeed, as one can decide which objectives and genes to use for each system, different combinations or recipes will provide different levels of accuracy that can be adapted to the problem at hand.  The correct recipe depends on the user's interest but results are already quite impressive by simply using a fast filtering objective (typically clashes), a placement objective (i.e., hydrogen bonds) and a field-tested scoring function (i.e., DrugScoreX or LigScore).  The two scoring functions implemented in GaudiMM have been previously validated and compared with other functions in several other works published the literature. DrugScoreX was successfully tested on the set of 195 protein-ligand complexes prepared by Cheng et al.[ 46 ] obtaining the best results with respect to docking power with a success ratio close to the 95%.[ 27 ] LigScore was validated on the Wang_AutoDock testing set[ 47 ] of 100 protein-ligand complexes obtaining a success ratio close to the 90%.[ 48 ]  Calculations performed under our competitive recipe provide very interesting results that show the versatility of GaudiMM at facing different problems. For example, fast docking jobs could be carried out using rigid bodies and simplistic terms such as minimization of steric clashes, maximization of WWW.CHEMISTRYVIEWS.COM hydrophobic interactions, and optimization of H-bonds networks, while more accurate dockings could be performed with normal modes analysis, rotameric side chains and force field energy minimization. Non-conventional docking studies are also possible in GaudiMM thanks to its plurigenetic features, such as systems where multiple ligands compete for the same binding site simultaneously, or problems that require to optimize the size of certain substituents of a ligand within the cavity of a protein. However, exotic challenges like those are outside the scope of this publication and will be further discussed in submissions to come.  A first attempt to benchmark a simple LigScore recipe against the original GOLD dataset[ 12 ] successfully reproduces 57.6% of the 100 crystallographic structures, which is comparable to well-established scoring functions already tested in several works published in the literature (GOLD reported a 69.7% success rate in its original publication).[ 12,27,46-49 ] The benchmark is described in detail in the Supporting Information and the results are summarized in Supporting Information table ESI.6. While this is not the target usage of GaudiMM, we firmly believe that with directed efforts toward optimizing the sampling stage the number of hits would surely increase and rival those docking methods with higher accuracy. The following \u2020 Table obtained using a scoring cutoff of DSX &lt; 0. [a] DSX score value. [b] Clashes measured in A˚ 3. [c] Sum of L.J.-like potential obtained for vdW hydrophobic contacts per the following formula: U5Pi;j4e rrij 122 rij r 6  with r 5 1 and e 5 0.25. [d] RMSD (in A˚ ) calculated on the heavy atoms via UCSF Chimera software. 880-061 complex[ 50 ] was correctly reproduced with minimization of clashes, maximization of hydrophobic interactions and  [b] 3 3 6 3 1 3 3 5 1 4 2 2 2 2 1 2 2 1 0 1 RMSDmean[e] 11.716 5.209 3.321 25.337 9.167 10.737 4.936 9.356 46.005 7.107 6.647 6.043 31.899 42.476 40.404 39.935 29.729 35.954 25.041 39.028 230.057 226.851 230.641 233.067 230.919 231.106 231.378 232.467 236.124 230.781 231.594 232.433 233.437 240.087 236.300 241.143 241.298 243.426 236.522 247.753 1.772 0.802 0.869 1.321 1.275 1.005 1.152 1.178 1.679 1.159 1.118 1.110 1.803 1.964 1.637 1.731 1.601 1.605 1.474 1.800 1.552 optimized DrugScoreX analysis. The complete input is reported in Supporting Information. The top seven solutions according to DrugScoreX reported less than 15 A˚ 3 of volumetric overlap (clashes). The experimental structure is reproduced with high accuracy reporting the best RMSD of 0.682 A˚ . The best solutions are shown in Figure 4 and reported in Table 5. A larger sample of solution is reported in Supporting Information table ESI.3. 2PHH. Crystalographic structure of adenosine 5-diphosphoribose in Pseudomonas fluorescens p-hydroxybenzoate hydroxylase[ 51 ] was correctly reproduced in the first cluster. The experimental structure is predicted with the best RMSD of 0.802 A˚. In this case, clashes \u2020 Table obtained using a scoring cutoff of LigScore &lt; 0. [a] LigScore score value. [b] Number of Hbonds. [c] Clashes measured in A˚ 3. [d] Sum of L.J.-like potential obtained for vdW hydrophobic contacts per the following formula: U5Pi;j4e rrij 122 rrij 6 with r 5 1 and e 5 0.25. [e] RMSD mean (in A˚ ) of the complete cluster calculated on the heavy atoms via UCSF Chimera software. minimization and hydrophobic interactions maximization were kept, but hydrogen bond network optimization was added and the scoring function was replaced with LigScore. The best poses are shown in Figure 5 and the first 20 solutions of the first cluster are reported in Table 6. A more complete sample can be found in Supporting Information table ESI.4. 1LAH. The same recipe used for 2PHH worked excellently for L-ornithine in Salmonella enterica periplasmic ornithine-binding protein.[ 52 ] In this case, the best RMSD between the experimental structure and the simulated one results in 0.648 A˚ . The top 20 solutions are reported in Table 7 and the best are shown in Figure 6. A more complete sample of solutions is reported in Supporting Information table ESI.5.  Our initial tests on the viability of GaudiMM to deal with protein-ligand docking problems have been particularly encouraging, illustrate its versatility and how multi-objective recipes provide this framework with a wide range of applications.     Conclusions\r\n  As molecular modelers strive to push the limits of the software they use, a shift from descriptive to predictive applications is occurring. The final goal of that quest is the generation of accurate 3D models from scratch. But to get there one must conquer a number of smaller milestones.  In principle, GaudiMM could be expected to be as accurate as major molecular modeling programs, as its accuracy mainly depends on which quality of the objectives and genes could be reached. While nothing prevents from developing, for example, a DFT objective to evaluate single-point energies of the candidates along the simulation, its main interest pivots toward getting molecular guesses in a reasonable amount of time to present physically and chemically sound models as the starting points of a multiscale protocol involving additional software: with a first set of GaudiMM solutions, one could set a long MD simulation, which in turn would return suitable models for QM/MM optimizations. However, this should not discourage from programming this hypothetical QM objective, as it could be indeed very useful in other computational chemistry fields, such as catalysis and surface science.  In a way, GaudiMM is more about answering the question: \u201cif a molecule could respond to these restrictions, what geometry could be acceptable in that case.\u201d As long as the adequate genes and objectives are provided, the answer is guaranteed. We firmly believe that it can become a particularly important asset in nowadays molecular modeling community.    Further Work\r\n  The current state of GaudiMM can be considered the reference implementation of our proof-of-concept and we are now hardly working in getting the code to highest level of performance.  For example, the evaluation stage, typically the most timeconsuming step, is performed individually for each candidate solution by design, which allows for easy parallelization with a multi-process strategy. This can be coupled with the idea that GaudiMM jobs can be thought as a series of evaluations of different set of coordinates generated by the exploration stage for one or more given molecular topologies, which means that some objectives could be re-engineered to make use of highly optimized trajectory analysis tools, such as MDTraj[ 53 ] or PyTraj.[ 54 ] Of course, if after these changes some parts are still slow, the standard procedures of optimization could still be applied: wrapping those methods with Numba[ 55 ] or rewriting them as C extensions.  Acknowledgment Support of COST Action CM1306 is kindly acknowledged. Keywords: molecular modeling protein-ligand docking objective optimization genetic algorithms metallopeptides multiHow to cite this article: J. Rodrıguez-Guerra Pedregal, G. Sciortino, J. Guasp, M. Municoy, J.-D. Marechal. J. Comput. Chem. 2017, 38, 2118-2126. DOI: 10.1002/jcc.24847 ] Additional Supporting Information may be found in the online version of this article.    ",
    "sourceCodeLink": "https://github.com/insilichem/gaudi",
    "publicationDate": "0",
    "authors": [],
    "status": "Success",
    "toolName": "gaudi",
    "homepage": "http://gaudi.readthedocs.io"
  },
  "66.pdf": {
    "forks": 1,
    "URLs": [
      "github.com/namshik/tigeri/",
      "github.com/namshik/tigeri/.Conclusions:",
      "bmcbioinformatics.biomedcentral.com/articles/supplements/volume-18-supplement-7"
    ],
    "contactInfo": [
      "namshik.han@gurdon.cam.ac.uk",
      "andy.brass@manchester.ac.uk"
    ],
    "subscribers": 0,
    "programmingLanguage": "",
    "shortDescription": "TIGERi: T FA I llustrator for G lobal E xplanation of R egulatory i nteractions",
    "publicationTitle": "TIGERi: modeling and visualizing the responses to perturbation of a transcription factor network",
    "title": "TIGERi: modeling and visualizing the responses to perturbation of a transcription factor network",
    "publicationDOI": "10.1186/s12859-017-1636-6",
    "codeSize": 14251,
    "publicationAbstract": "Background: Transcription factor (TF) networks play a key role in controlling the transfer of genetic information from gene to mRNA. Much progress has been made on understanding and reverse-engineering TF network topologies using a range of experimental and theoretical methodologies. Less work has focused on using these models to examine how TF networks respond to changes in the cellular environment. Methods: In this paper, we have developed a simple, pragmatic methodology, TIGERi (Transcription-factor-activity Illustrator for Global Explanation of Regulatory interaction), to model the response of an inferred TF network to changes in cellular environment. The methodology was tested using publicly available data comparing gene expression profiles of a mouse p38α (Mapk14) knock-out line to the original wild-type. Results: Using the model, we have examined changes in the TF network resulting from the presence or absence of p38α. A part of this network was confirmed by experimental work in the original paper. Additional relationships were identified by our analysis, for example between p38α and HNF3, and between p38α and SOX9, and these are strongly supported by published evidence. FXR and MYC were also discovered in our analysis as two novel links of p38α. To provide a computational methodology to the biomedical communities that has more user-friendly interface, we also developed a standalone GUI (graphical user interface) software for TIGERi and it is freely available at https://github.com/namshik/tigeri/.Conclusions: We therefore believe that our computational approach can identify new members of networks and new interactions between members that are supported by published data but have not been integrated into the existing network models. Moreover, ones who want to analyze their own data with TIGERi could use the software without any command line experience. This work could therefore accelerate researches in transcriptional gene regulation in higher eukaryotes.",
    "dateUpdated": "2016-07-23T16:37:00Z",
    "institutions": [
      "University of Cambridge",
      "University of Liverpool",
      "University of Manchester"
    ],
    "license": "No License",
    "dateCreated": "2015-03-14T09:36:19Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     BMC Bioinformatics     10.1186/s12859-017-1636-6   TIGERi: modeling and visualizing the responses to perturbation of a transcription factor network     Namshik Han  namshik.han@gurdon.cam.ac.uk  0  2    Harry A. Noyes  1    Andy Brass  andy.brass@manchester.ac.uk  2    0  Gurdon Institute, University of Cambridge ,  Cambridge ,  UK    1  School of Biological Sciences, University of Liverpool ,  Liverpool ,  UK    2  School of Computer Science and School of Health Sciences, University of Manchester ,  Manchester ,  UK     2017   18   Background: Transcription factor (TF) networks play a key role in controlling the transfer of genetic information from gene to mRNA. Much progress has been made on understanding and reverse-engineering TF network topologies using a range of experimental and theoretical methodologies. Less work has focused on using these models to examine how TF networks respond to changes in the cellular environment. Methods: In this paper, we have developed a simple, pragmatic methodology, TIGERi (Transcription-factor-activity Illustrator for Global Explanation of Regulatory interaction), to model the response of an inferred TF network to changes in cellular environment. The methodology was tested using publicly available data comparing gene expression profiles of a mouse p38α (Mapk14) knock-out line to the original wild-type. Results: Using the model, we have examined changes in the TF network resulting from the presence or absence of p38α. A part of this network was confirmed by experimental work in the original paper. Additional relationships were identified by our analysis, for example between p38α and HNF3, and between p38α and SOX9, and these are strongly supported by published evidence. FXR and MYC were also discovered in our analysis as two novel links of p38α. To provide a computational methodology to the biomedical communities that has more user-friendly interface, we also developed a standalone GUI (graphical user interface) software for TIGERi and it is freely available at https://github.com/namshik/tigeri/.Conclusions: We therefore believe that our computational approach can identify new members of networks and new interactions between members that are supported by published data but have not been integrated into the existing network models. Moreover, ones who want to analyze their own data with TIGERi could use the software without any command line experience. This work could therefore accelerate researches in transcriptional gene regulation in higher eukaryotes.    Machine Learning  Transcriptional regulatory network  Transcription factor binding site  Gene expression       -\r\n  From DTMBIO 2016: The Tenth International Workshop on Data and Text Mining in Biomedical Informatics Indianapolis, IN, USA. 24-28 October 2016    Background\r\n  Integrated functional genomics attempts to utilize the vast wealth of data produced by modern large scale genomic and post-genomic projects to understand the functions of cells and organisms [ 1 ]. The rapidly increasing amount of high throughput sequencing data makes it essential to develop new analytical tools that can systematically process and integrate those datasets. This presents both challenges and opportunities to the computer science community.  Transcription factor (TF) proteins bind to promoter elements on genomic DNA at TF binding sites (TFBS), to help control the transfer of genetic information from gene to mRNA [ 2 ]. Understanding the mechanisms underlying mRNA transcription is one of the \u201cgrand challenges\u201d in modern biology. Experimental techniques allow direct measurement of individual gene transcription, but the contribution of multiple TFs is hard to determine [ 3-5 ]. Measuring the concentration of TF proteins and their affinity for the promoter region of genes is difficult because concentrations are low and protein-DNA interactions are subject to multiple controls, resulting in measurement artifacts [ 6-8 ]. Post transcriptional regulation compounds these difficulties because other molecules modify mRNA stability and hence the signals from the TFs [ 3, 9-11 ]. In such a complex environment, in-silico techniques can provide insights and hypotheses into the underlying TF regulatory activity, although they clearly have limitations.   Reverse-engineering of TF network and TFBS information\r\n  A number of techniques are available to uncover the topology of the TF network-the networks of complex reactions and interactions in the cell that control transcript levels [ 12 ]. One strategy is to use principals of reverseengineering and use gene expression data to infer regulatory interactions [ 13, 14 ]. Various reverse-engineering methods can reduce the dimensionality of the classic combinatorial search problem and utilize genome sequence data to enhance the sensitivity and specificity of predictions. However, they have difficulties in describing regulatory control by mechanisms other than TFs. Reverse-engineering of TF networks in the lower eukaryotes has been well developed [ 15-17 ]. However, the problems in mapping the regulatory mechanisms in cells of higher eukaryotes have made such global studies either impossible or impractical. Some recent studies have begun to address this issue [ 18-20 ], but have tended to focus only understanding which TFs bind to which genes-not looking in detail at the nature of the TF/ TFBS interaction. A recent study [ 21 ] identified key biological features in transcriptional changes, however this method has difficulties in inferring the dynamics of the interactions. Furthermore, TF concentrations were not considered during the identification of the features.  To date, various reverse-engineering methods can reduce the dimensionality of the reverse-engineering problem and utilize genome sequence data to enhance the sensitivity and specificity of predicted interactions. However, they have difficulties in describing regulatory control by mechanisms other than TFs. To address this issue TFBSs information is required to complement the gene expression data. We used a list of 132,654 TFBSs between 20,920 genes and 174 TFs that had been identified by searching an alignment of five mammal species for conserved 5' and 3' regions [ 22 ]. Connectivity data is notorious for high false positive rates; however, our connectivity data is robust against the problem because it extracts binding information from well conserved upstream regions. A more detailed explanation is addressed in the Methods and Results section and a schematic diagram of the connectivity data is presented in Fig. 1.    Identifying regulation type by combining TFA and TFC analysis\r\n  Transcription factor activities (TFAs) are the intensity of the interactions between a certain transcription factor (TF) and its targets at a certain experimental point [ 23 ]. Thus, the estimated strength of TFAs between each TF and its target gene are useful to know which TF is acting on which gene at a given time point or experiment condition. However, simply knowing the regulatory activities under a single experimental condition provides limited information about the transcriptional network. To understand the mechanism of regulatory interactions, we developed a method that identifies statistically significant differences in TFAs under two different conditions. The significant differences indicate the changing level of TFAs between two conditions, so varying trends of TFAs in whole experimental process are easily detected and can be used to identify TF-specific regulatory patterns (up and down-regulation).  A highly concentrated TF induces more gene expressions rather than a lower concentrated TF. High-affinity binding sites induce the gene expressions at any level of TF concentration (TFCs), but low-affinity binding sites require high level of TF concentration for induction [ 24 ]. Thus, we might assume that TF concentration level is an important factor for investigating TFAs, and TFA investigation with considering TFC provides more reliable and accurate results closer to the complex reality of biology. To address this problem we proposed a probabilistic variational inference method to infer the concentration of each TF protein (TFC) and the regulatory intensities (TFA) of each TF and gene pair [ 4 ].  Aside from the method, there have been some notable attempts to infer TFAs based on integrating gene expression data and TFBSs information. The approaches use various well-known statistical inference techniques such as network component analysis [ 25 ], support vector machine [ 26 ], multivariate regression plus backward variable selection [ 27 ] and partial least squares [ 28 ]. However, the TFAs, which are inferred by these methods, do not contain any information on the strength and the sign of the physical interaction between a TF and its target genes. Moreover, the regulatory interactions can change easily in response to changing experimental conditions and over time. Since the methods are not fully probabilistic, they are not ideal for investigating the stochastic interactions. A linear regression based probabilistic method to model the full probability distribution of each TFA on each gene was developed [ 23 ]. The limitation of this method, however, is that it does not infer the TFAs and TFCs separately. This is a serious problem in subsequent analysis and prediction.     Methods\r\n   Transcription regulatory circuits and mathematical model\r\n  Transcription regulatory circuits can be thought of as having trans- and cis-inputs that are transformed into genetic information at mRNA level [ 29 ]. These circuits are a key component in the regulation of mRNA levels in the cell, and have a number of components (shown in Fig. 1): TFs, whose concentration can change, bind to TFBSs upstream of genes with a strength that is a function of the particular TF-gene interaction, to control the concentration of mRNA produced. A number of mathematical models have been developed which attempt to describe these interactions [ 15-17, 19, 21, 30 ]. For example Sanguinetti et al. [ 4 ] model the log gene expression in the form: i) e is a set of logged gene expression measurements. ii) T is a binary matrix capturing the connection topology-the specific set of TFBS upstream of genes and the TFs that bind to them. If TF f binds upstream of gene g then T gf ¼ 1. iii) W is a weight matrix that captures the nature of the interaction strengths between TF-gene pairs in regulating expression of a specific gene.  e ¼ T W c þ v Where: ð1Þ iv) c is the vector of concentrations of each of the TFs. v) v is a vector of independent and identically distributed variables modeling the noise in the system. The model assumes that a spherical Gaussian term could explain all noise on gene expression profiling data.  Typically, we have knowledge of e (from gene expression profiling experiments, such as microarray or RNAseq) and would like to infer the set of TFC c , and TFA W giving rise to this signal. Given T and e Sanguinetti et al. [ 4 ] then show how it is possible to solve for c and W using a discrete time state space model (Eq. 1) with expectation-maximization (EM) algorithm. In the model, elements of the c matrix indicate the concentration level of a given TF protein (TFC) at a specific time. Elements of the W matrix represent the regulatory intensity (TFA) between a given TF protein and its binding affinity to its target genes. The baseline expression level is the mean vector. The measurement noise v follows zero-mean i.i.d. Gaussian noise. To estimate the c and W matrices, the model used posterior estimation of Bayer's theorem. During this estimation, EM algorithm allowed the model to efficiently approximate the log likelihood. However, it is rare to have a complete knowledge of T -we simply do not know the binding sites for all TFs in a typical higher eukaryotic cell. Recent experimental techniques, such as ChIP-chip and ChIP-seq can provide useful data to help construct the connection topology T [ 31 ], however they have clear limitations if we are looking for a complete topology [ 32, 33 ]. A number of theoretical techniques are also available to uncover the connection topology [ 34-36 ]. The techniques generally use principals of reverse-engineering and use gene expression and genome sequence data to infer regulatory interactions.    Gene expression data\r\n  Gene expression datasets were downloaded from Gene Expression Omnibus (accession number GSE7342 for p38α and GSE36890 for STAT5) [ 37, 38 ]. The expression profiling data of GSE7342 dataset was normalized by the robust microarray average (RMA) method. The read counts of GSE36890 dataset was normalized to the reads per kilobase of exon per mega-base of library size (RPKM).    Generating T , the connection topology\r\n  In this paper we have taken a conservative strategy for generating T which looks at upstream region of genes that are well-conserved in multiple mammalian genomes. We used a published catalogue of common regulatory motifs that were overrepresented in gene upstream regions [ 22 ]. These motifs were identified by constructing genome-wide alignments for four mammalian species in promoter regions and 3' UTRs relating to well-annotated genes from the RefSeq database. The same TFs were assumed to bind the same TFBSs in mice since the TFBS had been discovered in an alignment of human, mouse, rat and dog promoter regions. TFBS upstream of human 13,330 RefSeq genes were predicted. Mouse genes corresponding to the published list of human genes [ 22 ] were identified using Ensembl mouse gene annotation.    Estimation of statistically significant changes\r\n  We were specifically interested in any TF activity that exhibits statistically significant changes between the two conditions. In particular, we are interested in changes that may be due to a change in activity of the TF, and not just in its concentration. We therefore scaled the TFA by the predicted TFC as a measure for changes in activity [ 24 ]. A joint analysis of TFA and TFC should provide more robust predictions of those TFs whose activity has changed for reasons beyond those of a simple change in concentration. To compare two different conditions, the normalized TFA by TFC in wild-type condition were subtracted by the normalized TFA in knock-out condition. We therefore determined those interactions for which:  W gf cf  W T −  W gf cf  KO &gt; Cutoff ð2Þ  The value of the Cutoff was chosen such that all differences at the 95% confidence interval were considered significant (±2 standard deviations). ±2SD limit is widely chosen as a normal limit because it fits well into two important categories: (1) confident interval and (2) testing hypothesis.    Gene Ontology (GO) analysis\r\n  GO analysis was performed by using DAVID [ 39 ]. The sets of genes showing significant changes identified in Eq. 2 were submitted to DAVID using the default parameters in order to obtain the GO term classifications of each gene. Our computational pipeline utilized the results to investigate the functionality of genes and their regulatory TFs. The detailed methods and the result figures of GO analysis are supplied in Additional file 1: Supplementary Text and Figure S1-S3.     Results\r\n   Estimating the responses to perturbation of transcription networks\r\n  We have developed a strategy which used forwardengineering to construct the connection topology (see Fig. 1, Methods, and Additional file 1: Supplementary Text and Figure S1-S3), based on a previous study of regions upstream of genes conserved in multiple mammalian genomes [ 22 ]. The structure of this network of transcriptional regulatory interactions between TFs and the genes whose transcription they control is described by a binary matrix T ∈ ℜn m, where n is the number of TFs and m is the number of genes; An element (i, j) of the matrix is '1' if TF i binds to the upstream control region of gene j, '0' otherwise. We have then employed a mathematical model to integrate the connection topology data and a gene expression dataset from a higher eukaryote in which we are interested in modeling the changes that occur in the TF network in response to a change in the cellular environment (Fig. 2). Our approach could be seen as complementary to 'Integrative methods', as defined in [ 40 ], as it provides a strategy for creating an approximate connection topology if more detailed information is not available. The connection topology that is being used for this analysis contains many approximations and is certainly incomplete. However, it should be noted that we are looking at the differences between the models, for example between a wild-type and knock-out state, and those differences will be in parts of the model for which we do have data.  The results of our approach provide a set of TFs and their target genes which are related by significant up- or down-regulation in transcription. It provides a clear indication of the changes in TFA and TFC of TFs that are controlling transcriptional regulatory mechanisms in response to a specific stimulus. We therefore showed an \u201cintegrated\u201d approach for network inference, based on a forward-engineered connection topology, can produce plausible and testable hypotheses about the responses to perturbation of transcription networks in higher eukaryotes.    Illustrating interpretable images of complex data\r\n  The visualization tools then make patterns apparent that would be difficult to detect in numerical data (Fig. 2). To distinguish regulation patterns between different experimental conditions, recognizing at a glance is important. However, computing results are formed in large numerical matrix, thus it is not only difficult to navigate through the whole matrix but also impossible to present the results in one page.  Figure 3 shows a graphical representation of the significant changes in TFA matrices W (n by m) and TFC vectors c (n) obtained from this analysis. The patterns of the responses to perturbation of TF networks are readily observed in this single-shot image that presents approximately 2000 significant changes of varying TF activities on the 132,654 TFBSs after deleting p38α. In upper part of the plots, the TFs place in the functional group order. The genes, which have at least one significant interaction with TFs, locate in bottom part of the plots. A line in the plots presents a regulatory interaction (normalized TFA by TFC) between a TF and its target gene, and line color indicates a significant difference between the strengths of the regulatory interaction of two conditions. For example, we can easily find in visualized format (Fig. 3) that TF group three has distinct patterns (down-regulation at E13.5, up-regulation at E15.5) between two time points.    Modelling the changes of transcription factor network in\r\n  p38α deficient mice The computational pipeline as highlighted in Fig. 2 was applied to a published study of the effect p38α knockout in mouse embryos [ 37 ]. This study developed four gene expression profiling datasets (Gene Expression Omnibus, accession number GSE7342) comprising of two time points at days 13.5 and 15.5 of embryonic development (E13.5 and E15.5) for p38α knock-outs and their wild-type controls. This data set was chosen for this study as it includes experimental measurements of gene expression in the wild-type and knock-out mice and showed that p38α deficient mice have significantly different phenotype. Thus, the experimental datasets were used as positive controls for our theoretical study.  The TF-gene interaction strengths (TFAs) W and TF concentration levels (TFCs) c in each of these four data sets were then inferred to produce four weight matrices of TFAs: hW i  WT@E13:5 ; hW i  WT@E15:5 ; hW i  ; hW i KO@E13:5  KO@E15:5 and four concentration vectors of TFCs: From the TFA weight W and connection topology T matrices, the average strength of TFA, Sf, for each TF in the datasets was calculated:  By comparing the average strengths between wild-type and knock-out mice, it is possible to see which of the TFs have significantly changed as a consequence of the removal of p38α.  Figure 4a, b show the changes in TFA strengths between wild-type and knock-out mice at E13.5 and E15.5. It can be seen that a number of TFs show a significant signal (&gt;2 s.d.) in this data. These are shown with more detail in Table 1. Figure 4c, d show the inferred TFCs c obtained for the E13.5 and E15.5 time points. Again, from this graph it is possible to see that a number of TFs appear to be responding to the p38α status. These are shown in more detail in Table 2.    Transcriptional regulatory network for p38α\r\n  Gene Ontology (GO) analysis on the target genes of the TFs with strongly changed activity showed enrichment for three GO terms and provided insight into the functional role of the TFs (see Methods, Tables 1 and 2, and Additional file 1: Supplementary Text and Figure S1-S3). The three GO terms are the regulations of the apoptosis (programmed cell death), the downward spiral of the developmental process, and the immune system development. The JNK-c-Jun pathway stimulates the apoptosis, and the I-kB kinase/NF-kB cascade acts as a suppressor of the JNK-c-Jun pathway [ 41 ]. Inhibition of p38α MAPK retards another JNK-c-Jun pathway inhibitor NF-kB cascade, but promotes JNK-c-Jun pathway which induces the apoptosis by expressing the Bcl2 protein family [ 20, 42 ]. On the other hand, developmental process related genes are down-regulated in the p38α knock-out mice. The study of p38α MAPK [ 37 ] reported that the p38α knock-out mice die within days after birth. We do not have enough gene expression profiling data (either other time points in the embryonic period or postnatal period) to investigate TFAs in whole developmental process of the p38α knock-out mice; we cannot confirm but suppose that it might be the reason of the death of the knock-out mice. Further, the genes interact with the TFs which are reported as crucial TFs in the developmental The TFs predicted to have significantly different behaviors between the wild-type and knock-out mice. \u201cTrans Name\u201d-the official gene symbol of the TF. \u201cCis Name\u201d-the name given to the binding site of the TF. TFs were characterized into different functional groupings (see Fig. 3c for details: Dev-cell-type specific developmental TFs, Res-signal dependent resident nuclear factors, Lat-signal dependent latent cytoplasmic factors, Ste-signal dependent steroid receptor group, Unk-unknown). \u201cRegulation Type\u201d, the way in which the TF regulates its target genes in the absence of p38α. \u201cGO Analysis\u201d, provides more functional classification for the identified TFs (see Methods and Additional file 1: Figure S1 to S3 for details). The abbreviations of the GO terms are: Apo, Apoptosis; Dev, Developmental Process; Imm, Immune System Development. \u201cKnown Biological Functions\u201d summarizes the findings from the recent biological literature as shown in the \u201cReferences\u201d The boldface ones are the main node in Fig. 5 process and the immune system development. Our results are therefore in broad accordance with the experimentally validated results, so it confirmed that our pipeline produces reliable results.  Combining the data in Tables 1 and 2 with those obtained from the literature it is possible to build a putative model for the effects of p38α knock-out (Fig. 5).  This figure shows TFs with a strong response in our analysis as nodes, with links that demonstrate regulatory interactions between them. The TF network therefore comprehensively shows the biological consequences of p38α knock-out at transcriptional level.     Discussion\r\n  We have developed a novel strategy for discovering changes in transcriptional regulatory networks of higher eukaryotes.  It integrates methods for inferring TF-gene interaction strengths (TFAs) and TF concentration levels (TFCs); identifying statistically significant changes in TFAs and TFCs; analyzing the changes; classifying TFs into functional groups; and visualizing the changes. To our knowledge, this is the first ensemble approach for characterizing the transcriptional function of TF proteins and their target genes in higher eukaryotes. Reverse-engineering of TF networks has been well developed in the lower eukaryotes [ 15, 17 ]. However, the problems in mapping the regulatory mechanisms in cells of higher eukaryotes have made such global studies either impossible or impractical. Some recent studies have begun to address this issue [ 16, 19, 30 ], but have tended to focus only on understanding which TFs bind to which genes-not looking in detail at the nature of the TF-gene interaction. Other studies [ 5, 21 ] identified key biological features in transcriptional changes, however the methods have difficulties in inferring the dynamics of the interactions. A recent review [ 40 ] has categorized techniques for network inference and listed their limitations.  We validated our computational pipeline using the p38α gene expression profiling data and our connectivity data.  The study of p38α MAPK [ 37 ] used various experimental methods including a gene expression profiling analysis to Table 2 TFs showing significant changes in concentration between wild-type and knock-out mice Trans name TF group Cis name Level GO analy-sis Known biological functions Ref. AREB6 1.Dev ZEB1 Down - p38 → IFNγ → ZEB1 → Immune [ 58 ] PITX2 1.Dev PITX2 Down Apo p38 → PITX2 → Development [ 63, 64 ] → Apoptosis STAT1 6. Lat STAT1 Down - STAT1 → Development [ 57, 69 ] → Immune SOX9 7. Unk SOX9 Down Apo, Dev p38 → SOX9 → Apoptosis [ 59-62 ] TFs changing their concentration levels significantly between wild-type and knock-out mice. \u201cLevel\u201d, the changes of TF concentration level in the absence of p38α. Other column headings and abbreviations are the same as those in Table 1 The boldface ones are the main node in Fig. 5 show that p38α negatively regulates cell proliferation by antagonizing the JNK-c-Jun pathway. We utilized the published gene expression profiling dataset from their study, to demonstrate that our computational pipeline is able to infer from the gene expression profiling data the same insilico conclusions that the authors obtained from their invitro experiments. Therefore, our analysis focused on the JNK-c-Jun pathway to validate the accuracy, robustness and reliability of our strategy. The results are consistent with the experimentally validated inhibitory effect of p38α on transcriptional networks [ 37 ]. Their published data confirmed that the most important TF involved in the response to the knock-out was c-Jun, with a clear change observed in both its activation and concentration. In our theoretical work, we also showed a significant change in TFA of c-Jun, but we did not see any corresponding change in the predicted TFC, which is disappointing.  The p38α MAPK is one of many signal transduction pathways and works in both cell-type specific and cellcontext specific manner. It plays a pivotal role in converting extra-cellular signal into a wide range of cellular response [ 43 ]. We classified a set of TFs that responded to the deletion of p38α into functional groups (Tables 1 and 2, Fig. 3c), that are either developmental factors (group 2) or extracellular signal dependent factors (group 3). Developmental factors are also dependent on extra-cellular signals because cells may require such signals to generate developmental factors [ 44 ]. In Fig. 3a, it can be seen that the main factors that responded to the knock-out are the extra-cellular signal dependent factors. None of the TFs that significantly respond in the knock-out are constitutive factors. Our results are consistent with recent publications on the JNK-c-Jun pathway (see citations in Tables 1 and 2).  Our analyses generated a comprehensive transcriptional regulatory network for p38α. The network and a detailed description are shown in Fig. 5. The nodes in the graph were generated from our analysis of responding TFs. The edges in this network were derived from the literature or GO analysis (citations in Tables 1 and 2). The edges or links in the network of p38α regulated TFs have mostly been previously reported, but none of the reports had integrated all these p38α related TFs into a single comprehensive network diagram. Together these results predict a set of TFs that are in some way regulated by p38α, a set somewhat larger than that identified in the original paper.  For example, we predict that Foxm1 (HNF3) responds to the p38α status. Recent papers, published since the original study, provide some support for this hypothesis [ 45, 46 ]. Most parts of the network are reported in numerous biological studies. However, our network reveals novel links such as p38α─FXR and p38α─MYC. The inferred links are supported by direct experimental evidence so validating the approach, but that in addition novel links have been proposed that are now testable.  The data shown in Tables 1 and 2 and visualized in Fig. 5, provide evidence that the methodology described in this paper is capable of generating plausible hypotheses about linkage between p38α and a range of different TFs.  The hypotheses presented in this table have been generated solely from our input data (the connection topology data and gene expression profiling data), but are wellsupported by the literature. The methodology has therefore demonstrated that it can produce plausible and testable hypotheses, even if the specific details of those interactions may not be completely accurate. This is not surprising given the fact that we only have an incomplete model of the transcription process. Any in-silico techniques which uses predicted TF/TFBSs interactions can provide only a limited view of the complete complexity of transcription control due to the nature of the binding between the TF and the TFBS and the complex effect of gene expression on the TFBS-for example dependent on the epigenetic factors, such as the pattern of histones or DNA methylation at the binding site-as well as the state and concentration of the TF itself. Analysis is complicated by the fact that there are other processes in the cell that act to control mRNA concentration. Such as the rate of RNAi regulated mRNA degradation [ 9, 10 ] or susceptibility to attack by RNAses [ 3, 11 ]. TFBS can hidden by histones [ 7, 8 ], or made more accessible by genomic uncoiling [ 6 ]. Furthermore, most TF binding may be cell or species specific not all sites are functional even if occupied, and many functional sites have low levels of conservation [ 47 ]. This rather undermines the commonly accepted assumption that TFBSs can be discovered by conservation [ 22 ]. However although the exact binding sites may not be conserved the set of TFs that bind a gene somewhere probably is.  p38α deficient mice showed significantly different phenotype which indicates its role is critical. The p38α study also provided the gene expression profiling dataset of wild-type mice as well as p38α deficient mice, so that we could apply our pipeline on the dataset to investigate TFAs and TFCs. It allowed us to directly compare our in-silico results to the experimental in-vitro results, and it validated our findings. However, the experiment was done on two time-points that could limit our validation.  Thus, we tested our pipeline on a larger dataset from a recent STAT5 transcription factor study [ 38 ] which is consisted of 18 samples in five time-points. This study showed the critical role of STAT5-tetramer in immune system. To do this, the authors made STAT5-tetramer deficient mice by generating STAT5A-STAT5B doubleknockin mice. Interleukin 2 (IL2) and IL15 are two of well-known upstream regulators of STAT5A-STATB, so they measured IL2- and IL15-induced gene expression profiling in both wild-type mice and STAT5-tetramer deficient mice. We downloaded the RNA-seq gene expression dataset from this study and analyzed with our pipeline. TF activities were decreased in STAT5tetramer deficient mice (both IL2- and IL15-induced), particularly at 4, 24, 48 h (Fig. 6). This general trend is well-corresponded to the experimental findings as the author reported IL2- and IL15-induced gene expression were both down-regulated in STAT5-tetramer deficient  Fig. 6 TFA and TFC changes in STAT5-tetramer deficient mice. TFA and TFC of 65 TFs were estimated from IL2- and IL5-induced RNA-seq datasets and compared between wild-type and STAT5-tetramer deficient mice. Thus, TFA or TFC of a given TF is shown in red color if it is higher in STAT5-tetramer deficient mice than wild-type mice. If the level of TFA or TFC is higher, the color is darker. The numbers in right-side of heat-map indicates TF functional group (please see legend in Fig. 3c) mice. However, most TFs were re-activated at the last time-points which is also exactly same observation in the experimental result. In particular, STAT5A is in our TF list so we closely investigated its activation patterns.  STAT5A showed weak activities and concentration level at control sample, but it dramatically de-activated at 4, 24, 48 h and then re-activated at 72 h. More interestingly, all TFs in cytoplasmic factor group (marked as number 6 in Fig. 6) including STAT5A have same activation patterns with STAT5. Moreover, there are a few interesting TFs which are not classified as cytoplasmic factor but also followed same up- and down-regulated patterns with STAT5 (e.g. SP1, NFY, E12, MEIS1, PAX4, AP1, NRF1, TCF11, AP4, GABP, TATA, E4F1). We considered these are new findings and could lead new insight and testable hypotheses.    Conclusions\r\n  Our objective was to develop an effective computational pipeline which produces reliable and explicit models of transcriptional regulatory networks. Even though the TFBS information is incomplete due to the difficulties in identifying them, our pipeline predicts new biological hypotheses on a genome-wide scale by combining TFBS and gene expression information. TIGERi is publicly available as a stand-alone GUI software, so ones who have their own gene expression profiling data could easily use the TIGERi software to analyze their data on their fingertips. It would facilitate transcriptional gene regulation researches in the biomedical community.  Our approach can be applied to other gene expression datasets to provide a display of the transcriptional regulatory networks and identify novel candidate genes and TFs underlying specific phenotypes. For example, our methodology has been successfully applied to three recent studies [ 48-50 ]. The pipeline would be particularly valuable if it were run on large-scale multi-time point genomic data. It is also the case that we might expect the method to become increasingly predictive with improved connection topologies created from large scale experimentally validated TF/TFBS datasets as opposed to those generated from simple conservation data.    Additional file\r\n  Additional file 1: Supplementary Text and Figures. (DOC 7227 kb) Acknowledgements Our thanks to Magus Rattray and Guido Sanguinetti for many constructive discussions.  Funding This work was supported by European Research Council CRIPTON Grant (RG59701), and Institutional funding to the Gurdon Institute by Wellcome Trust Core Grant (092096) and Cancer Research UK Grant (C6946/A14492). The publication charges for this article was funded by European Research Council CRIPTON Grant.  Availability of data and materials TIGERi is a standalone GUI software for all platforms. The TIGERi software and manual can be freely downloaded at https://github.com/namshik/tigeri/.Authors' contributions Conceptualization, NH, HAN and AB; Methodology, NH; Software, NH; Analysis, NH and AB; Writing, NH, HAN and AB; Supervision, AB; Funding Acquisition, NH and AB All authors have read and approved the final manuscript. Competing interests The authors declare that they have no competing interests.  Consent for publication NA.  Ethics approval and consent to participate NA.  About this supplement This article has been published as part of BMC Bioinformatics Volume 18 Supplement 7, 2017: Proceedings of the Tenth International Workshop on Data and Text Mining in Biomedical Informatics. The full contents of the supplement are available online at https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-18-supplement-7.    Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/namshik/tigeri",
    "publicationDate": "0",
    "authors": [
      "Namshik Han",
      "Harry A. Noyes",
      "Andy Brass"
    ],
    "status": "Success",
    "toolName": "tigeri",
    "homepage": ""
  },
  "36.pdf": {
    "forks": 0,
    "URLs": ["github.com/ChengchenZhao/DrSeq2"],
    "contactInfo": ["yzhang@tongji.edu.cn"],
    "subscribers": 1,
    "programmingLanguage": "C++",
    "shortDescription": "",
    "publicationTitle": "Dr.seq2: A quality control and analysis pipeline for parallel single cell transcriptome and epigenome data",
    "title": "Dr.seq2: A quality control and analysis pipeline for parallel single cell transcriptome and epigenome data",
    "publicationDOI": "None",
    "codeSize": 5220,
    "publicationAbstract": "An increasing number of single cell transcriptome and epigenome technologies, including single cell ATAC-seq (scATAC-seq), have been recently developed as powerful tools to analyze the features of many individual cells simultaneously. However, the methods and software were designed for one certain data type and only for single cell transcriptome data. A systematic approach for epigenome data and multiple types of transcriptome data is needed to control data quality and to perform cell-to-cell heterogeneity analysis on these ultra-high-dimensional transcriptome and epigenome datasets. Here we developed Dr. seq2, a Quality Control (QC) and analysis pipeline for multiple types of single cell transcriptome and epigenome data, including scATAC-seq and Drop-ChIP data. Application of this pipeline provides four groups of QC measurements and different analyses, including cell heterogeneity analysis. Dr.seq2 produced reliable results on published single cell transcriptome and epigenome datasets. Overall, Dr.seq2 is a systematic and comprehensive QC and analysis pipeline designed for parallel single cell transcriptome and epigenome data. Dr.seq2 is freely available at: http://www.tongji.edu.cn/~zhanglab/drseq2/ and https://github.com/ChengchenZhao/DrSeq2.",
    "dateUpdated": "2017-02-07T04:50:00Z",
    "institutions": [
      "Emory University Rollins School of Public Health",
      "Tongji University"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2017-02-06T15:12:22Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     July      Dr.seq2: A quality control and analysis pipeline for parallel single cell transcriptome and epigenome data     Chengchen Zhao  0  1    Sheng'en Hu  0  1    Xiao Huo  0  1    Yong Zhang  yzhang@tongji.edu.cn  0  1    0  Editor: Zhaohui Qin, Emory University Rollins School of Public Health ,  UNITED STATES    1  Translational Medical Center for Stem Cell Therapy &amp; Institute for Regenerative Medicine, Shanghai East Hospital, School of Life Science and Technology, Shanghai Key Laboratory of Signaling and Disease Research, Tongji University ,  Shanghai ,  China     3  7  2017   3  2017  4  17    16  6  2017    22  2  2017     An increasing number of single cell transcriptome and epigenome technologies, including single cell ATAC-seq (scATAC-seq), have been recently developed as powerful tools to analyze the features of many individual cells simultaneously. However, the methods and software were designed for one certain data type and only for single cell transcriptome data. A systematic approach for epigenome data and multiple types of transcriptome data is needed to control data quality and to perform cell-to-cell heterogeneity analysis on these ultra-high-dimensional transcriptome and epigenome datasets. Here we developed Dr. seq2, a Quality Control (QC) and analysis pipeline for multiple types of single cell transcriptome and epigenome data, including scATAC-seq and Drop-ChIP data. Application of this pipeline provides four groups of QC measurements and different analyses, including cell heterogeneity analysis. Dr.seq2 produced reliable results on published single cell transcriptome and epigenome datasets. Overall, Dr.seq2 is a systematic and comprehensive QC and analysis pipeline designed for parallel single cell transcriptome and epigenome data. Dr.seq2 is freely available at: http://www.tongji.edu.cn/~zhanglab/drseq2/ and https://github.com/ChengchenZhao/DrSeq2.       -\r\n  Data Availability Statement: The MARS-seq files were available from NCBI Gene Expression Omnibus (GEO) database under accession GSE54006. The 10x genomics datasets were available from 10x genomic data support (https://support.10xgenomics.com/single-cell/datasets). The scATAC-seq datasets were available from NCBI Gene Expression Omnibus (GEO) database under accession GSE65360. The Drop-seq samples were available from NCBI Gene Expression Omnibus (GEO) database under accession GSM1626793.    Introduction\r\n  To better understand cell-to-cell variability, an increasing number of transcriptome technologies, such as Drop-seq [1, 2], Cyto-seq [3], 10x genomics [4], MARS-seq [ 5 ], and epigenome technologies, such as Drop-ChIP [6], single cell ATAC-seq (scATAC-seq) [7], have been developed in recent years. These technologies can easily provide a large amount of single cell transcriptome information or epigenome information at minimal cost, which makes it possible to perform analysis of cell heterogeneity on the transcriptome and epigenome levels, deconstruction of a cell population, and detection of rare cell populations. However, different single cell transcriptome technologies have their own features given their specific experimental design, such as cell sorting methods, RNA capture rates, and sequencing depths. But the methods and software such as Dr.seq [8] were developed for one single cell data type with certain functions Funding: This work was supported by National Natural Science Foundation of China (31571365, 31322031 and 31371288), National Key Research and Development Program of China (2016YFA0100400), Specialized Research Fund for the Doctoral Program of Higher Education (20130072110032), and Program of Shanghai Academic Research Leader (17XD1403600). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.  Competing interests: The authors have declared that no competing interests exist. (S1 File). Furthermore, the quality control step of single cell epigenome data is more challenging than for transcriptome data given the amplification noise caused by the limit number of DNA copy in single cell epigenome experiments. But few quality control and analysis method was developed specific for single cell epigenome data. Thus a comprehensive QC pipeline suitable for multiple types of single cell transcriptome data and epigenome data is urgently needed. Here, we provide Dr.seq2, a QC and analysis pipeline for multiple types of parallel single cell transcriptome and epigenome data, including recently published scATAC-seq data. Dr.seq2 can systematically generate specific QC, analyze, and visualize unsupervised cell clustering for multiple types of single cell data. For single cell transcriptome data, the QC steps of Dr.seq2 are primarily derived from Dr.seq [8] and the output of Dr.seq2 on these data will not be described in details in this paper.    Materials and methods\r\n   Drop-seq data\r\n  The Drop-seq samples were obtained from NCBI Gene Expression Omnibus (GEO) database under accession GSM1626793.    MARS-seq data\r\n  The MARS-seq samples were obtained from NCBI Gene Expression Omnibus (GEO) database under accession GSE54006. These samples were combined as a MARS-seq dataset and analyzed by Dr.seq2 using three different dimension reduction methods. 10x genomics data The 10x genomics datasets were obtained from 10x genomic data support (https://support.10xgenomics.com/single-cell/datasets). The sample named ª50%: 50% Jurkat: 293T Cell Mixtureº was analyzed by Dr.seq2 using three different dimension reduction methods. scATAC-seq data The scATAC-seq datasets were obtained from NCBI Gene Expression Omnibus (GEO) database under accession GSE65360. We combined 288 scATAC datasets (GSM1596255 ~ GSM1596350, GSM1596735 ~ GSM1596830, GSM1597119 ~ GSM1597214) from three cell types and analyzed by Dr.seq2. Cell clustering was conducted for the combined scATAC-seq dataset. We also plotted the cell type labels using different colors on the clustering plot and found consistent classifications with the clustering results.    Drop-ChIP data\r\n  The Drop-ChIP datasets were obtained from NCBI Gene Expression Omnibus (GEO) database under accession GSE70253.    Implementation of Dr.seq2\r\n  Dr.seq2 was implemented using Python and R. Linux or MacOS environment with Python (version = 2.7) and R (version&gt; = 2.14.1) was suitable for Dr.seq2. It was distributed under the GNU General Public License version 3 (GPLv3). A detailed tutorial was provided on the Dr. seq2 webpage (http://www.tongji.edu.cn/~zhanglab/drseq2) and source code of Dr.seq2 was available on github (https://github.com/ChengchenZhao/DrSeq2).2 / 14    Quality control components\r\n  Dr.seq2 conducted four groups of QC measurements on single cell epigenome data: (i) reads level QC; (ii) bulk-cell level QC; (iii) individual-cell level QC; and (iv) cell-clustering level QC.  Reads level QC and bulk-cell level QC. We used a published package called RseQC [9] for reads level QC of Drop-ChIP data and scATAC-seq data to measure the general sequence quality. In bulk-cell level QC, a Drop-ChIP dataset (or scATAC-seq datasets combined from several scATAC-seq samples) was regarded as a bulk-cell ChIP-seq (or bulk-cell ATAC-seq) data. Next, ªcombined peaksº were detected with total reads from the ªbulk-cellº data using MACS[10] for output and the following steps. Different MACS parameters were applied to Drop-ChIP and scATAC-seq data. We used the published package CEAS to measure the performance of ChIP for ChIP-seq data (or Tn5 digestion for scATAC-seq data) [11].  Individual-cell level QC. The reads number distribution was calculated by counting the number of reads assigned to each single cell. A single cell referred to a unique cell barcode in Drop-ChIP data. For scATAC-seq data, the peak number in each cell was defined as the number of ªcombined peaksº occupied by the reads in the cell. The distribution of different peak numbers in each cell indicated the different amount of information the cell contains.  Cell-clustering level QC. Cells were first clustered based on their occupancy of ªcombined peaksº using hierarchical clustering. Next, cells in each cluster were regarded as the same cell type (or same cell sub-type), and reads from the same cell type were merged. For each cell type, unique peaks from other cell types were defined as specific peaks in this cell type. Specific peaks in different cell types were displayed with different colors according to genomic locations. Silhouette method is used to interpret and validate the consistency within clusters defined in previous steps.  Note that reads with no overlap with ªcombined peaksº were discarded in this step and the following steps. Clusters containing less than 3 single cells were also discarded.    Simulation of scATAC-seq datasets\r\n  To measure the tolerance of Dr.seq2 for low sequencing depth and small numbers of cells of a certain cell type, we simulated datasets from 3 cell types with different cell proportions and sequencing depths using scATAC-seq data (Table 1). To test the effect of low sequencing depth, we sampled the reads count from 10,000 reads to 100,000 reads for each cell and compared these results with the Goodman-Kruskal's lambda index [12] of clustering results using cells with a certain number of reads.  To test the effect of low cell numbers of a certain cell type (defined as a target cell type) on cell clustering, we defined 1 of the 3 cell types as the ªtarget cell typeº, whereas the other cell types were defined as the ªregular cell typeº, and sampled cells with following compositions: 10:70:70 (10 for target cell type, 70 for the two regular cell types), 15:67:67, 20:65:65, 25:62:62, 30:60:60, 35:57:57, 40:55:55, 45:52:52 and 50:50:50. Then, we called ªcombined peaksº and clustered cells on the simulated dataset. The Goodman-Kruskal's lambda index [12] was 3 / 14 calculated to evaluate the cell clustering performance. The average Goodman-Kruskal's lambda index and 95% confidence intervals were calculated from 20 simulations.     Results and discussion\r\n   Dr.seq2 overview\r\n  The Dr.seq2 QC and analysis pipeline is suitable for both single cell transcriptome data and epigenome data. Multiple types of single cell transcriptome data (including scRNA-seq, Dropseq, inDrop, MARS-seq and 10x genomics data) and epigenome data (including scATAC-seq and Drop-ChIP) are acceptable for Dr.seq2 with relevant functions (S1 Fig).  Recently many methods and software were developed for single cell RNA-seq data. However most of them were suitable for certain data types with limited functions. We compared the major function of Dr.seq2 to existing state-of-the-art methods (Table 2). Dr.seq2 provides two advantages: 1) Dr.seq2 supports different types of single cell transcriptome data and single cell epigenome data. 2) Dr.seq2 provides both multifaceted QC reports and cell clustering ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ ✔ We compare the major function of Dr.seq2 to existing state-of-the-art methods. Each column shows different functions of these methods and software. 4 / 14 results. Then We used the simulated single cell RNA-seq data from seven RNA-seq datasets from ENCODE (S2 File) to estimate the performance of our Dr.seq2 pipeline (using different dimensional reduction methods: SIMLR and t-SNE) in cell clustering comparing to three existing methods (SINCERA, SNN-Cliq, BackSPIN). We applied these five methods on ten datasets with different numbers of reads per cell range from 100 to 10,000 to measure the accuracy and time cost of each method on different sequencing depth. SIMLR shows more accurate clustering results than t-SNE on the datasets with small number of reads per cell and comparable clustering results on the datasets with large number of reads per cell. And Dr.seq2 (using either SIMLR or t-SNE) shows better clustering accuracy than SNN-Cliq, and comparable clustering accuracy with BackSPIN and SINCERA on the datasets with large number of reads per cell. On the datasets with small number of reads per cell, SINCERA clustering result shows better accuracy than Dr.seq2 (using either SIMLR or t-SNE) and SNN-Cliq. However SINCERA takes a great mount of time on all these datasets comparing with Dr.seq2. As for BackSPIN, it does not support for these datasets with small number of reads per cell. Overall, Dr. seq2 (using either SIMLR or t-SNE) provides reliable cell clustering results with acceptable time cost (S2 Fig).    QC and analysis workflow\r\n  Dr.seq2 uses raw sequencing files in FASTQ format or alignment results in SAM/BAM format as input with relevant commands and generates four steps of QC measurements and analysis results (Fig 1).  For transcriptome data, the QC steps of Dr.seq2 are primarily derived from Dr.seq [8]. However, almost all data types are now supported, and more dimension reduction methods, including PCA, t-SNE and SIMLR[35], are supported. For single cell epigenome data, technologies like scATAC-seq and Drop-ChIP are increasingly common. However few quality control and analysis approaches have been developed for these data. Dr.seq2 conducts QC measurements on single cell epigenome data from four aspects: (i) reads level QC, including sequence quality, nucleotide composition and GC content of reads inherited from previous work; (ii) bulk-cell level QC, including genomic distribution of ªcombined peaksº and average profile on regulatory regions; (iii) individual-cell level QC, including the distribution of the number of reads and the peak number distribution; and (iv) cell-clustering level QC, including Silhouette score[36] and cell type-specific peak detection.    Cell clustering for different single cell transcriptome data types using different dimension reduction methods\r\n  We applied our pipeline to three different types of single cell transcriptome data (Drop-seq, MARS-seq and 10x genomics data) using three different dimension reduction methods (PCA, t-SNE and SIMLR[35]) to evaluate the performance of Dr.seq2 on different types of single cell transcriptome data (Fig 2). Due to the different distance calculation method and kernel function the method used, Dr.seq2 represented cluster results from different dimensions.    Bulk-cell level QC of scATAC-seq data to measure the performance of\r\n    Tn5 digestion\r\n  To evaluate the performance of Dr.seq2 on single cell epigenome data, we combined 288 scATAC datasets (GSM1596255 ~ GSM1596350, GSM1596735 ~ GSM1596830, GSM1597119 ~  GSM1597214) from three cell types and applied Dr.seq2 to it. ªCombined peaksº were detected 5 / 14 Fig 1. Flowchart illustrating the Dr.seq2 pipeline with default parameters. The workflow of the Dr.seq2 pipeline includes QC and analysis components for parallel single cell transcriptome and epigenome data. The QC component contains reads level, bulk-cell level, individual-cell level and cell-clustering level QC. with total reads from the combined dataset using MACS for output and the following steps.  We measured the scATAC data quality in bulk-cell level from 4 aspects (Fig 3): 1) Peak distribution on each chromosome; 2) Open regions distributed over the genome along with their scores; 3) Average profiling on different genomic features; 4) Fragment length distribution.  The peak distribution on each chromosome and the open region distributed over the genome showed the general quality of Tn5 digestion. The average profiling on different genomic features represented the quality of Tn5 digestion around specific regions. And the periodicity fragment length distribution indicated factor occupancy and nucleosome positions due to different Tn5 digestion degrees. 6 / 14 Fig 2. Dimensional reduction results for different single cell transcriptome data types. (A-I) Cell clustering results using dimensional reduction methods (PCA, t-SNE and SIMLR) on different types of single cell transcriptome data (Drop-seq, 10x genomics and MARS-seq).    Cell clustering for scATAC-seq datasets with three clusters that were consistent with the cell type labels\r\n  To measure the cell clustering performance of Dr.seq2 on epigenome data, cells from the combined scATAC-seq dataset were firstly clustered based on their occupancy of ªcombined peaksº using hierarchical clustering. Then cell type labels were marked by different colors according to the original cell type information (red stand for H1 cells, yellow stand for GM12878 cells and blue stand for K562 cells). Cells were clearly separated into different groups that were consistent with the cell type labels by Dr.seq2 (Fig 4A). 7 / 14 Fig 3. Bulk-cell level QC for scATAC-seq datasets. A) Peak region number distribution on each chromosome. The blue bars represent the percentages of the whole tiled or mappable regions in the chromosomes (genome background) and the red bars showed the percentages of the whole open region. These percentages are also marked right next to the bars. P-values for the significance of the relative enrichment of open regions with respect to the gnome background are shown in parentheses next to the percentages of the red bars. B) Open region distribution over the genome along with their scores or peak heights. The line graph on the top left corner illustrates the distribution of peak score. The x-axis of the main plot represents the actual chromosome sizes. C) Average profiling on different genomic features. The panels on the first row display the average enrichment signals around TSS and TTS of genes, respectively. The bottom panel represents the average signals on the meta-gene of 5 kb. D) Red line shows number distribution of different fragment length. 8 / 14 Fig 4. Cell-clustering level QC and single-cell level QC for scATAC-seq data. A) Upper panel shows cellclustering results for combined scATAC samples generated from 3 different cell types. Bottom panel shows corresponding cell type labels of each cell marked by different colors (red stand for H1 cells, yellow stand for GM12878 cells and blue stand for K562 cells). The clustering step of Dr.seq2 clearly separated the scATACseq samples from three different cell types into different groups that were consistent with the cell type labels.  B) Distribution of peak number for each single cell. C) Cell Clustering tree and peak region in each cell. The 9 / 14 upper panel represents the hieratical clustering results based on each single cell. The second panel with different colors represents decision of cell clustering. The bottom two panels (heatmap and color bar) represent the ªcombined peaksº occupancy of each single cell. D) Barplot shows Silhouette score of each cluster. Silhouette method is used to interpret and validate the consistency within clusters defined in previous steps. E) Cluster specific regions in each chromosome. Specific regions for different cell clusters are marked by different colors and ordered according to genomic loci.    Single-cell level QC and post analysis of scATAC-seq data\r\n  In the single-cell level QC of Dr.seq2 on scATAC-seq data, the peak number of in each cell was defined as the number of ªcombined peaksº occupied by the reads in the cell. The distribution of different peak numbers in each cell indicated the different amount of information the cell contains (Fig 4B). Cell clustering was conducted based on the peak information in each cell using hierarchical clustering and open region was shown in the order of genomic location (Fig 4C). And Silhouette score [36] validated the consistency of each cluster (Fig 4D). Then cells in the same clusters were considered as cells in the same cell type and combined for the detection of cell type specific regions, which were defined as the peak regions that only covered in this cell type. Specific regions for different cell clusters were marked by different colors and ordered according to genomic loci (Fig 4E).  Fig 5. Cell clustering stability on simulated scATAC-seq data. A) Clustering stability of Dr.seq2 on simulated data with different numbers of reads per cell. The lambda index (y-axis) is plotted as a function of the number of reads per cell (x-axis). Error bars represent 95% confidence intervals calculated from 20 simulations. B) Clustering stability of Dr.seq2 on simulated data with different cell proportion depths. The lambda index (y-axis) is plotted as a function of the target cell number (x-axis). Error bars represent 95% confidence intervals calculated from 20 simulations. 10 / 14 288 scATAC datasets from three cell types were used to evaluate the runtime of Dr.seq2. The running time for each step was calculated using a single CPU (Intel® Xeon® CPU E5-2640 v2 @ 2.00 GHz).    Cell clustering stability on simulated scATAC-seq data\r\n  To measure the tolerance of Dr.seq2 for low sequencing depth and small numbers of cells of a certain cell type, we simulated datasets with different cell proportions and sequencing depths by using scATAC-seq datasets from three cell types (Table 1).  We selected cells in different proportion with 100,000 reads per cell and then performed cell clustering using Dr.seq2. The performance of cell clustering methods was evaluated by Goodman-Kruskal's lambda index. And the average Goodman-Kruskal's lambda index calculated from 20 simulations indicated that Dr.seq2 was suitable for cell clustering with different cell proportions (Fig 5A). We also selected fifty cells from each cell type with the reads count range from 10,000 reads to 100,000 reads for each cell to measure the tolerance of Dr.seq2 on low sequence depth. Dr.seq2 produced stable clustering results with greater than 40,000 reads per cell (Fig 5B).    Computational cost of Dr.seq2\r\n  We also measured the computational time cost of Dr.seq2 by applied Dr.seq2 on combined scATAC-seq datasets (Table 3). The running time of each step was calculated using a single CPU (Intel1 Xeon1 CPU E5-2640 v2 @ 2.00 GHz).     Conclusions\r\n  In summary, Dr.seq2 is designed for QC and analysis components of parallel single cell transcriptome and epigenome data. Parallel single cell transcriptome data generated by different technologies can be transformed to the standard input for Dr.seq2 with contained functions. Using relevant commands, Dr.seq2 can also be used to report quality measurements based on four aspects and generate detailed analysis results for scATAC-seq and Drop-ChIP datasets.    Supporting information\r\n  S1 Fig. Workflow displays the software structure and detailed QC steps of Dr.seq2. A) Dr. seq2 provides QC and analysis for three major data types: single cell transcriptome data (DrSeq part), Drop-ChIP data (DrChIP part) and scATAC-seq data (ATAC part). For single cell RNA-seq data, two additional step-by-step functions are included: 1. Expression matrix generation for amounts of single cell RNA-seq datasets (GeMa step) and 2. Cell clustering and analysis for the single cell expression matrix (comCluster step). For different parallel single cell RNA-seq technologies, input data are standardized for DrSeq part. B) Four groups of QC measurements are conducted on single cell transcriptome data and epigenome data: 1.Reads level QC including reads quality, reads nucleotide composition and reads GC content 2.Bulk-cell level QC including reads alignment summary and gene body coverage for transcriptome data; 11 / 14 peak distribution; average profile on regulatory region and the distribution of different numbers of fragment length for epigenome data. 3. Individual-cell level QC including duplicate rate distribution, covered gene number and intron rate distribution and intron rate distribution for transcriptome data; peak number distribution and fragment length distribution for epigenome data. 4. Cell-clustering level QC including Gap statistics score and Silhouette score for transcriptome data, h-clustering and cluster specific peaks for epigenome data. (TIF) S2 Fig. Comparing the performance of Dr.seq2 and three existing state-of-the art methods on cell clustering. A) Clustering accuracy measured by the Goodman-Kruskal's lambda index of Dr.seq2 t-SNE, Dr.seq2 SIMLR methods and three published methods on simulated data with different numbers of reads per cell. The lambda index (y-axis) is plotted as a function of the number of reads per cell (x-axis). B) Running time of Dr.seq2 t-SNE, Dr.seq2 SIMLR methods and three published methods on simulated data with different numbers of reads per cell. The running time (y-axis) is plotted as a function of the number of reads per cell (x-axis). The running time for each method was calculated using a single CPU (Intel1 Xeon1 CPU E52640 v2 @ 2.00 GHz). (TIF) S1 File. Comparison of functions between Dr.seq2 and other software developed for single cell transcriptome data. (XLSX) S2 File. Meta data and accession ID for the bulk-cell RNA-seq data used in simulation. (XLSX) S3 File. Dr.seq2 QC and analysis output report for the scATAC-seq dataset. (PDF) S4 File. Dr.seq2 QC and analysis output report for the Drop-ChIP dataset. (PDF) S5 File. Dr.seq2 QC and analysis output report for the 10x genomics dataset. (PDF)    Acknowledgments\r\n  We thank Shiyang Zeng and Yiying Lang for their suggestions.    Author Contributions\r\n   Conceptualization: CZ SH XH YZ.\r\n    Data curation: CZ.\r\n    Formal analysis: CZ.\r\n    Funding acquisition: YZ.\r\n    Methodology: CZ SH YZ.\r\n    Resources: CZ.\r\n    Software: CZ.\r\n    Supervision: YZ.\r\n  12 / 14    Writing ± original draft: CZ.\r\n    Writing ± review &amp; editing: CZ SH XH YZ.\r\n  13 / 14 20. 21. 22. 23. 24. 25. 26. 27. 28. 30. 31. 32. 34. 35. 36.     ",
    "sourceCodeLink": "https://github.com/ChengchenZhao/DrSeq2",
    "publicationDate": "0",
    "authors": [
      "Chengchen Zhao",
      "Sheng'en Hu",
      "Xiao Huo",
      "Yong Zhang"
    ],
    "status": "Success",
    "toolName": "DrSeq2",
    "homepage": ""
  },
  "96.pdf": {
    "forks": 4,
    "URLs": [
      "www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-3657/",
      "qtltools.github.io/qtltools/",
      "www.nature.com/naturecommunications",
      "www.ebi.ac.uk/arrayexpress/files/E-GEUV-1/processed/"
    ],
    "contactInfo": [
      "olivier.delaneau@gmail.com",
      "emmanouil.dermitzakis@unige.ch"
    ],
    "subscribers": 4,
    "programmingLanguage": "C++",
    "shortDescription": "A complete tool set for molecular QTL discovery and analysis",
    "publicationTitle": "A complete tool set for molecular QTL discovery and analysis",
    "title": "A complete tool set for molecular QTL discovery and analysis",
    "publicationDOI": "10.1038/ncomms15452",
    "codeSize": 40434,
    "publicationAbstract": "Population scale studies combining genetic information with molecular phenotypes (for example, gene expression) have become a standard to dissect the effects of genetic variants onto organismal phenotypes. These kinds of data sets require powerful, fast and versatile methods able to discover molecular Quantitative Trait Loci (molQTL). Here we propose such a solution, QTLtools, a modular framework that contains multiple new and well-established methods to prepare the data, to discover proximal and distal molQTLs and, finally, to integrate them with GWAS variants and functional annotations of the genome. We demonstrate its utility by performing a complete expression QTL study in a few easy-to-perform steps. QTLtools is open source and available at https://qtltools.github.io/qtltools/.",
    "dateUpdated": "2017-10-12T07:30:23Z",
    "institutions": [],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2016-07-11T08:52:13Z",
    "numIssues": 5,
    "downloads": 0,
    "fulltext": "     NATURE COMMUNICATIONS |     10.1038/ncomms15452   A complete tool set for molecular QTL discovery and analysis     Olivier Delaneau  olivier.delaneau@gmail.com    Halit Ongen    Andrew A. Brown    Alexandre Fort    Nikolaos I. Panousis    Emmanouil T. Dermitzakis  emmanouil.dermitzakis@unige.ch     18  5  2017   8  15452   Population scale studies combining genetic information with molecular phenotypes (for example, gene expression) have become a standard to dissect the effects of genetic variants onto organismal phenotypes. These kinds of data sets require powerful, fast and versatile methods able to discover molecular Quantitative Trait Loci (molQTL). Here we propose such a solution, QTLtools, a modular framework that contains multiple new and well-established methods to prepare the data, to discover proximal and distal molQTLs and, finally, to integrate them with GWAS variants and functional annotations of the genome. We demonstrate its utility by performing a complete expression QTL study in a few easy-to-perform steps. QTLtools is open source and available at https://qtltools.github.io/qtltools/.       -\r\n  T association studies (GWAS), many genetic studies now o increase the explanatory power of genome-wide routinely combine genetic information with one or multiple molecular phenotypes such as gene expression1-3, protein abundance4, metabolomics5, methylation6 and chromatin activity7. This makes the discovery of molecular Quantitative Trait Loci (molQTL) possible; a key step towards better understanding the effects of genetic variants on the cellular machinery and eventually on organismal phenotypes. In practice, this requires analysing data sets comprising of millions of genetic variants and thousands of molecular phenotypes measured on a population scale; a design that aims to perform orders of magnitude more association tests than in a standard GWAS, which prevents the use of standard tools designed to handle only few phenotypes8,9. To face this computational and statistical challenge, there is a clear need of computational methods that are (i) powerful enough to handle the multiple testing problem, (ii) fast enough to easily process large amounts of data in reasonable running times and (iii) versatile enough to adapt to new data sets as they are being generated. Here, we present such an integrated framework, called QTLtools, which allows users to transform raw sequence data into collections of molQTLs in a few easy-to-perform steps, all based on powerful methods that either match or improve those employed in large scale reference studies such as Geuvadis1 and GTEx10.  QTLtools is a modular framework designed to accommodate new analysis modules as they are being developed by our group or the scientific community. In its current state, QTLtools performs multiple key tasks (Fig. 1) such as checking the quality of the sequence data, checking that sequence and genotype data match, quantifying and stratifying individuals using molecular phenotypes, discovering proximal or distal molQTLs and integrating them with functional annotations or GWAS data. To demonstrate the utility of this new tool with real data, we used it to perform a complete expression QTL (eQTL) study for 358 European samples where genotype and expression data were generated as part of the 1,000 Genomes11 and Geuvadis1 projects (Supplementary Data 1).    Results\r\n  Controlling the quality of the sequence data. To control the quality of the sequence data, QTLtools proposes two complementary approaches. First, it can measure the proportions of reads (i) mapping to the reference genome and (ii) falling within an annotation of interest (Supplementary Note 1), such as GENCODE for RNA-seq12. Second, it can ensure that the sequence data matches the corresponding genotype data; the opposite being an evidence of sample mislabelling13. To achieve this, QTLtools measures concordance between genotypes and sequencing reads, separately for heterozygous and homozygous genotypes (Supplementary Note 2). Low values in any of the two measures indicate problems such as sample mislabelling, contamination or amplification biases (Supplementary Fig. 1). When performed on Geuvadis, these two approaches demonstrate the high quality of the RNA-seq data (Supplementary Fig. 2) and the good match with available genotype data (Supplementary Fig. 3).  Quantifying gene expression. To quantify gene expression, QTLtools counts the number of sequencing reads overlapping a set of genomic features (for example, exons) listed in a given annotation file (Supplementary Note 3). We quantified both exon and gene expression levels in all 358 Geuvadis samples using this approach and find 22,147 genes with non-zero quantifications in more than half of the samples (Supplementary Fig. 4). Then,   Sequence data (BAM)\r\n    QTLtools bamstat\r\n  Controls the quality of the sequence data    QTLtools quant\r\n  Measures expression at genes and exons    QTLtools cis\r\n  Performs QTL mapping in cis    QTLtools rtc\r\n  Integrates QTL data with GWAS data    QTL data\r\n    Genotype data (VCF)\r\n    QTLtools mbv\r\n  Ensures good matching between sequence and genotype data    QTLtools pca\r\n  Performs PCA on phenotype and genotype data    QTLtools trans\r\n  Performs QTL mapping in trans    QTLtools func\r\n  Integrates QTL data with functional annotation we run principal component analysis (PCA) on these quantifications, as implemented in QTLtools (Supplementary Note 4), to capture any stratification in the sequence data or in the genotype data. In the Geuvadis data, we did not observe any unexpected clusters in the expression data or in the genotype data (Supplementary Fig. 5) and used the resulting weights on the first few principal components as latent variables to increase discovery power of any downstream association testing (Supplementary Note 5).  Mapping proximal molQTLs. A core task of QTLtools is to discover proximal (that is, cis-acting) molQTLs. To do so, it extends the QTL mapping method introduced by FastQTL14 and offers multiple key improvements that make this step fast and easy-to-perform. First, it uses a permutation scheme that needs a relatively small number of permutations to adjust nominal P values for multiple testing (see Methods section and Supplementary Fig. 6). As a consequence, the whole-Geuvadis eQTL analysis can be performed in short running times (B32 CPU hours) which has previously been proved to be an order of magnitude faster than a widely used tool, Matrix eQTL15 and provides adjusted P values without any lower bounds (Supplementary Fig. 7). The running times are actually so small that it becomes possible to process rapidly massive data sets such as the GTEx v6p study16 (7,051 samples in B870 CPU hours; Supplementary Fig. 8) and to repeat the whole analysis multiple times across different sets of quantifications, covariates and QC filters to determine the optimal configuration which maximizes the number of discoveries (Supplementary Figs 9 and 10). In addition, QTLtools also provides ways to easily extract subsets of data and therefore facilitate detailed inspection of particular eQTLs (Supplementary Fig. 11).  Mapping proximal molQTLs for groups of phenotypes. As multiple molecular phenotypes can belong to higher order biological entities, for example exons lying within genes or histone modification peaks which form larger variable chromatin modules (VCMs7), we also implemented two methods to maximize the discoveries in such particular cases (Methods section). Specifically, QTLtools can either (i) aggregate multiple phenotypes in a given group into a single phenotype via PCA or (ii) directly use all individual phenotypes in an extended permutation scheme that accounts for their number and correlation structure. In our experiments, the permutationbased approach seems to outperform the PCA-based approach in terms of number of discoveries in the two data sets we tested (Fig. 2a, Supplementary Data 2, Supplementary Fig. 12). In Geuvadis, the permutation-based approach is able to discover an additional set of B1,056 eQTLs compared to the standard gene-level quantifications, most of them being for genes containing many exons (Supplementary Fig. 13).  Mapping proximal molQTLs using conditional analysis. Furthermore, QTLtools can also perform conditional analysis to discover multiple proximal molQTLs with independent effects on a molecular phenotype. To do so, it first uses permutations to derive a nominal P value threshold per molecular phenotype that varies and reflects the number of independent tests per cis-window. Then, it uses a forward-backward stepwise regression to (i) learn the number of independent signals per phenotype, (ii) determine the best candidate variant per signal and (iii) assign all significant hits to the independent signal they relate to (Methods section). We applied this conditional analysis on Geuvadis and discovered that B38% of the significant genes have actually more than one eQTL (Fig. 2b); some have up to six independent eQTLs (Fig. 2c). Interestingly, we also find that combining the conditional analysis with the phenotype grouping approach described above could help to discover even more signals (Fig. 2b,c). The new discoveries resulting from theses analyses in Geuvadis have high replication rates within an independent data set (GTEx10) suggesting that these are genuine discoveries (Supplementary Note 6, Supplementary Fig. 14). Mapping distal molQTLs. Beyond mapping proximal molQTLs, QTLtools also includes methods to discover distal (that is, trans-acting) molQTLs. The first method we implemented relies on permuting all phenotypes together to draw from the null distribution of associations while preserving the correlation structure within genotype and phenotype data intact (Methods section). By repeating this permutation scheme multiple times (for example, 100 times in our experiments), we can obtain an empirically calibrated Quantile-Quantile plot that properly shows signal enrichment (Fig. 2d) and can estimate the false discovery rate (FDR) for all the most significant associations: in Geuvadis, we could find 52 genes with at least one significant signal in trans at 5% FDR. Given that this full permutation scheme is computationally intensive (B450 CPU hours for 100 permutations), we also designed an approximation of this process that gives reasonably close FDR estimates while being multiple orders of magnitude faster (B7 CPU hours; Methods section). Given that the whole genome is effectively tested for each phenotype, we quickly build a null distribution of associations for a single phenotype by permutations. We then use this null distribution to adjust each nominal P value for the number of variants being tested and then use standard FDR methods17 on the resulting set of adjusted P values to correct for the multiple phenotypes being tested. In practice, this approach can be seen as an extension of the mapping strategy we use in cis for trans analysis, and gives FDR estimates that are close to those obtained with the full permutation pass (Supplementary Fig. 15) while being much faster to obtain (B64 times faster in our experiments). Integrating molQTLs with GWAS and functional data. Finally, we also implemented multiple methods to integrate collections of molQTLs with two types of external data: functional genome annotations and GWAS results. First, QTLtools can estimate if a molQTL and a variant of interest (typically a GWAS hit) pinpoint the same underlying functional variant. To do so, it uses regulatory trait concordance18 (Supplementary Note 7); a sophisticated conditional analysis scheme designed to account for linkage disequilibrium as a confounding factor when co-localizing molQTLs and GWAS hits. This can be used, for instance, to determine the subset of GWAS hits that are likely mediated by molQTLs; a useful piece of information to understand the function of GWAS hits. When applied on Geuvadis and the NHGRI-EBI GWAS catalogue19, we estimated to which extent the disease associated variants reported in this catalogue overlap with eQTLs for lymphoblastoid cell lines (Supplementary Fig. 16). Alternatively, QTLtools can also look at the overlap between molQTLs and functional annotations such as those provided by ENCODE12. Specifically, it can compute the density of annotations around molQTL locations and, when they do overlap, estimate if it is more often than what is expected by chance (Methods section). This allows the distribution of functional annotations around molQTLs to be inspected visually (Fig. 2e) and statistically (Fig. 2f). When using this on the various sets of eQTLs we have discovered so far, we find that they tend to fall within transcription factor binding sites and open chromatin regions (Fig. 2f), in line with previous knowledge on eQTLs1. Computational efficiency. All functionality described above has been implemented in C þþ for high performance and in a modular way to facilitate future implementation of additional functionalities by the community. In practice, this allows all the experiments described above to be run in a relatively short time (Supplementary Table 1); the full set of analyses described above were completed in B1,327 CPU hours ( ¼ B55 CPU days). In addition, QTLtools has been designed so that the computational load can be easily distributed across the multiple CPU cores that are typically available on a compute cluster. The tasks run on individual samples (for example, QC the sequence data) are simple to parallelize as one compute job per individual. For population-based tasks, such as QTL mapping, the input data is a automatically split into small genomic chunks that are then run conveniently and independently on distinct CPU cores.     Discussion\r\n  Population scale studies combining genetic variation and molecular phenotypes have become a standard to detect molecular QTLs. This requires multiple computational steps to go from the raw sequence and genotype data to collections of molecular QTLs. So far, this can be done using multiple tools that are often hard to combine and/or adapt to the amount of data involved. We propose in this paper, QTLtools, a software package that integrates all functionalities required to easily and rapidly perform this task. It includes multiple new and powerful statistical methods to prepare and control the quality of the data, to map proximal and distal QTLs and to integrate those with GWAS results and functional annotations. It also offers a unique framework for the community to develop further additional methods or alternative to the ones already included, so that molecular QTL analysis can be more seamless among laboratories. By its integrative design and efficient implementation, QTLtools dramatically decreases the time needed to set up and run the various analysis pipelines traditionally needed by molecular QTL studies, freeing researchers to spend more effort on the interpretation and validation of their results.    Methods\r\n    Mapping proximal molQTLs using permutations. Mapping proximal molecular\r\n  QTL consists of finding statistically significant associations between molecular phenotypes and nearby genetic variants; a task commonly undertaken using linear regressions1,10,14. In practice, this requires millions of association tests to scan all possible phenotype-variant pairs in cis (that is, variants located within a specific window around a phenotype), resulting in millions of nominal P values. Due to the large number of tests performed per molecular phenotype, multiple testing has to be accounted for to assess the significance of any discovered candidate molQTL. A first naive solution to this problem is to correct the nominal P values for the number of tested variants using the Bonferroni method. However, due to the specific and highly variable nature of each genomic region being tested in terms of allele frequency and linkage disequilibrium, the Bonferroni method usually proves to be overly stringent and results in many false negatives. To overcome this issue, a commonly adopted approach is to analyse thousands of permuted data sets for each phenotype to empirically characterize the null distribution of associations (that is, the distribution of P values expected under the null hypothesis of no associations). Then, we can easily assess how likely an observed association obtained in the nominal pass originates from the null, resulting in an adjusted P value. In practice, thousands of permutations are required in this context and therefore fast methods able to absorb such substantial computational loads in reasonable running times are needed. FastQTL has recently emerged as a good candidate for this task by proposing a fast and efficient permutation scheme in which the null distribution of associations for a phenotype is modelled using a beta distribution14. This allows approximating the tail of the null distribution relatively well using only few permutations and also accurately estimating adjusted P values at any significance level in short running times. In the original FastQTL paper, it has been shown that running 1,000 permutations gives accurate adjusted P values while being B17 times faster than when implementing the standard permutation scheme with MatrixeQTL running on a BLAS-optimized R version15. In QTLtools, we use exactly the same approach than in FastQTL: we approximate the permutation outcome with a beta distribution. We run this method on Geuvadis using 1,000 permutations and a cis-window of 1 Mb. And since the eQTL mapping is quick and easy, we repeated the whole-mapping pass multiple times across multiple conditions. Specifically, we repeated the whole-Geuvadis analysis across multiple missing data proportion filters (that is, %genes with RPKM ¼ 0 between 0 and 100; Supplementary Fig. 10) and numbers of expression-derived Principal Components (that is, PCs between 0 and 100; Supplementary Fig. 9). We therefore determine that the optimal configuration to maximize the number of discoveries relies on filtering out genes with more than 50% of the samples with non-zero quantifications and using 50 expression-based PCs as covariates. In all downstream analyses, we used this configuration when not specified otherwise.    Mapping proximal QTLs for groups of phenotypes. It is common that some\r\n  kinds of molecular phenotypes may belong to higher order biological entities. For instance, a given gene often contains multiple exons. Similarly, nearby regulatory elements may cooperate within some module structures such as VCM7 or topologically associated domains20. To map molecular QTLs at the level of these higher order biological entities, we need methods able to properly combine information at all the multiple molecular phenotypes they contain. In the context of genes, this has traditionally been done at the quantification level: read counts at multiple exons are summed up to get gene-level quantifications that are then used to discover gene-level eQTL. In QTLtools, we introduced two approaches to combine locally multiple molecular phenotypes belonging to a given group. First, we extended the permutation scheme described above to deal with each group of phenotype independently. Assuming that a phenotypic group P (for example, gene) contains M phenotypes (for example, exons) and that the corresponding cis-window G contains L genetic variants, QTLtools proceeds as follows: ( 1 ) All MxL possible variant-phenotype pairs are tested using linear regressions.  The pair with the smallest nominal P value is stored as best candidate QTL for this group of phenotype. ( 2 ) Permute simultaneously all phenotypes in P using the same random number sequence. As a result, the inner correlation structure within both P and G remains completely unchanged, while the correlation in between P and G is broken. ( 3 ) Draw from the null distribution of association between P and G by scanning all MxL possible variant-phenotype pairs in the permuted data set and by retaining the best association. ( 4 ) Build empirically the null distribution of association between G and P by repeating the steps ( 2 ) and ( 3 ) as many times as needed (typically 1,000 times is enough). ( 5 ) Fit a beta distribution on this empirically defined null distribution using expectation-maximization14. This effectively makes the null distribution continuous. ( 6 ) Adjust the nominal P value of the best pair obtained in step ( 1 ) using the fitted beta distribution. ( 7 ) Repeat step ( 1 ) to ( 6 ) for all groups of phenotypes to get a candidate QTL together with an adjusted P value of association for each. ( 8 ) Determine all significant QTLs at a given FDR (typically 5%) using a FDR procedure such as Storey-Tibshirani implemented in R/q value17 on the adjusted P values.  Note that this permutation scheme corrects for both the number of genetic variants and the number of molecular phenotypes being tested while properly accounting for their inner correlation structure. As a consequence, when the beta distribution is fitted in step ( 5 ), we get an estimate of the effective number of independent tests corresponding to the actual MxL tests we performed. Alternatively to this extended permutation scheme, we also implemented an approach based on dimensionality reduction. This has been previously used to discover QTLs for VCMs from single-ChIP-seq peak quantifications7. Here, a PCA is first performed on the M phenotypes and the loadings on the first PC are used as a quantification vector for the entire group of phenotypes. We can then perform the standard mapping approach implemented in QTLtools to discover a QTL for P. We applied these two approaches on Geuvadis to discover gene-level eQTL from exonic quantifications and compared them with the standard gene-level quantifications. We find that the largest number of eQTL is obtained with the extended permutation scheme and the smallest with the PCA-based approach; the gene-level quantifications lying in between. The boost provided by the extended permutation scheme is really appreciable since we get an additional set of 1,019 eQTLs that the gene-level quantification is unable to discover (Fig. 2a). Of note, it really helps to discover eQTL for genes having a high number of exons (Supplementary Fig. 13). In addition to this, we also applied both approaches on the histone modification data to discover vcmQTL and find similar results (Supplementary Fig. 12). Despite the lower performance of the PCA-based approach in this context, we decided to keep it in QTLtools since we believe it can still be useful in a different context; such as for instance when we are more interested in capturing instead the common trend between multiple phenotypes within a group.  Mapping proximal molQTLs using conditional analysis. The two mapping approaches above only report a single candidate QTL per phenotype or group of phenotypes. In some cases, this limitation may reduce significantly the number of discoveries. For example, it is relatively frequent that expression for a given gene is affected by multiple proximal eQTLs1. A well-established approach to discover multiple QTLs with independent effects on a given phenotype relies on conditional analysis: new discoveries are made by conditioning on previous ones. In QTLtools, we implemented a conditional analysis scheme based on stepwise linear regression that is fast, accounts for multiple testing and automatically learns the number of independent signals per phenotype. Specifically, we implemented it as follows for both grouped and ungrouped phenotypes: ( 1 ) Initialization. We determine a nominal P value threshold of significance on a per-phenotype basis. To do so, we first perform a permutation pass as described above which gives us an adjusted P value per phenotype (or group of phenotypes) together with its most likely beta parameter values. Next, we determine the adjusted P value threshold corresponding to the targeted FDR level (for example, 5% FDR) and feed the beta quantile function (for example, R/q beta) with it to get a specific nominal P value threshold for each phenotype. Here, the beta quantile function allows us to use the Beta distribution in a reversed way: from adjusted P value to nominal P value. Note that the resulting nominal P value thresholds vary from one phenotype to the other depending on the complexity of the cis regions being tested and the effective number of independent tests they encapsulate. ( 2 ) Forward pass. We next learn the number of independent signals per phenotype using stepwise regressions with forward variable selection. More specifically, we start from the original phenotype quantifications and search for the variant in cis with the strongest association. When the corresponding nominal P value of association is below the threshold defined in step ( 1 ), we store the variant as additional and independent discovery and residualize its genotypes out from the phenotype quantifications. We then repeat these two steps until no more significant discovery is made: this immediately gives us the number of independent molQTLs together with a best candidate variant for each. ( 3 ) Backward pass. Finally, we try to assign nearby genetic variants to the various independent signals we discovered in step ( 2 ). To do so, we define a linear regression model that contains all candidate QTLs discovered so far in the forward pass: P ¼ Q1 þ y þ Qi þ y þ QR where R is the number of independent signals and {Q1, y, Qi, y, QR} are the corresponding best molQTL candidates. Then, we test all possible hypotheses by fitting this model Rx(L-R) times each time fixing { Q1, y, Qi 1, Qi þ 1,y, QR } and setting Qi as another variant in cis (L-R variants in cis not being a candidate molQTL times R independent signals). We then end up with a vector of R nominal P values for each variant in cis which allows us to determine the signal the variant belongs to by simply finding the smallest P value in this vector and comparing it to the significance threshold obtained in step ( 1 ).  Mapping distal molQTLs. Another common problem in the field of QTL discovery relates to mapping distal QTLs (that is, trans-QTL). This presents multiple computational and statistical challenges related to multiple testing, computational feasibility and confounding factors such as read misalignment, gene homology or incorrect gene location. In the context of this work; we only address two particular problems: how to correct for multiple testing and how to perform this analysis in reasonable running times. We solved this problem by testing all possible phenotype-variant pairs for association excluding all those in cis (that is, implying that the phenotype and the variant cannot be proximal, typically o5 Mb) using linear regressions with high computational performance as we do for cis mapping. In practice, we manage to perform B1.3 M linear regressions per second for 358 individuals on an AMD Opteron(tm) Processor 6,174 at 2.2 GHz. To minimize the RAM usage, the phenotype data is stored in memory and the genotype data streamed as we move along the genome and tested against all phenotypes at once. To minimize the size of the output files, we only report detailed information for associations below a given threshold (typically 10 5 for nominal P values); all those above are simply binned to have an idea of the overall P value distribution. Once the nominal pass done, we correct for multiple testing using one of these two approaches: ( 1 ) Full permutation scheme. We permute all phenotypes using the same random number sequence to preserve the correlation structure unchanged. By doing so, the only association we actually break in the data is between the genotype and the phenotype data. Then, we proceed with a standard association scan identical to the one used in the nominal pass. In practice, we repeat this for 100 permutations of the phenotype data. Then, we can proceed with FDR correction by ranking all the nominal P values in increasing order and by counting how many P values in the permuted data sets are smaller. This immediately gives an FDR estimate: if we have 500 P values in the permuted data sets being smaller than the 100th smallest nominal P value, we can then assume that the FDR for the 100 first associations is around 5% ( ¼ 500/(100 100)). ( 2 ) Approximate permutation scheme. To enable fast screening in trans, we also designed an approximation of the method described just above based on what we already do in cis. To make it possible, we assume that the phenotypes are independent and normally distributed (which can be enforced in practice). Then, we draw from the null by permuting only one randomly chosen phenotype, testing for associations with all variants in trans and storing the smallest P value. When we repeat this many times (typically 1,000 or 10,000 times), we effectively build a null distribution of the strongest associations for a single phenotype. We then make it continuous by fitting a beta distribution as we do in cis and use it to adjust every nominal P value coming from the initial pass for the number of variants being tested. To correct for the number of phenotypes being tested, we estimate FDR (using R/q value) again as we do in cis; that is onto the best adjusted P values per phenotype (one per phenotype). As a by-product, this also gives an adjusted P value threshold that we finally use to identify all phenotype-variant pairs that are whole-genome significant. In our experiments, this approach gives similar results to the full permutation scheme both in term of FDR estimates and number of discoveries (Supplementary Fig. 15).    Integrating molQTLs with functional annotations. QTLtools includes two\r\n  approaches to integrate molQTLs with functional annotations. First, it can measure the density of functional annotations around the genomic positions of molQTLs. To do so, we first enumerate all annotations within a given window around the molQTLs (by default 1 Mb). Then, we split this window into small bins (default 1 kb) and count the number of functional annotations overlapping each bin. This produces an annotation count per bin that can be then plotted to see if there is any peak or depletion around the molQTLs (Fig. 2e). Complementary to this density-based representation, QTLtools can also assess if the molQTLs overlap the functional annotations more often than what we expect by chance. Here, we mean by chance what is expected given the non-uniform distributions of molQTLs and functional annotations around the genomic positions of the molecular phenotypes. To do so, we first enumerate all the functional annotations located nearby (for example, within 1 Mb) a given molecular phenotype. In practice, for X phenotypes being quantified, we have X lists of annotations. And, for the subset Y of those having a significant molQTL, we count how often the Y molQTLs overlap the annotations in the corresponding lists: this gives the observed overlap frequency fobs(Y) between molQTLs and functional annotations. Then, we permute randomly many times (typically a 1,000 times) the lists of functional annotations across the phenotypes (for example, phenotype A may be assigned the list of annotations coming from phenotype B) and for each permuted data set, we count how often the Y molQTLs do overlap the newly assigned functional annotations: this gives the expected overlap frequency fexp(Y) between molQTLs and functional annotations. By doing this permutation scheme, we keep unchanged the distribution of functional annotations and molQTLs around molecular phenotypes. Now that we have the observed and expected overlap frequencies, we use a fisher test to assess how fobs(Y) and fexp(Y) differ. This gives an odd ratio estimate and a tow-sided P value which basically tells us first if there is enrichment or depletion and second how significant this is. Then, we typically plot these two quantities on a scatter plot with the x axis and y axis being the odd ratio and the significance of the enrichment/depletion, respectively (Fig. 2f). In our experiments, we use three types of functional annotations generated by ENCODE12 for lymphoblastoid cell lines: open chromatin regions given by DNAse footprinting, a union of all transcription factor binding sites assayed by ChIP-seq and transcribed regions as predicted by ChromHMM21.  Data availability. The Geuvadis RNA-seq data corresponds exactly to what has been generated in the original Geuvadis study, so please consult the Supplementary Materials of the paper1 for a more detailed description of the experimental protocol used for RNA-seq data generation. In our experiments, we focus our attention on a subset of 358 European samples for which we also have complete DNA sequence data generated as part of the phase 3 of the 1,000 Genomes project22. All variant sites with a minor allele frequency across all 358 samples below 5% or exhibiting more than two possible alleles have been removed which resulted in a set of 6,241,929 single-nucleotide variants and 843,851 short insertion-deletions or structural variants left for the analysis. All the raw sequence data can be downloaded from http://www.ebi.ac.uk/arrayexpress/files/E-GEUV-1/processed/(RNA-seq data) and ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ (for DNA-seq data)  The histone modification data set contains ChIP-seq across for 3 histone modifications across 47 European samples: H3K4me1, H3K4me3 and H3K27ac that are known to usually tag enhancers, promoters and active regions. Please consult this paper7 for more detailed description of the experimental protocols used for the ChIP-seq data generation. In this data set, the samples have been either sequenced or imputed from an Illumina OMNI2.5 M as part of the phase 1 of the 1,000 Genomes project11. Again, all variant sites with a minor allele frequency across the 47 samples below 5% or exhibiting more than two possible alleles have been removed which resulted in a set of 6,085,881 single-nucleotide variants and 606,344 short insertion-deletions or structural variants. All the raw ChIP-seq data can be downloaded from https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-3657/.  QTLtools is open source and available for download at https://qtltools.github.io/qtltools/.    Acknowledgements\r\n  This research is supported by grants from European Commission SYSCOL FP7, European Research Council, Louis Jeantet Foundation, Swiss National Science Foundation, SystemsX, the NIH-NIMH (GTEx) and Helse Sør Øst. The computations were performed at the Vital-IT Swiss Institute of Bioinformatics.    Author contributions\r\n  O.D., H.O., A.A.B. and E.T.D. designed the research. O.D. and H.O. implemented the methods. O.D. analysed data. A.F. and N.I.P. helped to test various functionalities. O.D. and E.T.D. supervised the research. O.D. wrote the paper.    Additional information\r\n  Supplementary Information accompanies this paper at http://www.nature.com/naturecommunications Competing interests: The authors declare no competing financial interests. Reprints and permission information is available online at http://npg.nature.com/reprintsandpermissions/ How to cite this article: Delaneau, O. et al. A complete tool set for molecular QTL discovery and analysis. Nat. Commun. 8, 15452 doi: 10.1038/ncomms15452 (2017).  Publisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/qtltools/qtltools",
    "publicationDate": "0",
    "authors": [
      "Olivier Delaneau",
      "Halit Ongen",
      "Andrew A. Brown",
      "Alexandre Fort",
      "Nikolaos I. Panousis",
      "Emmanouil T. Dermitzakis"
    ],
    "status": "Success",
    "toolName": "qtltools",
    "homepage": ""
  },
  "79.pdf": {
    "forks": 2,
    "URLs": ["github.com/ding-lab/BreakPointSurveyor,"],
    "contactInfo": ["lding@wustl.edu"],
    "subscribers": 17,
    "programmingLanguage": "Shell",
    "shortDescription": "A comprehensive pipeline to analyze and visualize structural variants",
    "publicationTitle": "BreakPoint Surveyor: a pipeline for structural variant visualization",
    "title": "BreakPoint Surveyor: a pipeline for structural variant visualization",
    "publicationDOI": "10.1093/bioinformatics/btx362",
    "codeSize": 147366,
    "publicationAbstract": "Summary: BreakPoint Surveyor (BPS) is a computational pipeline for the discovery, characterization, and visualization of complex genomic rearrangements, such as viral genome integration, in paired-end sequence data. BPS facilitates interpretation of structural variants by merging structural variant breakpoint predictions, gene exon structure, read depth, and RNA-sequencing expression into a single comprehensive figure. Availability and implementation: Source code and sample data freely available for download at https://github.com/ding-lab/BreakPointSurveyor, distributed under the GNU GPLv3 license, implemented in R, Python and BASH scripts, and supported on Unix/Linux/OS X operating systems. Contact: lding@wustl.edu Supplementary information: Supplementary data are available at Bioinformatics online.",
    "dateUpdated": "2017-08-24T16:50:37Z",
    "institutions": [
      "University of Texas MD Anderson Cancer Center",
      "Department of Genetics",
      "Department of Mathematics",
      "Department of Pediatrics",
      "McDonnell Genome Institute",
      "Department of Medicine",
      "Washington University School of Medicine"
    ],
    "license": "GNU General Public License v3.0",
    "dateCreated": "2017-02-06T17:45:14Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Bioinformatics     10.1093/bioinformatics/btx362   BreakPoint Surveyor: a pipeline for structural variant visualization     Matthew A. Wyczalkowski  4  5    Kristine M. Wylie  3  4    Song Cao  4  5    Michael D. McLellan  4    Jennifer Flynn  1    Mo Huang  4  5    Kai Ye  4    Xian Fan  0    l C. W    Li Ding  1  4  5  6    0  Department of Bioinformatics and Computational Biology, University of Texas MD Anderson Cancer Center ,  Houston, TX 77230 ,  USA    1  Department of Genetics    2  Department of Mathematics ,  Washington University in St. Louis, St. Louis, MO 63108 ,  USA    3  Department of Pediatrics    4  McDonnell Genome Institute    5  Oncology Division, Department of Medicine    6  Siteman Cancer Center, Washington University School of Medicine ,  St. Louis, MO 63110 ,  USA     2017   1  1  2   Summary: BreakPoint Surveyor (BPS) is a computational pipeline for the discovery, characterization, and visualization of complex genomic rearrangements, such as viral genome integration, in paired-end sequence data. BPS facilitates interpretation of structural variants by merging structural variant breakpoint predictions, gene exon structure, read depth, and RNA-sequencing expression into a single comprehensive figure. Availability and implementation: Source code and sample data freely available for download at https://github.com/ding-lab/BreakPointSurveyor, distributed under the GNU GPLv3 license, implemented in R, Python and BASH scripts, and supported on Unix/Linux/OS X operating systems. Contact: lding@wustl.edu Supplementary information: Supplementary data are available at Bioinformatics online.       1 Introduction\r\n  The expanding scale and diversity of human sequence data present opportunities to investigate structural variants (SVs) in the human genome with unprecedented resolution  (Feuk et al., 2006) . Viral integration, a type of SV where the inserted sequence has a viral origin, has been implicated in the initiation and progression of a number of cancers  (Martin and Gutkind, 2008) . Yet, the examination, interpretation, and visualization of SVs, especially at humanvirus boundaries ('breakpoints'), is complicated by the large number of features associated with such events, including their size, orientation, state (inserted, inverted, deleted, copied, translocated), number of copies, source of inserted DNA, and whether the event results in a gene fusion.  There are a number of steps in an SV calling pipeline, including data preprocessing, SV discovery, verification, annotation and visualization  (Guan and Sung, 2016) . Existing SV visualization tools include linear genome browsers which indicate breakpoint positions along a single reference genome, and tools which represent breakpoints explicitly as arcs connecting linear or circular genomic segments (see Supplementary Material for references). A major limitation of the latter is that large numbers of breakpoints result in a thicket of overlapping lines, making details of breakpoint structure difficult to discern. The toolkit presented here encodes breakpoints as points on a grid, with coordinates indicating the junction position along the chromosome or virus. Such an approach provides a clear representation of breakpoint clusters and promotes efficient visual inspection of integration events and their fine structures.  BreakPoint Surveyor (BPS) is a new enterprise-level pipeline for examining breakpoint predictions, together with their associated structural variation and gene context. Its rendering engine is coupled with a flexible pipeline to detect structural variants, and it accommodates a variety of toolsets and analyses of whole genome sequencing (WGS) and RNA-sequencing (RNA-Seq) data. We illustrate the ability of BPS to combine multiple data types from The Cancer Genome Atlas (TCGA), including breakpoint predictions, copy number, gene/exon annotation and relative gene expression, into a comprehensive visual representation of the structural variation associated with viral integration in tumors and their effects on expression of genes adjacent to the event. Online documentation includes two additional reference workflows to illustrate functionality with human/human breakpoints.    2 Description of pipeline\r\n  Starting with WGS and RNA-Seq BAM files, BPS multi-stage workflow yields illustrations of inter- and intra-chromosomal SV events through a series of sequential data processing steps (Supplementary Fig. S1a). The pipeline consists of a series of Unix shell scripts that invoke core apps written primarily in R and Python (Supplementary Fig. S1b).  Preliminary steps include realignment and breakpoint detection: (i) If investigating human-virus breakpoints, WGS data must be realigned to a reference which includes both human and specific virus sequences so that virus reads are properly mapped and used to detect the presence of viruses. (ii) Context-specific breakpoint detection (i.e. human-virus) must be performed on the sequence alignment data using any suitable algorithm  (Guan and Sung, 2016)  to yield breakpoint positions (see Supplementary Material), with discordant read pair, Pindel and contig assembly techniques illustrated.  Breakpoint analysis and impact evaluation then involves several tasks: (iii) We combine any breakpoints which occur within a given distance of one another (50 Kbp in this example) on the same chromosome pair into a single integration event; positions of such events can be further refined manually. This clustering process defines the list of regions ('PlotList') for further analysis and visualization. (iv) Optionally, RNA-Seq-based expression values are calculated for genes near integration events, or preprocessed (e.g. TCGA RSEM) expression data are used. The algorithm compares expression of genes in cases against their corresponding controls to discern over- or under-expression (see Supplementary Material). (v) Breakpoints may be more precisely defined with other SV callers or contig assembly (see Supplementary Material).  Finally, figures are generated: (vi) Structure plots consist of four panels (Fig. 1a, Supplementary Fig. S2) illustrating breakpoint positions, copy number, and gene annotations for a selected target region. (vii) The expression levels of genes in the vicinity of integration events, relative to a population of controls, are illustrated in a gene expression plot (Fig. 1b, Supplementary Fig. S3). Figure 1 provides a comprehensive visual summary of several important features of virus integration events (see also Supplementary Figs S2-S8 for examples from additional samples and workflows).  BPS is a pipeline for integrating large, complex data sets with a scalable architecture supporting analysis from individual samples on a laptop to very large data sets on compute clusters. Additional example workflows provided with the online distribution demonstrate how BPS can be applied to the analysis of non-viral structural variants.    Acknowledgements\r\n  We acknowledge critical reading by Kuan-lin Huang and Robert Jay Mashl and valuable discussions with members of the TCGA Research Network.    Funding\r\n  This work was supported by the National Cancer Institute [R01CA178383, R01CA180006, 1U24CA211006-01, and 1U24CA210972-01 to L.D., R01CA172652 to K.C.]; and National Human Genome Research Institute [U01HG006517 to L.D.].  Conflict of Interest: none declared.    ",
    "sourceCodeLink": "https://github.com/ding-lab/BreakPointSurveyor",
    "publicationDate": "0",
    "authors": [
      "Matthew A. Wyczalkowski",
      "Kristine M. Wylie",
      "Song Cao",
      "Michael D. McLellan",
      "Jennifer Flynn",
      "Mo Huang",
      "Kai Ye",
      "Xian Fan",
      "l C. W",
      "Li Ding"
    ],
    "status": "Success",
    "toolName": "BreakPointSurveyor",
    "homepage": ""
  },
  "23.pdf": {
    "forks": 1,
    "URLs": [
      "github.com/ZFMK/COGNATE",
      "standage.github.io/AEGeAn",
      "ftp.ncbi.nlm.nih.gov/genomes/",
      "www.zfmk.de/en/COGNATE",
      "ftp.ncbi.nlm.nih.gov/genomes/all/",
      "github.com/ihh/gfftools",
      "rstb.royalsocietypublishing.org/highwire/filestream/32237/field_highwire_adjunct_files/0/rstb20140331supp1.xlsx",
      "www.ncbi.nlm.nih.gov/genome/annotation_euk/Apis_mellifera/103/"
    ],
    "contactInfo": [
      "j.wilbrandt@leibniz-zfmk.de",
      "bmisof@uni-bonn.de"
    ],
    "subscribers": 3,
    "programmingLanguage": "Perl",
    "shortDescription": "",
    "publicationTitle": "COGNATE: comparative gene annotation characterizer",
    "title": "COGNATE: comparative gene annotation characterizer",
    "publicationDOI": "10.1186/s12864-017-3870-8",
    "codeSize": 922,
    "publicationAbstract": "Background: The comparison of gene and genome structures across species has the potential to reveal major trends of genome evolution. However, such a comparative approach is currently hampered by a lack of standardization (e.g., Elliott TA, Gregory TR, Philos Trans Royal Soc B: Biol Sci 370:20140331, 2015). For example, testing the hypothesis that the total amount of coding sequences is a reliable measure of potential proteome diversity (Wang M, Kurland CG, Caetano-Anollés G, PNAS 108:11954, 2011) requires the application of standardized definitions of coding sequence and genes to create both comparable and comprehensive data sets and corresponding summary statistics. However, such standard definitions either do not exist or are not consistently applied. These circumstances call for a standard at the descriptive level using a minimum of parameters as well as an undeviating use of standardized terms, and for software that infers the required data under these strict definitions. The acquisition of a comprehensive, descriptive, and standardized set of parameters and summary statistics for genome publications and further analyses can thus greatly benefit from the availability of an easy to use standard tool. Results: We developed a new open-source command-line tool, COGNATE (Comparative Gene Annotation Characterizer), which uses a given genome assembly and its annotation of protein-coding genes for a detailed description of the respective gene and genome structure parameters. Additionally, we revised the standard definitions of gene and genome structures and provide the definitions used by COGNATE as a working draft suggestion for further reference. Complete parameter lists and summary statistics are inferred using this set of definitions to allow down-stream analyses and to provide an overview of the genome and gene repertoire characteristics. COGNATE is written in Perl and freely available at the ZFMK homepage (https://www.zfmk.de/en/COGNATE) and on github (https://github.com/ZFMK/COGNATE).Conclusion: The tool COGNATE allows comparing genome assemblies and structural elements on multiples levels (e.g., scaffold or contig sequence, gene). It clearly enhances comparability between analyses. Thus, COGNATE can provide the important standardization of both genome and gene structure parameter disclosure as well as data acquisition for future comparative analyses. With the establishment of comprehensive descriptive standards and the extensive availability of genomes, an encompassing database will become possible.",
    "dateUpdated": "2017-10-18T18:26:04Z",
    "institutions": [
      "Institut für Biologie I (Zoologie)",
      "Zentrum für Molekulare Biodiversitätsforschung (zmb)"
    ],
    "license": "https://github.com/ZFMK/COGNATE/blob/master/LICENSE",
    "dateCreated": "2017-03-31T13:44:13Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     Wilbrandt et al. BMC Genomics     10.1186/s12864-017-3870-8   COGNATE: comparative gene annotation characterizer     Jeanne Wilbrandt  j.wilbrandt@leibniz-zfmk.de  1    Bernhard Misof  bmisof@uni-bonn.de  1    Oliver Niehuis  0    0  Abteilung Evolutionsbiologie und Ökologie, Albert-Ludwigs-Universität Freiburg, Institut für Biologie I (Zoologie) ,  Freiburg ,  Germany    1  Zoologisches Forschungsmuseum Alexander Koenig (ZFMK), Zentrum für Molekulare Biodiversitätsforschung (zmb) ,  Bonn ,  Germany     2017   18    19  6  2017    31  3  2017     Background: The comparison of gene and genome structures across species has the potential to reveal major trends of genome evolution. However, such a comparative approach is currently hampered by a lack of standardization (e.g., Elliott TA, Gregory TR, Philos Trans Royal Soc B: Biol Sci 370:20140331, 2015). For example, testing the hypothesis that the total amount of coding sequences is a reliable measure of potential proteome diversity (Wang M, Kurland CG, Caetano-Anollés G, PNAS 108:11954, 2011) requires the application of standardized definitions of coding sequence and genes to create both comparable and comprehensive data sets and corresponding summary statistics. However, such standard definitions either do not exist or are not consistently applied. These circumstances call for a standard at the descriptive level using a minimum of parameters as well as an undeviating use of standardized terms, and for software that infers the required data under these strict definitions. The acquisition of a comprehensive, descriptive, and standardized set of parameters and summary statistics for genome publications and further analyses can thus greatly benefit from the availability of an easy to use standard tool. Results: We developed a new open-source command-line tool, COGNATE (Comparative Gene Annotation Characterizer), which uses a given genome assembly and its annotation of protein-coding genes for a detailed description of the respective gene and genome structure parameters. Additionally, we revised the standard definitions of gene and genome structures and provide the definitions used by COGNATE as a working draft suggestion for further reference. Complete parameter lists and summary statistics are inferred using this set of definitions to allow down-stream analyses and to provide an overview of the genome and gene repertoire characteristics. COGNATE is written in Perl and freely available at the ZFMK homepage (https://www.zfmk.de/en/COGNATE) and on github (https://github.com/ZFMK/COGNATE).Conclusion: The tool COGNATE allows comparing genome assemblies and structural elements on multiples levels (e.g., scaffold or contig sequence, gene). It clearly enhances comparability between analyses. Thus, COGNATE can provide the important standardization of both genome and gene structure parameter disclosure as well as data acquisition for future comparative analyses. With the establishment of comprehensive descriptive standards and the extensive availability of genomes, an encompassing database will become possible.    Comparative genomics  Protein-coding genes  Gene annotation  Gene repertoires  Gene structure   Standardization       Background\r\n  As more and more sequenced genomes become available, studying the commonalities and differences in the structure of genes and genomes has become an exciting and a rapidly expanding research field. Examples of comparative studies of intron size are those published by Yandell et al. [ 1 ], Moss et al. [ 2 ], and Zimmer et al. [ 3 ], who found that intron length evolution behaves clocklike, that ancient bursts of repetitive elements can be responsible for an unusual intron length distribution, and that there is a trend towards shorter introns in the evolution of land plants, respectively. These studies were restricted to a rather unrepresentative selection of animal, fish, and plants species, respectively, due to the lack of genome sequences. Studies with much larger species numbers and a broader taxonomic coverage are becoming feasible.  Elliott &amp; Gregory [ 4 ] recently published a seminal meta-analysis of the genome and gene summary statistics of animals, land plants, fungi, and 'protists', relying on 521 species. The large number of species and genomes considered in their analysis allowed the authors to robustly detect statistical trends in genome evolution, such as a positive correlation between genome size and both gene and intron content, while taking phylogenetic relationships into account. These trends have been previously observed (e.g., [ 5, 6 ]), but were based on a much smaller taxonomic sampling. Yet, despite the evidently improved availability of sequenced genomes, Elliott &amp; Gregory [ 4 ] struggled with a lack of standards in the disclosure of genome characteristics when compiling data for their analyses; they evaluated 28 parameters of the genomes of 521 species (see Supplement of [ 4 ]), for which only 48% of all possible values were provided in the publications to the respective genomes (cf. Fig. 2) and thus available for the meta-analysis.  The lack of standardization in the publication of gene structure characteristics is a general problem. Not only are some basic gene content and structure statistics frequently presented in a non-standardized manner, it often remains unclear whether or not terms describing gene structure were consistently applied to achieve comparability between analyses. For example, gene counts may or may not be inferred from tallying all predicted transcripts, thus bearing the risk of including alternative transcripts or isoforms as pseudo-replicates in metaanalyses. Furthermore, GC content may be reckoned without respect to IUPAC base-calling ambiguity in the total sequence lengths, which predicates the resulting value on sequencing and assembly quality. Finally, it can be difficult to trace inconsistencies in the use of terms, like 'exon' versus 'coding sequence (CDS)' despite existing standard vocabularies like the Sequence Ontology [ 7 ]. Clearly, comparability and traceability of published data can greatly benefit from standardized analyses of genome organization and gene structure (see also [ 8 ]).  A partial explanation for the lack of a standardized analysis and presentation of fundamental genomic features referring to protein-coding genes is a lack of software that infers the desired statistics. Available tool suites like BEDtools [ 9 ], genomeTools [ 10 ], AEGeAN,1 and gfftools2 are mostly intended for processing rather than describing annotations. While various programming libraries, such as BioPerl3 and SeqAn [ 11 ] provide suitable methods, their usage is demanding to researchers without programming experience and fosters the development of custom scripts by researchers with programming skills. The former likely limits the number of scientists who can infer the desired statistics, while the latter increases the risk of inferring incompatible results due to errors and/or misconceptions in analyses and definitions. Thus, there is a need for easy to use software that provides the facility to examine genome annotations for a wealth of structural features of the protein-coding gene repertoire in a concise way and that provides basic and standardized statistics as well as results suitable for downstream applications.  Here we present the tool COGNATE, a Comparative Gene Annotation Characterizer. It fills the above identified gap of software for structural characterization of the annotated protein-coding gene repertoire of a genome. COGNATE allows a quick and easy extraction of basic genome features and gene repertoire data; it is thus a tool to primarily describe a genome and its annotated protein-coding gene repertoire, which is an essential prerequisite for comparative analyses. Given the ongoing genome sequencing efforts, especially by large consortia like 10 k [ 12 ] and i5k [ 13 ], we see an increasing demand for a standardization of large-scale comparisons of genome and gene structure.    Implementation\r\n  With COGNATE, we promote a tool to simultaneously analyze a given protein-coding gene annotation and the corresponding assembled sequences of a genome, here referred to as scaffold or contig sequence (SCS). An overview of the software's input, work flow, analyzed parameters, and output is visualized in Fig. 1. A complete list of analyzed parameters is given in Additional file 1, a glossary with the definitions of all terms used in this publication and by COGNATE is provided in Additional file 2.  COGNATE requires as input: (1) a gff file in GFF3 format4 containing the annotation of protein-coding genes; (2) a fasta file, containing the corresponding genomic nucleotide sequences, which are exploited to infer the length, GC content, and amino acid sequences of the assembled SCSs and of the predicted protein-coding genes, respectively. The gene annotation has to include at least (See figure on previous page.) Fig. 1 Overview of the information flow in the software package COGNATE. The Perl script COGNATE requires two files per run as input (blue): a fasta file containing the assembled nucleotide sequences and a GFF3 file with the protein-coding gene annotation information. The input (blue) is used to analyze genomic and genic features (green) on the level of assembly, SCSs, transcripts, CDSs, exons, and introns. Each complex of analyzed features is evaluated individually and the analyzed parameters are condensed in a step-wise manner by calculating means and medians (red). As output (yellow), 21 files are generated, of which all except two are in TSV format (the exceptions are: 00, protein fasta; 20, bash commands). The output files are split according to the analyzed features and parameters. All data files (02-13) are ordered by the ID of the respective feature. BATCH files (14-20) contain one entry line per genome and thus data of multiple COGNATE runs to facilitate direct comparisons of genomes. CDS: CoDing Sequence; GFF: Generic Feature Format; SCS: Scaffold or Contig Sequence; TSV: Tab-Separated Values the features 'gene', 'mRNA', and 'exon', as provided by, for example, BRAKER1 [ 14 ] and MAKER2 [ 15 ]. Thus, the analysis of partial and pseudogenes depends on their annotation in the analyzed gff file; non-coding genes (i.e., genes without mRNA) are not considered in the analysis. Further technical requirements are several standard Perl libraries as well as the GAL::Annotation and GAL::List libraries to allow gff-handling. The latter two libraries are available from the Sequence Ontology Project5 and are also included in the COGNATE software package. COGNATE is written in Perl and has been tested under Ubuntu 12.04 and 14.04. COGNATE analyzes one genome at a time. Providing multiple genomes (i.e., a batch) for serial processing is possible with a special input file (see README, Additional file 4). Serial, single-threaded processing leads to a linear relationship of processed genomes and required time. As a gauge, the analysis of the latest Apis mellifera gene set (see Results and Discussion), which has a genome size of 250.3 Mb and 10,733 annotated protein-coding genes, takes with COGNATE up to 4 h, using up to 600 MiB RAM. For comparison, COGNATE requires a very similar amount of time for the analysis of the gene set6 of Ixodes scapularis (genome size: 1765.4 Mb, 20,467 annotated protein-coding genes). A benchmark comparison of COGNATE to other software, such as genomeTools [ 12 ], AEGeAN1, or gfftools2, is not meaningful due to major differences between these software packages in focus and aim. At the moment, no tool yields the wide array of metrics that COGNATE delivers by default.  COGNATE infers the following major metrics (for a full list of the 296 parameters, see Additional file 1): summary counts of the analyzed features, including L90pcG7, i.e., the number of SCSs needed to cover 90% of all annotated protein-coding genes; strandedness of transcripts and features (CDSs, exons, and introns); lengths and length statistics (nucleotide/amino acid sequences), including N50/L50, 75/L75, N90/L90; intron length distribution [ 16 ]; percental GC content statistics in two different ways, namely using a calculation that explicitly considers IUPAC ambiguity codes (G, C, S per total length excluding N, R, Y, K, M, B, D, H, V); using the previously prevailing calculation of GC per total length, which is inappropriate for genome comparisons due to its dependence on assembly quality; statistics of CpG dinucleotide depletion (CpG observed/expected), normalized by C and G content of the respective region [ 17 ]; density statistics (ratio of the length of a feature covered by another, number-wise); coverage statistics (ratio of the length of a feature covered by another, length-wise).  In summary, the output parameters can be classified as computations of the eight above major metrics or feature types, some with child types (e.g., added length), of six structural entities (e.g., assembly/annotation, SCSs, introns). In other words, parameters are inferred on several levels. For example, the total count of CDSs in analyzed transcripts is given for the entire assembly as well as on a per transcript basis. For the latter, COGNATE also calculates the mean and median count of CDSs per transcript as well as the mean/median of these medians over all transcripts. As another example, the intron density of a gene is calculated as the total number of introns divided by the length of the gene (i.e., genomic length of the transcript, including introns and exons) and also given as mean/median intron density per gene over the whole annotation. For each gene, only one representative (optionally the longest [default], shortest, or median-length) transcript is evaluated.  The analysis is independent of homology hypotheses (i.e., not limited to gene families), thus comprising information on a genome's entire annotated proteincoding gene repertoire.  As output, COGNATE provides various result tables in TSV format: a concise overview (summary) of measured variables; lists of all measured variables referring to features of a given SCS, transcript, or individual CDSs, exons, or introns, respectively; 'batch' files, which contain one line of summary statistics per analyzed genome. There are individual files for general genome data and means and medians of SCS and transcript data, respectively; a component size overview (i.e., the added length [in bp] of all coding and intron sequences, respectively), which offers a basis for a comparison of these values with statistics of other genomic features inferred with other tools, for example non-coding elements;  All above specified files (except the one providing an overview) facilitate tests for correlations between parameters within and among genomes. The output files are formatted specifically to allow easy import in statistical software, such as R [ 18 ] and SPSS [ 19 ]. COGNATE also provides a fasta file ('analyzed_transcripts') containing the predicted amino acid sequences inferred from the CDSs of the one analyzed transcript per gene. This file can be used, for example, as input for BUSCO [ 20 ] to test for the completeness of the gene set, which is facilitated by the ready-made bash commands supplied in the 'bash commands' text file. The generation of all output files can be controlled directly by the user.  The output of COGNATE can be used in manifold analyses, ranging from a descriptive characterization to an in-depth comparative analysis of gene organization across multiple genomes. This is further exemplified in the discussion.    Results and discussion\r\n  It is an essential feature of COGNATE to provide not only descriptive statistics but also the complete primary data, since \u201can over-reliance on simple summary statistics [\u2026] can obscure real biological trends and differences\u201d ([ 2 ], p. 1191). Apart from other already mentioned potential applications, COGNATE output can be used to study the variability of gene structure within a genome and to compare it with that in other genomes. In such an instance, the list of transcript features can be exploited to analyze the range of exon lengths, intron lengths, and their distribution over genes of a certain GC content. Another example would be a comparison of GC content in coding and non-coding regions of genes across a genome. Having the characteristics of a gene repertoire at hand, they can be compared to those of other species and used in phylogenomic analyses (e.g., [ 21 ]). COGNATE results can also serve as a starting point to find genes of interest and relate them to functions, e.g., looking for very long or short genes or investigating genes containing exactly two CDSs. Hypotheses like 'Flying birds have shorter introns than birds of non-volant sister lineages due to energetic demands of powered flight' [ 6 ], 'Evolutionary changes in intron lengths correlate with co-expression of genes' [ 22 ], or 'Strategies of splice-site recognition are influenced by differences in GC content between exons and introns' [ 23 ] could thereby be tested in more detail. Thus, COGNATE provides data to facilitate downstream analyses, and in addition, provides summary statistics that can help standardizing genome parameter disclosure.  Missing standardization in comparative genomics can easily lead to problems in meta-analyses and consequently result in biased conclusions. As Elliott &amp; Gregory [ 4 ] noted during their tremendous effort of data compilation, there are problems of standardization in terms of parameter listing and source disclosure as well as of definitions of descriptive terms. Some of these subtle and sometimes deemphasized problems are elucidated here in more detail to raise and sustain the awareness for them.  One problem in compiling data for meta-analyses are missing values. The data matrix compiled by Elliott &amp; Gregory (Supplement of [ 1 ]12) contains overall 52% missing values due to incomplete data disclosure by publications or missing entries in databases. This lack of data introduces a potential bias in correlative analyses of genome structures, which has not been systematically investigated. Thus, without in-depth parameter disclosure, the enormous effort of collecting data from open sources for genome and gene structure comparison potentially yields unreliable results. The general distribution of missing data in the matrix compiled by Elliott &amp; Gregory [ 1 ] is noteworthy in that the GC content is almost always given while values related to gene structure including intron size values are missing for half of the genomes in the data matrix (see Fig. 2). It is surprising to find that for 38% of the genomes in their dataset no assembly genome size was included in the original publications or databases. To further illustrate the problem of missing data in comparative genomics, we analyzed the genome (version 4.5, downloaded 31 August 2015, from NCBI8) and latest protein-coding gene annotation (release 103, downloaded 20 March 2017 from NCBI9) of Apis mellifera. Compared to the 144 values recorded by COGNATE that can readily be given as a single number, the publications covering the official gene sets 1 [ 24 ] and 3.2 [ 25 ] offer only eight and nine comparable values, respectively; NCBI offers a report site10 for the most recent annotation release (103), where we found 14 comparable values (Additional file 1, sheet 2). The obtained values differ on a small scale (for example, the count of protein-coding genes differs by 5 for a total of circa 10,730), most likely due to the different annotation versions or deviating definitions. Generally, COGNATE can help to mitigate the problem of missing values by easing their acquisition and has the benefit of providing tractable values with a transparent method.  Problems of fuzzy terminology become apparent when, for example, the coding amount (i.e., the total length of protein-coding sequences within a genome) is given in exonic megabases (Mb) (Fig. 2; [ 4 ]). Given the functional and structural similarity of exons and CDSs and their often complete overlap in automated annotations, it is an understandable, yet potentially misleading lack of differentiation. In contrast to CDSs, annotated exons can include untranslated regions (UTRs) and stop codons; not every exon is a coding sequence [ 26 ]. Most of the automated annotations do not include UTRs, which are difficult to delineate de novo (e.g., [ 27, 28 ]); nevertheless, a future project is to include the analysis of UTR annotations in COGNATE. Thus, in this instance, it remains unclear in which form exons and CDSs were evaluated and contributed to a summary statistic. With the above example, we are illustrating why we stress the importance of clear definitions and applications of these to genome and gene structure characterizations. Accordingly, COGNATE differentiates between CDSs and exons, but it can only be as accurate as the given annotation. For a complete list of our definitions, compared to Sequence Ontology terms 11, see the glossary in Additional file 2. The problems of defining a universally needed term such as 'gene' (described in [ 29 ]) as well as the various ways and needs of gene annotation [ 30 ] render the ongoing efforts of finding precise and useful definitions both essential and exacting.  Another problem of terminological and methodological nature is the widespread use of means as descriptive summary statistic. Since many gene structure features are not normally distributed within a genome, the mean is an inappropriate summary statistic of these features. Yet, in many investigations, only the mean is calculated as a summary statistic of gene structure features (see [ 4 ] as well as the publications cited therein). Doing so can bias analyses and severely mislead comparisons between genomes, especially when one is represented by a mean, the other by a median. To illustrate this, we used results of COGNATE from analyzing the latest gene set of Apis mellifera (see above) and compared the obtained values of mean and median of exon size and intron size per transcript, respectively (Fig. 3, data in Additional file 3). In normally distributed data, means and medians are expected to be (nearly) identical, which is clearly not the case in A. mellifera. COGNATE calculates both means and medians for a wealth of parameters.  A third example of unclear usage of terms relates to the evaluation of intron density. The two above evaluated parameters - exon size and intron size per transcript together with intron density per transcript can be understood as a proxy for gene structure, as demonstrated by Yandell et al. [1] 1 ], and are thus of great interest in structural gene characterization. Note however that intron density as calculated by Yandell et al. [1] 1 ] relates to protein length (i.e., count of introns/protein length). We advocate (and implemented in COGNATE) the relation of intron density to gene length as described above, since proteins as well as mature mRNAs are spliced and thus intron-free.  Aside from reporting important insights, Elliott &amp; Gregory [ 4 ] advocated the need for standardization in large-scale comparisons of genomes. The inevitable problems of analyzing datasets with missing data could, in the future, be extenuated by a common, comprehensive set of basic parameters published together with genomic data. When publishing a genome and its annotation of protein-coding genes, it would be most beneficial to attach the complete set of COGNATE results to it to avoid problems resulting from changing versions of genomes and/or annotations. A set of standard metrics to advance standardization of parameter publishing was proposed by Elliott &amp; Gregory [ 4 ], including \u201cdetails of base pair composition, gene number, intron number and size, total repeat content, and TE abundance, diversity and activity\u201d ([ 4 ], p. 8). Many other parameters can and should be used to describe the features of a genome completely, most of which go beyond the scope of COGNATE (e.g., properties of repetitive elements). Regarding protein-coding genes, we suggest to cover the descriptive parameters more broadly and to provide the following parameters as a minimum: assembly size (i.e., total added length of all SCSs, with and without Ns), assembly GC content (with and without ambiguity), gene count, median transcript length (tallying one representative transcript per gene), median CDS length, median CDS count per transcript (i.e., density), median CDS length per gene (i.e., coverage), coding amount (i.e., total added length of all CDSs), intron count, median intron length, median intron count per transcript (i.e., density), median intron length per gene (i.e., coverage), intron amount (i.e., total added length of all introns).  Following the establishment of standard parameters of gene model properties and the institution of a standard tool to acquire these, the next desirable step is the constitution of a \u201ccurated, user-friendly, open-access database [to] make this information accessible and usable in largescale comparative analyses\u201d ([ 4 ], p. 8).  Finally, we would like to draw the readers' awareness also to a frequently encountered problem in comparative genomics: the source of primary sequence data or the version of gene annotations are often not clearly stated, which hampers reproducibility of the published analyses. Therefore, we emphasize the need for disclosing used databases, genome versions, and other source information in combination with data and results.    Conclusion\r\n  Comparative meta-analyses of gene and genome characteristics, testing, for example, whether potential proteome diversity is reliably reflected by the total amount of coding sequences [ 31 ], rely on descriptive statistics of primary genome sequences and gene annotations. However, comprehensive standard statistics of genome organization and gene structure have not been fully or consistently defined with the effect that they are inconsistently collected or often incomplete. Due to this problem, comparative meta-analyses of gene and genome characteristics can be severely handicapped and are potentially unreliable. Obviously, this problem can be solved with the routine application of standard tools. The here presented software COGNATE allows effortless and flexible parameter disclosure as well as genome comparisons within its designated scope. Its merits include the comprehensive evaluation of an extensive set of standard and non-standard parameters of protein-coding genes, the provision of both primary data and summary statistics, and the use of explicit term definitions. COGNATE was developed in the hope to further promote and ease comparative studies, which should eventually yield insights into the evolution of genomes and gene repertoires.    Availability and requirements\r\n  COGNATE is provided as a package, including source code, helper scripts (e.g., to check the presence of required Perl libraries), example data, GAL libraries, and manual at the ZFMK website and together with this publication as Additional file 4.  Project name: COGNATE Project home page: https://www.zfmk.de/en/COGNATE and https://github.com/ZFMK/COGNATE Operating system(s): platform independent Programming language: Perl Other requirements: GAL libraries (included) License: GNU GPLv3  The datasets analyzed during the current study are available in the NCBI RefSeq repositories7,8 and from the supplement12 of [ 4 ].  Endnotes  1Standage DS. AEGeAn: an integrated toolkit for analysis and evaluation of annotated genomes. 2010-2015. http://standage.github.io/AEGeAn. Last accessed 20 March 2017.  2GitHub: Holmes I. gfftools. 2011. https://github.com/ihh/gfftools. Last accessed 20 March 2017.  3The BioPerl Project. 2016. http://bioperl.org. Last accessed 20 March 2017.  4The Generic Model Organism Database: GFF format definition. 2016. http://gmod.org/wiki/GFF3. Last accessed 20 March 2017.  5The Genome Annotation Library. 2016. http://www.sequenceontology.org/software/GAL.html. Last accessed 20 March 2017.  6Data of Ixodes scapularis: NCBI: FTP directory of the Ixodes scapularis genome version JCVI_ISG_i3_1.0 and the corresponding protein-coding gene annotation (NCBI RefSeq). 2017. ftp://ftp.ncbi.nlm.nih.gov/genomes/all/ GCF/000/208/615/GCF_000208615.1_JCVI_ISG_i3_1.0/. Last accessed 20 March 2017.  7L90pcG, the count of SCSs necessary to cover 90% of the annotated protein-coding genes in an assembly, is here, to our knowledge, explicitly termed for the first time. Similar metrics have been described in other publications, for example, the \u201cnumber of whole genome CARs [Contiguous Ancestral Regions] that cover 90% of one-to-one orthologous families\u201d ([32], SOM, page 57). Although the notation of L for a number (instead of a length) appears to be counter-intuitive, we deliberately decided to follow the already established convention of N50 and L50, with N50 designating \u201cmaximum length L such that 50% of all nucleotides lie in contigs (or scaffolds) of size at least L\u201d [33], and L50 designating the \u201cnumber of sequences evaluated at the point when the sum length exceeds 50% of the assembly size\u201d (Bradnam K. ACGT. 2015. http://www.acgt.me/blog/2015/6/11/l50-vs-n50-thats-another-fine-mess-that-bioinformaticsgot-us-into. Last accessed 23 May 2017).  8NCBI: FTP directory of the Apis mellifera genome version 4.5 (NCBI RefSeq). 2016. ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/195/GCF_000002195.4_ Amel_4.5/. Genome file downloaded 31 August 2015. Last accessed 20 March 2017.  9NCBI: FTP directory of the Apis mellifera annotation release 103. 2017. ftp://ftp.ncbi.nlm.nih.gov/genomes/ Apis_mellifera/GFF/. Annotation file downloaded 20 March 2017. Last accessed 20 March 2017.  10NCBI: NCBI Apis mellifera Annotation Release 103 report site. 2016. https://www.ncbi.nlm.nih.gov/genome/annotation_euk/Apis_mellifera/103/. Last accessed 20 March 2017.  11The Sequence Ontology. 2016. http://www.sequen ceontology.org. Last accessed 20 March 2017.  12Elliott TA, Gregory TR. Supplement 1 - Genome data used in the analyses. 2015. http://rstb.royalsocietypublishing.org/highwire/filestream/32237/field_highwire_adjunct_files/0/rstb20140331supp1.xlsx. Last accessed 20 March 2017.    Additional files\r\n  Additional file 1: Parameter table. List of parameters recorded by COGNATE. The first sheet of this table contains all 296 parameters evaluated by COGNATE, including the output file in which to find them and explanatory comments. Sorting for parameters, the individual feature ('of') or the feature location ('per'), and files allows to quickly find a parameter of interest. The second sheet contains a comparison of the values recorded by COGNATE when analyzing the latest annotation of the Apis mellifera genome (genome version 4.58, annotation release 1039) to those values given in the publications of the official gene sets version 1 [ 24 ] and 3.2 [ 25 ] and in the annotation report by NCBI11. As an addition, we included the results of GenomeTools' 'gt stat' command applied to the annotation release 103 GFF file for comparison. (XLSX 43 kb) Additional file 2: Definition table. Glossary and definitions used by COGNATE. This document contains the definitions used by COGNATE and in this manuscript for structural entities and measured parameters. Where available, we added matching Sequence Ontology terms. (PDF 110 kb) Additional file 3: Result table. COGNATE results of analyzing exon and intron lengths of Apis mellifera. This data sheet contains the mean and median lengths of exons and introns, which are part of the 10,733 transcripts analyzed by COGNATE (default run, i.e., using the longest of each gene's alternative transcripts). In total, 76,276 exons and 65,543 introns were taken into account. The data is visualized in Fig. 2. (XLSX 225 kb) Additional file 4: The COGNATE package. This archive file contains the COGNATE package, including Perl scripts, Additional file 1: Parameter table, Readme, example data and output, and the GAL library. (ZIP 566 kb) Abbreviations Bp: Nucleotide basepairs; CDS: Coding sequence; CpG o/e: Cytosine-guanine dinucleotides observed/expected; GC: Guanine, cytosine; Mb: Megabases (1 Mb = 1000 basepairs [bp]); MiB: Megabinary byte (1 MiB = 1,048,576 bytes); RAM: Random access memory ('working memory' of a computer); SCS: Scaffold or contig sequence; UTR: Untranslated region Acknowledgements We thank Malte Petersen, Jan Philip Oeyen, and Tanja Ziesmann for beta-testing COGNATE and Joshua D Gibson as well as three anonymous reviewers for helpful feedback on the manuscript. We further acknowledge the students of the Leibniz Graduate School on Genomic Biodiversity Research, the i5K community, and especially Anna Childers, for valuable feedback on ideas put forth in this study. JW thanks Barry Moore for help implementing GAL in the software package COGNATE. Finally, BM, JW and ON thank the German Research Foundation for support of this study and acknowledge the Leibniz Association for funding the Graduate School on Genomic Biodiversity Research. Funding This study was supported by the Leibniz Graduate School on Genomic Biodiversity Research and by the German Research Foundation (MI 649/16-1, NI-1387/3-1). The funding agencies did not influence the design of the study, the collection, analysis, and interpretation of data, or the manuscript writing. Authors' contributions JW conceived this study. BM, JW, and ON designed the study. JW developed the software package. BM, JW, and ON wrote the manuscript. All authors read and approved the final manuscript.  Authors' information During the becoming of this study, JW was a PhD candidate in entomological phylogenomics. Amidst the quest to tackle insect genomes and evaluate their commonalities and differences, the lack of a very basic tool became apparent and pressing. Thus, COGNATE was written to establish such a tool and to subsequently provide it to the community. Ethics approval and consent to participate Not applicable.  Consent for publication Not applicable.  Competing interests The authors declare that they have no competing interests.    Publisher\u2019s note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/ZFMK/COGNATE",
    "publicationDate": "0",
    "authors": [
      "Jeanne Wilbrandt",
      "Bernhard Misof",
      "Oliver Niehuis"
    ],
    "status": "Success",
    "toolName": "COGNATE",
    "homepage": ""
  },
  "53.pdf": {
    "forks": 51,
    "URLs": [
      "github.com/lh3/wgsim",
      "github.com/bergmanlab/mcclintock"
    ],
    "contactInfo": [],
    "subscribers": 11,
    "programmingLanguage": "C",
    "shortDescription": "Reads simulator",
    "publicationTitle": "McClintock: An Integrated Pipeline for Detecting Transposable Element Insertions in Whole-Genome Shotgun Sequencing Data",
    "title": "McClintock: An Integrated Pipeline for Detecting Transposable Element Insertions in Whole-Genome Shotgun Sequencing Data",
    "publicationDOI": "None",
    "codeSize": 131,
    "publicationAbstract": "Transposable element (TE) insertions are among the most challenging types of variants to detect in genomic data because of their repetitive nature and complex mechanisms of replication . Nevertheless, the recent availability of large resequencing data sets has spurred the development of many new methods to detect TE insertions in whole-genome shotgun sequences. Here we report an integrated bioinformatics pipeline for the detection of TE insertions in whole-genome shotgun data, called McClintock (https://github.com/bergmanlab/mcclintock), which automatically runs and standardizes output for multiple TE detection methods. We demonstrate the utility of McClintock by evaluating six TE detection methods using simulated and real genome data from the model microbial eukaryote, Saccharomyces cerevisiae. We find substantial variation among McClintock component methods in their ability to detect nonreference TEs in the yeast genome, but show that nonreference TEs at nearly all biologically realistic locations can be detected in simulated data by combining multiple methods that use split-read and read-pair evidence. In general, our results reveal that split-read methods detect fewer nonreference TE insertions than read-pair methods, but generally have much higher positional accuracy. Analysis of a large sample of real yeast genomes reveals that most McClintock component methods can recover known aspects of TE biology in yeast such as the transpositional activity status of families, target preferences, and target site duplication structure, albeit with varying levels of accuracy. Our work provides a general framework for integrating and analyzing results from multiple TE detection methods, as well as useful guidance for researchers studying TEs in yeast resequencing data.",
    "dateUpdated": "2017-10-03T01:59:21Z",
    "institutions": ["University of Manchester"],
    "license": "No License",
    "dateCreated": "2011-01-22T22:57:48Z",
    "numIssues": 11,
    "downloads": 0,
    "fulltext": "      McClintock: An Integrated Pipeline for Detecting Transposable Element Insertions in Whole-Genome Shotgun Sequencing Data     Michael G. Nelson  0    Raquel S. Linheiro  0    Casey M. Bergman  0    0  Faculty of Life Sciences, University of Manchester ,  M13 9PL ,  United Kingdom     2017   7   Transposable element (TE) insertions are among the most challenging types of variants to detect in genomic data because of their repetitive nature and complex mechanisms of replication . Nevertheless, the recent availability of large resequencing data sets has spurred the development of many new methods to detect TE insertions in whole-genome shotgun sequences. Here we report an integrated bioinformatics pipeline for the detection of TE insertions in whole-genome shotgun data, called McClintock (https://github.com/bergmanlab/mcclintock), which automatically runs and standardizes output for multiple TE detection methods. We demonstrate the utility of McClintock by evaluating six TE detection methods using simulated and real genome data from the model microbial eukaryote, Saccharomyces cerevisiae. We find substantial variation among McClintock component methods in their ability to detect nonreference TEs in the yeast genome, but show that nonreference TEs at nearly all biologically realistic locations can be detected in simulated data by combining multiple methods that use split-read and read-pair evidence. In general, our results reveal that split-read methods detect fewer nonreference TE insertions than read-pair methods, but generally have much higher positional accuracy. Analysis of a large sample of real yeast genomes reveals that most McClintock component methods can recover known aspects of TE biology in yeast such as the transpositional activity status of families, target preferences, and target site duplication structure, albeit with varying levels of accuracy. Our work provides a general framework for integrating and analyzing results from multiple TE detection methods, as well as useful guidance for researchers studying TEs in yeast resequencing data.    transposable elements bioinformatics genomics yeast       -\r\n  The widespread availability of genomic data over the last two decades has provided unparalleled opportunities to learn about the abundance, diversity, and functional consequences of transposable elements (TEs) in modern genomes. However, the computational analysis of TE sequences in both reference and resequenced genomes remains a challenging area of bioinformatics research because of the repetitive nature of these sequences. Development of bioinformatics tools for the detection and annotation of TEs in reference genomes is now a relatively mature field  (Bergman and Quesneville 2007; Saha et al. 2008; Lerat 2010) , although many open questions remain about choosing the best tools for specific biological applications ( Hoen et al. 2015 ). In contrast, detection of reference and nonreference TE insertions in whole-genome shotgun (WGS) resequencing data are an active research area  (reviewed in Ewing 2015) , with a large number of methods published in recent years  (Sackton et al. 2009; Ewing and Kazazian 2010, 2011; Hormozdiari et al. 2010; Quinlan et al. 2010; FistonLavier et al. 2011, 2015; Kofler et al. 2012, 2016; Lee et al. 2012; Linheiro and Bergman 2012; Nellaker et al. 2012; Platzer et al. 2012; Chen et al. 2013, 2017; Cridland et al. 2013; Robb et al. 2013; Gilly et al. 2014; Nakagome et al. 2014; Thung et al. 2014; Wu et al. 2014; Zhuang et al. 2014; Hawkey et al. 2015; Hénaff et al. 2015; Jiang et al. 2015; Rahman et al. 2015; Quadrana et al. 2016) .  Because of the wide array of available methods, it remains unclear which method for detecting TEs in resequenced genomes is best suited for particular genomic problems, leading to substantial investigator effort in terms of installation and testing, or the application of suboptimal bioinformatic approaches. Most papers reporting new methods to detect reference or nonreference TEs in WGS data provide some measure of their own performance relative to using simulations, benchmark genomic data, or PCR-based validation. However, only a handful of papers have reported new methods that include performance evaluation relative to other methods  (Gilly et al. 2014; Ewing 2015; Hawkey et al. 2015; Hénaff et al. 2015; Jiang et al. 2015; Rahman et al. 2015; Chen et al. 2017) , and these are often limited in scope to only a single organism or TE family. In addition to being incomplete, comparative analysis of bioinformatic systems in papers that report new methods can fall victim to the \u201cselfassessment trap\u201d  (Norel et al. 2011) . Moreover, there is no common format for the annotation of nonreference TE insertions  (Bergman 2012; Rishishwar et al. 2016) , making direct comparison of predictions from different methods more challenging. Recently,  Rishishwar et al. (2016)  performed an independent comparative evaluation of seven WGS-based TE detection methods using human genomic data, which revealed many method-specific predictions and recommended combining the results of multiple systems followed by manual curation  (see also Ewing 2015) .  Rishishwar et al. (2016)  also highlighted the challenges users face when installing and running multiple TE detection methods, and provide helpful advice for users and developers.  As a step toward a fully automated framework for running and evaluating multiple methods to detect TEs in WGS resequencing data, we have developed an integrated pipeline called McClintock (https://github.com/bergmanlab/mcclintock) that generates standardized output for multiple WGS-based TE detection methods. The primary goal of the McClintock pipeline is to lower the barrier to installation, use, and evaluation of multiple WGS-based TE detection methods. Several key features of the McClintock pipeline are that it automates formatting of key input files and standardizes output of multiple TE detection methods to allow easy comparisons of results from different systems, as recommended by R Rishishwar et al. (2016) . In the initial version of McClintock, we incorporate six complementary TE detection methods that make predictions based on split-read- or read-pair-based evidence in Illumina WGS data. Here we describe the McClintock system and its component methods, and perform a comparative evaluation using simulated and real yeast genome data. Our analysis supports previous conclusions that no single TE detection method provides a comprehensive detection of nonreference TEs ( (Ewing 2015; Rishishwar et al. 2016) , but provides a framework for further testing, development, and integration to achieve this ultimate aim, as well as useful guidance for yeast researchers to select appropriate TE detection tools.    MATERIALS AND METHODS\r\n  Analysis of simulated WGS data sets with single artificial    TE insertions\r\n  To investigate the performance of McClintock component methods on data containing known, nonreference TE insertions, we created simulated Saccharomyces cerevisiae genomes, each containing a single synthetic nonreference TE insertion from one of the four active TE families in an otherwise unmodified S. cerevisiae reference genome. Since active S. cerevisiae TEs (Ty1, Ty2, Ty3, and Ty4) are known to target tRNA genes  (Ji et al. 1993; Chalker and Sandmeyer 1990, 1992; Devine and Boeke 1996; Kim et al. 1998; Baller et al. 2012; Mularoni et al. 2012; Qi et al. 2012) , each synthetic insertion was placed upstream of a different annotated tRNA gene in the reference genome, taking the orientation of the tRNA gene into consideration. The annotation for 299 tRNAs was extracted from the SGD genome annotation for sacCer2 (SGD version R61.1.1). Ty1, Ty2, and Ty4 have been shown to insert predominantly within the first 200 bp upstream of tRNA genes, and Ty3 appears to target more specifically the region of RNA polymerase III transcription initiation, 16 or 17 nucleotides from the 59 ends of tRNA genes  (Ji et al. 1993; Chalker and Sandmeyer 1990, 1992; Devine and Boeke 1996; Kim et al. 1998; Baller et al. 2012; Mularoni et al. 2012; Qi et al. 2012) . All active S. cerevisiae TEs produce 5-bp target site duplications (TSDs) on insertion  (Gafner and Philippsen 1980; Rinckel and Garfinkel 1996; Chalker and Sandmeyer 1990; Kim et al. 1998; Zou et al. 1996) . To mimic these insertion preferences in our simulations, Ty1, Ty2, Ty3, and Ty4 were alternately selected for insertion; a 5-bp TSD was created (either 200-195 bp upstream of a tRNA gene for Ty1, Ty2, and Ty4; or 17-12 bp upstream of tRNA genes for Ty3); and the corresponding full-length Ty canonical sequence was inserted in the reference genome. 299 insertions were produced with the TE sequence inserted on the positive strand of the genome, and 299 were produced with the TE sequence reverse complemented to test the effects of TE orientation on method performance.  We simulated resequencing of single-insertion synthetic genomes using Wgsim (https://github.com/lh3/wgsim)  (Li et al. 2009)  with a 1% base error rate (2e 0.01). Read lengths were chosen to be 101 bases each with an insert size of 300 bp (42-bp SE) and 100· coverage to mimic the properties of a large sample of WGS data sets collected by  Strope et al. (2015) , which we use in our analysis of real yeast genomes (see below). To generate an average read depth of 100· across the length of sacCer2 reference genomes with additional single TE insertions, in silico WGS samples were created with 6,024,220 read pairs for Ty1 insertions, 6,024,237 read pairs for Ty2 insertions, 6,023,936 read pairs for Ty3 insertions, and 6,024,369 read pairs for a Ty4 insertion.  McClintock (version e945d20da22dc1186b97960b44b86bc21c96ac27) was run on each of these simulated data sets using reference TE annotations and canonical TE sequences from  Carr et al. (2012) , plus a manually produced hierarchy file based on the reference TE annotation in  Carr et al. (2012) . We used the standard, unmodified reference genome sequence option of McClintock for these single synthetic insertion simulations. The mean of the number of nonreference and reference TEs predicted per sample was calculated across all 299 simulated samples for each strand. The proportion of correct predictions of nonreference TEs was calculated at four thresholds of accuracy: (i) requiring the exact TSD to be annotated correctly, (ii) requiring a prediction to be within a 100-bp window either side of the TSD, (iii) within a 300-bp window either side of the TSD (the insert size of the simulated sequencing data set), or (iv) within a 500-bp window either side of the TSD. BEDtools window  (Quinlan and Hall 2010)  was used to calculate correct predictions within the given windows. A prediction was classified as exactly correct only if the same TE family was predicted to occur at the exact coordinates of the TSD of the synthetic TE insertion location. For nonexact overlaps, BEDtools window allows a permissive definition of a true positive, where a correct TE prediction is counted when any part of a predicted insertion falls within the given threshold distance if the correct TE family is predicted. The orientation of a predicted insertion was not taken into account for determining a correct prediction because some methods do not predict orientation.  To visualize the accuracy of nonreference TE predictions, the results files for the 299-positive strand and 299-negative strand single-insertion samples were converted into two bigWig files (one for each strand) using BEDtools and wigToBigWig  (Kent et al. 2010) . This was performed for each TE family and each component method of McClintock. SeqPlots  (Stempor 2014)  was then used to produce plots of the genome coverage of predictions for each TE family, centered around the simulated insertion locations for that family. Visualization of predicted insertions negative strand simulations were reverse complemented and depicted on the same plot as positive strand simulations in different colors. Plots were centered on the 5-bp TSD and extended 610 bp for split-read methods, and 6500 bp for read-pair methods, respectively. Results for TEMP were partitioned based on whether or not split-read support was available for a prediction. Prior to visualization, we attempted to filter out any obvious false-positive predictions using the fact that each synthetic insertion location should only be predicted in one simulated sample. Thus, any locations where a predicted nonreference insertion was observed across multiple simulated samples indicated a potential false positive. This filtering was necessary to prevent a nonreference insertion that was predicted by RelocaTE in the same location in 149 single synthetic insertion samples from dominating the visualization for this component method. False-positive filtering prior to visualization only affected five other potential insertions for PoPoolationTE, and thus this filtering procedure does not substantially alter positional accuracy results. To further investigate the accuracy of TSDs predicted by split-read methods, the length of the predicted TSD was plotted for each active yeast TE family. To be consistent with analysis of real yeast genomes (see below) and to mitigate effects of false-positive predictions found at the same site in multiple samples, TSD lengths predicted in simulated data were only plotted for unique insertion sites rather than all insertions.  To investigate the concordance of nonreference TE predictions made by different McClintock component methods, we first determined whether or not each method had made a \u201ccorrect\u201d prediction in each of the simulated samples with a synthetic TE insertion. Predictions for ngs_te_mapper, RelocaTE, TEMP (both split read and read pair), and PoPoolationTE were classified as correct if they overlapped with the true location of the TSD. Predictions for RetroSeq and TE-locate were classified as correct if they occurred within a 100- or 500-bp window of the correct location of the TSD, respectively. The orientation of a prediction was not taken into account when classifying a prediction as \u201ccorrect\u201d or not, because not all methods predict orientation. The overlap of these correct predictions was then plotted as Venn diagrams using jvenn  (Bardou et al. 2014) , comparing split-read methods, read-pair methods, and finally the total set of correct predictions from all split-read vs. all read-pair methods.    Analysis of real WGS data sets\r\n  To assess the relative performance characteristics of the component methods on real data, McClintock was run on a large sample of S. cerevisiae data sets from  Strope et al. (2015)  that includes 93 S. cerevisiae strains from different geographical locations and clinical origins. T he Strope et al. (2015 ) samples were sequenced on an Illumina HiSeq 2000 with paired-end reads of 101 bases each, an average insert size of 300 bases, and a median coverage of .117·. We used these general library characteristics in our single synthetic insertion simulations (see above) to allow more direct comparison with analysis of these real yeast genomes. The raw fastq files for the 93 sequenced strains were obtained from the EBI Sequence Read Archive (SRA072302).  McClintock (version 354acec977e37c354f6f05046940b0dabf09b331) was run on each of these samples using reference TE annotations and canonical TE sequences from  Carr et al. (2012) , and a manually produced hierarchy file based on the annotation in  Carr et al. (2012) . The McClintock version used for the analysis of real yeast data differs slightly from that used for simulated data in terms of three small improvements that were required to handle variation in sample names (for ngs_te_mapper) and differences in read lengths of paired-end fragments (for PoPoolationTE) which were encountered when analyzing real yeast genome data. We used the standard, unmodified reference genome sequence option of McClintock for these analyses. The average number of nonreference and reference TEs predicted per strain was plotted as box plots for each method. In addition, the total numbers of nonreference and reference TE insertions per TE family were summarized across all strains for each McClintock component method, both genome wide and in tRNA gene regions.  To biologically validate results of different component methods of McClintock, we took advantage of the fact that Ty elements are known to insert in close proximity to tRNA genes in S. cerevisiae  (Ji et al. 1993; Chalker and Sandmeyer 1990, 1992; Devine and Boeke 1996; Kim et al. 1998; Baller et al. 2012; Mularoni et al. 2012; Qi et al. 2012) . A prediction was counted as within a tRNA gene region if any part of the annotation was with 1000 bp upstream or 500 bp downstream of the transcription start site of one of the 299 annotated tRNA genes, taking tRNA gene orientation into account. To visualize the patterns of nonreference TE predictions around tRNA genes, all results for all 93 samples were converted to a single genome coverage bigWig file for each TE and each component method. SeqPlots  (Stempor 2014)  was used to produce plots of the genome coverage averaged across the 299 tRNA genes. Plots were centered on the start of the tRNA gene and extended 1000 bp upstream and 500 bp downstream, taking into account the orientation of each tRNA gene. Results for TEMP were subset into two groups based on whether split-read support for a prediction was available or not.  The lengths of TSDs for nonreference TE insertions predicted by the split-read methods were plotted by TE family. To prevent any nonreference TE insertions present at the same location in multiple samples from biasing the results, only unique insertion sites were plotted. If a method called an insertion at nearly the same location but with a longer or shorter TSD in different samples, these were classed as unique sites.    Data availability\r\n  The McClintock pipeline is available under the FreeBSD license at https://github.com/bergmanlab/mcclintock. Supplemental Material, File S1 contains a combined supplement text including: detailed descriptions of McClintock components; an overview of the McClintock execution process; details of postprocessing of component method output; methods, results, and discussion for analysis of McClintock applied to simulated resequencing data created for unmodified S. cerevisiae reference genomes; and Figures S1-S4 and Tables S1 and S2. Supporting data sets of McClintock predictions for real yeast genomes in SRA072302 are available in File S2. Code used to generate simulated yeast genomes and apply McClintock to simulated and real yeast genome data are provided in File S3.    RESULTS AND DISCUSSION\r\n    McClintock component methods and their dependencies\r\n  We initiated our design of McClintock with a literature search for candidate bioinformatic systems that can detect TE insertions from NGS data in 2014, which yielded 33 potential systems. Our main project objective was to develop a system that automatically detects nonreference TE insertions in raw WGS data for any species. Thus, we excluded systems that required any wet-laboratory enrichment from further consideration. Systems that did not make their code available were also rejected. This left a list of 12 candidate software systems. After preliminary testing of these 12 methods, six were rejected from further testing because of difficulties during installation [Tangram (Wu et al.  McClintock: Pipeline for TE Detection | 2765 Split read and read pair refer to what type of evidence is used to make TE-insertion predictions (see main text for details). aTEMP reports whether a reference TE is absent from the resequenced sample rather than providing direct evidence for the presence of a reference TE. bRelocaTE output provides information about the orientation of nonreference TEs, but not for reference TEs. McClintock annotates the orientation of reference TEs in cTREel-oloccaaTtEe opurotpviudteussiinnfgortmheatoioringianbaoluretftehreenocreieTnEtatainonnootaftnioonn.reference TEs where possible, but not for reference TEs. McClintock annotates the orientation of reference TEs in TE-locate output using the original reference TE annotation. edTREetMroPSeoqnlycamnadkeetseTctSDTEpfraemdiicliteiosnnsoftoprriensseenrttioinnsthweithresfeprleitn-rceeadgesnuopmpoertw.hen using Exonerate to generate a reference TE annotation, but not when using a usersupplied reference TE annotation which is the default option in McClintock. 2014) and VariationHunter  (Hormozdiari et al. 2010) ], reliance on data for a specific organism [TEA  (Lee et al. 2012)  and VirusSeq  (Chen et al. 2013) ], inability to detect nonreference insertions [T-lex  (Fiston-Lavier et al. 2011) ], or the inability to distinguish general structural variations from TE insertions [HYDRA  (Quinlan et al. 2010) ]. Six remaining methods [ngs_te_mapper  (Linheiro and Bergman 2012) , TE-locate  (Platzer et al. 2012) , PoPoolationTE  (Kofler et al. 2012) , RetroSeq  (Keane et al. 2013) ,  RelocaTE (Robb et al. 2013 ), and TEMP  (Zhuang et al. 2014) ] had publicly available code that could be installed reproducibly and met project objectives were selected for incorporation into the initial McClintock pipeline. Since the original selection of methods for inclusion in McClintock, a number of additional methods that meet the initial project requirements [\u201cpecnv teclust\u201d  (Cridland et al. 2013) , TIF  (Nakagome et al. 2014) , TE-Tracker  (Gilly et al. 2014) , Mobster  (Thung et al. 2014) , ITIS  (Jiang et al. 2015) , Jitterbug ( Hénaff et al. 2015 ), TIDAL (Ra hman et al. 2015 ), ISmapper  (Hawkey et al. 2015) , MELT  (Sudmant et al. 2015) , SPLITREADER  (Quadrana et al. 2016) , and TEPID  (Stuart et al. 2016) ] and new versions of some methods [PoPoolationTE2  (Kofler et al. 2016)  and RelocaTE2  (Chen et al. 2017) ] have been released. These methods have not yet been incorporated into McClintock, but the flexible architecture of our system permits their inclusion in the future.  A summary of the main features of the six component methods included in McClintock is shown in Table 1. A more detailed overview of the component methods, their original use case, software/data dependencies, and limitations is provided in the \u201cDescription of McClintock Component Methods\u201d section of File S1. While none of the McClintock component methods were originally designed for detecting TEs in yeast, using the yeast system as a test bed does not favor any particular component method and realistically models application of component systems to a new species. The six component systems each have many dependencies on other pieces of software, which must all be correctly installed before the component system will function correctly. These software dependencies are listed in Table 2. Several of these component dependencies require end-user licenses, and thus it was not possible to fully automate installation of all component methods. McClintock therefore assumes component dependencies are installed system wide, but automates installation of the component methods themselves. A passive check is performed during installation of McClintock that reports whether component dependencies are available, though installation is not halted if they are missing. Because of the large number of component dependencies and subsequent development of components themselves, we developed McClintock to use specific versions of components and their dependencies. Table 2 also lists the version of each dependency that was used with McClintock to obtain the results presented here.  McClintock component methods also have a variety of data dependencies that are required as inputs, which are listed in Table 3. The component methods incorporated into McClintock together require a total of 13 different data dependencies to run. However, since many of these data dependencies can be automatically generated or are format alterations that can be automatically achieved with simple preprocessing steps, the number of data dependencies can be reduced to three required inputs for McClintock: a fasta file of the reference genome, a fasta file of the canonical TE sequences, and fastq files of NGS reads (paired or single ended).    The McClintock pipeline\r\n  An overview of the data flow and processing steps performed by the McClintock pipeline is shown in Figure 1. A detailed description of how the McClintock pipeline is executed can be found in the \u201cOverview of the McClintock process\u201d section of File S1. In the following sections, we describe the options for running the McClintock pipeline, then describe how component methods are parsed in the context of the McClintock pipeline to create standardized output for downstream analysis.  Reference TE annotation options: Several McClintock component systems rely on information about TEs in the reference genome as part of their workflow, which can be either supplied by the user or automatically generated by McClintock. If a preexisting annotation of the TE sequences in the reference genome is available, a one-based GFF file of this data can be used as input for the McClintock pipeline. If such a reference TE annotation is provided, then the user must also create and supply a TE \u201chierarchy\u201d file as another input. The hierarchy file contains two tab-delimited columns, the first listing the name of each instance in the reference TE annotation and the second listing the canonical TE family to which that instance belongs. If no reference TE annotation is provided, then a reference TE annotation and hierarchy file is created automatically by running RepeatMasker and postprocessing RepeatMasker output files.  Reference genome sequence options: McClintock provides options to automatically create various different modified reference genomes.  These options were implemented because some component methods abTOhnislyscpoemcipfiactvibelresiwonithofSABWMTAoioslsne0e.1d.e1d9 otor eeanrsluierre(Rcoismhipshawtibairlietyt bale. t2w0e1e6n). PoPoolationTE, which uses BWA-ALN, and other component methods which use BWAMEM. (RetroSeq and TE-locate) require an instance of a TE to exist in the reference genome for nonreference instances of that family to be detected in a resequenced sample. This is important because, in some cases, like the Drosophila melanogaster P-element  (Kaminker et al. 2002) , the reference genome does not include any copies of a TE family that occurs in natural populations. This situation may also occur when a TE family has been introduced experimentally into a strain lacking that TE to study its transposition. To allow for these cases, McClintock has an option to generate modified reference genomes that include additional \u201cchromosomes\u201d comprised of canonical TE sequences or TE sequences extracted from the reference genome. An annotation of TEs in the additional \u201cchromosomes\u201d is then appended to the reference TE annotation file.  PoPoolationTE requires a modified reference genome with canonical TE sequences and reference TE sequences added as additional \u201cchromosomes.\u201d Thus these reference genome modifications are always made specifically for PoPoolationTE, regardless of whether user-supplied options to modify the reference genome are provided globally for other component methods.  Run options: McClintock offers additional options to customize the way the pipeline is run. It is possible to specify which component methods are executed, allowing tailored output and shorter run times. McClintock and its component methods produce short-read alignment files and other intermediate files that can be very large, and thus an option is provided to remove unwanted intermediate files. BAM files output by McClintock may be useful for other purposes, so an option is provided to eliminate all intermediate files other than BAM files. The location of all output files can be changed to any absolute path that the user requests.  Within the specified location, all output files will be produced in a directory named after the reference genome sequence with results for each sample stored in subdirectories named after the fastq files for that sample, allowing multiple samples for the same reference genome to reuse common index files.  Postprocessing and standardization of output format: The component methods within McClintock produce their output in different file formats and annotation frameworks  (see Bergman 2012 for discussion) .  Therefore, McClintock performs a number of postprocessing steps to standardize outputs from different methods into a common annotation framework. Details of the native annotation framework for component methods and the postprocessing steps made by McClintock can be found in the \u201cPostprocessing and Standardization of Component Method Output\u201d section of File S1. Before performing these steps, the original (unedited) results for each method are saved in the output directory for that sample. If TE predictions are made by any component method in the additional \u201cchromosomes\u201d added in modified reference genomes (see above), these results are removed from the standard results files and retained in a subdirectory within the results directory called \u201cnon-ref_chromosome_results.\u201d  The output file format chosen to standardize results for all component methods is a zero-based BED6 format because it allows easy integration with the BEDTools and UCSC genome browser. The BED format provides a fourth column to contain a name for the annotated feature. All records in these BED files contain the name of the TE family predicted at that location and whether the prediction is of a nonreference or reference TE. The name column also reports the sample ID from the fastq input file and the name of the component method that made the prediction. The type of evidence used for the prediction is also listed, either \u201csr\u201d representing a prediction made from split-read evidence, \u201crp\u201d representing a prediction made from read-pair evidence, or \u201cnonab\u201d for TEMP reference TE predictions that rely on no evidence for the absence of the TE in the sample. In addition, filtering and redundancy removal was performed within the result file for each component method. No redundancy filtering is performed by McClintock across component methods, allowing users to more directly compare output from different methods. To facilitate viewing of results on the UCSC genome browser, a header is included in each BED file. This header is read by the UCSC browser and lists the sample name and McClintock component system that produced the results as the track name and description, allowing multiple result files for the same sample to be merged and visualized simultaneously.    Application of McClintock to simulated S. cerevisiae genomes with single synthetic TE insertions\r\n  To test McClintock and its component methods, we used simulated WGS data sets based on the genome of the model eukaryote, S. cerevisiae.  We chose S. cerevisiae for testing McClintock because its reference genome is relatively small and has been completely determined  (Goffeau et al. 1996) , it has large samples of publicly available resequenced genomes  (Liti et al. 2009; Almeida et al. 2015; Strope et al. 2015) , and the genome biology of its TEs is relatively simple and well characterized  (Kim et al. 1998; Carr et al. 2012) . Briefly, the 12-Mb S. cerevisiae reference genome contains 483 annotated TEs from six long terminal repeat (LTR) retrotransposon families (Ty1, Ty2, Ty3,  McClintock: Pipeline for TE Detection | 2767 n Table 3 Data dependencies required to successfully run each component of the McClintock pipeline aMust include an entry in the format \u201cTSD=. . .\u201d for each TE in the file on the same line as the header, where \u201c. . .\u201d is the TSD sequence if known, or a string of periods bwith equal to the TSD length if the TSD sequence is unknown. If neither length nor the sequence of the TSD is known, \u201cTSD=UNK\u201d can be supplied. Must be formatted as one fasta file per TE family and a file of files listing their locations. cMust be one BED file for each entry in the reference TE annotation and a file of files listing their locations.  Ty3_1p, Ty4, and Ty5)  (Carr et al. 2012) , a type of TE that can be processed effectively by all six McClintock component methods. Ty1 and Ty2 share a nearly identical LTR sequence but differ in their internal regions  (Kim et al. 1998) , while Ty3 and Ty3_1p have 82% nucleotide identity over their entire length  (Fingerman et al. 2003) . Most TEs in S. cerevisiae are solo LTRs or otherwise truncated copies, with only 50 full-length elements from four active families in the reference genome (Ty1, Ty2, Ty3, and Ty4)  (Kim et al. 1998; Carr et al. 2012) . Ty1 and Ty2 have the most full-length copies in the S. cerevisiae reference genome, with very few full-length copies being observed for Ty3 and Ty4  (Kim et al. 1998; Carr et al. 2012) . The active TE families in S. cerevisiae are known to target tRNA genes  (Ji et al. 1993; Chalker and Sandmeyer 1990, 1992; Devine and Boeke 1996; Kim et al. 1998; Baller et al. 2012; Mularoni et al. 2012; Qi et al. 2012)  and create a 5-bp TSD on insertion  (Gafner and Philippsen 1980; Rinckel and Garfinkel 1996; Chalker and Sandmeyer 1990; Kim et al. 1998; Zou et al. 1996) .  We first performed control analyses by simulating WGS resequencing of unmodified S. cerevisiae reference genome samples and applying McClintock to these data sets (see \u201cSimulating Resequencing of the S. cerevisiae Reference Genome\u201d in File S1). While not the major focus of this study, these reference genome simulations allowed us to evaluate how often McClintock component methods detected reference TEs and, more importantly, how often component methods detected false-positive nonreference TEs (in the absence of any true, nonreference TE insertions). An example of reference TE predictions for all six component methods is shown in Figure S1A in File S1. In general, analysis of unmodified simulated reference genomes showed that McClintock component methods cannot detect all reference TEs (Table S1 in File S1), but also typically have low false-positive rates for predicting nonreference TE insertions when they are truly absent (Table S2 in File S1). Additionally, these simulations showed that McClintock had better performance at 100· vs. 10· coverage, and that neither the choice of reference TE annotation nor reference genome options substantially affected the detection of reference or nonreference TEs for most McClintock component methods.  Next, we simulated WGS samples for reference genomes that include a single synthetic TE insertion from one of the four active TE families (placed at biologically realistic locations upstream of tRNA genes) to evaluate the ability of McClintock component methods to detect true positive nonreference TE insertions. To do this, WGS reads were simulated for 598 samples, each with a different synthetic TE insertion placed upstream of one of the 299 tRNA genes in the yeast genome. 299 samples were created for single synthetic insertions in the positive orientation upstream of tRNA genes, and 299 samples for single synthetic insertions in the negative orientation. Genomes with synthetic insertions were created by alternately selecting one of the four active TE families and creating a 5-bp sequence 12-17 bp upstream of a tRNA start site for Ty3 or 195-200 bp upstream of a tRNA start site for Ty1, Ty2, and Ty4. This 5-bp sequence formed the basis of a synthetic TSD and became the location into which a full-length Ty canonical sequence was inserted in the sacCer2 reference genome. All single-insertion samples were simulated at 100· coverage since the ability of component methods to detect reference TEs improved with increasing coverage and to better match properties of the real yeast genomes analyzed below. An illustration of nonreference TE predictions for all six component methods in a genomic segment containing a synthetic TE insertion is shown in Figure S1B in File S1. In the following sections, we detail the analysis of these single synthetic insertion simulated samples in terms of overall numbers of reference and nonreference TE predictions and positional accuracy of nonreference TE predictions.  Numbers of reference and nonreference TE predictions: Table 4 shows the mean number of reference and nonreference TE insertions predicted across all 299 simulated single-insertion samples on the positive and negative strands, respectively. The proportion of correct predictions of nonreference TEs was calculated at four thresholds of accuracy: (i) requiring the exact TSD to be annotated correctly, (ii) requiring a prediction to be within a 100-bp window either side of the TSD, (iii) within a 300-bp window either side of the TSD (the insert size of the simulated sequencing libraries), or (iv) within a 500-bp window either side of the TSD. If all single TE insertion samples were predicted correctly for a method, it would lead to an average value of exactly one nonreference TE predicted per sample. Comparing row one of Table 4 (single-insertion simulation) with row nine of Table S1 in File S1 (unmodified reference simulation), we can infer that the inclusion of single synthetic insertions into the yeast genome does not substantially alter the ability of any McClintock component method to predict reference TEs. As expected, comparing row two of Table 4 (single-insertion simulation) with row nine of Table S2 in File S1 (unmodified reference simulation), we see gains in the numbers of nonreference TE insertions predicted for all methods; demonstrating that McClintock components can detect true positives above false-positive baselines in our simulation framework.  For ngs_te_mapper, the average number of nonreference predictions shows this method systematically underpredicts nonreference TE insertions. However, the average number of predictions made overall per sample is only slightly higher than the average number of exact predictions. Consistent with unmodified reference genome simulations (see row nine of Table S2 in File S1), this result indicates that only a small number of nonreference predictions made by ngs_te_mapper are false positives. Moreover, whenever ngs_te_mapper makes a prediction of a nonreference TE (that is within 500 bp of the true insertion site), the prediction was always at the exact TSD, suggesting high accuracy in terms of position and TSD structure for this method (see below). We also observed that ngs_te_mapper detected fewer insertions when the synthetic insertion is on the negative strand relative to the tRNA gene, suggesting there can be strand bias in the detection of nonreference TEs. This bias could be due to yeast genome organization, our simulation framework, the ngs_te_mapper algorithm, or a combination of these factors.  RelocaTE produced, on average, slightly more than one nonreference TE prediction per sample. At face value, this result suggests that RelocaTE may detect essentially every synthetic insertion, but also makes occasional false-positive predictions. In fact, the average excess number of predictions made by RelocaTE in single-insertion simulated genomes is very close to the false-positive rates observed in simulations of unmodified reference genomes (see row nine of Table S2 in File S1).  However, only 50% of the total RelocaTE predictions are made within 500 bp of the true insertion. Thus, it appears that the inclusion of single synthetic insertions increases the rate of false-positive nonreference TE predictions by RelocaTE relative to unmodified reference genomes.  Nevertheless, RelocaTE produces more correct predictions within 100 bp of the true insertion site than ngs_te_mapper, the other purely  McClintock: Pipeline for TE Detection | 2769 n Table 4 Average numbers of predictions and correct predictions, by method, for simulated yeast WGS samples with a single synthetic TE insertion upstream of tRNA genes split-read method, despite producing fewer exact predictions than ngs_te_mapper. Thus many of the nonexact RelocaTE predictions within 100 bp of the true location are likely to be accurately positioned, but simply not have the correct TSD structure (see below). Like ngs_te_mapper, RelocaTE also appears to have a slightly higher truepositive rate for positive strand insertions, with the difference in the number of correct predictions on the positive strand being greater in the exact prediction category.  The average total number of nonreference TE predictions for TEMP is nearly one (0.90), confirming results from unmodified reference genome simulations (see row nine in Table S2 in File S1) that TEMP makes very few false-positive nonreference predictions. Moreover, the total number of nonreference TE predictions for TEMP is the same as the average number that are accurate within 100 bp of the true insertion site. These results suggest TEMP is correctly predicting most simulated insertions, but not to base pair accuracy (see below). Some positional inaccuracy is expected for TEMP since not all predictions for this method are supported by split-read evidence. For TEMP, there appears to be no difference in detection ability for TE insertions on the positive or negative strand.  RetroSeq predicted nearly as high an average number of nonreference TE predictions per sample as TEMP, but the proportion predicted correctly was lower than TEMP for all length thresholds. The fact that not all RetroSeq predictions are within 500 bp of the true insertion suggests that RetroSeq can produce some false-positive predictions of nonreference TE insertions when the sample is not identical to the reference genome, unlike what was observed for simulations of unmodified reference genomes (see row nine of Table S2 in File S1).  Because RetroSeq does not use split-read information, no predictions from this method were exact, however most predictions were generally within 100 bp of the true location. For RetroSeq there is a only slight reduction in ability to detect nonreference TE insertions on the negative strand compared with the positive strand at all length thresholds.  PoPoolationTE produces an average of slightly more than one nonreference TE prediction per sample, but this method shows the lowest proportion of true-positive predictions at the most permissive length thresholds, suggesting most predictions are false positives. This result supports those obtained from unmodified reference genomes that PoPoolationTE makes approximately one false-positive prediction per genome in the absence of any synthetic nonreference TE insertions (see row nine of Table S2 in File S1). Because PoPoolationTE does not use split-read information and the span predicted by this method is often large (see Figure S1B in File S1), no predictions made by PoPoolationTE were exact. For PoPoolationTE there appears to be no difference in ability to detect nonreference TE insertions correctly on the positive or negative strand.  TE-locate produced an average of nearly one nonreference TE prediction per sample. However, these include some false-positive predictions or at least predictions that are .500 bp from the actual insertion location. The proportion of correct nonreference TE insertions predicted by TE-locate drops steadily from 500 to 100 bp, with TE-locate predicting the lowest number of correct insertions for any method at the 100-bp scale. As with the other read-pair methods, no predictions could be considered exact because TE-locate does not predict a TSD. These numbers indicate that, though the ability of TE-locate to detect the presence of a TE in the general vicinity of its true location is good, the annotation will not be as positionally accurate as other read-pair methods like TEMP or RetroSeq. For TE-locate there appears to be a reduction in detection ability at all thresholds for TE insertions on the negative strand compared with the positive strand.  Positional accuracy of nonreference TE predictions: To visualize more clearly the positional accuracy of McClintock component methods, predicted nonreference insertions were plotted around the known location of synthetic insertions (Figure 2 and Figure 3). Plots were produced for each TE family and method to determine if the family of the synthetic TE insertion affected results for a particular method.  Table 4 showed that for split-read methods, there was no increase in the accuracy at thresholds of 100 bp and many predictions were exactly correct. For read-pair methods, it appeared predictions could be several hundred base pairs from the correct location. As such, split-read (Figure 2) and read-pair (Figure 3) results were plotted on different spatial scales. Since TEMP could use both split-read and read-pair evidence, results for this method were partitioned into two categories for visualization. For a small number of cases, RelocaTE (one location) and PoPoolationTE (five locations) predicted nonreference TE insertions at the same genomic location in multiple samples. These predictions must include false positives based on the fact that each synthetic genome had only a single insertions at different genomic locations. Inclusion of these high-frequency, false-positive predictions dominated the visualization of results for these two methods, and thus predictions for these six cases were filtered prior generating Figure 2 and Figure 3 (see Materials and Methods for details).  Figure 2 shows that when ngs_te_mapper makes a prediction, it produces the TSD at the correct location, apparently with no TSDs called too long or too short. Direct analysis of TSD length distributions supports this conclusion: for simulated data, ngs_te_mapper always predicts the correct TSD length for nonreference insertions (Figure  S2 in File S1). For Ty1, Ty2, and Ty4, ngs_te_mapper detected insertions on the positive or negative strand with similar accuracy. Thus, the main difference in detection rates on the positive and negative strands for ngs_te_mapper observed in Table 4 appears to be for Ty3 insertions, where many fewer insertions were detected correctly on the negative strand. For RelocaTE, the predicted TSDs of nonreference insertions are in approximately the correct locations but with coordinate ranges that are frequently too short (see also Figure S2 in File S1). As with ngs_te_mapper, RelocaTE shows the biggest difference in ability to detect Ty3 insertions on the negative strand relative to the positive strand. TEMP split-read predictions for Ty1, Ty3, and Ty4 are often predicted correctly but with the TSD often annotated to be longer than its true length (see also Figure S2 in File S1). Surprisingly, TEMP made no predictions for nonreference Ty2 insertions using split-read evidence, perhaps because of the ambiguous signal arising from the similarity of Ty1 and Ty2 LTR sequences. For TEMP, there is no difference in detection ability for insertions on the positive or negative strand for any family.  Results of the positional accuracy for read-pair methods are shown in Figure 3. For Ty1, Ty3, and Ty4 there were very few insertions (only three per family) that TEMP did not have split-read supporting evidence for, and thus few insertions for these families are plotted in Figure 3. In contrast, all Ty2 predictions made by TEMP in the single-insertion simulations had read-pair evidence. For all families, when only readpair evidence is used, TEMP generally predicts an insertion at the correct site, but with some slight inaccuracy on either side. The majority of RetroSeq predictions appear to be clustered close to the true insertion locations, but there appears to be a slight bias for RetroSeq to predict insertions 39 of where the true TE is located on reference genome coordinates. This bias is potentially introduced by the breakpoint determination step of RetroSeq, which always scans in the 59 to 39 direction (see section \u201cDescription of McClintock Component Methods\u201d in File S1). PoPoolationTE produced the highest number false-positive predictions (Table 4). When these false-positive nonreference predictions are filtered from the results, all predictions for Ty1 and Ty2 in the windows around simulated insertions are eliminated. The effect of removing false positives is probably most pronounced for Ty1 because it is the most common TE family in S. cerevisiae, and thus would be the most likely family to have a reference insertion with sequence similarity to the synthetic insertion in the vicinity of tRNA genes. PoPoolationTE makes no predictions for Ty2, even including false positives. For Ty3 and Ty4, PoPoolationTE has the capability of producing relatively accurate predictions, albeit with low resolution (nearly 100 bp around the true insertion site). For TE-locate, many predictions are made within 500 bp of the true insertion, but they are clearly spread further from the true insertion location than other methods. TE-locate also appears to have a slight bias to predict insertions 59 of the true insertion location on reference genome coordinates.  Overlap between methods: To understand the concordance of predictions made by the McClintock components, we investigated the overlap among methods for predictions that were made correctly at the sites of synthetic insertions. As shown in Figure 2 and Figure 3, different methods have different positional accuracy, and thus we used different windows to classify if a method made a \u201ccorrect\u201d prediction for a known insertion or not. Predictions for ngs_te_mapper, RelocaTE, TEMP (both split read and read pair), and PoPoolationTE were classified as correct if they had any overlap with the true location of the TSD; while predictions for RetroSeq and TE-locate were classified as correct if they occurred within a 100- or 500-bp window, respectively, of the correct location of the TSD. Neither the orientation nor the TE family was taken  McClintock: Pipeline for TE Detection | 2771 into account when classifying a prediction as correct or not. The overlap of correctly detected insertions are shown in Figure 4, A and B, for splitread and read-pair insertions, respectively. The overlap of correct predictions made by all split-read methods vs. all read-pair methods is shown in Figure 4C.  Figure 4A shows that the majority of split-read predictions are supported by at least two methods (n = 340, 57%) but that each method made many correct TE predictions that were not made by any other method. RelocaTE and TEMP made a greater number of correct overlapping predictions with each other than either of these method did with ngs_te_mapper. Figure 4A also shows that 16% (n = 94) of synthetic insertions were not predicted by any split-read method at the threshold of positional accuracy used here. Figure 4B shows that the vast majority of synthetic TE insertions (n = 428, 72%) are predicted by at least two of the read-pair methods, but that only 24% (n = 143) of insertions are supported by three or more methods. RetroSeq and TE-locate make the highest number of unique correct predictions.  10% (n = 58) of synthetic insertion samples were not predicted by any read-pair method at the threshold of positional accuracy used here.  Finally, Figure 4C shows that, while the overwhelming majority of insertions are predicted by at least one split-read and one read-pair method (n = 470, 79%), there are many insertions that are only predicted using one type of evidence or the other (n = 104, 17%) given the thresholds of positional accuracy used here. Nevertheless, use of all six methods recovers nearly 96% of synthetic insertions, demonstrating the utility of integrating multiple TE-identification methods enabled by McClintock.    Application of McClintock to 93 yeast genomes\r\n  The previous sections presented results on the accuracy of McClintock component methods on simulated resequencing data. Simulations are useful for testing methods under controlled settings, but do not capture all aspects of how methods perform when applied to real data. Since much is known about the expected insertion preferences of TEs in S. cerevisiae  (Gafner and Philippsen 1980; Ji et al. 1993; Chalker and Sandmeyer 1990, 1992; Devine and Boeke 1996; Rinckel and Garfinkel 1996; Zou et al. 1996; Kim et al. 1998; Baller et al. 2012; Mularoni et al. 2012; Qi et al. 2012) , analysis of real WGS data sets can be used as an alternative approach to evaluate if McClintock component methods can recapitulate the known genome biology of yeast TEs. To do this, we analyzed 93 highcoverage S. cerevisiae WGS data sets from  Strope et al. (2015)  using McClintock to generate TE predictions for all six component methods.  Figure 3 and Figure S3 in File S1 show how many of the nonreference and reference TEs per strain, respectively, are detected by the different McClintock component methods across all 93 samples. In general, splitread methods predict between 5 and 20 nonreference TE insertions per strain, whereas read-pair methods predict 40-100 nonreference TE insertions per strain (Figure 3). Numbers of reference TEs predicted per strain in real data (Figure 5) are generally lower than in simulated genomes (Table 4 and Table S1 in File S1). The exceptions to this pattern Figure 4 Concordance of correctly predicted nonreference insertions among McClintock component methods. (A) The concordance of nonreference predictions by methods that use split-read evidence that overlap with the true location of a synthetic insertion. (B) The concordance of nonreference predictions made by methods that use read-pair evidence that either overlap (TEMP, PoPoolationTE), or are within 100 bp (RetroSeq), or 500 bp (TE-locate) of the true location of a synthetic insertion. (C) The concordance of correctly predicted synthetic nonreference TEs with split-read or read-pair evidence.  Predictions for TEMP were partitioned based on whether they had split-read evidence (split-read) or not (read-pair). Counts in all diagrams total 598, the number of simulated samples with single synthetic insertions. are TEMP and PoPoolationTE, which show similar or higher numbers of reference TE predictions per strain in real data relative to simulations.  We note that for a few strains in t he Strope et al. (2015 ) data set, TE-locate predicted several hundred nonreference insertions; these Figure 5 Numbers of nonreference TE insertions per strain predicted by McClintock component methods in real yeast genomes. Predictions for TEMP were partitioned based on whether they had split-read evidence (split-read) or not (read-pair). Data are from 93 yeast strains taken from  Strope et al. (2015) . Methods are classified based on whether they use split-read or read-pair evidence to make a nonreference TE prediction. The box plot is shown on a log10 scale. The thick line indicates the median, the colored box is the interquartile range, the whiskers mark the most extreme data point which is no more than 1.5 times the interquartile range from the box, and the s's are outliers.  Note that for TE-locate, several outlier samples generated hundreds of predicted nonreference TE insertions. strains did not appear to be outliers in terms of their nonreference TE content based on other methods (results not shown).  We evaluated the quality of nonreference TE predictions made by McClintock component met hods on the Strope et al. (2015 ) data set using three aspects of the known biology of TEs in S. cerevisiae: (i) activity of families, (ii) tRNA targeting, and (iii) TSD length. Our expectations based on prior knowledge of yeast TE biology are that methods that make high quality nonreference TE predictions should (i) show few nonreference predictions for inactive TE families (Ty3_1p and Ty5), (ii) show a high proportion of nonreference predictions in the vicinity of tRNA genes, and (iii) show characteristic 5-bp TSDs for nonreference predictions made by split-read methods.  Prediction of active and inactive families: Table 5 shows numbers of nonreference TE predictions made by McClintock component methods across all strains in t he Strope et al. (2015 ) data set. As expected, all methods predicted multiple nonreference insertions for TE families that are known to be active in this species. Additionally, ngs_te_mapper and TEMP make no nonreference TE predictions for both inactive families in S. cerevisiae, supporting simulation results above that show these methods have low false-positive rates. RelocaTE makes nonreference TE predictions for Ty3_1p but not Ty5, PoPoolationTE makes nonreference TE predictions for Ty5 but not Ty3_1p, and both RetroSeq and TE-locate predict nonreference insertions for Ty3_1p and Ty5.  RelocaTE is the only split-read method that predicts nonreference insertions for an inactive family, suggesting that split-read methods  McClintock: Pipeline for TE Detection | 2773 n Table 5 Number and location of nonreference TEs predicted by McClintock component methods in 93 yeast genomes Each cell shows the number of nonreference TEs predicted in tRNA regions followed by the total number of nonreference TEs predicted genome wide. Data are for numbers of insertions, not numbers of nonredundant insertion sites, so TE insertion alleles present in more than one sample are counted independently. A prediction is counted in a tRNA region if any portion of the annotation is within 1000 bp upstream and 500 bp downstream of the tRNA start site, taking into account the orientation of the tRNA gene. The first column applies the same analysis to the reference TE annotations from  Carr et al. (2012) . N.A. indicates that no nonreference TE insertions were predicted by a method for that TE family. generally have a higher ability to discriminate active from inactive TE families. Compared to the total numbers predicted for other active TE families, the three pure read-pair methods predicted fewer nonreference insertions for both Ty3_1p and Ty5, suggesting false-positive rates for these methods are not so high as to overwhelm true signal. The one exception is for TE-locate, which predicted relatively high numbers of Ty5 insertions, which is likely related to the outlier samples noted above where TE-locate predicts hundreds of presumably false-positive nonreference insertions.  Predicted insertions in tRNA regions: Active TE families in S. cerevisiae are known to target tRNA genes  (Ji et al. 1993; Chalker and Sandmeyer 1990, 1992; Devine and Boeke 1996; Kim et al. 1998; Baller et al. 2012; Mularoni et al. 2012; Qi et al. 2012) . The highest density of Ty1 and Ty2 insertions are in the 200 bp upstream of the tRNA transcription start site  (Ji et al. 1993; Devine and Boeke 1996; Kim et al. 1998; Baller et al. 2012; Mularoni et al. 2012) . Ty3 targets a specific location just upstream of tRNA gene transcription start sites  (Chalker and Sandmeyer 1990, 1992; Kim et al. 1998; Qi et al. 2012) .  Patterns of Ty4 insertions have not been experimentally determined, although the locations of insertions in the reference genome suggest a similar pattern to that of Ty1 and Ty2  (Kim et al. 1998) .  To evaluate if nonreference TE insertions predicted by McClintock component methods show expected hallmarks of tRNA targeting, we plotted locations of nonreference TE insertions identified in t he Strope et al. (2015 ) strains using split-read evidence and read-pair evidence in Figure 6 and Figure 7, respectively. The expected profiles of insertion into tRNA gene regions is observed for all Ty families for ngs_te_mapper, RelocaTE, TEMP, and RetroSeq, albeit with the different levels of resolution that are characteristic of each method. Consistent with simulation data (Figure 2), TEMP appears to have difficulty predicting Ty2 using split-read data in real yeast genomes, and this effect also appears to impact prediction of Ty1 insertions using split-read data in real data (Figure 6). PoPoolationTE can predict meaningful profiles of insertion for Ty3 and Ty4 (Figure 7), as expected based on simulation data (Figure 3). However, in contrast to simulation data where only putative false positives are predicted (Figure 3), PoPoolationTE also predicts nonreference insertions for Ty1 and Ty2 in real data (Figure 7). Since  PoPoolationTE predicts reference and nonreference insertions in the same way, and since many Ty1 and Ty2 insertions exist in the reference genome upstream regions of tRNA genes, it is possible that these Ty1 and Ty2 insertions predicted in t he Strope et al. (2015 ) data set are actually reference insertions that are mislabeled by PoPoolationTE as nonreference insertions. Finally, nonreference insertions predicted by TE-locate are only weakly enriched in tRNA regions for all families, and the positional profiles produced by TE-locate are shifted relative to expectations and predictions made by other methods.  To quantify the proportion of nonreference TEs that were predicted in tRNA regions, we counted predictions 1000 bp upstream and 500 bp downstream of a tRNA gene, taking into account the orientation of the tRNA gene but not the orientation of the TE insertion. The expected percentage of TEs located in these regions if they were inserted randomly in the genome would be 0.037% [(299 tRNA genes · 1500 bp window)/12,162,995 bp genome]. Previous analyses of tRNA targeting of TEs in the S. cerevisiae reference genome  (Kim et al. 1998)  assessed whether TEs were within 750 bp of a tRNA gene or other RNA polymerase III gene (excluding other intervening TE sequences). Here we use extended regions for tRNA targeting based on the inaccuracy in nonreference predictions observed for some methods in the simulations above. For comparison with previous results, we first applied our definition of tRNA targeting to the reference TE annotation from  Carr et al. (2012)  (Table 5). Estimated proportions of Ty elements in tRNA regions for the  Carr et al. (2012)  reference annotation are lower than those reported by  Kim et al. (1998) , however, they still show highly biased targeting toward tRNA regions.  Nonreference TE predictions of all four active Ty elements show the expected enrichment in tRNA regions for each McClintock component method (Table 5). For all methods, Ty3 is the active TE family most strongly associated with tRNA regions, consistent with experimental data and observations based on the reference genome  (Chalker and Sandmeyer 1990, 1992; Kim et al. 1998; Qi et al. 2012) . Split-read methods predict a higher proportion of nonreference TEs in tRNA regions relative to expectations based on TEs in the reference genome.  For read-pair methods, at least one TE family showed a lower proportion of nonreference TEs in tRNA regions relative to reference TEs.  We interpret this observation to be due to the lower positional accuracy of read-pair methods. TE-locate consistently predicted the lowest number of TEs in tRNA regions for active Ty families, though predicted insertions for this method still showed an enrichment in tRNA regions relative to random expectation. We interpret the low tRNA enrichment for TE-locate to be a consequence of the low positional accuracy of read-pair methods combined with the presence of outlier samples for this method which have very high numbers of nonreference predictions.  As discussed above, nonreference predictions were made by RelocaTE, RetroSeq, and TE-locate for the inactive Ty3_1p family. Despite most likely being false positives, these predictions were predominantly  McClintock: Pipeline for TE Detection | 2775 in tRNA regions, suggesting they could either be nonreference Ty3 insertions that are miscalled as nonreference Ty3_1p, or reference Ty3_1p insertions called as nonreference Ty3_1p insertions. Nonreference predictions were also made by RetroSeq, PoPoolationTE, and TE-locate for the inactive Ty5 family. The majority of these predictions are made outside of the tRNA regions, as is expected based on the known location of Ty5 insertions in the reference genome prior knowledge about Ty5 target preferences  (Zou et al. 1996; Kim et al. 1998; Baller et al. 2011) . These nonreference TE predictions may be false positives (possibly caused by mapping inconsistencies in heterochromatic regions where Ty5 elements typically insert) or real nonreference \u201cinsertions\u201d that arose by recombination events rather than transposition events  (Zou et al. 1995) .  Prediction of TSDs by split-read methods: Finally, we evaluated the performance of split-read methods to predict the known TSD lengths of active yeast Ty families in real WGS data. All available experimental and genomic data indicates that active yeast Ty families create 5 bp TSDs on insertion  (Gafner and Philippsen 1980; Rinckel and Garfinkel 1996; Chalker and Sandmeyer 1990; Kim et al. 1998; Zou et al. 1996) . TSD length distributions of unique insertion sites for McClintock predictions in real yeast genomes are shown for Ty1, Ty2, Ty3, and Ty4 in Figure S4 in File S1. As observed in simulated data (Figure 2 and Figure S2 in File S1), ngs_te_mapper predictions had the highest proportion of correct TSD lengths predicted per family. However, in contrast to simulated data, ngs_te_mapper can infrequently make incorrect TSD-length predictions in real data. Confirming simulation results, RelocaTE generally underpredicts the length of TSDs, and TEMP consistently overpredicts the lengths of TSDs for all families in real data.  For all split-read methods, the modal value of the TSD-length distribution reflects the true TSD length for all families. Thus, the modal TSD length provided by each of the split-read methods yields biologically meaningful inferences about TSD structure.    CONCLUSIONS AND FUTURE DIRECTIONS\r\n  Here we describe McClintock, an integrated pipeline for detecting TE insertions in WGS resequencing data. McClintock offers many advantages relative to running multiple TE detection methods in isolation. Specific versions of compatible software dependencies required to run each method are fully documented, allowing users to easily set up their environment. The number of input files required to run all methods is reduced and complex processing of input files to create the correct custom formats and file relationships is automated. In addition, the pipeline is structured to allow parallel computations for multiple samples, so population data sets can be analyzed more quickly. Finally, results from individual methods are standardized to facilitate comparisons across methods and easy visualization in the UCSC genome browser. Overall, McClintock greatly lowers the barriers to running multiple TE detection methods, allowing users to gain more insight into how various methods work for their samples. McClintock does not currently include all published TE detection methods, although additional methods can be easily incorporated into the pipeline due to the flexible architecture and open-source nature of the project.  In addition, we have applied McClintock to simulated and real yeast WGS samples to evaluate the performance of McClintock component methods. Simulations on the unmodified S. cerevisiae reference genomes reveal that sequencing coverage influences detection of reference TEs, but that recovery of reference TE insertions and false-positive rates for nonreference TE insertions are generally low even at high sequencing coverage. Simulations on S. cerevisiae reference genomes 2776 | including a single nonreference insertion showed that pure split-read methods may detect fewer TE insertions than read-pair methods, but they have much higher positional accuracy. Single-insertion simulations also revealed that the TE family affects the ability of methods to detect nonreference TE insertions. We find substantial difference in the ability of McClintock component methods to detect subsets of nonreference insertions in the yeast genome, but that by combining multiple methods that use split-read and read-pair data, nonreference TEs at nearly all biologically realistic locations can be detected in simulated data. Finally, application of McClintock to a large sample of real yeast genomes reveals that most but not all McClintock component methods can recover known aspects of TE biology in yeast such as family activity status, tRNA gene targeting, and TSD structure. Together, our results suggest that even in the context of a simplified model eukaryotic genome like S. cerevisiae, current TE detection methods using short-read data do not provide comprehensive recovery of all TE insertions in WGS resequencing samples. Further performance studies in other genomic contexts, including newer methods not currently included in McClintock, are needed to generalize the results presented here, and to provide a road map for developing more advanced systems for the detection of TEs in unassembled short-read genomic data.    ACKNOWLEDGMENTS\r\n  We thank Matthew Ronshaugen, Douda Bensasson, and members of the Bergman Laboratory for helpful comments throughout the project; Robert Kofler, Thomas Keane, Alexander Platzer, Sofia Robb, and Jiali Zhuang for code fixes and insights into McClintock component methods; and Nick Gresham for high-performance cluster computing assistance. This work was supported by Wellcome Trust Award 096602/B/11/Z (M.G.N.) and Human Frontier Science Program grant RGY0093/2012 (C.M.B.).  August 2017 |  McClintock: Pipeline for TE Detection | 2777    ",
    "sourceCodeLink": "https://github.com/lh3/wgsim",
    "publicationDate": "0",
    "authors": [
      "Michael G. Nelson",
      "Raquel S. Linheiro",
      "Casey M. Bergman"
    ],
    "status": "Success",
    "toolName": "wgsim",
    "homepage": ""
  },
  "40.pdf": {
    "forks": 5,
    "URLs": [
      "github.com/ikalatskaya/ISOWN",
      "www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id=phs000598.v2.p2",
      "www.genecards.org/cgi-bin/carddisp.pl?gene=MUC4",
      "dcc.icgc.org/pcawg",
      "portal.gdc.cancer.gov/",
      "dcc.icgc.org/releases/PCAWG/cell_lines",
      "dockstore.org/containers/quay.io/pancancer/pcawg-dk"
    ],
    "contactInfo": [],
    "subscribers": 3,
    "programmingLanguage": "Perl",
    "shortDescription": "",
    "publicationTitle": "ISOWN: accurate somatic mutation identification in the absence of normal tissue controls",
    "title": "ISOWN: accurate somatic mutation identification in the absence of normal tissue controls",
    "publicationDOI": "10.1186/s13073-017-0446-9",
    "codeSize": 12228,
    "publicationAbstract": "Background: A key step in cancer genome analysis is the identification of somatic mutations in the tumor. This is typically done by comparing the genome of the tumor to the reference genome sequence derived from a normal tissue taken from the same donor. However, there are a variety of common scenarios in which matched normal tissue is not available for comparison. Results: In this work, we describe an algorithm to distinguish somatic single nucleotide variants (SNVs) in next-generation sequencing data from germline polymorphisms in the absence of normal samples using a machine learning approach. Our algorithm was evaluated using a family of supervised learning classifications across six different cancer types and ~1600 samples, including cell lines, fresh frozen tissues, and formalin-fixed paraffin-embedded tissues; we tested our algorithm with both deep targeted and whole-exome sequencing data. Our algorithm correctly classified between 95 and 98% of somatic mutations with F1-measure ranges from 75.9 to 98.6% depending on the tumor type. We have released the algorithm as a software package called ISOWN (Identification of SOmatic mutations Without matching Normal tissues). Conclusions: In this work, we describe the development, implementation, and validation of ISOWN, an accurate algorithm for predicting somatic mutations in cancer tissues in the absence of matching normal tissues. ISOWN is available as Open Source under Apache License 2.0 from https://github.com/ikalatskaya/ISOWN.",
    "dateUpdated": "2017-09-30T08:17:40Z",
    "institutions": [
      "University of California Davis",
      "University of Toronto",
      "University of Edinburgh",
      "Ontario Institute for Cancer Research"
    ],
    "license": "Apache License 2.0",
    "dateCreated": "2016-10-17T20:25:01Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     Kalatskaya et al. Genome Medicine     10.1186/s13073-017-0446-9   ISOWN: accurate somatic mutation identification in the absence of normal tissue controls     Irina Kalatskaya  4    Quang M. Trinh  4    Melanie Spears  1  5    John D. McPherson  0    John M. S. Bartlett  1  3  5    Lincoln Stein  2  4    0  Department of Biochemistry and Molecular Medicine, University of California Davis ,  Sacramento, California ,  USA    1  Department of Laboratory Medicine and Pathobiology, University of Toronto ,  Toronto, ON ,  Canada    2  Department of Molecular Genetics, University of Toronto ,  Toronto, Ontario ,  Canada    3  Edinburgh Cancer Research UK Centre ,  MRC IGMM ,  University of Edinburgh ,  Edinburgh ,  UK    4  Informatics and Bio-computing, Ontario Institute for Cancer Research ,  Toronto, Ontario ,  Canada    5  Transformative Pathology, Ontario Institute for Cancer Research ,  Toronto, Ontario ,  Canada     2017   9  2  19    6  6  2017    9  1  2017     Background: A key step in cancer genome analysis is the identification of somatic mutations in the tumor. This is typically done by comparing the genome of the tumor to the reference genome sequence derived from a normal tissue taken from the same donor. However, there are a variety of common scenarios in which matched normal tissue is not available for comparison. Results: In this work, we describe an algorithm to distinguish somatic single nucleotide variants (SNVs) in next-generation sequencing data from germline polymorphisms in the absence of normal samples using a machine learning approach. Our algorithm was evaluated using a family of supervised learning classifications across six different cancer types and ~1600 samples, including cell lines, fresh frozen tissues, and formalin-fixed paraffin-embedded tissues; we tested our algorithm with both deep targeted and whole-exome sequencing data. Our algorithm correctly classified between 95 and 98% of somatic mutations with F1-measure ranges from 75.9 to 98.6% depending on the tumor type. We have released the algorithm as a software package called ISOWN (Identification of SOmatic mutations Without matching Normal tissues). Conclusions: In this work, we describe the development, implementation, and validation of ISOWN, an accurate algorithm for predicting somatic mutations in cancer tissues in the absence of matching normal tissues. ISOWN is available as Open Source under Apache License 2.0 from https://github.com/ikalatskaya/ISOWN.    Next-generation sequencing  Somatic mutation  Matching normal tissue  Variant classification       Background\r\n  Somatic, or acquired, mutations are genetic changes that accumulate in the non-germline cells of an individual during his or her lifetime. Somatic mutations that disrupt genes involved in one or more of the pathways that regulate cell growth, programmed cell death, neovascularization, and other \u201challmarks of cancer\u201d can lead to the development of a neoplasm [ 1-4 ]. The use of nextgeneration sequencing to comprehensively characterize cancer genomes has led to multiple breakthroughs in the understanding of driver genes and pathways involved in cancer [ 5-7 ], the interaction between environmental exposures and patterns of mutations [ 8, 9 ], tumor classifications [ 10, 11 ], and the evolution of tumors in the presence and absence of therapy [ 12, 13 ].  Accurate identification of somatic mutations is an essential first step for many cancer studies. There are many challenges in mutation calling, including but not limited to: (a) the admixture of multiple tumor subclones with each other and with normal tissue; (b) the frequent presence of copy number alterations in tumors; and (c) a raw error rate from sequencing instruments that is comparable to the variant allele frequency of mutant alleles in admixed samples. Nevertheless, the current generation of somatic mutation calling tools are highly accurate, even in the presence of admixed samples with low variant allele frequencies [ 14-17 ]. However, all these tools require both patient's tumor and normal tissues (typically white blood cells or adjacent normal tissue in the tumor resection specimen) in order to distinguish somatic mutations from uncommon germline polymorphisms. These tools construct a multiple alignment with both the tumor and normal reads, and then scan down the columns of the alignment to identify tumor-specific alterations, using statistical models of sequencing error rates and base quality scores to reduce false positives.  In some commonly encountered scenarios, however, matching normal tissues are not available. This may be because normal samples were not collected in the first place, or because the patient consent was obtained in a way that precludes examination of normal tissue or germline variants. This is most commonly encountered when performing analysis on retrospective studies with human material from clinical trials, pathology archives, and legacy biobanks, a strategy that may be required when building a cohort of a rare cancer type or subtype, or when executing secondary studies on clinical trials. Another common scenario is the use of a cancer cell line as an experimental model, many of which have no information on the donor's normal genomes. There may also be financial considerations; sequencing both tumor and normal genomes not only roughly doubles the cost but also increases data storage and computational requirements. In these cases, there is a need to identify somatic mutations from tumor tissues without the presence of the normal tissues.  One of the main challenges for accurate identification of somatic mutations in the absence of normal DNA is to distinguish somatic mutations from germline polymorphisms (single nucleotide polymorphisms (SNPs)). On average, the genome of any human individual contains ~3,300,000 SNPs [ 18 ]. Roughly 20,000-25,000 of those are coding variants and 9000-11,000 are nonsynonymous [ 19 ]. All common SNPs with population frequencies of 1% or greater in the major world population groups have been extensively catalogued [ 20 ], and these can be excluded from consideration by a simple filtering step. Some ethnic subpopulations are under-represented and appropriate calibration within these groups may be required. In addition, however, each individual is estimated to carry 400,000-600,000 rare SNPs specific to the individual or his or her close family [ 19 ], and these cannot easily be excluded by comparison with SNP databases or with recent large-scale exome sequencing projects.  In this study, we describe an algorithm that uses supervised machine learning to distinguish simple substitution somatic mutations in coding regions from germline variants in the absence of matching normal DNA. The accuracy of this approach, calculated based on the whole-exome sequencing data from The Cancer Genome Atlas (TCGA), as well as targeted (gene-panel) sequencing performed on formalin-fixed paraffinembedded (FFPE) tissue, lies in a range that would be acceptable for most applications.    Implementation\r\n   Validation sets\r\n  Protected datasets in VCF format (containing both somatic and germline variants) were downloaded directly from TCGA portal. Only one sample (TCGA-IB-7651-01A from PAAD) was excluded from the analysis based on its extremely high mutational loads (~300-fold in comparison to the median for this cancer set). According to the headers of the retrieved VCF files, variant calling in KIRC (kidney renal clear cell carcinoma), PAAD (pancreatic adenocarcinoma), and COAD (colon adenocarcinoma) sets was done using the Baylor College of Medicine (BCM) CARNAC (Consensus And Repeatable Novel Alterations in Cancer) pipeline (version 1.0) [ 21 ]; in BRCA (breast invasive carcinoma) and UCEC (uterine corpus endometrial carcinoma) sets with the bambam pipeline (version 1.4) from University of California at Santa Cruz (UCSC; Sanborn JZ, Haussler D; University of California; Bambam: parallel comparative analysis of high-throughput sequencing data. Patent. EP2577538 A1). During quality control of the validation sets, we noticed that, of the five TCGA datasets used for validation, the KIRC, PAAD, and COAD sets did not contain any homozygous variants, possibly a consequence of CARNAC filtering. To maintain consistency across all five data sets, we removed all homozygous variants from UCEC and BRCA as well.  In addition, we downloaded 145 ESO (esophageal adenocarcinoma) BAM files from dbGAP portal (https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id=phs000598.v2.p2 [2 22 ]). We extracted the raw reads from the BAM files and aligned them to human genome hg19 using BWA (v0.6.2) [2 23 ]. Collapsed reads that aligned in the correct orientation were passed to Mutect2 (bundled with GATK v3.6) [1 17 ] to call variants. MuTect2 was run twice on each sample in two different modes: (1) in the usual mode with pair matching normal to retrieve gold-standard somatic mutation calls; and (2) in so called tumor_only_mode to call all variants (including all somatic and some germlines). This mode mimics the situation when matching normal data are not available. Variants from 100 ESO samples were randomly selected and used for training set generation and the remaining samples for validation.  ANNOVAR (version released on 2012-03-08) was used for coding region functional annotations [ 24 ]. Variants were filtered based on the following criteria: (1) minimum coverage of at least 10×; (2) PASS filtering; (3) exclusion of all non-single nucleotide variants (non-SNVs; e.g., indels or multiple base substitutions); (4) removing all variants with \u201cN\u201d as reference alleles; and (5) exclusion of all variants that were labeled as \u201cunknown\u201d by ANNOVAR. The basic statistics of each dataset are shown in Table 1. The use of TCGA and ESO data sets was authorized under dbGaP project #6257.    Variant annotations\r\n  Each variant in every validation cancer set was annotated using COSMIC v69 [ 25 ], dbSNP v142 [ 20 ], Mutation Assessor [ 26 ], ExAC r0.3 [ 27 ], and PolyPhen-2 [ 28 ]. Annotation against the dbSNP database produced two outputs: (1) whether a variant was catalogued by the \u201ccommon_all\u201d division of dbSNP (found in ≥1% of the human population by definition); or (2) represents a rare polymorphism. COSMIC v69 was released prior to the availability of TCGA or ESO data sets used for validation, and is therefore not contaminated with somatic mutations from those sets. (The first COSMIC release to contain data from any of these sets was version 72). Future users of ISOWN are encouraged to use the latest version of COSMIC.    Supervised learning\r\n  WEKA (Waikato Environment for Knowledge Analysis) software v3.6.12 suite [ 29 ], a mature Java-based machine learning toolkit, was employed for the variant classification task. The WEKA toolkit provided a collection of machine learning algorithms for data mining together with graphical user interfaces. Algorithms used in the study are described in Additional file 1: Supplemental methods.  The performance of all classifiers was evaluated by tenfold cross-validation, and the following six measures were used to estimate classifier performances: 1. Recall (or sensitivity or true positive rate) measures the proportion of the known somatic variants that are correctly predicted as those and is defined as TP/(TP + FN), where TP is true positive and FN is false negative. 2. Precision is a fraction of the correctly called somatic mutations to all variants that are labeled as somatic by the classifier and is defined as TP/(TP + FP), where FP is false positive. 3. F1-measure [ 30 ] is the harmonic mean of precision and recall: 2 × (Precision × Recall)/ (Precision + Recall). 4. False positive rate (FPR) is the fraction of germline variants incorrectly classified as somatic and is defined as FP/(FP + TN), where TN is true negative. 5. Accuracy (ACC) is the proportion of variants that are correctly predicted and is defined as (TP + TN)/ (TP + FN + TN + FP). 6. Area under ROC curve (AUC) denotes the probability that a classifier assigns a higher score to the positive instance than a randomly chosen negative sample. It measures the general ability of the classifier to separate the positive and negative classes. The best performing classifier for each cancer dataset was selected based on AUC and F1-measure.    External and internal features\r\n  All features used for variant classification are shown in Table 2. Variants are described by ten features that ultimately contributed to subsequent machine learning training and evaluation steps. One class of features came from external databases, and the other class was derived from the characteristics of the variants themselves.  Features based on external databases: 1. The Catalogue Of Somatic Mutations In Cancer (COSMIC) [ 25 ] is by far the richest database of the cancer-related somatic mutations. The presence of a candidate variant in COSMIC is predictive, but not definitive, of a somatic origin. The biggest drawback of COSMIC (v69) usage is that more than 90% of all coding somatic SNVs catalogued by COSMIC were submitted from a single sample. Most of these are random passenger mutations. In practice, therefore, we used the COSMIC CNT (instead of just acknowledging the presence of a variant in this database) attribute as the feature presented to machine learning. CNT is an attribute assigned to each coding variant catalogued by COSMIC and represents a number of samples with a mutation across all tumor types. The CNT value was used as a feature in the classifier. If the variant wasn't catalogued by COSMIC, this value of the numeric feature was assigned to zero. Thus, CNT varies from 0 to 19,966 (a well-described mutation in BRAF). 2. Correspondingly, the Exome Aggregation  Consortium (ExAC) has collected germline variants from ~60,000 independent individuals and is one of the richest databases of common germline polymorphisms. A boolean feature based on the presence in ExAc (is.in.ExAc/not.in.ExAc) was assigned to each variant in our validation sets and used as an independent feature. 3. The dbSNP resource, another widely used collection of the common germline variants, classifies submitted variants into common (≥1% of the human population) and rare polymorphisms. All variants in validation sets were annotated against dbSNP/ common_all and dbSNP/rare databases. The information from the first set was used for variant pre-labeling (see the \u201cVariant pre-labeling\u201d section) ) to g 5 .  0 .5  ]2 ] ] ] ] ] aI] .,45062 .849062 .058129 .,301078 .522758 .595855 C , , , , , ,  3 6 1 8 1 6 ilen [59% .,2279 .2150 .0628 .6297 .7785 .,1703 rm le 1 2 8 7 8 5  [ [ [ [ [ [ e p  2 6 1 7 8 1 g m .9 .8 .4 .8 .4 .5 n sa 8 4 8 7 6 1 ae re ,0 ,0 ,9 ,9 ,6 ,4  0 7 8 4 5 5 M p 2 2 8 7 8 5 ] ] ] 6 8 5 s aI] .6 .5 .0 V C 50 ] 16 ]1 ]8 31 SN 5% ,2 .48 ,3 .55 .05 ,2 c 9 6 2 8 3 4 5 it [ .3 1 .7 , , .6 a le 88 ,5 91 .60 .59 50 m p [ .0 1 1 3 1 so m 5 [9 [ [3 [3 [  1 8 5 n sa .0 7 .6 6 8 .8 ae re 7 .7 6 .5 .0 1  4 0 7 3 7 8 M p 1 1 2 3 3 1 t s n le u fo ilen p 0 5 c o r am 41 32 ,51 ,15 18 51 ca ebm r/egm llsa ,420 ,743 ,329 ,164 ,396 ,009 iton ltaun itcamaisVn ,/5210 ,/0165 ,/1426 ,/2984 ,/2139 ,/7890 tkaen o o N 8 5 0 0 ,5 6 re T s S 3 5 6 1 5 2 e g n lli a c and therefore was not used again for the classifier. The second annotation was used as an independent feature in the classifier. 4. Sequence context is defined as the three-base sequence comprising the variant and its flanking bases. It is known that different cancer types have different mutational signatures [ 31 ]. In addition, sequence context can help to distinguish germline from somatic mutations due to the differences in the mutational processes that often, but not always, generate these two types of change. For example, we have noticed that in all six cancer sets somatic mutations are significantly enriched in the AGA pattern and germline polymorphisms in the ATG pattern. 5. Mutation Assessor predicts the functional impact of amino acid substitutions in proteins based on evolutionary conservation of the affected amino acid in protein homologs. We assume that, on average, the impact of the somatic mutation on protein function will be significantly higher than a germline polymorphism. Categorical output from Mutation Assessor (high, medium, low, or neutral) was used as a feature in the classifier. Stop loss and especially stop gain mutations (annotated by ANNOVAR) usually have greater impact on protein function and predominantly occur as somatic alterations. As variants that introduce stop gain or stop loss are ignored by Mutation Assessor and mutually exclusive to its output; these mutation types were added as categories of the feature. 6. PolyPhen-2 is a tool that predicts damaging effects of missense mutations based on both sequence and structural information. It was also used as an independent feature in the classifier.  With respect to the use of functional impact features, while a small number of germline polymorphisms may have high protein structure impact, we confirmed that in all sets used for validations, somatic mutations are significantly enriched in \u201chigh\u201d and \u201cmedium\u201d impacts, whereas germline polymorphism are enriched in \u201cneutral\u201d impacts. For example, the ratio of germline polymorphisms scored as neutral impact by Mutation Assessor ranged from 40 to 45% depending on cancer data set, while neutral somatic mutations occurred 23-27% of the time (Additional file 1: Table S6). A similar difference was observed for PolyPhen-2 output (Additional file 1: Table S7).  The following four features are generated based on internal characteristics of the variants themselves: sample frequency, variant allele frequency, substitution pattern, and flanking regions (Table 2).  Internal annotations: 7. Sample frequency is calculated as the fraction of samples carrying that particular variant over the total number of samples in the particular dataset. Variants with high sample frequencies are more likely to be germline polymorphisms. More detailed justification of this feature is provided in the  Additional file 2: Figure S4. 8. Variant allele frequency (VAF) is calculated as the ratio of number of reads supporting the variant allele over the total number of reads. The heterozygous VAF distribution is centered at 50% [ 32 ] for germline polymorphisms; however, germline VAFs can deviate from 50% when they are involved in a somatic copy number alteration event. VAFs for somatic mutations are more likely to have values below 50% due to copy number variation, admixture with normal tissues and/or tumor subclonality, and, on average, range from 22% to 50% [ 7 ] and in some cases reach values greater than 50% due to amplification events (Additional file 2: Figure S3). 9. Flanking regions: The VAF of each variant is an informative feature due to the fact that somatic mutations tend to be subclonal, while heterozygous SNPs will have a VAF close to 50%. To use VAF as a predictive feature, we examine regional differences in VAF between the candidate variant and flanking polymorphisms. For each candidate variant (X) we searched for flanking polymorphisms (that were catalogued by dbSNP/common) within 2 Mbp of flanking 5\u2032 or 3\u2032 regions from X (Additional file 2: Figure S1a). The 5\u2032 and 3\u2032 flanking region polymorphisms are labeled as V1 and V2, respectively. If both V1 and V2 exist and the 95% confidence intervals (CIs) of their VAFs, as determined by the binomial distribution, overlap the 95% CI of X, then X is more likely a germline variant. On the other hand, if the VAF CI for X overlaps the CI for neither V1 nor V2, while the V1 and V2 CIs overlap with each other, then X is most likely a somatic variant. In all other cases, including where V1 and/or V2 were not found within the 2-Mbp flanking regions, this feature is marked as NA (not applicable). The flanking region feature measures whether the VAF of an unknown variant is similar to the VAF of flanking known germline polymorphisms. Because copy number alterations are often quite large, germline polymorphisms are expected to have similar VAFs to those of flanking SNPs, while a somatic mutation VAF should be different from its flanking SNPs. This feature strongly depends on the presence of known germline polymorphisms in close proximity to an unclassified variant, and because of this and the strict conditions for defining informative flanking SNPs, this feature is unavailable for up to 50% of the variants in a typical cancer exome. 10.Substitution pattern is defined as a two base sequence that contains the reference (wild type) and the newly introduced variant base of the mutation. For example, the substitution pattern of chr3,178936094C &gt; G mutation is \u201cCG\u201d. All substitution patterns are combined into six categorical subtypes: \u201cCA\u201d, \u201cCG\u201d, \u201cCT, \u201cTA\u201d, \u201cTC\u201d, and \u201cTG\u201d. We determined that somatic mutations (as well as germline polymorphisms) are often enriched in the particular substitution pattern. For example, across all tested datasets somatic mutations were significantly enriched in C &gt; A/G &gt; T substitutions and germline variants were significantly enriched in T &gt; C/A &gt; G exchanges.    Feature selection\r\n  We used the WEKA-InfoGain feature selection tool to ensure all features we selected are relevant and not redundant [ 33 ].    Variant collapsing\r\n  For the somatic/germline classification task, we assumed that variants that share the same genomic position and substitution pattern are either somatic or germline across all samples within a particular cancer data set (Additional file 2: Figure S2). We distinguished between the set of unique variants, defined as the unique union of all variants (genomic positions + substitution patterns) in the data sets, from the set of total variants, which includes all variants across all samples. This simplifies the classification problem: instead of making predictions on a large number of variants (ranges in million; see column 6 in Table 1), we only need to do predictions on a few hundreds of thousands unique variants (Additional file 1: Table S5). Justification of this step is provided in Additional file 1: Supplemental methods (Additional file 1: Table S5). Variant collapsing is the process of transforming the set of total variants into the set of unique variants.    Adapting internal machine learning features to the mono-labeled approach\r\n  After variant collapsing, the features generated based on external annotations will be identical for all samples in which this variant was found. For example, chr7,140453136A &gt; T in COAD detected in 27 out of 215 samples will have identical values for CNT, ExAC, dbSNP, Mutational Assessor, PolyPhen, and sequence context annotations across all 27 samples. However, as a consequence of variant collapsing, VAF and flanking region annotations might be different for the same variant from sample to sample. Thus, if a variant was called in one sample, its actual VAF value was used in the classifier; otherwise, if a variant was called across two or more samples, the mean of VAFs of all variants is used.  Flanking region assessment was calculated for each variant as either \u201ctrue\u201d, \u201cfalse\u201d, or \u201cNA\u201d (described above). If a variant was called in only one sample, flanking region assessment equals \u201ctrue\u201d was converted into a flanking region feature equals \u201c1\u201d and \u201cfalse\u201d to \u201c0\u201d. Multiple ambiguous decisions for the same variant across multiple samples were collapsed in the following way: a weight ranging from 0 to 1 for each collapsed variant is calculated as the ratio of \u201ctrue\u201d counts over the total number of samples with this variant (Additional file 2: Figure S1b). If flanking regions across all samples were all NAs, then the weight is NA.    Supervised learning algorithms\r\n  The full list of the tested supervised learning algorithms together with their short descriptions as well as settings and optimization strategies can be found in Additional file 1: Supplemental methods. In summary, seven algorithms were tested: JRip [ 34 ], J48 [ 35 ], random forest [ 36 ], LADTree [ 37 ], naïve Bayes classifier (NBC) [ 38 ], logistic regression [ 39 ], and support vector machine (SVM) [ 40 ]. selected polymorphisms for each cancer type. The best classification algorithm was chosen using tenfold cross-validation based on the highest AUC.    Variant pre-labeling\r\n  Some subsets of variants do not require classification. For example, the variants that are in dbSNP/common_all and not in COSMIC are most likely germline in origin and were pre-labeled as such; justifications are provided in Additional file 1: Table S3. High values for COSMIC CNT is a good indicator that variants are true somatic mutations (Additional file 1: Table S4), and all variants with CNT ≥100 were pre-labeled as somatic. Pre-labeled variants were not subjected to the classification step (Fig. 1).    Tenfold cross-validation\r\n  Tenfold cross-validation was used to perform the primary assessment of the algorithm performance and to choose the best classification strategy. We generated 1000 training subsets each containing 700 randomly selected somatic mutations and 700 randomly    Validation on independent sets\r\n  The best classification algorithm chosen during tenfold cross-validation was trained using a linearly increasingly number of samples from 1 to 100 for each cancer set. The validation was done using a separate validation dataset (not used in training) based on: (1) only nonsilent variants; (2) only silent variants; (3) somatic mutations occupying different VAF tiers. We also performed cross-cancer validation by training in one cancer type and validating in a different cancer type. The algorithm was also evaluated on an independent pancreatic cancer dataset and a series of cell lines.     Results\r\n   Development of a somatic prediction pipeline\r\n  In this work we focused on predicting single-base substitution somatic mutations in coding regions. Figure 1 illustrates the overall architecture of our prediction algorithm. The design of our pipeline can be summarized as follows: VCF files containing both somatic and germline variants from five cancer types were downloaded from TCGA portal. Only those variants that passed a somatic mutation caller filter (marked with \u201cPASS\u201d in VCF files) with read depth at least 10× were used in the prediction pipeline. Each variant was annotated against ANNOVAR, dbSNP, ExAC, COSMIC, Mutation Assessor, and PolyPhen. Based on functional annotations from ANNOVAR, we removed all non-coding variants as well as variants with unknown annotations.  We chose validation data sets that represent a range of somatic mutation loads and mutation-calling pipelines. For the five validation datasets from TCGA, we used the published somatic mutations and germline polymoprhisms, which were in turn derived from paired tumor-normal samples processed by either the CARNAC or the bambam pipelines (Table 1). In addition, we generated validation data for a sixth data set (145 esophageal adenocarcinoma (ESO) samples) using the popular Mutect2 paired mutation caller [ 17 ], starting with unaligned BAM files. Mutect2 was first ran in paired mode on tumor and matched normal to generate the gold standard list of somatic mutations. We then ran Mutect2 in tumor-only mode on the tumor sample only to generate somatic mutations together with germline variants to present to the classifier. The second mode completely mimics the situation when matching normal tissues are not available.  To validate different supervised learning algorithms provided by WEKA, for each tumor type we generated 1000 training sets in Attribute-Relation File Format (ARFF), each containing 700 randomly selected somatic mutations and 700 randomly selected germline polymorphisms. The performance of the machine learning classifiers was evaluated using tenfold cross-validation based on the training sets. This was repeated using classifiers representative of each of the major classification methods (see \u201cList of tested learning algorithms\u201d in Additional file 1: Supplemental materials). The best classification method was chosen based on the highest AUC.  For validation purposes, the sample set was then randomly divided into a training sample subset (100 samples) and a held-out validation sample subset (the remaining samples). Each of the six cancer type data sets was preprocessed and collapsed independently. Using the best classification methods (NBC and LADTree), the classifier was trained with a gradually increasing number of samples from the training set and the accuracy was calculated using the held-out validation sample set.    Datasets\r\n  Evaluation of the classifiers was performed on six different cancer datasets: UCEC (uterine corpus endometrial carcinoma), KIRC (kidney renal clear cell carcinoma), COAD (colon adenocarcinoma), BRCA (breast invasive carcinoma), ESO (esophageal adenocarcinoma), and PAAD (pancreatic adenocarcinoma).  In total, six different tumor types were used for ISOWN validation. All datasets were sequenced using Illumina technology. Average read depth ranged from 58× to 363× (Table 1). The number of samples in each dataset as well as the number of the coding non-silent variants per data set are provided in Table 1. The average number of somatic non-silent mutations in the coding regions per sample ranged across an order of magnitude from 10.77 for BRCA to 276.68 in COAD (Table 1).  Because of the range in somatic mutation and germline polymorphism rate, each of the testing sets contained different ratios of positive (somatic mutation) and negative (germline polymorphism) instances, which allowed us to validate the algorithm in several different settings (Table 1, last column). The ratio of somatic to germline variants ranged from 2:1 in the UCEC set to 1:10.5 in the PAAD set and, surprisingly, did not always correlate with mutational load. For example, BRCA has the lowest mutational load (~10 somatic SNVs per sample; Table 1) but the number of germline variants is only six times higher than somatic variants (in the collapsed set), whereas PAAD has 37 somatic SNVs per sample but the ratio of somatic to germline variants reaches 1:10. It is unlikely that the rate of germline SNPs varies to this extent across TCGA cancer cohorts, and most likely these differences reflect disparities in the approaches used to call and filter variants in these datasets. Our algorithm was nevertheless able to learn and correctly discriminate somatic from germline variants across a wide range of absolute variation counts and somatic to germline ratios.    Tenfold cross-validation and the best classification method selection\r\n  We first set out to select the best classifier(s) for each cancer dataset, investigate whether the best classifier is cancer-specific, and to compare performance measures across different cancer types. We present the results from the best trained models for only the seven supervised learning algorithms we selected, although several others were investigated (Additional file 1: Supplemental methods).  The performance measures presented here were retrieved based on collapsed datasets (see the \u201cVariant collapsing\u201d section) without taking into account prelabeled variants. Cross-validation was done based on 1000 training sets, each balanced with 700 somatic and 700 germline variants randomly selected from each cancer set (Fig. 1 and \u201cImplementation\u201d section).  Figure 2 shows performance measures from tenfold cross-validation for all cancer datasets. The top panel shows similar performances for five out of six cancer datasets regardless of which supervised learning method was used. ESO is the only dataset with slightly lower F1-measure (ranges from 88 to 95%). Overall, all seven selected classifiers showed comparable performances in each of the six cancer data sets we tested, ranging from ~3-4%.  The false positive rate (FPR) was less than 7% for all datasets except ESO. Usage of NBC consistently shows the lowest FPR below 5% for all but the ESO set. The FPR for the ESO set ranges from 6 to 12% (Fig. 2, middle panel).  Based on the AUC, the worst classifier in all six cases was SVM in spite of the fact that both kernels, Polykernel and RBF-kernel, were tested and optimized. The AUC for the best classifiers was estimated to be ~99% for COAD, UCEC, KIRC, and BRCA, ~98% for PAAD, and ~96% for ESO (Additional file 1: Table S1). Based on mean AUC value, NBC and LADTree were chosen as the best classification algorithms (Fig. 2, bottom panel) for all cancer sets but ESO. Random forest and LADTree were used for ESO.    Classifier validation and effect of training set size on performance\r\n  The final assessment of the classifier performance was done based on the held-out validation testing sample sets that had not been used in the training procedure (see pipeline description and Fig. 1). In addition, we investigated the effect of the size of the training set on the final performance measures. The validation was performed as follows: the indicated classifier was trained based on gradually increasing number of samples (starting from 2 to 100 with increments of one) and for each case, accuracy, F1-measure, and FPR were calculated based on the held-out testing set. The training set was generated based on all somatic variants retrieved from the indicated number of samples plus an equal number of randomly selected germlines.  The overall accuracies for all six cancer sets is over 99.0% for almost all training sets (Additional file 3: Table S2). But the FPR and F1-measure are better measurements of a classifier's performance when the data set is unbalanced, as it is in the validation sets used in this study. The FPR was below 0.5% if the classifier was trained with at least 25 samples for the COAD, UCEC, KIRC, and BRCA sets, and at least 50 samples for PAAD and ESO (Additional file 2: Figure S5). The F1-measure was high (above 90%) in four out of six studied cancer sets and reached 91.1% for KIRC, 93.2% for ESO, 96.6% for COAD, and 98.6% for UCEC. BRCA, with a max F1-measure of 88%, showed slightly reduced but still acceptable performance. PAAD had the worst accuracy, with the F1-measure reaching a maximum of just 76% (Fig. 3).  The comparison of performance between the two best classifiers, LADTree and NBC (random forest for ESO), is depicted in Fig. 3 and Additional file 2: Figure S5. When applied to the BRCA, COAD, and UCEC tumor types, NBC and LADTree classifiers were indistinguishable. In KIRC and PAAD, NBC significantly outperformed LADTree in terms of accuracy. LADTree and random forest showed no differences in performance in ESO.  The F1-measure plateaus for all cancer sets but PAAD, most likely due to low mutation load. Thus, we recommend using at least 25 samples for training of highly mutated cancer types (like COAD, ESO, and UCEC) and 50-100 samples for medium mutated types (like BRCA and KIRC) and &gt;100 samples for cancers with a low mutation load (like PAAD). Recall and precision for the above described experiments are listed in Additional file 3: Table S2.    ISOWN performance on silent mutations\r\n  Some applications require a list of silent somatic mutations in addition to non-synonymous ones. We evaluated the accuracy of our classifier for distinguishing silent somatic mutations in coding regions. In this scenario, PolyPhen and Mutation Assessor do not provide functional annotations for most variants. Thus, we expected that the performance of the classifier would be slightly lower due to missing functional annotation features.  We performed training and validation in a similar manner as described earlier: training using nonsynonymous variants from increasing number of samples from each cancer set and validating with either non-silent variants only (as it was done in the previous experiment) or silent variants only. As LADTree showed better or comparable performance (see \u201cClassifier validation and effect of training set size on performance\u201d section) in the majority of the datasets, it was selected for this and following experiments. For the purposes of comparison, F1-measures are shown for predictions of both silent and non-silent somatic mutations in Additional file 2: Figure S6. In all six tumor types the F1-measure was reduced for silent mutation prediction versus non-silent. The effect was relatively small for UCEC, ESO, and COAD, with reductions in F1-measure of 1.9, 2.3, and 3.5%, correspondingly. Other tumor types showed a stronger effect: F1 was reduced by 8.9, 11.9, and 17.7% in KIRC, PAAD, and BRCA, respectively, when applied to silent variants (Additional file 2: Figure S6). We also observed that the classifiers plateaued at roughly the same number of training samples regardless of whether silent or nonsilent variants were tested (Additional file 2: Figure S6).  In summary, the ISOWN algorithm can correctly classify silent coding variations at acceptable levels in tumor types with high and moderate mutational loads (F1 92-97% for COAD, ESO, and UCEC, 80-87% for BRCA and KIRC), but has error rates that are unacceptably high in tumors with low mutational loads (69.2% for PAAD).    ISOWN performance in relationship to VAF\r\n  Depending on the cellularity and heterogeneity of the tumor sample, the VAF of somatic mutations may vary significantly. Accurate calling of low-VAF mutations is important for identification and characterization of subclones present in the tumor. To address this issue, we studied the impact of VAF on ISOWN accuracy. For this experiment, we trained the LADTree classifier according to the protocol described earlier, but divided the somatic mutations used in the testing sets into two sets based on their collapsed VAF values: low VAF variants (VAF ≤ median of all collapsed somatic variants) and high VAF. To maintain the original ratio of somatic and germline variants in the testing set, germline polymorphisms were randomly divided among the two test sets.  As we expected, ISOWN shows consistently better performance for predicting somatic mutations with low VAF in comparison to high VAF. The median VAF varied from 11.3% in the PAAD set to 31.7% in the UCEC set (Additional file 2: Figure S2). In spite of this wide variation, we observed only minor differences in the F1-measure (in the range of 0.1-2.9% differences) in the majority of tumor types. The most significant differences were observed in ESO, where we observed a reduction of 4.3% in the F1-measure for somatic mutation classification for low versus high VAF test sets (Additional file 2: Figure S7). In conclusion, ISOWN performs well in predicting somatic mutations across differing VAF tiers.    ISOWN performance on cross-cancer type training and testing\r\n  In some cases, it may be difficult to find a sufficient number of samples sequenced with matching normal tissues to train the classifier, especially for rare cancer types. We decided to test ISOWN in a setting in which the classifier was trained using one cancer type and then tested on another cancer type.  Figure 4 shows the results from cross-cancer type testing. The first conclusion is that in all six cancer types (with minor exceptions), training and testing using the same cancer type give the best accuracy. This is explained by the following differences between cancer types: (a) VAF distributions; (b) different patterns of sample frequencies; (c) different mutation signatures; and probably (d) different calling biases in among TCGA variant call sets. The second observation is that the somatic mutation prediction in the PAAD set posed the greatest difficulty for the classifier among all six training sets, most likely due its high ratio of germline to somatic mutations.  It is interesting to note that the UCEC and KIRC training sets in combination with NBC demonstrated relatively good accuracy across all six sets; these training sets can probably serve as uniformly applicable training sets for cancers with medium to high mutational loads. The LADTree classifier was consistently worse than NBC in this experiment. In summary, cross-tumor type training can produce accurate classifiers, and in at least one case differences in the paired variant calling methodology are more important than differences between the tumor types.    Misclassified variants\r\n  To understand the source of misclassifications, we examined these variants in greater detail. To do so, we trained the classifier on variants retrieved from 100 samples in each cancer data set and collected all misclassified variants. In the discussion below, germline variants misclassified as somatic by our algorithm are called false positive variants (FPVs), while somatic mutations classified as germline by ISOWN are called false negative variants (FNVs).  One common theme across all tumor types tested is that FPVs are enriched with low VAF variants. For example, 23.8% of all FPVs in KIRC have VAF &lt;20%, while just 0.52% of variants correctly predicted as germline have VAF &lt;20%. In BRCA, 21% of all FPV versus 0.4% of all germlines have VAF &lt;20%. For PAAD, the different is even more drastic: 55.4 versus 2.88%. This suggests that one source of classifications comes from unbalanced copy number variations affecting germline SNPs.  We detected 63.11% of all FPVs in PAAD in one sample only, whereas only 5.14% of true germline polymorphisms appear only once in the sample population. In KIRC, 87.81% of all FPVs are seen in a single sample, in contrast to 2.93% of germline polymorphisms. Similar ratios were observed in the other cancer types. These results indicate that the majority of the incorrectly predicted somatic mutations were called in single samples only. Because of this, these FPVs are unlikely to have a major effect on downstream analyses, as they would most likely be treated as low frequency passenger mutations.  Another interesting observation is that, in three out of six cancer sets, the gene most frequently involved with FPVs was MUC4. This gene accounted for 1.9% of all FPVs in BRCA, 3.5% in KIRC and 5.8% in COAD. This is significantly higher than expected by chance even after taking into account the gene length. According to Genecards (http://www.genecards.org/cgi-bin/carddisp.pl?gene=MUC4), this gene contains a region in the coding sequence which has a variable number (&gt;100) of a 48-base tandem repeat. We hypothesize that the tandem repeat is responsible for mapping errors during the alignment and variant calling steps of upstream processing. The other genes affected by the same issue in at least one out of six datasets are MUC2, MUC6, and TTN, each of which contained tandem repeats and may be subject to similar issues. These observations highlight the fact that our classification method is not designed to identify sequencing errors and mapping artifacts. We recommend using ISOWN only after pre-filtering for possible artifacts (for example, sequencing and/or FFPE artifacts).  Turning to FNVs, one source of FNVs came from the classification of variants present in dbSNP/common_all but not in COSMIC as germline variants (Additional file 1: Table S3). Depending on the cancer type, between 0.9 and 9.3% of all FNVs are explained by this classification error. In addition, the VAFs for FNVs are significantly higher than the average VAF for all somatic mutations. For example, 38.8% of all FNVs in UCEC have VAF &gt;40%, while only 20.7% of somatic mutations have VAF &gt;40%. Because of this, FNV classification errors may be biased towards clonal driver somatic mutations that arise early in tumor development and have a high VAF, as well as oncogenes that are involved in amplification events. This is part of the rationale for the algorithm's preprocessing step of labeling all known drivers with COSMIC CNT ≥100 as somatic and skipping the machine learning classification step.  One of the major concerns for proper somatic mutation classification is its accuracy with respect to the subset of \u201cnovel\u201d variants that are catalogued by neither dbSNP/ExAC nor COSMIC. The ratio of novel variants among true somatic mutations ranges from 2.0% in COAD to 52.1% in PAAD. Interestingly, in five out of six cancer types, we find a smaller proportion of novel somatic mutations among the FNVs than among all somatic mutations, meaning that FNVs were depleted from novel mutations. For example, in the PAAD data set the percentage of novel variants dropped from 52.1% in all somatic mutations to 6% in FNVs (p value &lt;0.0001 by Fisher proportional test). In the sixth cancer type (COAD), the FNV rate among novel and known somatic mutations was comparable. This means that ISOWN is no more likely to miss novel somatic mutations than it is to miss known ones.    Application to cell lines\r\n  Cell lines represent a specific case for somatic mutation prediction where we expected a reduction in ISOWN performance. First, the number of samples are usually low (only two lines in the case presented below) and the sample frequency feature is not applicable. Second, because cell lines have cellularity close or equal to 100%, the VAF distribution for somatic and germline variants should show comparable patterns. In addition, the flanking region VAF feature may also be less relevant due to the high levels of cellularity. Thus, only seven out of ten features are fully applicable to this particular scenario.  VCF files with somatic and germline variants for the HCC1143 and HCC1954 breast cancer cell lines were downloaded from Cancer Genome Collaboratory (http://www.cancercollaboratory.org/). We used variants called using the DKFZ variant-calling pipeline (https://dockstore.org/containers/quay.io/pancancer/pcawg-dk fz-workflow) for the ICGC/TCGA PanCancer Analysis of Whole Genomes Project (https://dcc.icgc.org/pcawg). In this case, matching normal DNA (isolated from normal B lymphoblasts) was available to provide a gold standard for somatic mutations called from the cell lines. We considered only non-silent calls in coding regions, and the ratio of SNPs to somatic mutations was 8 to 1.  We trained NBC and LADTree using increasing numbers of TCGA BRCA (breast cancer) samples. Because of the limited number of cell lines, we removed the sample frequency feature from both the training and testing sets. The average recall across all training sets was 85% and the precision 63% (F1-measure 71.4%). We found that both NBC and LADTree had similar accuracies, but NBC generated more stable results with lower accuracy variance across the training sets (Additional file 2: Figure S8).    Application to archival FFPE specimens\r\n  A major use case for ISOWN is the identification of somatic mutations in archival FFPE specimens, which often do not have accompanying blood or other normal tissue. To test the algorithm's accuracy in this scenario, we sequenced 1491 estrogen receptor-positive (ER+) early breast cancer FFPE samples (see Additional file 1: Supplemental methods for more details) from the Tamoxifen versus Exemestane Adjuvant Mulitcentre (TEAM) clinical trial [ 41 ], which didn't have matching normal tissues. ISOWN was used to call somatic SNVs in this set. To validate the call sets, the final list of TEAM somatic mutations was compared with three other publicly available breast cancer mutation sets (TGCA BRCA ER+ [ 42 ] and results published in [ 43 ]) to determine whether the somatic mutation frequency in each gene matched expectations.  Overall mutation loads in the genomic regions sequenced using our targeted sequencing panel were similar between TEAM samples and those from other data sources. We found no significant differences in gene mutation frequency between the ISOWN-processed TEAM samples and previously published breast cancer mutation frequencies using Fisher's proportional test (false discovery rate &gt;10%). For example, 30.5, 29.6, and 34.1% of samples contain mutations in the PIK3CA gene in the TEAM, TCGA BRCA, and Stephen et al. [ 43 ] sets, respectively. We also calculated the proportion of samples carrying at least one non-silent somatic mutation in each independent dataset. In the TEAM data set, 71.8% of samples carried at least one non-silent mutation, which is not significantly different from the 69.0% observed in the ER+ subset of breast cancer samples in TCGA BRCA, and 69.4% of ER+ samples in Stephen et al. (p value 0.558 from Fisher's proportional test). In addition, the pattern of somatic mutations within genes matched the expected distribution.  Based on these three assessment criteria (mutational load, mutated gene frequency, and samples carrying at least one mutation) we conclude that the somatic mutation call set produced by ISOWN on a targeted FFPE sample set is comparable to the data sets produced by paired somatic mutation callers across three similar breast cancer data sets.     Discussion\r\n  We describe the development and implementation of ISOWN, an accurate algorithm for discriminating germline polymorphisms from somatic mutations in cancer tissues in the absence of matching normal tissues. We achieved F1-measures ranging from 75.9-98.6% across multiple tumor types. The algorithm was validated using different sequencing strategies, including whole-exome sequencing and deep targeted sequencing, and different tissue types, including fresh frozen tumor tissues, cell lines, and FFPE samples.  The major challenge for this discrimination is the greatly unbalanced nature of the classification problem. After the various quality control and preprocessing steps, the number of germline polymorphisms is up to 500 times larger than somatic mutations, depending strongly on cancer type. ISOWN uses two mechanisms to overcome this imbalance. The first takes advantage of the fact that the vast majority of variants catalogued by dbSNP/common_all but not by COSMIC are germline polymorphisms. Removing this subset reduces the number of germline variants by roughly 70%, but the number of germline polymorphisms still greatly outweighs the somatic mutations. The second approach uses a data collapsing step in which we assume that any variant occurring in multiple samples is either somatic or germline. This assumption reduces the ratio of germline to somatic to 0.5-10 times depending on the cancer type.  The subsequent machine-learning classification step is based on ten different features, the most predictive of which are the three extrinsic features of the variants' presence in the COSMIC, ExAC, and dbSNP databases, and the two intrinsic features sample frequency and VAF. As these databases grow and expand, we can expect the performance of the classifier to improve. In addition, because sample frequency is one of the strongest intrinsic features, the performance of the classifier improves as the number of samples in the training and testing sets increases. Interestingly, the predicted functional impact of the variant, while helpful in discriminating non-silent variants, is not essential for correct classification, as shown in the relatively good performance of the algorithm on silent mutations.  ISOWN was designed to accommodate multiple underlying supervised machine learning systems. Of the seven machine learning systems we evaluated, NBC and LADTree were consistently the best, achieving comparable accuracies across all cancer data sets. While there were no major differences between NBC and LADTree, the former is computationally faster.  We benchmarked ISOWN against six TCGA wholeexome sequencing datasets that had been generated using conventional matched normal sequencing and variant calling. The data sets varied both biologically (a range of mutational loads and mutational spectra) and technically (different paired variant callers and preprocessing steps). Using a set of ten features we were able to identify nonsilent somatic mutations with an overall accuracy of ~99.5% across all six datasets. Cancer types with a high mutational load and a low germline:somatic ratio (COAD and UCEC) had the best performance, with an F1-measure ranging from 95-98%. Tumor types with a lower mutational load and a higher germline:somatic ratio (BRCA, ESO, and KIRC) had a reduced accuracy with F1-measures ranging from 85 to 93%. The worst performance was observed in PAAD (pancreatic adenocarcinoma), which has the highest germline:somatic ratio.  Some cancer driver prediction algorithms, for example, OncodriveCLUST [ 44 ], require a list of both non-silent and silent (synonymous) mutations. When applied to the task of predicting silent somatic mutations located in coding regions, ISOWN's accuracy is reduced, but remains in the range of 69-97% (F1-measure). We have not evaluated ISOWN on whole genome sequences because several of the intrinsic features we use for discrimination, such as PolyPhen-2 functional impact, do not apply. In addition, COSMIC is currently heavily biased towards coding mutations obtained from exome sequencing studies, and the COSMIC CNT feature would bias the classifier away from non-coding somatic mutations.  In a recently published paper [ 45 ], nine somatic variant callers were evaluated and benchmarked against a set of high-confidence somatic mutations generated using alternative calling algorithms together with manual curation. Widely used paired somatic mutation callers such as Strelka [ 15 ] and MuTect [ 17 ] demonstrated the best sensitivity rates of ~83 and ~89%, respectively. When benchmarked against paired call sets, ISOWN demonstrates sensitivities ranging from 86.7% (for PAAD) to 98% for the rest of the datasets, indicating that ISOWN's accuracy lies within the range that would be acceptable for the majority of research and clinical projects. The caveat, of course, is that ISOWN is trained against paired variant call sets from the appropriate tumor type, and its accuracy can never exceed that of the paired caller it is trained on. The variation in the number of germline SNPs per sample called by the different TCGA projects (Table 1) illustrates the strong effect that the choice of the paired variant calling pipeline may have on the training set.  The ISOWN algorithm works across multiple experimental designs, including whole-exome sequencing and targeted sequencing, and samples derived from freshfrozen tissue, FFPE tissue blocks, and cell lines. For a large cohort of ER+ breast cancer patients with unpaired FFPE samples, ISOWN produced somatic mutation call rates that, on a per-sample and per-gene basis, were consistent with the values reported by several large paired sample studies of similar cohorts. In cell lines, we were able to predict somatic mutations in two breast cancer cell lines, achieving an F1-measure close to 75% when the classifier was trained on a breast cancer data set. The great majority of the cell lines registered with the Cancer Cell Line Encyclopedia (CCLE) portal are missing matching normal tissues, and only common germline polymorphisms are removed based on dbSNP and other external databases. Provided that an appropriate training set is used, ISOWN can be used for identifying somatic mutations in these cell lines.  ISOWN is applicable to two research scenarios. First is the case where a researcher has access to matched normal tissue for some, but not all, of the members of a cancer cohort. In this case, he or she will be able to call somatic mutations using a conventional paired variant caller like MuTect2. For the rest of the samples without matching normals, all variants including somatic and germlines are called in tumor-only mode using existing tools such as GATK or MuTect2. The somatic mutations are then used to train and validate ISOWN. Once trained and validated, ISOWN can be used to predict which of those variants called from the tumor-only samples are somatic mutations. Our benchmarks demonstrate that 25-50 samples are adequate for training ISOWN on highly mutated cancer types (&gt;100 nonsilent somatic mutations per sample), 50-100 samples for cancers with a moderate mutational load (10-100 non-silent somatic mutations per sample), and &gt;100 samples for cancers with a high ratio of germline variants to somatic mutations (like PAAD). A researcher might also wish to reduce the overall cost of a cancer sequencing study by sequencing only sufficient matched normals to adequately train the classifier, and then using the classifier to call somatic mutations on unpaired tumor sequences obtained from the remainder of the donors.  The second research scenario is where no matched normal tissue is available at all, either because it was never collected (e.g., cell lines, pathology archives) or because donor consent was obtained in a narrow fashion that forbids examination of the germline. In such cases, ISOWN can be trained on a reference data set that has similar biology to the cohort of interest. For example, we demonstrate that ISOWN's accuracy is degraded but still usable when the classifier is trained on one tumor type and then tested with another that has a similar mutational load (F1-measure 98% for training with COAD and testing with UCEC). Even in the worst case, in which paired variant calls from breast cancer primaries were used to train the classifier to detect somatic mutations in two breast cancer cell lines, still had an accuracy in the 70% range (F1 measure). For convenience, we have included six standard training sets in the ISOWN software package.  Like many other software, ISOWN also has a few limitations. First, its accuracy suffers with cancers with low mutational load and small sample sets. Second, the algorithm isn't trained to recognize sequencing artifacts related to FFPE damage or other artifacts; these must be removed via upstream filters prior to the classification task. Third, for best results the algorithm requires a set of 25-100 samples to train the classifier; one of the standard training sets provided with ISOWN can be used, but accuracy might be moderately reduced. Fourth, the algorithm has only been tested on variants that fall in coding regions and is unlikely to work on whole genomes until the databases of somatic mutations become more comprehensive. Lastly, the current version of ISOWN is not set up to call small insertions/deletions (indels), a task that is challenging due to the high rate of sequencing and mapping artifacts that contribute to indel calls, and their relative scarcity. These challenges will be addressed in the next releases of ISOWN.  Future work will focus on improving the classifier performance for cancer types with low mutation frequencies, datasets with low numbers of samples, indels, and non-coding mutations. In addition, we plan to add additional reference training sets to the ISOWN package.    Conclusions\r\n  In this work we have presented a novel and accurate computational algorithm called ISOWN for predicting somatic mutations from cancer tissues in the absence of matching normal samples. ISOWN uses machine learning and external databases along with the sequencing characteristics information retrieved from the samples themselves. ISOWN was extensively validated across six different cancer types with different mutation loads where F1-measures range from 75.9 to 98.6%. In addition, ISOWN was tested on FFPE, fresh frozen, and cell line tissues.  ISOWN can help researchers to accelerate sequencing process, reduce financial investment in sample sequencing and storage requirements, or increase the power of analysis by increasing the number of tumor samples sequenced with the same resources. In addition, ISOWN is useful in cases where the patient consent prevents normal tissue collection or when a study is based on retrospective biopsies where normal tissues were not collected. ISOWN is freely available on GitHub together with a detailed manual of how to install and use it.    Availability and requirements\r\n  Project name: ISOWN (Identification of Somatic mutations Without Normal tissues) Project home page: https://github.com/ikalatskaya/ISOWN Operating system(s): Linux, iOS Programming language: C, Perl, Java Other requirements: Tabix, Annovar, Weka License: GNU Any restrictions to use by non-academics: please contact the authors    Additional files\r\n  Additional file 1: Supplemental material including supplementary methods and supplementary results as well as Tables S1 and S3-S8. Table S1: Results from 10-fold cross-validation. Table S3: Comparison of the somatic mutation ratio in the whole dataset vs in the subset of variants that were catalogued by dbSNP/common_all but not by COSMIC. Table S4: Number of germline variants with high CNT in different cancer sets. Table S5: Number of variants with \u201cmixed\u201d labels in different cancer sets. Table S6: Uneven distribution of the somatic mutations and germline polymorphisms in Mutation Assessor categories. Table S7: Uneven distribution of the somatic mutations and germline polymorphisms in PolyPhen-2 categories. (PDF 208 kb) Additional file 2: Figures S1-S8. Figure S1: Concent of flanking regions. Figure S2: Mono-labeled approach expanation: variant labels are allele- specific, not sample specific. Figure S3: VAF density distribution for different cancer sets. Figure S4: Sample frequencies for somatic mutations and germline polymorphisms. Figure S5: ISOWN validation (False Positive Rate). Figure S6: ISOWN testing on silent variants (F1-measure). Figure S7: ISOWN validation on variants with different VAF (F1-measure). Figure S8: ISOWN validation on cell lines. (PDF 18546 kb) Additional file 3: Table S2. Table giving the ISOWN performance measures (F1-measure, Recall, False Positive Rate, Precision and Accuracy) calculated based on held-out independent sample set across six cancer datasets. (XLSX 125 kb) Abbreviations BRCA: Breast invasive carcinoma; CARNAC: Consensus And Repeatable Novel Alterations in Cancer; CI: Confidence interval; COAD: Colon adenocarcinoma; ER: Estrogen receptor; ESO: Esophageal adenocarcinoma; ExAC: Exome Aggregation Consortium; FFPE: Formalin-fixed paraffin embedded; FNV: False negative variant; FPR: False positive rate; FPV: False positive variant; KIRC: Kidney renal clear carcinoma; NBC: naïve Bayes classifier; PAAD: Pancreatic adenocarcinoma; SNP: Single nucleotide polymorphism; SNV: Single nucleotide variant; SVM: Support vector machine; TCGA: The Cancer Genome Atlas; UCEC: Uterine corpus endometrial carcinoma; VAF: Variant allele frequency Acknowledgements We thank Jeremy Adams and Peter Ruzanov for their technical support, Jeff Luong for ISOWN testing and providing valuable comments, and Paul Boutros for helpful suggestions during the design of the ISOWN algorithm. Funding This work was supported by funding from the Ontario Institute for Cancer Research, provided through the Ontario Ministry of Research, Innovation and Science.  Availability of data and materials The TEAM dataset is available upon request from the TEAM trial group. VCF files containing somatic and germline variants from HCC1143 and HCC1954 cell lines can be downloaded from the ICGC portal (https://dcc.icgc.org/releases/PCAWG/cell_lines). The ESO dataset is publicly available from the dbGAP under accession number phs000598.v2.p2. BRCA, UCEC, KIRC, PAAD, and COAD data sets are available for download from the Genomic Data Commons (https://portal.gdc.cancer.gov/) under project IDs TCGA-BRCA, TCGA-UCEC, TCGA-KIRC, TCGA-PAAD, and TCGA-COAD. ISOWN is available free of charge for academic and commercial use under the Open Source Apache 2.0 license. The package and documentation can be downloaded from https://github.com/ikalatskaya/ISOWN.  Authors' contributions IK, QT, MS, JB, JM, and LS participated in the experimental design and analysis pipeline. IK and QT designed, wrote, and validated the ISOWN algorithm. IK, QT, and LS compared and interpreted results generated by ISOWN. MS and JB managed and coordinated the FFPE specimen (TEAM) study. JM supervised TEAM sequencing. IK drafted the manuscript. QT packaged, organized, and made ISOWN available on GitHub. LS supervised the project. All authors have read and approved the final manuscript. Competing interests The authors declare that they have no competing interests.  Consent for publication Not applicable.  Ethics approval and consent to participate The TEAM study (NCT00279448) complied with the Declaration of Helsinki, individual ethics committee guidelines, and the International Conference on Harmonisation and Good Clinical Practice guidelines; all patients provided informed consent. The use of TEAM trial data for the study reported here was approved by University of Toronto ethics committee protocol number 29019. The use of TCGA and ESO data sets was authorized by TCGA Data Access Committee (DAC) project #6257.    Publisher\u2019s Note\r\n  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.    ",
    "sourceCodeLink": "https://github.com/ikalatskaya/ISOWN",
    "publicationDate": "0",
    "authors": [
      "Irina Kalatskaya",
      "Quang M. Trinh",
      "Melanie Spears",
      "John D. McPherson",
      "John M. S. Bartlett",
      "Lincoln Stein"
    ],
    "status": "Success",
    "toolName": "ISOWN",
    "homepage": ""
  },
  "19.pdf": {
    "forks": 1,
    "URLs": [
      "www.github.com/PapenfussLab",
      "www.ncbi.nlm.nih.gov/sra/SRX803011"
    ],
    "contactInfo": ["papenfuss@wehi.edu.au"],
    "subscribers": 10,
    "programmingLanguage": "Java",
    "shortDescription": "classification of higher-order structural variants from breakpoint data",
    "publicationTitle": "CLOVE: classification of genomic fusions into structural variation events",
    "title": "CLOVE: classification of genomic fusions into structural variation events",
    "publicationDOI": "10.1186/s12859-017-1760-3",
    "codeSize": 30,
    "publicationAbstract": "Background: A precise understanding of structural variants (SVs) in DNA is important in the study of cancer and population diversity. Many methods have been designed to identify SVs from DNA sequencing data. However, the problem remains challenging because existing approaches suffer from low sensitivity, precision, and positional accuracy. Furthermore, many existing tools only identify breakpoints, and so not collect related breakpoints and classify them as a particular type of SV. Due to the rapidly increasing usage of high throughput sequencing technologies in this area, there is an urgent need for algorithms that can accurately classify complex genomic rearrangements (involving more than one breakpoint or fusion). Results: We present CLOVE, an algorithm for integrating the results of multiple breakpoint or SV callers and classifying the results as a particular SV. CLOVE is based on a graph data structure that is created from the breakpoint information. The algorithm looks for patterns in the graph that are characteristic of more complex rearrangement types. CLOVE is able to integrate the results of multiple callers, producing a consensus call. Conclusions: We demonstrate using simulated and real data that re-classified SV calls produced by CLOVE improve on the raw call set of existing SV algorithms, particularly in terms of accuracy. CLOVE is freely available from http://www.github.com/PapenfussLab.",
    "dateUpdated": "2017-10-06T23:17:27Z",
    "institutions": [
      "Peter MacCallum Cancer Centre",
      "University of Melbourne",
      "G Royal Parade",
      "Johannes Gutenberg Universität Mainz"
    ],
    "license": "No License",
    "dateCreated": "2015-08-21T04:55:16Z",
    "numIssues": 1,
    "downloads": 0,
    "fulltext": "     Schröder et al. BMC Bioinformatics     10.1186/s12859-017-1760-3   CLOVE: classification of genomic fusions into structural variation events     Jan Schröder  0  1  4    Adrianto Wirawan  5    Bertil Schmidt  5    Anthony T. Papenfuss  papenfuss@wehi.edu.au  0  2  3  4  6    0  Bioinformatics and Cancer Genomics, Peter MacCallum Cancer Centre ,  East Melbourne, VIC 3000 ,  Australia    1  Department of Computing and Information Systems, University of Melbourne ,  Melbourne, VIC ,  Australia    2  Department of Mathematics and Statistics, University of Melbourne ,  Melbourne, VIC 3010 ,  Australia    3  Department of Medical Biology, University of Melbourne ,  Melbourne, VIC 3010 ,  Australia    4  G Royal Parade ,  Parkville, VIC 3052 ,  Australia    5  Institut für Informatik, Johannes Gutenberg Universität Mainz ,  Mainz ,  Germany    6  Sir Peter MacCallum Department of Oncology, University of Melbourne ,  Melbourne, VIC 3010 ,  Australia     2017   18    13  7  2017    20  10  2016     Background: A precise understanding of structural variants (SVs) in DNA is important in the study of cancer and population diversity. Many methods have been designed to identify SVs from DNA sequencing data. However, the problem remains challenging because existing approaches suffer from low sensitivity, precision, and positional accuracy. Furthermore, many existing tools only identify breakpoints, and so not collect related breakpoints and classify them as a particular type of SV. Due to the rapidly increasing usage of high throughput sequencing technologies in this area, there is an urgent need for algorithms that can accurately classify complex genomic rearrangements (involving more than one breakpoint or fusion). Results: We present CLOVE, an algorithm for integrating the results of multiple breakpoint or SV callers and classifying the results as a particular SV. CLOVE is based on a graph data structure that is created from the breakpoint information. The algorithm looks for patterns in the graph that are characteristic of more complex rearrangement types. CLOVE is able to integrate the results of multiple callers, producing a consensus call. Conclusions: We demonstrate using simulated and real data that re-classified SV calls produced by CLOVE improve on the raw call set of existing SV algorithms, particularly in terms of accuracy. CLOVE is freely available from http://www.github.com/PapenfussLab.    Structural variations  Genomic rearrangements       Background\r\n  A structural variant (SV) is a rearrangement of the genome caused by at least two double strand DNA breaks followed by DNA repair. Typically, the term SV is used for events that are greater than 1 kb in size [ 1 ]. SVs include large insertions, inversions, balanced or unbalanced translocations, and amplifications and large deletions, collectively referred to as copy number variations (CNVs). A precise understanding of SVs is important in the study of population diversity, cancer [ 2-4 ] and other diseases (e.g. Charcot-Marie Tooth [ 5 ] and autism [ 6 ]).  The increasing usage of high throughput sequencing technologies has led to advances in the discovery and genotyping of structural variants in germline and somatic cells [ 7-9 ]. Consequently, a variety of methods have been developed to detect SVs from DNA sequencing data. Different approaches can be classified into four distinct categories: read depth (RD), discordant read pair (DR), split reads (SR), and de novo assembly (DN). RD methods involve counting reads in windows and segmenting the counts [ 10 ]. They identify only one class of structural variation (CNVs) and provide neither direct evidence for breakpoints, nor information about genomic organization. Their resolution and accuracy is dependent on sequencing coverage and window size, but is typically of the order of kilobases. Examples of RD methods include readDepth [ 10 ] and CNVnator [ 11 ]. DR methods use pair endsequenced DNA fragments that span a breakpoint (typically in the un-sequenced region between the reads). These reads map anomalously or discordantly to the reference genome-further apart or closer together than expected based on the selected fragment size, to different chromosomes or with inverted orientation [ 12 ]. The signal of a rearrangement is a cluster of anomalous alignments. The resolution of DR methods is related to fragment size distribution and genome coverage. However, in general, single-nucleotide resolution is not possible with DR methods. BreakDancer [ 13 ] is an example of a DR method. SR methods rely on individual reads, which span the breakpoint and are capable of single-nucleotide resolution, although micro-homologies at breakpoint sites may introduce uncertainty. Examples of SR methods include Splitread [ 14 ] and Socrates [ 15 ]. DN methods use some form of assembly following other evidence to locate the locus of a rearrangement. DN methods typically provide single nucleotide resolution, but can be slow. Examples of DN methods are Cortex [ 16 ] and SOAPdenovo [ 17, 18 ].  Many tools utilize a hybrid approach that combines multiple lines of evidence to predict SVs. For example, Delly [ 19 ] and PRISM [ 20 ] use DR evidence and incorporate SR evidence through a targeted Smith-Waterman alignment. CNVer [ 12 ] uses DR and RD signals to identify potential copy number changes. CREST [ 21 ] uses SR then DN to directly map SVs at single nucleotide resolution, while SMUFIN [ 22 ] uses DN first and SR subsequently.  Another strategy is the \u201cconsensus\u201d caller approach. For example, MetaSV [ 23 ] integrates a set of multiple tools into a pipeline. This approach aims to leverage the specific strengths of tools regarding different aspects of SV calling as well as confidence through agreement of multiple tools, and is therefore considered a meta-caller.  Nevertheless, the identification of SVs remains challenging. Existing methods suffer from a variety of issues relating to sensitivity and precision, positional accuracy and error profiles, and classification into one of the various types of SV. The majority of existing methods only identify breakpoints (a pair of connected breakends), also called genomic fusions (henceforth referred to as fusions), but do not classify the rearrangements further. Several SV callers are capable of limited classification of genomic events, such as insertions and deletions, but fail to classify more complex rearrangements. SVs may be simple (involving only a single fusion, such as an deletion) or more complex events (involving two or more fusions, such as a balanced translocation or inversion). Yang et al. [ 24 ] noted the lack of more complex events in the output of SV algorithms and introduced complex deletions, as well as inference of underlying DNA repair mechanisms. The study by Sudmant et al. [ 25 ] analysed a large cohort of human genomes for SVs including mobile element insertions. Another notable exception is the work by Escaramis et al. [ 26 ], which detects more complex events from aligned read data. However, the method is neither widely used nor cited, and failed to run on our data for purposes of comparison.  Here, we present a new method, CLOVE, which integrates calls from one or more breakpoint (or SV) detection methods and (re-)classifies the SV. Our method creates a graph data structure from the provided breakpoint information and then looks for patterns that are characteristic of more complex rearrangement types (e.g. balanced translocations). CLOVE is not another SV caller, but integrates multiple independent breakpoint predictions from other tools into a single, more accurate and potentially more complex event. This makes the output of these other tools more interpretable and increases the precision. A better-categorized output allows for better filtering or prioritization of specific events that are most relevant to the biological interpretation or experimental validation. CLOVE is the first meta-SV-caller that (i) can use any set of input (from current or future SV algorithms) to (ii) re-classify the data into more complex SVs than the original call sets.    Methods\r\n  Our algorithm is capable of handling the breakpoint or SV calls produced by a variety of tools - in fact, it can handle multiple sets of fusion at the same time, benefiting from the increase in sensitivity in raw calls. CLOVE can be thought of as augmenting existing breakpoint callers. It allows for stratification of SVs into (i) sets of simple events that collectively present the signature of a complex event, and (ii) remaining simple events that pass or don't pass a read depth check. CLOVE presents the stratified SVs in VCF format with additional SV types, statistics about the read depth of events, and the levels of support for events (i.e. how many SV callers support the SV). In the following, we distinguish between the notion of basic and complex SV types. Simple or basic SV types are those that are fully represented by a single fusion. Complex SVs contain more than one fusion.   Basic SV types\r\n  We introduce the following terminologies and conventions in order to discuss the different types of SVs that can be identified in sequencing data: (1)A fusion is a pair of loci that are adjacent in the donor genome but on different chromosomes or separated on the same chromosomes in the reference. The two separate locations of a fusion are referred to as breakends. (2)Each breakend has an orientation. We define the orientation to be \u201c+\u201d if the breakend occurs on the 3\u2032 end of the fused region (on its right) in the reference, and \u201c−\u201d if it occurs at the 5' end (on its left). If a fusion has DR support, the discordant reads flanking the breakend will be aligned \u201cpointing\u201d towards the break and the read strand will coincide with breakend orientation. Similarly, for a fusion with SR support, if the aligner maps the paired reads to the same side (left) of the breakend with the 3\u2032 end of the fragment clipped, then the breakend has a \u201c+\u201d orientation. Figure 1 illustrates an example of a breakend in an intra-chromosomal event and DR support. (3)Based on the definitions above, we define a fusion as a pair of loci and directions: chromosome1 (chr1), position1, orientation1, chromosome2 (chr2), position2, orientation2. (4)If a fusion is an intra-chromosomal event, i.e. the event occurs in the same chromosome (chr1==chr2), by convention, we assume position1 ≤ position2.  Based on our definitions, a single fusion can be classified into eight categories (Table 1). Furthermore, we have assigned an event type to each category. These definitions are in line with the literature [ 19 ]. The labels are motivated by the fact that some of the more basic structural rearrangements in DNA bear the corresponding signatures. Note that the sub-classifications of inter-chromosomal translocation types require a chromosome ordering and assume chr1 &lt; chr2. This definition is arbitrary but necessary to distinguish the two non-inverted events. Most SV callers that do provide event classification often follow the naming scheme presented in Table 1, or something similar. However, while the breakend signatures for these types are indeed consistent with the events that give them their names, these are not the only events that can cause such a signature. Overlooking this fact is often a source of classification error and confusion when interpreting fusions.    Complex SV types\r\n  The computation and output of more complex SVs, such as translocations or inversions, is typically not addressed by existing SV calling algorithms (or only parts thereof as discussed below). Figure 2a and b illustrate the rearrangement patterns created by interspersed duplications and translocations on a single chromosome. Both patterns consist of two to three lower order events of the tandem duplication or the deletion type. This observation motivates our approach to further classify SV calls: the SV output referring to deletions and duplications at sites where a more complex event has occurred is often confusing to the user, and not easily identifiable from the list of breakends. Note that duplications and translocations upstream of the insertion site have a slightly different pattern with the deletion event pointing at the insertion site. Figure 2c shows the signature of an inversion event. Some SV algorithms, e.g. CREST, can classify this type of rearrangement. Figure 2d shows the signature of inverted interspersed duplications on the same chromosome. Similar to the classifications in Fig. 2a-d, parts e-g show the signatures for complex inter-chromosomal events. The list of rearrangements presented here is probably not complete as far as all potential complex events go, but we believe covers the majority of relevant classes. The signal of complex events shown above is consistent with existing discussion of structural variants (such as by [ 26 ]). To our knowledge CLOVE is the first meta caller that categorises such a comprehensive list of SVs from fusion calls.  The complex SV events described above share a common principle: there is at least one locus in the rearrangement pattern where two or more breakpoints (or simple SV events) have a common breakend coordinate. This observation guides the design of CLOVE to discover rearrangement patterns in SV data.  With one exception, the patterns for each of the rearrangement events are also unique and specific. Due to the symmetric nature of the intra-chromosomal translocation event, it cannot be determined whether the block has moved as shown in Fig. 2b, or indeed the block marked with an \u201cA\u201d has translocated to just after the second deletion event. Notice that both alternatives result in the same string of DNA. We define the convention that the smaller of the two alternative blocks is considered to have translocated.  Breakpoint graph construction and analysis CLOVE has two major components, i.e. complex rearrangement pattern matching and read depth validation. Before these stages commence, the breakend calls fusion p2r1 p1r1 5' 3' + bp1  bp2 3' 5' p1r2 p2r2 Fig. 1 Example of a simple structural variant that illustrates how the signatures of fusions are defined. The horizontal structure in the middle represents the double-stranded DNA of a chromosome. Two read pairs are depicted as horizontal arrows mapping to the positive and negative strand of the DNA (the reads in the pairs are enumerated as pxry - pair x, read y). Assuming that the insert size of the two pairs are significantly above their expected value, an SV caller would call a deletion event from the two read pairs. The two dashed vertical lines indicate two breakends in the DNA. The breakends are connected by an arrow labelled \u201cfusion\u201d, which corresponds to the deletion event. The orientation signature of the fusion is indicated as \u201c+\u201d and \u201c−\u201cnext to the breakends according to the mapping orientation of the reads that constitute the evidence to the fusion call Rows refer to the orientation of the first breakend and columns to the orientation of the second breakend. Simple events may be combined into complex event types; for example, an inversion (an inverted segment of DNA) is comprised of the two simple events INV1 and INV2 − Deletion type (DEL) Inversion type 2 (INV2) Inter-chromosomal translocation type 1 (ITX1) Inter-chromosomal translocation type 2 (INVTX2) are parsed into the algorithm. At the time of publication, the method supports outputs from Socrates, Delly, CREST, Gustaf [ 27 ], and BedPE (a standard format for fusion data). Furthermore, any SV caller can be reclassified, as long as it provides the sextet of information for each fusion introduced in Section 2.1. Any number of call sets can be input to CLOVE at the same time, which increases sensitivity if this adds non-redundant information.  The main internal data structure of CLOVE is a graph where nodes represent genomic coordinates (or coordinate intervals) and edges the fusions predicted by the SV callers.  Fusions are added one at the time, and their coordinates are compared to the existing set of nodes. If a coordinate is within a user defined distance of a node, it is added to that node (modifying its coordinate interval if necessary). Otherwise, a new node (or pair of nodes) is created and added to the graph. Furthermore, one edge per fusion is added to the appropriate node(s). The edges (fusions) are labelled with one of the eight variant types introduced in Table 1. This data structure is similar, but not identical, to the breakpoint graph proposed by Bafna and Pevzner [ 28 ].  After the graph is constructed, CLOVE refines the existing edges in the graph. CLOVE scans the graph for redundant edges, i.e. edges between the same pair of nodes with identical SV type and merges them.  The refined graph is subsequently analysed to identify complex rearrangement patterns. Afterwards, the classified SV events are validated using read depth information from the original short read dataset. Figure 3 illustrates the workflow of CLOVE to classify provided fusion data into improved SV calls.  The two main steps are now explained in more detail. (1)Complex rearrangement pattern matching: The initial classification step analyses the breakpoint graph in order to search for patterns described in Section 2.2. We iterate over the nodes in ascending coordinate order on each chromosome. If a node is adjacent with at least two edges, every edge-pair is investigated upon matching one of the complex SV rearrangement patterns (illustrated in Fig. 2). Intra-chromosomal duplication types are investigated first by matching tandem duplication and deletion types (or inversion1/ 2) accordingly. A further search for a deletion of the duplicated interval may then change the classification to a translocation. Any identified complex event is added to the graph and the (up to three) contributing events are removed from the graph. This step concludes once every node has been visited. (2)Read depth validation: The second stage of the classification procedure again traverses the graph in order to analyse the breakpoint consistency with the read depth within the according intervals. Deletion events are expected to include a relatively low read depth in-between the two breakends, while tandem duplications are expected to include an increased coverage. The read depth is established for each individual interval by querying the input BAM file.  This value is compared to the expected coverage and standard deviation. A single deviation value is used in this step independent of the size of the interval. Although this is not the most rigorous approach, it works well in practice. The expected deviation is a user-supplied parameter - to be chosen reflecting the coverage spread of the analysed read data. It is key for this step to take place after the classification step. This way deletion types and tandem duplication type events have already been merged into higher order events that do not change the read depth (for example, translocations). Events that do not fulfil the expected coverage response are rejected from the classification, and instead output as events of secondary interest due to read depth inconsistency.  After the classification stages, CLOVE presents the results in a new output file in VCF format. The read depth is presented along with the levels of support for each event (informing about the number of tools that support an event and the number of events within the call sets).  Clove is implemented in java and makes use of the htsjdk samtools implementation to handle genomic intervals.     Results and Discussion\r\n  To evaluate our method, we investigate results on simulated and real genomic data. The motivation behind the   a) Intrachromosomal duplication:\r\n  d) Intrachromosomal inverted duplication (translocation): e) Interchromosomal duplication (translocation): f) Interchromosomal inverted duplication (translocation):  Insertion site ITX1  ITX2 Duplicated (translocated) segment  Insertion site INVTX2  INVTX1  Duplicated (translocated) segment    g) Balanced translocation:\r\n  Exchanged chromosome arms  ITX2  ITX1 TAN TAN  INV1 INV1 usage of simulated data is to allow full control over the sequence content, making an assessment of the sensitivity and precision of different tools possible. In the ideal case every simulated structural variant should be found and classified in the sequencing data, and no additional ones. Application to real data is important, but our knowledge about the variants present is incomplete.  We demonstrate that CLOVE is able to assign meaningful labels to rearrangement events. Furthermore, it increases the accuracy of the output of existing SV tools by removing a large proportion of false positive events. In the following two subsections, we compare the initial call set of different SV algorithms with the re-classified calls produced by CLOVE. For these comparisons, we investigate the sensitivity, precision, and accuracy statistics, which calls for the classic contingency tables of true positives (TP), false positives (FP), false negatives (FN) (and true negatives). Furthermore, we modify the standard approach as follows: we introduce a fifth contingency called half-true-positives (HTP). HTPs are defined as SV calls at the correct location but with the wrong event label (and potentially with too few fusions). This type allows us to make a fair comparison between the raw output (which often has few actual TPs) and the classified calls, without being overly harsh on existing methods - meaning, in the following HTPs are going to be counted as TPs for the calculations of recall, precision, and accuracy.   Simulated data results\r\n  For this experiment, we use chromosomes 21 and 22 of the human reference (hg19) as the underlying genome.  The simulation workflow is as follows: (1)The genetic material is divided into non-overlapping bins of length 500kbp. The binning strategy prevents events from overlapping and thus generating unresolvable complexities. (2)A random event type is chosen for each bin. The types we use here are deletion, tandem duplication, inversion, intra- and inter-chromosomal duplications, intra- and inter-chromosomal translocations, intraand inter-chromosomal inverted duplications, and intra- and inter-chromosomal inverted translocations. Algorithm 1  SV calls Algorithm 2  SV calls  ...  Algorithm n  SV calls  BAM file  Stage 1: classification  Stage 2: read-depth check  VCF file of improved  SV calls  For each event a random starting or insertion position is chosen within the bin, as well as a random length or duplication/translocation interval, respectively. (3)A new reference is generated that contains the  above modifications. (4)Reads are simulated from the alternative reference (using SimSeq [ 29 ]) at a sequencing depth of 30× and 100 bp read length. (5)The reads are aligned to the original reference containing chromosomes 21 and 22 with Bowtie2 [ 30 ] in local alignment mode. (6)SVs are called with a variety of tools from the aligned read data. All tools are run with standard parameters. (7)CLOVE re-classifies the output of the SV methods generated in the previous step, and the results are compared to the list of variants created in Step 2).  Clove is run without any parameters, except the coverage option, which is set to \u201c-c 30 8\u201d for this experiment.  Steps 1-3 of the simulations is done by using tools introduced in [ 31 ]. This simulation workflow is repeated five times, generating new random events and reads each time. The classified SV calls of CLOVE (v0.14) are based on the outputs of Socrates (v1.13.1), Delly (v0.76) and CREST (v0.0.1) for each simulated dataset. Tests are conducted using both the output of each individual tool as input and outputs combined from two or all three tools as input. CLOVE results are then compared to the raw results of the corresponding SV algorithm(s) that have been used as input. Additionally, we include information about runs of MetaSV (using version 0.5.4 and calls provided by Pindel, Lumpy, and BreakDancer) by itself. The performance is measured in terms of sensitivity, precision, and accuracy. Figure 4 summarises the results for the accuracy metric. Note that Fig. 4 is based on calculations treating HTPs as TPs for the purpose of calculating accuracy - specifically, accuracy ≔ (TP + HTP)/(TP + HTP + FP + FN). The detailed individual results and performance per SV type are listed in the supplement (Additional file 1: Table S1). These results include the performance of Lumpy - not shown in Fig. 4.  Lumpy performs with an average raw accuracy of 0.71, which improves to 0.82 when applying CLOVE. Interestingly, Lumpy's performance is better than MetaSV's, which uses its data as input.  The results show the following trends. First, there is increased accuracy due to the classification of events. Although some tools already call complex variants, there is not a single tool capable of the same level of accuracy as clove when run on multiple inputs. Consolidating the trend with more specific results in the supplementary material, we can conclude that this effect is caused by the increase in precision of re-classified breakpoints.  Conversely, the recall is decreased after classification.  Obviously, the recall cannot increase, since CLOVE does not create any additional events in the SV input - and a decrease is inevitable if an event does not pass the read depth check, or a complex event is missing an edge leaving the remaining one(s) to be discarded, whereas it is counted as HTP in the raw input. A summary of the sensitivity for each SV tool and CLOVE can be found seen in Additional file 2: Figure S1. It shows that there is no clear strengths and weaknesses for individual types (except for MetaSV struggling with anything but inversions, deletions, and tandem duplications), and that CLOVE's sensitivity is competitive, but not (greatly) superior to that of individual tools.  Do these results allow us to answer the question if the genotyping is improving a given set of SV calls? This depends on the application. It is obvious that the precision is greatly improved, which is often desirable, sometimes critical to an experiment. This improvement effect is in fact stronger than the loss of sensitivity caused by the genotyping step, leading to overall increased accuracy.  To demonstrate this, we can compare accuracy values before and after classification and apply a pairwise t-test.  The change in means is statistically significant for each of the analysed methods. Finally, CLOVE allows usage of separate call sets of SVs in a single run of the classification algorithm. The graph cleaning steps discussed above 0.8 0.6 raw classified raw classified raw classified raw classified Fig. 4 SV detection accuracy in simulated data before (raw) and after classification with CLOVE (classified). The scatter plots indicate performance for individual runs and the lines the average on the data. The significance of change in means is indicated by a p-value at the top of each panel (except for MetaSV, which is shown without CLOVE classification) facilitate the removal of redundant edges (events that have been called by two or more of the input methods).  Taking the union of all input events, CLOVE has (potentially) more data to classify, which reduces the drop-off in recall after classification. We can indeed observe that the maximum average accuracy is achieved by CLOVE when using input from Crest, Delly, and Socrates at the same time. This is not quite the case for MetaSV, which shows overall low performance values (for both precision and sensitivity) despite having access to the output of multiple tools.  The simulated data and predicted SVs are available from http://bioinf.wehi.edu.au/clove/.    Results for Escherichia coli K-12 strain\r\n  The utilized data set is a collection of single-end Illumina HiSeq 2500 reads from the K12 Beta 10 E. coli strain (http://www.ncbi.nlm.nih.gov/sra/SRX803011).The data is not affected by any specific conditions and thus should resemble the laboratory strain fairly closely. To introduce rearrangements into the sequence context, we compare the read data to a slightly distant reference strain: E. coli K12. This causes a number of relative genomic rearrangements in the donor genome on which we can test the effectiveness of CLOVE. Due to the single-ended nature of the data we are restricted to a subset of SV callers to evaluate for this experiment: Socrates and CREST. The experimental setup is as follows: (1)Align the reads to the K12 strain E. coli reference.  For this purpose we use Bowtie2 (version 2.2.3) with the \u201c-local\u201d option. 6,254,124 reads align, amounting to an average haploid sequence coverage of 135×. (2)Run the SV calling algorithms. The tools produce a number of breakpoints, of which most are in concordance with each other. Again, we use standard parameters for the algorithms, except for the runs of Socrates where we set the minimum mapping quality to 0, as indicated below. (3)Establish which calls relate to real rearrangement events in the data and which relate to false positive predictions. This is a crucial step and the nature of the data allows us to make the required distinctions with high confidence. For once, the data is from a haploid genome without any expected changes except for those differences in strains (and maybe some mutations acquired in culture). Consequently, the allele frequencies of rearrangement events should be close to 100% for real events, except those involving multiple copies of the same genomic region. We use this property effectively by establishing the mutant allele frequency at each break and observing its ratio over reads that do not support the break at the same locus, but support the reference allele. Secondly, E. coli being a relatively small genome of relatively low complexity, we can manually check the predictions made by the SV callers upon credibility. With this we came across the issue of transposable elements in the E. coli reference: There are at least two groups of repetitive sequence in the reference that occur at 6 and 15 spots in the sequence. These elements actually drive the majority of the complex rearrangements that we observe in the donor genome. The problem that they present is mapping ambiguity (and therefore also rearrangement ambiguity). For example, the first such mobile element mentioned here is inserted into a new locus in the donor genome, but the question is, which of its six instances has been moved (or copied) to the new locus? For the sake of simplicity, we do not dwell on this problem too long, but assign a correct breakpoint call if any of the instances has been identified. It still poses a challenge to SV calling because copying an element to a new locus requires two fusions (see Section 2), and if not both fusions have been called for the same instance, CLOVE is not able to establish the correct event. A possible solution to this might be the creation of a new reference strain that consolidates all such repetitive elements into a single instance for the sake of mapping specificity.  Unfortunately, this approach would also remove slight sequence differences that actually exist between the instances, and thus we refrained from this option. (4)Run CLOVE on the various input data for matched comparison to the raw output from SV calling algorithms. CLOVE is set to use the following coverage parameter for all input data sets: \u201c-c 135 20\u201d.  The following results are for the Socrates and Crest algorithms. Table 2 shows the results of SV discovery in the E. coli genome. We compare two different runs of Socrates where we select two different choices of the minimum mapping quality of re-aligned soft-clips. The motivation here is to demonstrate that CLOVE increases the precision of the re-classified output so much that it is beneficial to go for a more sensitive calling approach upfront. Table 2 demonstrates this gain in accuracy (from 0.6 on the standard parameters (M5) to 0.9 with heightened sensitivity (M0)). Unlike for the simulated data, the accuracy of Crest does not improve from the classification on this particular data set. However, precision is increased due to classification once again, but not Organism Ecoli (SRX803011)  Tool  Socrates Human (NA12878)  Crest Socrates + Crest Delly Socrates Delly + Socrates  Data M5R M5C M0R M0C R C R C R C R C R C  TP 8 12 8 17 7 10 8 17 1447 1437 900 894 1819 1816  HTP 7 0 13 13 2 7 0 3 0 0 0 0 0 0 by enough to make up for the drop in sensitivity. The reason for this strong decrease is the ambiguity in mapping location as highlighted above. The best results on this data are achieved by combining the available results from both algorithms (Crest plus Socrates), as was the case on simulated data. While the raw data suffers from high numbers of false positives after combination, the classified data has perfect precision making it the most useful set of SV events to investigate.    Results for NA12878\r\n  Finally, we demonstrate how CLOVE performs on the widely studied NA12878 cell line (Illumina sequencing data to 50× coverage with 100 bp PE reads; ENA accession: ERA172924). As there are previously validated deletion calls [ 32 ], we have a truth set for this type of variant to compare to. We subset the output of Delly and Socrates to those of deletion calls only and then run CLOVE on the data. Similar to our observations in Sections 3.1 and 3.2, the precision is greatly increased while the sensitivity suffers slightly. More specifically, the details of the experiment can be seen in Table 2, and highlight the gain in accuracy by up to 0.42.  When using the entire set of fusions produced by Delly and Socrates, CLOVE is able to classify complex events from the data. When filtering events which insertion point (where applicable) is in a repetitive region, there is a total of 1922 events in called for the cell line. The variants are deletions (1338), tandem duplications (332), inter-chromosomal (/inverted) duplications (104/87), interspersed (/inverted) duplications (35/12), and inversions (13). We randomly selected 4 such events (an inversion, duplication, interFP 33 0 49 50 0 4 0 0 0 0 0 6818 3781 10,394  FN 6 8 0 2 6 0 1 10 1932 1942 2361 2403 1464 1493  Sn (95% CI) 0.71 (.50,.86) 0.60 (.39,.78) 1.00 (.85,1.0) 0.90 (.71,.97) 0.70 (.48,.85) 0.50 (.30,.70) 1.00 (.85,1.0) 0.95 (.77,1.0) 0.43 (.41,.44) 0.43 (.41,.44) 0.28 (.26,.29) 0.27 (.26,.29) 0.55 (.54,.57) 0.55 (.53,.57)  Pr (95% CI) 0.31 (.20,.45) 1.00 (.76,1.0) 0.30 (.21,.42) 1.00 (.83,1.0) 0.77 (.55,.91) 1.00 (.72,1.0) 0.30 (.20,.41) 1.00 (.84,1.0) 0.18 (.17,.18) 1.00 (1.0,1.0) 0.19 (.18,.20) 1.00 (1.0,1.0) 0.14 (.14,.16) 1.00 (1.0,1.0)  Acc (95% CI) 0.28 (.18,.40) 0.60 (.39,.78) 0.30 (.21,.42) 0.90 (.71,.97) 0.58 (.39,.76) 0.50 (.30,.70) 0.30 (.20,.41) 0.95 (.77,1.0) 0.14 (.14.15) 0.43 (.41,.44) 0.13 (.12.14) 0.27 (.26,.29) 0.13 (.13,.14) 0.55 (.53,.57) Columns refer to true positive events (TP), half true positives (HTP), false positives (FP), false negatives (FN), sensitivity/recall (Sn), precision (Pr), and accuracy (Acc). Confidence intervals calculated through binconf in R are supplied for the latter three columns chromosomal duplication and inverted duplication) and were able to demonstrate that for each of these the mapping of Pacbio long read data [ 33 ] could be improved when aligning to an alternative reference containing the predicted reference (we consider a variant validated if a Pacbio read maps across the SV including all fusions, and the alignment has a higher score than in the original reference). For more detail see the Additional file 3: Data S1.      Future work\r\n  CLOVE has been developed for and experimented with on germline data. In other types of data, such as cancer genomes, types of variants other than those currently identified by CLOVE may be present. For example, there are large scale chromosomal rearrangements, such as chromothripsis and breakage fusion bridge, common among some cancers. Further, compound events, where two or more of the events described in this work cooccur at the same locus, have also been observed in tumour genomes. It would be desirable to add rules and functionality to CLOVE to deal with such events. However, this is met with technical challenges of potentially incomplete fusion signatures (false negative calls), and an explosion of the rule set (for all potential compound events).    Conclusions\r\n  We have presented a new method for classifying complex rearrangement from breakpoint calls generated by different algorithms. We have demonstrated CLOVE's ability to improve the output of standard SV methods by highlighting biologically relevant features, prioritizing, enhancing the precision of the calls, and generally improving accuracy. CLOVE's independence of the input algorithm makes it a flexible tool to utilize in any SV calling pipeline. Its ability to process joint inputs from multiple methods is an attractive feature, which often leads to even better rearrangement classification, as has been indicated by our results. CLOVE is the first metacaller that can use the input of any SV algorithm (provided it outputs sufficient information).    Additional files\r\n  Additional file 1: Figure S1. Description of data: Sensitivity of individual tools and one run on CLOVE for different event types. Sensitivity is measured including half true positives (wrong event type). Events are considered recalled if any one of its fusions is found in the output. (PDF 9 kb) Additional file 2: Table S1. Description of data: Detailed results of simulated data analysis. The spreadsheet shows runs of the tested structural variant tools as well as CLOVE re-classified results by variant type and for the individual runs of simulated data. (XLSX 139 kb) Additional file 3: Data S1. Description of data: VCF file of variant calls of CLOVE on the NA12878 genome. (VCF 271 kb) Abbreviations CNV: Copy number variation; DR: Discordant read pair; FN: False negative; FP: False positive; HTP: Half true positive; RD: Read depth; SR: Split read; SV: Structural variation; TP: True positive Acknowledgements The authors acknowledge useful input from Kangbo Mo and Geoff MacIntyre. Funding This research was supported by an Australian National Health and Medical Research Council (NHMRC) Program Grant (1054618) and NHMRC Senior Research Fellowship (1116955). The research benefitted by support from the Victorian State Government Operational Infrastructure Support and Australian Government NHMRC IRIISS. We also acknowledge partial funding by the Centre of Computational Science (CSM) at JGU Mainz. None of the funding organizations contributed to the design or conclusions of this study. Availability of data and materials Source code for CLOVE: http://www.github.com/PapenfussLab Sequencing reads for the e.coli experiment: http://www.ncbi.nlm.nih.gov/sra/SRX803011 Sequencing reads for NA12878: https://trace.ddbj.nig.ac.jp/DRASearch/submission?acc=ERA172924 Authors' contributions ATP and JS conceived the study. JS developed and tested the approach, and wrote the manuscript. ATP had input to the concepts of the method and was a major contributor in writing the manuscript. AW had contributions to the concepts of CLOVE, the implementation, and in writing the manuscript. BS contributed to testing the algorithm and writing the manuscript. All authors read and approved the final manuscript.  Ethics approval and consent to participate Not applicable.  Consent for publication Not applicable.  Competing interests The authors declared that they have no competing interests. Submit your next manuscript to BioMed Central and we will help you at every step:    ",
    "sourceCodeLink": "https://github.com/PapenfussLab/CLOVE",
    "publicationDate": "0",
    "authors": [
      "Jan Schröder",
      "Adrianto Wirawan",
      "Bertil Schmidt",
      "Anthony T. Papenfuss"
    ],
    "status": "Success",
    "toolName": "clove",
    "homepage": ""
  },
  "10.pdf": {
    "forks": 4,
    "URLs": [
      "github.com/fenghen360/HomBlocks.git,",
      "github.com/fenghen360/HomBlocks"
    ],
    "contactInfo": ["yxmao@ouc.edu.cn"],
    "subscribers": 2,
    "programmingLanguage": "Python",
    "shortDescription": "HomBlocks: A multiple-alignment construction pipeline for organelle phylogenomics based on locally collinear block searching",
    "publicationTitle": "HomBlocks: A multiple-alignment construction pipeline for organelle phylogenomics based on locally collinear block searching",
    "title": "HomBlocks: A multiple-alignment construction pipeline for organelle phylogenomics based on locally collinear block searching",
    "publicationDOI": "10.1016/j.ygeno.2017.08.001",
    "codeSize": 12045,
    "publicationAbstract": "A B S T R A C T Organelle phylogenomic analysis requires precisely constructed multi-gene alignment matrices concatenated by pre-aligned single gene datasets. For non-bioinformaticians, it can take days to weeks to manually create highquality multi-gene alignments comprising tens or hundreds of homologous genes. Here, we describe a new and highly efficient pipeline, HomBlocks, which uses a homologous block searching method to construct multiple sequence alignment. This approach can automatically recognize locally collinear blocks among organelle genomes and excavate phylogenetically informative regions to construct multiple sequence alignment in a few hours. In addition, HomBlocks supports organelle genomes without annotation and makes adjustment to different taxon datasets, thereby enabling the inclusion of as many common genes as possible. Topology comparison of trees built by conventional multi-gene and HomBlocks alignments implemented in different taxon categories shows that the same efficiency can be achieved by HomBlocks as when using the traditional method. The availability of Homblocks makes organelle phylogenetic analyses more accessible to non-bioinformaticians, thereby promising to lead to a better understanding of phylogenic relationships at an organelle genome level. Availability and implementation: HomBlocks is implemented in Perl and is supported by Unix-like operative systems, including Linux and macOS. The Perl source code is freely available for download from https://github.com/fenghen360/HomBlocks.git, and documentation and tutorials are available at https://github.com/fenghen360/HomBlocks. Contact: yxmao@ouc.edu.cn or fenghen360@126.com",
    "dateUpdated": "2017-10-12T08:20:25Z",
    "institutions": [
      "Ocean University of China",
      "Ministry of Education",
      "Qingdao National Laboratory for Marine Science and Technology"
    ],
    "license": "No License",
    "dateCreated": "2017-02-23T06:22:53Z",
    "numIssues": 0,
    "downloads": 0,
    "fulltext": "     10.1016/j.ygeno.2017.08.001   HomBlocks: A multiple-alignment construction pipeline for organelle phylogenomics based on locally collinear block searching     Guiqi Bi  0  1    Yunxiang Mao  yxmao@ouc.edu.cn  0  1  2    Qikun Xing  0  1    Min Cao  0  1    0  College of Marine Life Sciences, Ocean University of China ,  Qingdao ,  China    1  Key Laboratory of Marine Genetics and Breeding (OUC), Ministry of Education ,  Qingdao ,  China    2  Laboratory for Marine Biology and Biotechnology, Qingdao National Laboratory for Marine Science and Technology ,  Qingdao ,  China     2017     2  8  2017    23  5  2017    21  7  2017     A B S T R A C T Organelle phylogenomic analysis requires precisely constructed multi-gene alignment matrices concatenated by pre-aligned single gene datasets. For non-bioinformaticians, it can take days to weeks to manually create highquality multi-gene alignments comprising tens or hundreds of homologous genes. Here, we describe a new and highly efficient pipeline, HomBlocks, which uses a homologous block searching method to construct multiple sequence alignment. This approach can automatically recognize locally collinear blocks among organelle genomes and excavate phylogenetically informative regions to construct multiple sequence alignment in a few hours. In addition, HomBlocks supports organelle genomes without annotation and makes adjustment to different taxon datasets, thereby enabling the inclusion of as many common genes as possible. Topology comparison of trees built by conventional multi-gene and HomBlocks alignments implemented in different taxon categories shows that the same efficiency can be achieved by HomBlocks as when using the traditional method. The availability of Homblocks makes organelle phylogenetic analyses more accessible to non-bioinformaticians, thereby promising to lead to a better understanding of phylogenic relationships at an organelle genome level. Availability and implementation: HomBlocks is implemented in Perl and is supported by Unix-like operative systems, including Linux and macOS. The Perl source code is freely available for download from https://github.com/fenghen360/HomBlocks.git, and documentation and tutorials are available at https://github.com/fenghen360/HomBlocks. Contact: yxmao@ouc.edu.cn or fenghen360@126.com    Organelle phylogenomics  Locally collinear blocks  Alignment construction  Efficient pipeline       -\r\n  A R T I C L E I N F O    1. Introduction\r\n  The discord between gene trees and species trees is a common phenomenon when utilizing one or a few genes to infer species relationships [ 2,11 ]. Nevertheless, systemic error and the probability of false tree exploration will decline to satisfactory levels when a sufficiently long sequence length is used in alignment [ 5,12 ]. Therefore, the combination of multiple gene sequences has become the mainstream approach in phylogenetic studies. Owing to the characteristics of a high mutation rate and the near-absence of genetic recombination, along with the development of genome sequencing technology, organelle genomes are widely used in phylogenic and phylogeographic studies. Because genome rearrangements are frequent in some categories of taxa, it is impossible in most cases to carry out genome alignment directly. Manually constructing multi-gene alignments based on concatenation of pre-aligned single organelle gene datasets is a time-consuming and error-prone procedure, particularly when handling several dozens of genomes or genomes of large size, such as those of chloroplasts (typically greater than 100 kb with more than 70 common genes).  With the aim of improving the efficiency of sequence matrix construction derived from multitudes of organelle genomes, we developed a time-saving and accurate method that can be utilized in phylogenomics studies. In this pipeline, the core conserved fragment (protein coding genes, conserved non-coding regions, and rRNA genes) will be extracted and integrated into a long sequence from the same genome. This method avoids the time-consuming sequence alignment of every single gene and can generate a phylogenetically informative and highquality data matrix. In contrast to days-long manual work, it typically takes less only than an hour to construct the HomBlocks matrix with approximately two dozen organelle genomes. In addition, HomBlocks produces sequence optimal partition schemes and models of sequence evolution for RAxML, which are important in downstream phylogenic analysis.    2. Methods\r\n  The framework of HomBlocks is implemented by Perl. It utilizes progressive Mauve [ 7 ], which applies an anchored alignment algorithm, to identify locally collinear blocks (LCBs) shared by organelle genomes (chloroplast and mitochondrial genomes). The LCBs co-existing among all organelle genomes will be extracted and trimmed to screen out phylogenetically informative regions. HomBlocks offers four different methods for LCB trimming: Gblocks [ 3 ], trimAl [ 4 ], noisy [ 8 ], and BMGE [ 6 ]. Without settings, the default trimming method is Gblocks. The final alignment composed of trimmed LCBs can be used in downstream analysis. Additional parameters are provided for users to select the best-fit DNA substitution model and optimal partition schemes and models of sequence evolution for RAxML with the final alignment by PartitionFinder [ 10 ].  A working flow diagram is shown in Fig. 1.    3. Example applications\r\n  We demonstrated the accuracy and efficiency of HomBlocks by comparing phylogenic trees inferred from traditionally concatenated gene alignments and HomBlocks alignments, respectively. Comparisons were composed of datasets derived from the mitochondrial genomes of 41 red algae, chloroplast genomes of 52 higher plants [ 15 ] and mitochondrial genomes of 36 xenarthrans [ 9 ]. A concise overview of the species utilized and their data sources is provided in Supplementary  Tables S1-3.  For algal mitochondrial genomes, phylogenetic analyses were implemented through alignments composed of 13 pre-aligned protein coding genes and HomBlocks alignments by RAxML [ 14 ] and MrBayes 3.2.5 [ 13 ]. The topology comparison is shown in Fig. 2. Trees from two other datasets were built directly by ML and Bayes methods using HomBlocks alignments and were compared to topologies derived from corresponding references. These results are provided in Figs. 3, 4 and Supplementary Fig. S1-2, respectively. The tree topologies built using traditional and HomBlocks alignments are consistent, with the exception of certain minor differences in the bootstrap values on nodes. Moreover, all constructions of HomBlocks alignments were completed in less than half a day.  To elucidate the adaptability of HomBlocks when applied to different taxa, using an NJ tree inferred from HomBlocks matrices of the chloroplast genomes of 37 red algae (Supplementary Table S4) as a reference and the chloroplast genome of Wildemani schizophylla as an initial genome, we ran Homblocks iteratively by addition of a closely related species each time. These 36 alignments constructed by HomBlocks were blasted against the initial genome using BRIG [ 1 ] to determine the origins of the alignments. The results presented in Fig. 5 show that HomBlocks is adaptive and can retain different common genes when handling various taxa.  In conclusion, HomBlocks facilitates organelle phylogenomics analysis by application based on the locally collinear block searching method.    4. Implementation and features\r\n  HomBlocks is a command-line tool and should be functional under any version of Unix or Linux, including macOS. There is no requirement for external installation, with the exception of directory uncompression. The pipeline's documentation provides a typical LCB alignment construction tutorial with test datasets.  The main features of HomBlocks compared with the traditional manual construction method are described briefly below: Fast and efficient. The runtimes and memory requirements of HomBlocks are highly dependent on the use cases. The pipeline took only 20 min to construct whole LCB alignments with mitochondrial genomes from 36 xenarthran species (Fig. 3 &amp; Supplementary Table S3) using a desktop computer (IntelV R CoreTM i7-3770 CPU 3.40 GHz, 8G of RAM).  Comprehensive. Conserved sequence fragments, including noncoding regions, unannotated coding genes, and rRNA genes, are also taken into account in the final alignments (Fig. 4).  Adaptive. Genes shared by organelle genomes of different taxa are diverse and non-stable. HomBlocks consistently attempts to restore as many common genes as possible when dealing with variant datasets (Fig. 5).  Accurate. Abundant sites guarantee high accuracy when building phylogenic trees.  Convenient. Genome annotation is not required before implementation of HomBlocks. HomBlocks supports sequences in fasta or genbank format.    Funding\r\n  This work was supported by funding from National Natural Science Foundation of China (Grant No. 31372517), Scientific and Technological Innovation Project Financially Supported by the Qingdao National Laboratory for Marine Science and Technology (No. 2015ASKJ02), Project of National Infrastructure of Fishery Germplasm Resources (2016DKA30470), Fundamental Research Funds for the  Central Universities (201762016) and Program for Chinese Outstanding Talents in Agriculture Scientific Research..    Acknowledgements\r\n  We would like to thank Chengjie Chen (College of Horticulture, South China Agricultural University), Penghao Yu (Institute of Genetics and Developmental Biology, Chinese Academy of Sciences), and Xiwen Xu (College of Informatics, HuaZhong Agricultural University) for their aid in technical support and valuable suggestions.    Supplementary data\r\n    ",
    "sourceCodeLink": "https://github.com/fenghen360/HomBlocks",
    "publicationDate": "0",
    "authors": [
      "Guiqi Bi",
      "Yunxiang Mao",
      "Qikun Xing",
      "Min Cao"
    ],
    "status": "Success",
    "toolName": "HomBlocks",
    "homepage": ""
  },
  "49.pdf": {
    "institutions": [],
    "URLs": [
      "cran.r-project.org/web/packages/biclust/",
      "github.com/abhatta3/Condition-dependentCorrelation-Subgroups-CCS"
    ],
    "contactInfo": [
      "anindyamail123@gmail.com",
      "ycui2@uthsc"
    ],
    "fulltext": "     RepoRts |     10.1038/s41598-017-04070-4   A GPU-accelerated algorithm for biclustering analysis and detection of condition-dependent coexpression network modules     Anindya Bhattacharya  anindyamail123@gmail.com    Yan Cui  ycui2@uthsc    7  4162    9  5  2017    23  12  2016     OPEN Published: xx xx xxxx In the analysis of large-scale gene expression data, it is important to identify groups of genes with common expression patterns under certain conditions. Many biclustering algorithms have been developed to address this problem. However, comprehensive discovery of functionally coherent biclusters from large datasets remains a challenging problem. Here we propose a GPU-accelerated biclustering algorithm, based on searching for the largest Condition-dependent Correlation Subgroups (CCS) for each gene in the gene expression dataset. We compared CCS with thirteen widely used biclustering algorithms. CCS consistently outperformed all the thirteen biclustering algorithms on both synthetic and real gene expression datasets. As a correlation-based biclustering method, CCS can also be used to find condition-dependent coexpression network modules. We implemented the CCS algorithm using C and implemented the parallelized CCS algorithm using CUDA C for GPU computing. The source code of CCS is available from https://github.com/abhatta3/Condition-dependentCorrelation-Subgroups-CCS.       -\r\n  Clustering algorithms have been widely used to group genes based on their similarities in expression1-4. eTh gene expression coherence is often related to functional coherence. A recent comparative assessment of 21 existing clustering algorithms showed that the clustering algorithms report more functionally meaningful clusters by exploiting the relationships between all pairs of genes1. Clustering algorithms are also known to perform better grouping of co-functional genes if they search for similarities between expression patterns rather than similarities between expression values1, 5.  Functional relations between genes may vary over conditions6, 7. For example, a group of genes may act coherently under one set of conditions but may become inactive or perform diefrent functions separately under another set of conditions. Clustering algorithms that obtain grouping based on similarities over all the samples in a dataset are not eefctive for detecting condition-dependent coexpression patterns. Biclustering algorithms have been developed to address this problem1, 8-12. A bicluster consists of a group of genes and a set of conditions under which the genes are co-expressed. Searching for bicluster is a challenging problem because the number of potential biclusters is exponential to the number of genes and samples. An important diefrence between various biclustering algorithms is how they apply heuristic rules to detect biclusters. Most biclustering algorithms use local search heuristics that may miss many biclusters. Conventional way of finding biclusters depends on the selection of random seeds of genes and/or samples followed by their augmentation based on a scoring function. However, random selection of initial seeds is often unable to eficiently cover the search space and to obtain a consistent set of biclusters from multiple runs on the same input data. The selection of scoring function for the heuristic search is also important for finding the biologically meaningful biclusters. Most common biclustering approaches adopt arithmetic mean of the gene expression or an up/down-regulation patterns on corresponding discretized data matrix. Both are ineficient for finding the interesting co-regulatory modules where genes are expressed with similar or opposite patterns of expression but the expression values are very diefrent. Such modules are important as they may represent relations between genes in the same biological functions1, 13.  A Pearson Correlation Coeficient based scoring function was introduced by Correlated Pattern Biclusters (CPB)14. However, the performance CPB is still depends on the random selection of samples. Benetfis of Pearson Correlation Coeficient based similarity measures over the conventional mean square residue based bicluster scores again was demonstrated by Bi-Correlation Clustering algorithm (BCCA) that looks for positively correlated biclusters and reports biclusters for each pair of genes present in a dataset13. Initially, for each pair of genes all the samples are selected and then a greedy search is made to eliminate samples one at a time until the gene pair shows positive correlation higher than a predenfied high positive correlation threshold. eTh gene pair is then augmented with other genes that are correlated to form a bicluster. However the backward elimination of samples in a greedy search is subject to nfiding a local optimal subset of samples, hence in reality it imposes a restriction on the search space for finding the optimal set of biclusters. BIclustering by Correlated and Large number of Individual Clustered seeds (BICLIC) introduces another alternative correlation based biclustering. BICLIC forms biclusters by starting from a large number of clustered seeds followed by augmentation and deletion of rows and columns based on the correlations with seeds15. The main drawback of BICLIC which is also true for BCCA is finding a large number of overlapping biclusters which are very much identical. Moreover, BICLIC, BCCA and other correlation based biclustering methods, only look for nfiding positively correlated gene groups as bicluster. However, in reality negatively correlated genes are equally relevant and may represent important regulatory mechanisms in biological functions.  Here we propose a more efective correlation-based biclustering algorithm named Condition-dependent Correlation Subgroup (CCS). It integrates several important features for developing an eefctive algorithm for comprehensive discovery of functionally coherent biclusters1. A significant challenge in genomic data analysis is to utilize the fast growing high performance computing capacity to process and analyze large complex datasets eficiently 16-18. CCS is particularly suitable for parallel computing. We used the GPGPU computing for a parallel implementation of the algorithm in CUDA C which shows a substantial speedup compared to the sequential C program. The performance of CCS was compared with CPB, BCCA, BICLIC and ten other widely-used biclustering algorithms on 5 synthetic and 5 real gene expression datasets. CCS outperforms the other biclustering algorithms in all the comparisons. We also showed that there is equivalence between the CCS biclusters and condition-dependent coexpression network modules.    Methods\r\n  A bicluster is a group of genes with a related pattern of expression over a group of samples. We denfied the related pattern of gene expression in terms of positive and negative Pearson correlation coeficients. Let us consider a gene expression data set D = G × E where G = {g1, g2, \u2026, gn} is a set of \u201cn\u201d genes and E = {e1, e2, \u2026, es, \u2026, em} is a set of \u201cm\u201d samples. For each gene gi there is an m-dimensional vector xi. In vector xi, xis is the value of es for gene gi. In our algorithm we defined a bicluster as a group of genes \u201cI\u201d over a group of samples \u201cJ\u201d where each gene gi in \u201cI\u201d is correlated to all the other genes gj in \u201cI\u201d with an absolute correlation value greater than a threshold θ. Mathematically a bicluster \u201cC\u201d is represented as C = (I; J) where \u201cI\u201d is a subset of \u201cG\u201d and \u201cJ\u201d is a subset of \u201cE\u201d. hTe Pearson correlation coeficient between g i and gj over a subset of samples \u201cJ\u201d is represented as r(gi, gj)J and defined as r(gi, gj)J =  ∑sm=1((xis × bs − xi ×¯ b)(xjs × bs − xj ×¯ b)) (∑sm=1(xis × bs − xi ×¯ b)2)(∑sm=1(xjs × bs − xj ×¯ b)2) , where b is a bit vector of size \u201cm\u201d. If sample es is in \u201cJ\u201d then we set the sth bit of bs = 1, otherwise we set bs = 0. eTh terms xis and xjs represent sth sample values for gene gi and gj respectively. Average expression values of gi and gj for samples in \u201cJ\u201d are represented as xi × b and xj × b respectively. We considered a pair of genes gi and gj similar for a subset of samples \u201cJ\u201d if r(gi, gj)J &gt; θ, where θ is the correlation threshold and r(gi, gj)J is the absolute value of correlation between gi and gj for the subset of samples \u201cJ\u201d. For each gene gi in a dataset \u201cD\u201d, CCS considers gi as the base gene to start forming a bicluster for each gi.  Search space sorting and base-gene selection. hTe gene with the lower variability over the samples (commonly known as housekeeping genes) are oeftn considered as less significant for the corresponding biological conditions, hence the less important candidates for forming a bicluster. Prior to biclustering, the search space (input data matrix) was sorted by the standard deviations of the gene expression values. In an ordered data matrix the genes with the higher variability were placed before the other with the lower variability.  CCS algorithm selects 'base_number' of genes as base-gene from the sorted data matrix in the decreasing order of variability. Ordering of the search space ensured that a gene with higher variability considered for forming a bicluster and also for augmentation before the other with lower variability. The 'base_number' can be set between \u201c1\u201d to \u201cn\u201d (total number of genes). Here we set the base_number value equals 1,000 for the biological dataset to restrict the search time.  Similarity pattern classes and sample selections. CCS algorithm considers three classes of gene expression pattern similarities: (i) up-regulated positive correlation: gi and gj are positively correlated and the expression values of gi and gj for the selected samples are higher than the arithmetic mean expression values over all smaples; (ii) down-regulated positive correlation: gi and gj are positively correlated and the expression values of gi and gj for the selected samples are lower than the arithmetic mean expression values over all smaples; (iii) negative correlation: gi and gj are negatively correlated over the selected samples and their expression values are at the opposite sides to their respective arithmetic mean expressions. ( 1 ) 3.       I ← NULL 4.       J ← NULL 2. for all gi ∈ G, i ≤ base_number do 5.       for all gj ∈ G, i &lt; j ≤ n do 6.          apply Rules( 1,2,3 ) on {gi, gj} for sample sets Jk(k=1,2,3) 7.          for all Jk, k = 1:3 do 8.                if |r(gi, gj)Jk| &gt; θ then 9.                        Ii ← {gi, gj} 10.                       for all gp not in Ii do 11.                           if |r(gp, gq)Jk| &gt; θ, for all gq ∈ Ii then 12.                                 Ii ← Ii ∪ gp 13.                             end if 14.                       end for 15.                end if 16.                If (BScore(Ii,Ji) &lt; BScore(I,J) &lt; 0.01) or                         (BScore(Ii,Ji) = BScore(I,J) &lt; 0.01 and |Ii| &gt; |I|) then 19.                end if 20.          end for 21.       end for Continued 22.       if I ≠ NULL and J ≠ NULL then  For each pair of genes gi and gj the sample sets \u201cJ1: for up-regulated positive correlation\u201d, \u201cJ2: for down-regulated positive correlation\u201d and \u201cJ3: for negative correlation\u201d are selected from sample selection rules 1, 2 and 3 respectively. For each sample es, s = 1:m, we defined the following three rules to determine J 1, J2 and J3: Rule 1. IF (xis − xi) &gt; 0 AND (xjs − xj) &gt; 0 THEN include es to J1 [Expression values of gi and gj for the samples in J1 are higher than the arithmetic mean expression values], Rule 2. IF (xis − xi) &lt; 0 AND (xjs − xj) &lt; 0 THEN include es to J2 [Expression values of gi and gj for the samples in J2 are lower than the arithmetic mean expression values], Rule 3. IF (xis − xi) × (xjs − xj) &lt; 0 THEN include es to J3 [Expression values of gi and gj for the samples in J3 are opposite to their arithmetic mean expression values]. Here xi and xj are the mean expression value of gene gi and gj over all samples. eTh correlations between g i and gj for sample sets Jk=1:3 are determined first by computing Pearson Correlation Coeficient 'r' and then by comparing against a threshold 'θ ' which is denoted by |r(gi, gj)Jk| &gt; θ in the algorithm.  Defining biclusters as condition dependent correlation modules. CCS defines biclusters as con dition dependent correlation modules where the genes in a bicluster are expected to show correlations only for the samples in that bicluster. CCS introduces a scoring function named BScore as defined in Equation ( 2  ). The BScore is designed to compare two sets of correlated gene pairs N and M. eTh correlations in set 'N' are computed over the samples that are included in a bicluster while the correlations in set 'M' are computed over the rest of the samples that are not included in the same bicluster. uThs, the BScore measures the degree to which the gene coexpression in a bicluster is condition-specicfi. In this work, we used a small BScore threshold ( &lt;0.01) to ensure the discovered biclusters are based on condition-specific coexpression.  BScore =  N ∩ M  N ∪ M Algorithm. eTh algorithm at each iteration of step 2 starts with a new base-gene g i. In each iteration of step 5, gi is paired with a gene gj (i &lt; j ≤ n). In step 6, the sample sets are selected for gene pair gi, gj. In step 8, the algorithm computes the absolute value of the correlation |r(gi, gj)Jk| for each sample set Jk=1:3 (Equation ( 1 )). If |r(gi, gj)Jk|&gt;θ then the algorithm starts a bicluster with an initial gene set Ii = {gi, gj}. In step 10, augmentation of the gene set is performed by including a new gene gp in Ii. In step 16, BScore(Ii,Ji) is compared against the previous best BScore(I,J) to update the sets I and J. eTh most condition dependent bicluster for each base gene g i is selected in step 22. In step 26, all the overlapping biclusters are merged while keeping BScore &lt; 0.01 for the final set of biclusters 'S'.  Input: (i) A gene expression data set D = G × H, G = {g1, g2,\u2026, gn} is a set of n genes, H = {e1, e2,\u2026, em} is a set of m samples for gene set G. (ii) A correlation threshold θ.  Output: A set of biclusters S = {bicluster(g1), bicluster(g2),\u2026, bicluster(gbase_number)}, base_number ≤ n. ( 2 ) 23.        bicluster(gi) ← {I, J} 24.       end if 25. end for 26. for each bicluster({I, J}) do 27.       for each bicluster({K, L})≠ NULL do 28.          if BScore(I ∪ K, J ∪ L) &lt; 0.01 and I ∩ K ≠ NULL then 29.               bicluster({I, J}) ← {I ∪ K, J ∪ L} 30.                 bicluster({K, L}) ← NULL 31.          end if 32.   end for 33. end for  ehT input parameter for our biclustering algorithm is the correlation threshold θ , which is the minimum absolute value of correlation between genes in a bicluster. Allocco et al. investigated the relation between co-regulation and coexpression and came up with a conclusion that for a correlation greater than 0.84 there is at least 50% chance of co-regulation19. Here we set θ = 0.8. If the number of biclusters is zero for θ = 0.8 then we decrease θ by 0.05 until we get a nonempty set of biclustering result or θ drops to 0.  GPU computing for biclustering analysis of large datasets. GPU was initially introduced for rendering video graphics on display devices. eTh GPU designing company NVIDIA introduced their CUDA architecture in 2007 for general purpose computing on graphics processing unit (GPGPU computing). The single instruction set and multiple dataset architecture (SIMD) of GPU is particularly useful for executing a single instruction over thousands of core processors in a GPU card. CUDA is currently the most popular programming model for general purpose GPU computing. CUDA C is an extension of regular C that supports GPGPU computing. The parallel part is implemented as a CUDA C kernel function. Thousands of instances of a CUDA C kernel function run parallel on a group of GPU core processors. Each group of GPU cores is also known as a thread block. Each thread block is a virtual processor formed from one or more GPU core processors, registers, local memories, shared memory and global memory. CUDA C kernel functions are sent to the GPU for parallel execution, and sequential parts of the code run on the CPU.  We used GPGPU computing to reduce the execution time. In this work, we executed our CUDA C code for CCS on NVIDIA Tesla K20 (2,496 CUDA cores) and K80 GPU (4,992 CUDA cores) accelerator. For compilation of CUDA C code we installed CUDA 7.5 toolkit in our Linux workstation. Transferring big datasets between CPUs and GPUs is time consuming. To reduce the data transfer overhead we moved the entire process of finding new biclusters to GPU.  Step 2 of CCS executes \u201cn\u201d times when \u201cbase_number\u201d is set at \u201cn\u201d for a dataset with \u201cn\u201d rows. For each execution the inner loop at step 5 executes a maximum \u201cn-1\u201d times. To achieve increased speed from parallel GPGPU computing, we removed the step 2 loop from CCS algorithm and implement steps 5 to 25 of CCS as a CUDA C kernel function.  Synthetic datasets. Five synthetic datasets were generated using the BiBench-0.2 python library8. The synthetic datasets contains two types of biclustering: Constant biclusters and Shift-Scale biclusters. Constant biclusters are sub-matrices with a constant value and Shift-Scale biclusters are biclusters that are formed from both shifting and scaling a base row by random numbers. Table  1 describes the synthetic datasets for their types, dimensions and number of true biclusters.  Gene expression datasets. Five publicly available gene expression datasets GDS531, GDS589, GDS3603, GDS3966 and GDS4794 were downloaded from Gene Expression Omnibus (GEO). Genes with standard deviation less than 2.0 were removed. For genes appeared multiple times, only the row (probe set) with the highest standard deviation was kept. eTh dataset information is summarized in Table  2.  Evaluating biclustering results. We evaluated the results of the biclustering algorithms on synthetic datasets using Recovery and Relevance metrics8, 20. eThy compare the set of found biclusters 'F' against the expected biclusters 'E' (the true bicluster). Both for Recovery and Relevance the scores are ranging between '0' and '1'. eTh highest score '1' for Recovery denotes that all the expected biclusters are found. Similarly, the highest '1' for Relevance denotes that all the found biclusters are expected.  To evaluate the results on the real gene expression datasets, we performed functional enrichment tests for each bicluster1, 8, 13, 20. We used gProfileR 21 for selecting the enriched gene annotation terms. gProfilerR retrieves a comprehensive set of gene annotations from Gene Ontology (GO) terms, biological pathways, regulatory motifs in DNA, microRNA targets and protein complexes. From gProfiler search, all the annotation terms with a Benjamini-Hochberg FDR less than 0.01 were considered as enriched. From the biclustering results on all vfie gene expression datasets we computed the average number of enriched annotation terms. Higher average number of enriched terms indicates better functional grouping. We also evaluated the performance of biclustering algorithms from percentage of total biclusters that have one or more enriched annotation terms.  Dataset CNST.100.3 SS.150.4 SS.200.5 SS.200.6 SS.250.7  Type Constant Shift and Scale Constant Shift and Scale Shift and Scale 100 150 200 200 250 75 100 120 120 120  Results and Discussions We obtained the biclustering results from CCS and 13 other biclustering algorithms namely Cheng and Church Biclustering algorithm (CC)22, Iterative Signature Algorithm (ISA)23, Plaid24, Spectral25, Order Preserving Sub Matrix (OPSM)26, xMotifs27, SAMBA28, Bimax20, BicSPAM29, UniBic30, CPB14, BICLIC15 and BCCA13. We used BicAT20 software package for ISA, CC, OPSMs, xMotifs and Bimax. We also used 'biclust' R package (https://cran.r-project.org/web/packages/biclust/) for Plaid and Spectral. For UniBic, BicSPAM, BCCA, CPB and BICLIC the published versions of the software packages were used. SAMBA biclustering was performed using the Expander package31.  Performance on synthetic data. Figure 1 shows the Recovery and Relevance scores on five synthetic datasets (Table 1) for CCS and the best among the other 13 algorithms for comparison. For all of the vfie synthetic datasets CCS significantly outperformed other algorithms for finding the expected biclusters (recovery) and the most relevant set of biclusters (relevance). For constant bicluster on a datasets of 100 rows and 100 columns, CCS obtained 0.88 score for both recovery and relevance. For larger size datasets and shift and scale biclusters, still the recovery and relevance scores are higher than the other algorithms which clearly demonstrate the accuracy of CCS for recovering the relevant biclusters irrespective of the number and pattern of the biclusters and size of the datasets.  Performance on gene expression data. CCS biclustering algorithm obtained total 25 biclusters from vfie gene expression datasets (Table 2). Figure 2 shows the average number of enriched terms from gProfiler enrichment analysis. Figure 2 shows there are more enriched terms per bicluster from CCS than the others. Figure 3 compares the biclustering algorithms for the percentage of biclusters with at least one enriched term. Again, CCS obtained the highest percentage of enriched biclusters (Figure 3).  GPU accelerated biclustering. eTh CPU version of the CCS algorithm is computationally expensive. If the 'base_number' is 'n' then step 5 executes 'n' times. Again for 'n' iterations of step 5 the augmentation task at step 10 executes 'n' times. Hence, the worst case time complexity of CCS is O(n3). The CUDA C implementation of CCS eliminates the outer loop and runs from step 5 to step 25 as CUDA C kernel function. This reduces the time complexity to O(n2). We compared the execution time of the CPU and GPU implementations of CCS. Figure 4 shows the speedup gained from CCS running on the NVIDIA Tesla K20 and K80 GPUs. For larger datasets the GPU-based CCS runs more than 20 times faster than the CPU-based CCS. CCS biclusters are equivalent to condition-dependent coexpression network modules. A network module is usually defined as a highly connected sub-network. A module in coexpression network consists of a group of genes whose expression levels are highly correlated32. The correlative relations in a coexpression network oeftn depends on conditions such as genotype, environment, treatment, cell type, disease state or developmental stage33-38. Therefore, coexpression network modules may also change with those conditions. There is an interesting relation between CCS biclusters and coexpression network modules. A CCS bicluster consists of genes whose expression levels are highly correlated under a set of conditions. Therefore, a CCS bicluster is equivalent to a condition-dependent module in a coexpression network. This has been illustrated here from two CCS-biclusters. We picked two CCS-biclusters from GDS589 dataset. eTh rfist bicluster (bicluster 1) includes 166 genes and 70 samples. eTh second bicluster (bicluster 2) includes 81 genes and 60 samples. We also identified all the neighboring genes of the biclusters. Neighboring genes are the genes that are not from a bicluster but they are correlated with at least one gene in that bicluster. Figure 5(A) shows the coexpression network of the genes in two biclusters and their neighboring genes. Because this network shows coexpression over all the samples, none of the biclusters are forming a network module. In contrast, when the coexpression network is constructed using correlations over the samples of bicluster 1, the genes of this bicluster form a network module, but the genes of bicluster 2 do not form a module (Figure 5(B)). Similarly, genes of bicluster 2 form a module when the coexpression network is based on correlations over samples of bicluster 2, but genes from bicluster 1 do not form a module in this case (Figure 5(C)).    Conclusion\r\n  In this work we designed a novel algorithm, CCS, to discover the functionally coherent biclusters from large-scale gene expression datasets. The performance of CCS was compared with thirteen widely used biclustering algorithms. CCS consistently outperforms all the other algorithms in the comparison for obtaining true biclusters from synthetic datasets and discovering functionally enriched biclusters from the real gene expression datasets. Moreover, the CCS biclusters are equivalent to condition-dependent modules in coexpression networks. This important feature makes the CCS algorithm also useful for the study of the condition-dependent structural characteristics of the coexpression networks. eTh biclusters and network modules of co-regulated and co-functional genes discovered by the CCS algorithm may provide an important entry point for many other analyses such as gene set enrichment analysis, regulatory network inference and disease genes identification.  ehT recent development of fast and accurate data acquisitions and quantifications in genomics, transcriptomics and proteomics greatly increased the data volume that needs to be processed by clustering algorithms. To enable rapid discovery of high quality biclusters, we implemented CCS algorithm using CUDA C for parallel GPGPU computing. For large datasets, the GPU implementation of CCS achieved about 20 fold speedup compared to the sequential version of CCS. We expect that the GPU- accelerated biclustering will have important applications in the time-sensitive analysis of genomic medicine data.    Acknowledgements\r\n  ehT authors gratefully acknowledge the editorial assistance of Dr. Martha M. Howe provided through the Writing Assistance Program of the College of Graduate Health Sciences. Funding: This work was partly supported by hTe University of Tennessee Center for Integrative and Translational Genomics. A.B. was supported in part by P-41-RR24851.    Author Contributions\r\n  A.B. and Y.C. conceived the idea of the work, A.B. implemented and tested the algorithms, A.B. and Y.C. wrote the manuscript.    Additional Information\r\n  Competing Interests: eTh authors declare that they have no competing interests.  Publisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afiliations.  Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.    ",
    "publicationTitle": "A GPU-accelerated algorithm for biclustering analysis and detection of condition-dependent coexpression network modules",
    "title": "A GPU-accelerated algorithm for biclustering analysis and detection of condition-dependent coexpression network modules",
    "publicationDOI": "10.1038/s41598-017-04070-4",
    "publicationDate": "0",
    "publicationAbstract": "OPEN Published: xx xx xxxx In the analysis of large-scale gene expression data, it is important to identify groups of genes with common expression patterns under certain conditions. Many biclustering algorithms have been developed to address this problem. However, comprehensive discovery of functionally coherent biclusters from large datasets remains a challenging problem. Here we propose a GPU-accelerated biclustering algorithm, based on searching for the largest Condition-dependent Correlation Subgroups (CCS) for each gene in the gene expression dataset. We compared CCS with thirteen widely used biclustering algorithms. CCS consistently outperformed all the thirteen biclustering algorithms on both synthetic and real gene expression datasets. As a correlation-based biclustering method, CCS can also be used to find condition-dependent coexpression network modules. We implemented the CCS algorithm using C and implemented the parallelized CCS algorithm using CUDA C for GPU computing. The source code of CCS is available from https://github.com/abhatta3/Condition-dependentCorrelation-Subgroups-CCS.",
    "authors": [
      "Anindya Bhattacharya",
      "Yan Cui"
    ],
    "status": "Success",
    "toolName": "Condition-dependentCorrelation-Subgroups-CCS"
  }
}